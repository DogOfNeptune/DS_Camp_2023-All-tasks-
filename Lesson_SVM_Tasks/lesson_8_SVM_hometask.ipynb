{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "<font color = green>\n",
    "\n",
    "#  Home Task\n",
    "</font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font color = green>\n",
    "\n",
    "##  Sklearn SVM for Iris dataset\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC train accuracy: 0.967\n",
      "RBF kernel train accuracy: 0.980\n",
      "Poly kernel train accuracy: 0.993\n"
     ]
    }
   ],
   "source": [
    "# YOUR_CODE.  Try linear, rbf and poly kernels \n",
    "# START_CODE \n",
    "clf_svm = LinearSVC(C=1, max_iter=10000).fit(X, y)\n",
    "print(\"LinearSVC train accuracy: {:.3f}\".format(clf_svm.score(X, y)))\n",
    "\n",
    "clf_svm = SVC(kernel='rbf', C=1, gamma=1).fit(X, y)\n",
    "print(\"RBF kernel train accuracy: {:.3f}\".format(clf_svm.score(X, y)))\n",
    "\n",
    "clf_svm = SVC(kernel='poly', C=1, gamma=1).fit(X, y)\n",
    "print(\"Poly kernel train accuracy: {:.3f}\".format(clf_svm.score(X, y)))\n",
    "# END_CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font color = green>\n",
    "\n",
    "##  Spam/Non-spam classification\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green>\n",
    "\n",
    "###  Problem statement\n",
    "</font>\n",
    "\n",
    "Spam filter classifies emails into spam and non-spam email. \n",
    "For this task use SVMs to build your spam filter.\n",
    "This is binary classification  - whether a given email, x, is spam (y = 1) or non-spam (y = 0).\n",
    "In particular, you need to convert each email into a feature vector x. \n",
    "The dataset for this task is based on a a subset of the SpamAssassin Public Corpus.\n",
    "For the purpose of this exercise, it will use the body of the email (excluding the email headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green>\n",
    "\n",
    "###  Preprocessing mail\n",
    "</font>\n",
    "\n",
    "Before starting on a machine learning task, it is usually insightful to take a look at examples from the dataset. Below is a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts. While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to “normalize” these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, replace each URL in the email with the unique string “http_addr” to indicate that a URL was present.\n",
    "This lets the spam classifier to make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green>\n",
    "\n",
    "### Reveiw sample mail \n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "path = os.path.join(cwd, 'data')\n",
    "\n",
    "def get_sample(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "    \n",
    "fn=  os.path.join(path , 'emailSample1.txt')\n",
    "content = get_sample(fn)\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Preprocessing and normalization steps\n",
    "</font>\n",
    "\n",
    "Preprocessing and normalization includes the following steps:\n",
    "<ul>\n",
    "<li> <b>Lower-casing</b>: The entire email is converted into lower case, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "    \n",
    "<li> <b>Stripping HTML</b>: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "<li> <b>Normalizing URLs</b>: All URLs are replaced with the text “httpaddr”.\n",
    "<li> <b>Normalizing Email Addresses</b>: All email addresses are replaced\n",
    "with the text “emailaddr”.\n",
    "<li> <b>Normalizing Numbers</b>: All numbers are replaced with the text\n",
    "“number”.\n",
    "<li> <b>Normalizing Dollars</b>: All dollar signs ($) are replaced with the text\n",
    "“dollar”.\n",
    "<li> <b>Word Stemming</b>: Words are reduced to their stemmed form. For ex- ample, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips o↵ additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”.\n",
    "<li> <b>Removal of non-words</b>: Non-words and punctuation have been re- moved. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Word tokenization\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokeniize(content):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    return: list of tokens (str) e.g. ['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a']\n",
    "    '''\n",
    "    # YOUR_CODE.  Split the content to tokens. You may need re.split()\n",
    "    # START_CODE \n",
    "    tokens= re.split(r'\\s+', content)\n",
    "    # END_CODE \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>',\n",
       " 'Anyone',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'costs',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " '?',\n",
       " '>',\n",
       " 'Well,',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'how',\n",
       " 'many',\n",
       " 'visitors',\n",
       " \"you're\",\n",
       " 'expecting.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywhere',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " '10',\n",
       " 'bucks',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " '$100.',\n",
       " 'You',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'http://www.rackspace.com/',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'Amazon',\n",
       " 'EC2',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'running',\n",
       " 'something',\n",
       " 'big..',\n",
       " 'To',\n",
       " 'unsubscribe',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'this',\n",
       " 'mailing',\n",
       " 'list,',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to:',\n",
       " 'groupname-unsubscribe@egroups.com',\n",
       " '']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens  = word_tokeniize('''> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n''')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`array(['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
    "       'a', 'web', 'portal', '?', '>', 'Well', 'it', 'depends', 'on',\n",
    "       'how', 'many', 'visitors', \"you're\", 'expecting.', 'This', 'can',\n",
    "       'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'a',\n",
    "       'month', 'to', 'a', 'couple', 'of', '$100.', '', 'You', 'should',\n",
    "       'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'Amazon',\n",
    "       'EC2', '', 'if', 'youre', 'running', 'something', 'big..', '',\n",
    "       'To', 'unsubscribe', 'yourself', 'from', 'this', 'mailing', 'list',\n",
    "       'send', 'an', 'email', 'to:', 'groupname-unsubscribe@egroups.com',\n",
    "       '', ''], dtype='<U33')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Lower case \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of tokens in lower case (str)\n",
    "    '''\n",
    "    # YOUR_CODE.  Make all tokens in lower case\n",
    "    # START_CODE \n",
    "    tokens= [tokens.lower() for tokens in tokens]\n",
    "    # END_CODE \n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>',\n",
       " 'anyone',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'costs',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " '?',\n",
       " '>',\n",
       " 'well,',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'how',\n",
       " 'many',\n",
       " 'visitors',\n",
       " \"you're\",\n",
       " 'expecting.',\n",
       " 'this',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywhere',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " '10',\n",
       " 'bucks',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " '$100.',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'http://www.rackspace.com/',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'amazon',\n",
       " 'ec2',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'running',\n",
       " 'something',\n",
       " 'big..',\n",
       " 'to',\n",
       " 'unsubscribe',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'this',\n",
       " 'mailing',\n",
       " 'list,',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to:',\n",
       " 'groupname-unsubscribe@egroups.com',\n",
       " '']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = lower_case(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`array(['>', 'anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
    "       'a', 'web', 'portal', '?', '>', 'well', 'it', 'depends', 'on',\n",
    "       'how', 'many', 'visitors', \"you're\", 'expecting.', 'this', 'can',\n",
    "       'be', 'anywhere', 'from', 'less', 'than', '10', 'bucks', 'a',\n",
    "       'month', 'to', 'a', 'couple', 'of', '$100.', '', 'you', 'should',\n",
    "       'checkout', 'http://www.rackspace.com/', 'or', 'perhaps', 'amazon',\n",
    "       'ec2', '', 'if', 'youre', 'running', 'something', 'big..', '',\n",
    "       'to', 'unsubscribe', 'yourself', 'from', 'this', 'mailing', 'list',\n",
    "       'send', 'an', 'email', 'to:', 'groupname-unsubscribe@egroups.com',\n",
    "       '', ''], dtype='<U33')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Normalize\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens (tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of tokens replaced with corresponding unified words\n",
    "    '''\n",
    "    # YOUR_CODE.  \n",
    "    #     Remove html and other tags\n",
    "    #     mark all numbers \"number\"\n",
    "    #     mark all  urls as \"httpaddr\"\n",
    "    #     mark all emails as \"emailaddr\"\n",
    "    #     replace $ as \"dollar\"\n",
    "    #     get rid of any punctuation\n",
    "    #     Remove any non alphanumeric characters\n",
    "    #  You may  need re.sub()\n",
    "    # START_CODE\n",
    "    tokens= [re.sub(r'<.*?>', '', tokens) for tokens in tokens]\n",
    "    tokens= [re.sub(r'[0-9]+', 'number', tokens) for tokens in tokens]\n",
    "    tokens= [re.sub(r'(http|https)://[^\\s]*', 'httpaddr', tokens) for tokens in tokens]\n",
    "    tokens= [re.sub(r'[^\\s]+@[^\\s]+', 'emailaddr', tokens) for tokens in tokens]\n",
    "    tokens= [re.sub(r'[$]+', 'dollar', tokens) for tokens in tokens]\n",
    "    tokens= [re.sub(r'[^a-zA-Z0-9]', '', tokens) for tokens in tokens]\n",
    "    # END_CODE \n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'anyone',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'costs',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " '',\n",
       " '',\n",
       " 'well',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'how',\n",
       " 'many',\n",
       " 'visitors',\n",
       " 'youre',\n",
       " 'expecting',\n",
       " 'this',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywhere',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " 'number',\n",
       " 'bucks',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'dollarnumber',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'httpaddr',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'amazon',\n",
       " 'ecnumber',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'running',\n",
       " 'something',\n",
       " 'big',\n",
       " 'to',\n",
       " 'unsubscribe',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'this',\n",
       " 'mailing',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to',\n",
       " 'emailaddr',\n",
       " '']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = normalize_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`array(['', 'anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host',\n",
    "       'a', 'web', 'portal', '', '', 'well', 'it', 'depends', 'on', 'how',\n",
    "       'many', 'visitors', 'youre', 'expecting', 'this', 'can', 'be',\n",
    "       'anywhere', 'from', 'less', 'than', 'number', 'bucks', 'a',\n",
    "       'month', 'to', 'a', 'couple', 'of', 'dollarnumber', '', 'you',\n",
    "       'should', 'checkout', 'httpaddr', 'or', 'perhaps', 'amazon',\n",
    "       'ecnumber', '', 'if', 'youre', 'running', 'something', 'big', '',\n",
    "       'to', 'unsubscribe', 'yourself', 'from', 'this', 'mailing', 'list',\n",
    "       'send', 'an', 'email', 'to', 'emailaddr', '', ''], dtype='<U12')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Remove zero length tokens\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_tokens (tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of filtered tokens (str)\n",
    "    '''\n",
    "    original_tokens_len = len(tokens)\n",
    "    \n",
    "    # YOUR_CODE. Keep only tokens that lenght >0  \n",
    "    # START_CODE \n",
    "    tokens= [tokens for tokens in tokens if len(tokens)>0]\n",
    "    # END_CODE     \n",
    "   \n",
    "    print ('Original len= {}\\nRemaining len= {}'.format(original_tokens_len, len(tokens)))    \n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len= 65\n",
      "Remaining len= 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anyone',\n",
       " 'knows',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'costs',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " 'well',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'how',\n",
       " 'many',\n",
       " 'visitors',\n",
       " 'youre',\n",
       " 'expecting',\n",
       " 'this',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywhere',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " 'number',\n",
       " 'bucks',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'dollarnumber',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'httpaddr',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'amazon',\n",
       " 'ecnumber',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'running',\n",
       " 'something',\n",
       " 'big',\n",
       " 'to',\n",
       " 'unsubscribe',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'this',\n",
       " 'mailing',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to',\n",
       " 'emailaddr']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = filter_short_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`Original len= 69\n",
    "Remaining len= 61\n",
    "array(['anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depends', 'on', 'how', 'many',\n",
    "       'visitors', 'youre', 'expecting', 'this', 'can', 'be', 'anywhere',\n",
    "       'from', 'less', 'than', 'number', 'bucks', 'a', 'month', 'to', 'a',\n",
    "       'couple', 'of', 'dollarnumber', 'you', 'should', 'checkout',\n",
    "       'httpaddr', 'or', 'perhaps', 'amazon', 'ecnumber', 'if', 'youre',\n",
    "       'running', 'something', 'big', 'to', 'unsubscribe', 'yourself',\n",
    "       'from', 'this', 'mailing', 'list', 'send', 'an', 'email', 'to',\n",
    "       'emailaddr'], dtype='<U12')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Stem tokens\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of stemmed tokens e.g. array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani']...\n",
    "    '''\n",
    "    # YOUR_CODE. replace the tokens by stemmed form. You may need PorterStemmer.stem() \n",
    "    # START_CODE \n",
    "    tokens= [PorterStemmer().stem(tokens) for tokens in tokens]\n",
    "    # END_CODE     \n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anyon',\n",
       " 'know',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'cost',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " 'well',\n",
       " 'it',\n",
       " 'depend',\n",
       " 'on',\n",
       " 'how',\n",
       " 'mani',\n",
       " 'visitor',\n",
       " 'your',\n",
       " 'expect',\n",
       " 'thi',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywher',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " 'number',\n",
       " 'buck',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'coupl',\n",
       " 'of',\n",
       " 'dollarnumb',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'httpaddr',\n",
       " 'or',\n",
       " 'perhap',\n",
       " 'amazon',\n",
       " 'ecnumb',\n",
       " 'if',\n",
       " 'your',\n",
       " 'run',\n",
       " 'someth',\n",
       " 'big',\n",
       " 'to',\n",
       " 'unsubscrib',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'thi',\n",
       " 'mail',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to',\n",
       " 'emailaddr']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = stem_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani',\n",
    "       'visitor', 'your', 'expect', 'thi', 'can', 'be', 'anywher', 'from',\n",
    "       'less', 'than', 'number', 'buck', 'a', 'month', 'to', 'a', 'coupl',\n",
    "       'of', 'dollarnumb', 'you', 'should', 'checkout', 'httpaddr', 'or',\n",
    "       'perhap', 'amazon', 'ecnumb', 'if', 'your', 'run', 'someth', 'big',\n",
    "       'to', 'unsubscrib', 'yourself', 'from', 'thi', 'mail', 'list',\n",
    "       'send', 'an', 'email', 'to', 'emailaddr'], dtype='<U10')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Vocabulary\n",
    "</font>\n",
    "\n",
    "Ususally vocabulary is built from the top most frequent tokens within all available training set.\n",
    "For this task it is already prepared and provided in the `vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab)= 1,899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocabulary(fn):\n",
    "    '''\n",
    "    fn: str - full path to file \n",
    "    return: ndarray of str e.g. array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)\n",
    "    '''\n",
    "    vocab_list = pd.read_table(fn, header=None)\n",
    "    vocab = np.array(vocab_list)[:,1] # first columns is index, select only words column  \n",
    "    print ('len(vocab)= {:,}'.format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "fn=  os.path.join(path , 'vocab.txt')\n",
    "vocab = get_vocabulary(fn)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Feature reresentation\n",
    "</font>\n",
    "\n",
    "Every word in vocabulary is the feature of the mail - 1 is the word in in the mail and 0 otherwise. \n",
    "Thus every mail can be represented as the binary array of fixed length - number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_features(tokens,vocab):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of binary values 1 if word from vocabulary is in mail 0 otherwise\n",
    "    '''\n",
    "    # YOUR_CODE. Compute the array with 1/0 corresponding to is word from vocabulary in mail \n",
    "    # START_CODE \n",
    "    tokens_represented = [1 if word in tokens else 0 for word in vocab]\n",
    "    # END_CODE     \n",
    "\n",
    "    print ('{} word(s) from vocab are in the tokens.'.format(np.sum(tokens_represented)))\n",
    "\n",
    "   \n",
    "    return tokens_represented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 word(s) from vocab are in the tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_represented = represent_features(tokens, vocab)\n",
    "tokens_represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`44 word(s) from vocab are in the tokens.\n",
    "array([0, 0, 0, ..., 0, 0, 0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Composing all steps of preprocessing \n",
    "</font>\n",
    "\n",
    "Every word in vocabulary is the feature of the mail - 1 is the word in in the mail and 0 otherwise. \n",
    "Thus every mail can be represented as the binary array of fixed length - number of words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess (content,vocab):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    vocab: ndarray of str - list of considered words \n",
    "    '''\n",
    "    # YOUR_CODE. Compute the array with 1/0 corresponding to is word from vocabulary in mail \n",
    "    # START_CODE \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import PorterStemmer\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # tokenize content    \n",
    "    tokens  = word_tokeniize(content)\n",
    "    \n",
    "    # make lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # normalize tokens\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    # remove zero words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # stem words\n",
    "    stem = PorterStemmer()\n",
    "    tokens = [stem.stem(token) for token in tokens]\n",
    "    \n",
    "    # convert to binary array of features  \n",
    "    tokens_represented = np.array([1 if word in tokens else 0 for word in vocab], dtype=int)\n",
    "    # END_CODE     \n",
    "    \n",
    "    return tokens_represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess (content,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`Original len= 69\n",
    "Remaining len= 61\n",
    "44 word(s) from vocab are in the tokens.\n",
    "array([0, 0, 0, ..., 0, 0, 0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Training and test sets\n",
    "</font>\n",
    "\n",
    "All mails for training are preprocessed the same way as performed for sample above. \n",
    "The training set (array of feature representation) is provided in the `spamTrain.mat`\n",
    "The corresponding test set is provided in the `spamTest.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= {} (4000, 1899)\n",
      "y_train.shape= {} (4000,)\n",
      "X_test.shape= {} (1000, 1899)\n",
      "y_test.shape= {} (1000,)\n",
      "Sample with index  =0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "fn=  os.path.join(path , 'spamTrain.mat')\n",
    "\n",
    "mat= loadmat(fn)\n",
    "X_train= mat['X']\n",
    "y_train= mat['y'].ravel()\n",
    "\n",
    "print ('X_train.shape= {}',X_train.shape)\n",
    "print ('y_train.shape= {}',y_train.shape)\n",
    "\n",
    "fn=  os.path.join(path , 'spamTest.mat')\n",
    "mat= loadmat(fn)\n",
    "X_test = mat['Xtest']\n",
    "y_test = mat['ytest'].ravel() \n",
    "\n",
    "print ('X_test.shape= {}',X_test.shape)\n",
    "print ('y_test.shape= {}',y_test.shape)\n",
    "index = 0 \n",
    "print ('Sample with index  ={}: \\n{}'.format(index, X_train[index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Training  the model\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train= 0.99975\n",
      "Score test= 0.992\n"
     ]
    }
   ],
   "source": [
    "C = .1\n",
    "clf= LinearSVC(C=C)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Score train= {}'.format(clf.score(X_train,y_train)))\n",
    "print ('Score test= {}'.format(clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Determining most spam contributors\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('clf.intercept_={}'.format(clf.intercept_))\n",
    "# print ('clf.coef_={}'.format(clf.coef_))\n",
    "\n",
    "# YOUR_CODE. Compute top 20 largest coeficients and return the corresponding 20 words from vocabulary\n",
    "# START_CODE \n",
    "top_spam_contributors = [vocab[i] for i in np.argsort(clf.coef_[0])[-20:][::-1]]\n",
    "# END_CODE    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['our', 'remov', 'click', 'basenumb', 'guarante', 'visit', 'bodi', 'will', 'numberb', 'price', 'dollar', 'nbsp', 'below', 'lo', 'most', 'send', 'dollarnumb', 'credit', 'wi', 'hour']\n"
     ]
    }
   ],
   "source": [
    "print (top_spam_contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['our' 'remov' 'click' 'basenumb' 'guarante' 'visit' 'bodi' 'will'\n",
    " 'numberb' 'price' 'dollar' 'nbsp' 'below' 'lo' 'most' 'send' 'dollarnumb'\n",
    " 'credit' 'wi' 'hour']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "### Use model for prediction \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emailSample1.txt is Spam\n",
      "\n",
      "emailSample2.txt is Not Spam\n",
      "\n",
      "spamSample1.txt is Spam\n",
      "\n",
      "spamSample2.txt is Spam\n",
      "\n",
      "Latter sample:\n",
      "==================================================\n",
      "Best Buy Viagra Generic Online\n",
      "\n",
      "Viagra 100mg x 60 Pills $125, Free Pills & Reorder Discount, Top Selling 100% Quality & Satisfaction guaranteed!\n",
      "\n",
      "We accept VISA, Master & E-Check Payments, 90000+ Satisfied Customers!\n",
      "http://medphysitcstech.ru\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "for sfn in [ 'emailSample1.txt', 'emailSample2.txt', 'spamSample1.txt', 'spamSample2.txt']:\n",
    "    fn=  os.path.join(path,sfn)    \n",
    "    content = get_sample(fn)\n",
    "    \n",
    "    # YOUR_CODE.  Preprocess the sample and get prediction 0 or 1 (1 is spam)\n",
    "    # START_CODE \n",
    "    prediction = clf.predict(preprocess(content,vocab).reshape(1,-1))[0]\n",
    "    # END_CODE    \n",
    "    \n",
    "    print ('{} is {}\\n'.format(sfn, ('Not Spam','Spam')[prediction]))\n",
    "\n",
    "print ('Latter sample:\\n{1}\\n{0}\\n{1}'.format(content, '='*50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected output\n",
    "\n",
    "</font>\n",
    "\n",
    "`Original len= 69\n",
    "Remaining len= 61\n",
    "44 word(s) from vocab are in the tokens.\n",
    "emailSample1.txt is Not Spam`\n",
    "\n",
    "`Original len= 247\n",
    "Remaining len= 222\n",
    "122 word(s) from vocab are in the tokens.\n",
    "emailSample2.txt is Not Spam`\n",
    "\n",
    "`Original len= 141\n",
    "Remaining len= 97\n",
    "46 word(s) from vocab are in the tokens.\n",
    "spamSample1.txt is Spam`\n",
    "\n",
    "`Original len= 39\n",
    "Remaining len= 31\n",
    "18 word(s) from vocab are in the tokens.\n",
    "spamSample2.txt is Spam`\n",
    "\n",
    "`Latter sample:`\n",
    "\n",
    "`==================================================`\n",
    "\n",
    "`Best Buy Viagra Generic Online`\n",
    "\n",
    "`Viagra 100mg x 60 Pills $125, Free Pills & Reorder Discount, Top Selling 100\\% Quality & Satisfaction guaranteed!`\n",
    "\n",
    "`We accept VISA, Master & E-Check Payments, 90000+ Satisfied Customers!\n",
    "http://medphysitcstech.ru`\n",
    "\n",
    "\n",
    "`==================================================`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\n",
    "\n",
    "# Congratulation!\n",
    "</font>\n",
    "\n",
    "Now you know how to build Spam/Non-Spam classifier\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
