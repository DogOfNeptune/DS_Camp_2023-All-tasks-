{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Topic Modeling \n",
    "\n",
    "</font>\n",
    "\n",
    "[voted-kaggle-dataset](https://www.kaggle.com/canggih/voted-kaggle-dataset/version/2#voted-kaggle-dataset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "fn= 'voted-kaggle-dataset.csv'\n",
    "df = pd.read_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of texts= 2,150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('len of texts= {:,}'.format(len(df)))\n",
    "index = 10\n",
    "df.loc[index, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Versions</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>License</th>\n",
       "      <th>Views</th>\n",
       "      <th>Download</th>\n",
       "      <th>Kernels</th>\n",
       "      <th>Topics</th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit Card Fraud Detection</td>\n",
       "      <td>Anonymized credit card transactions labeled as...</td>\n",
       "      <td>Machine Learning Group - ULB</td>\n",
       "      <td>1241</td>\n",
       "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
       "      <td>crime\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>442,136 views</td>\n",
       "      <td>53,128 downloads</td>\n",
       "      <td>1,782 kernels</td>\n",
       "      <td>26 topics</td>\n",
       "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
       "      <td>The datasets contains transactions made by cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>European Soccer Database</td>\n",
       "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
       "      <td>Hugo Mathien</td>\n",
       "      <td>1046</td>\n",
       "      <td>Version 10,2016-10-24|Version 9,2016-10-24|Ver...</td>\n",
       "      <td>association football\\neurope</td>\n",
       "      <td>SQLite</td>\n",
       "      <td>299 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>396,214 views</td>\n",
       "      <td>46,367 downloads</td>\n",
       "      <td>1,459 kernels</td>\n",
       "      <td>75 topics</td>\n",
       "      <td>https://www.kaggle.com/hugomathien/soccer</td>\n",
       "      <td>The ultimate Soccer database for data analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TMDB 5000 Movie Dataset</td>\n",
       "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
       "      <td>The Movie Database (TMDb)</td>\n",
       "      <td>1024</td>\n",
       "      <td>Version 2,2017-09-28</td>\n",
       "      <td>film</td>\n",
       "      <td>CSV</td>\n",
       "      <td>44 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>446,255 views</td>\n",
       "      <td>62,002 downloads</td>\n",
       "      <td>1,394 kernels</td>\n",
       "      <td>46 topics</td>\n",
       "      <td>https://www.kaggle.com/tmdb/tmdb-movie-metadata</td>\n",
       "      <td>Background\\nWhat can we say about the success ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global Terrorism Database</td>\n",
       "      <td>More than 170,000 terrorist attacks worldwide,...</td>\n",
       "      <td>START Consortium</td>\n",
       "      <td>789</td>\n",
       "      <td>Version 2,2017-07-19|Version 1,2016-12-08</td>\n",
       "      <td>crime\\nterrorism\\ninternational relations</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>187,877 views</td>\n",
       "      <td>26,309 downloads</td>\n",
       "      <td>608 kernels</td>\n",
       "      <td>11 topics</td>\n",
       "      <td>https://www.kaggle.com/START-UMD/gtd</td>\n",
       "      <td>Context\\nInformation on more than 170,000 Terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bitcoin Historical Data</td>\n",
       "      <td>Bitcoin data at 1-min intervals from select ex...</td>\n",
       "      <td>Zielak</td>\n",
       "      <td>618</td>\n",
       "      <td>Version 11,2018-01-11|Version 10,2017-11-17|Ve...</td>\n",
       "      <td>history\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>119 MB</td>\n",
       "      <td>CC4</td>\n",
       "      <td>146,734 views</td>\n",
       "      <td>16,868 downloads</td>\n",
       "      <td>68 kernels</td>\n",
       "      <td>13 topics</td>\n",
       "      <td>https://www.kaggle.com/mczielinski/bitcoin-his...</td>\n",
       "      <td>Context\\nBitcoin is the longest running and mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kaggle ML and Data Science Survey, 2017</td>\n",
       "      <td>A big picture view of the state of data scienc...</td>\n",
       "      <td>Kaggle</td>\n",
       "      <td>574</td>\n",
       "      <td>Version 4,2017-10-28|Version 3,2017-10-03|Vers...</td>\n",
       "      <td>employment\\nsociology\\nartificial intelligence</td>\n",
       "      <td>CSV</td>\n",
       "      <td>28 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>95,587 views</td>\n",
       "      <td>9,390 downloads</td>\n",
       "      <td>244 kernels</td>\n",
       "      <td>10 topics</td>\n",
       "      <td>https://www.kaggle.com/kaggle/kaggle-survey-2017</td>\n",
       "      <td>Context\\nFor the first time, Kaggle conducted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Iris Species</td>\n",
       "      <td>Classify iris plants into three species in thi...</td>\n",
       "      <td>UCI Machine Learning</td>\n",
       "      <td>512</td>\n",
       "      <td>Version 2,2016-09-27|Version 1,2016-01-12</td>\n",
       "      <td>botany</td>\n",
       "      <td>SQLite</td>\n",
       "      <td>15 KB</td>\n",
       "      <td>CC0</td>\n",
       "      <td>162,706 views</td>\n",
       "      <td>24,361 downloads</td>\n",
       "      <td>3,394 kernels</td>\n",
       "      <td>14 topics</td>\n",
       "      <td>https://www.kaggle.com/uciml/iris</td>\n",
       "      <td>The Iris dataset was used in R.A. Fisher's cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>World Development Indicators</td>\n",
       "      <td>Explore country development indicators from ar...</td>\n",
       "      <td>World Bank</td>\n",
       "      <td>468</td>\n",
       "      <td>Version 2,2017-05-02|Version 1,2016-01-28</td>\n",
       "      <td>economics\\ninternational relations</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2 GB</td>\n",
       "      <td>Other</td>\n",
       "      <td>134,038 views</td>\n",
       "      <td>20,364 downloads</td>\n",
       "      <td>389 kernels</td>\n",
       "      <td>5 topics</td>\n",
       "      <td>https://www.kaggle.com/worldbank/world-develop...</td>\n",
       "      <td>The World Development Indicators from the Worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Daily News for Stock Market Prediction</td>\n",
       "      <td>Using 8 years daily news headlines to predict ...</td>\n",
       "      <td>Aaron7sun</td>\n",
       "      <td>438</td>\n",
       "      <td>Version 1,2016-08-25</td>\n",
       "      <td>news agencies\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>14 MB</td>\n",
       "      <td>CC4</td>\n",
       "      <td>95,770 views</td>\n",
       "      <td>10,820 downloads</td>\n",
       "      <td>293 kernels</td>\n",
       "      <td>8 topics</td>\n",
       "      <td>https://www.kaggle.com/aaron7sun/stocknews</td>\n",
       "      <td>Actually, I prepare this dataset for students ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pokemon with stats</td>\n",
       "      <td>721 Pokemon with stats and types</td>\n",
       "      <td>Alberto Barradas</td>\n",
       "      <td>428</td>\n",
       "      <td>Version 2,2016-08-29|Version 1,2016-08-23</td>\n",
       "      <td>popular culture\\ngames and toys\\nvideo games</td>\n",
       "      <td>CSV</td>\n",
       "      <td>43 KB</td>\n",
       "      <td>CC0</td>\n",
       "      <td>133,256 views</td>\n",
       "      <td>16,610 downloads</td>\n",
       "      <td>706 kernels</td>\n",
       "      <td>13 topics</td>\n",
       "      <td>https://www.kaggle.com/abcsds/pokemon</td>\n",
       "      <td>This data set includes 721 Pokemon, including ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title  \\\n",
       "0              Credit Card Fraud Detection   \n",
       "1                 European Soccer Database   \n",
       "2                  TMDB 5000 Movie Dataset   \n",
       "3                Global Terrorism Database   \n",
       "4                  Bitcoin Historical Data   \n",
       "5  Kaggle ML and Data Science Survey, 2017   \n",
       "6                             Iris Species   \n",
       "7             World Development Indicators   \n",
       "8   Daily News for Stock Market Prediction   \n",
       "9                       Pokemon with stats   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0  Anonymized credit card transactions labeled as...   \n",
       "1  25k+ matches, players & teams attributes for E...   \n",
       "2                Metadata on ~5,000 movies from TMDb   \n",
       "3  More than 170,000 terrorist attacks worldwide,...   \n",
       "4  Bitcoin data at 1-min intervals from select ex...   \n",
       "5  A big picture view of the state of data scienc...   \n",
       "6  Classify iris plants into three species in thi...   \n",
       "7  Explore country development indicators from ar...   \n",
       "8  Using 8 years daily news headlines to predict ...   \n",
       "9                   721 Pokemon with stats and types   \n",
       "\n",
       "                          Owner  Votes  \\\n",
       "0  Machine Learning Group - ULB   1241   \n",
       "1                  Hugo Mathien   1046   \n",
       "2     The Movie Database (TMDb)   1024   \n",
       "3              START Consortium    789   \n",
       "4                        Zielak    618   \n",
       "5                        Kaggle    574   \n",
       "6          UCI Machine Learning    512   \n",
       "7                    World Bank    468   \n",
       "8                     Aaron7sun    438   \n",
       "9              Alberto Barradas    428   \n",
       "\n",
       "                                            Versions  \\\n",
       "0          Version 2,2016-11-05|Version 1,2016-11-03   \n",
       "1  Version 10,2016-10-24|Version 9,2016-10-24|Ver...   \n",
       "2                               Version 2,2017-09-28   \n",
       "3          Version 2,2017-07-19|Version 1,2016-12-08   \n",
       "4  Version 11,2018-01-11|Version 10,2017-11-17|Ve...   \n",
       "5  Version 4,2017-10-28|Version 3,2017-10-03|Vers...   \n",
       "6          Version 2,2016-09-27|Version 1,2016-01-12   \n",
       "7          Version 2,2017-05-02|Version 1,2016-01-28   \n",
       "8                               Version 1,2016-08-25   \n",
       "9          Version 2,2016-08-29|Version 1,2016-08-23   \n",
       "\n",
       "                                             Tags Data Type    Size License  \\\n",
       "0                                  crime\\nfinance       CSV  144 MB    ODbL   \n",
       "1                    association football\\neurope    SQLite  299 MB    ODbL   \n",
       "2                                            film       CSV   44 MB   Other   \n",
       "3       crime\\nterrorism\\ninternational relations       CSV  144 MB   Other   \n",
       "4                                history\\nfinance       CSV  119 MB     CC4   \n",
       "5  employment\\nsociology\\nartificial intelligence       CSV   28 MB    ODbL   \n",
       "6                                          botany    SQLite   15 KB     CC0   \n",
       "7              economics\\ninternational relations       CSV    2 GB   Other   \n",
       "8                          news agencies\\nfinance       CSV   14 MB     CC4   \n",
       "9    popular culture\\ngames and toys\\nvideo games       CSV   43 KB     CC0   \n",
       "\n",
       "           Views          Download        Kernels     Topics  \\\n",
       "0  442,136 views  53,128 downloads  1,782 kernels  26 topics   \n",
       "1  396,214 views  46,367 downloads  1,459 kernels  75 topics   \n",
       "2  446,255 views  62,002 downloads  1,394 kernels  46 topics   \n",
       "3  187,877 views  26,309 downloads    608 kernels  11 topics   \n",
       "4  146,734 views  16,868 downloads     68 kernels  13 topics   \n",
       "5   95,587 views   9,390 downloads    244 kernels  10 topics   \n",
       "6  162,706 views  24,361 downloads  3,394 kernels  14 topics   \n",
       "7  134,038 views  20,364 downloads    389 kernels   5 topics   \n",
       "8   95,770 views  10,820 downloads    293 kernels   8 topics   \n",
       "9  133,256 views  16,610 downloads    706 kernels  13 topics   \n",
       "\n",
       "                                                 URL  \\\n",
       "0     https://www.kaggle.com/mlg-ulb/creditcardfraud   \n",
       "1          https://www.kaggle.com/hugomathien/soccer   \n",
       "2    https://www.kaggle.com/tmdb/tmdb-movie-metadata   \n",
       "3               https://www.kaggle.com/START-UMD/gtd   \n",
       "4  https://www.kaggle.com/mczielinski/bitcoin-his...   \n",
       "5   https://www.kaggle.com/kaggle/kaggle-survey-2017   \n",
       "6                  https://www.kaggle.com/uciml/iris   \n",
       "7  https://www.kaggle.com/worldbank/world-develop...   \n",
       "8         https://www.kaggle.com/aaron7sun/stocknews   \n",
       "9              https://www.kaggle.com/abcsds/pokemon   \n",
       "\n",
       "                                         Description  \n",
       "0  The datasets contains transactions made by cre...  \n",
       "1  The ultimate Soccer database for data analysis...  \n",
       "2  Background\\nWhat can we say about the success ...  \n",
       "3  Context\\nInformation on more than 170,000 Terr...  \n",
       "4  Context\\nBitcoin is the longest running and mo...  \n",
       "5  Context\\nFor the first time, Kaggle conducted ...  \n",
       "6  The Iris dataset was used in R.A. Fisher's cla...  \n",
       "7  The World Development Indicators from the Worl...  \n",
       "8  Actually, I prepare this dataset for students ...  \n",
       "9  This data set includes 721 Pokemon, including ...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\",\n",
       " 'The ultimate Soccer database for data analysis and machine learning\\nWhat you get:\\n+25,000 matches\\n+10,000 players\\n11 European Countries with their lead championship\\nSeasons 2008 to 2016\\nPlayers and Teams\\' attributes* sourced from EA Sports\\' FIFA video game series, including the weekly updates\\nTeam line up with squad formation (X, Y coordinates)\\nBetting odds from up to 10 providers\\nDetailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\\n*16th Oct 2016: New table containing teams\\' attributes from FIFA !\\nOriginal Data Source:\\nYou can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. I must insist that you do not make any commercial use of the data. The data was sourced from:\\nhttp://football-data.mx-api.enetscores.com/ : scores, lineup, team formation and events\\nhttp://www.football-data.co.uk/ : betting odds. Click here to understand the column naming system for betting odds:\\nhttp://sofifa.com/ : players and teams attributes from EA Sports FIFA games. FIFA series and all FIFA assets property of EA Sports.\\nWhen you have a look at the database, you will notice foreign keys for players and matches are the same as the original data sources. I have called those foreign keys \"api_id\".\\nImproving the dataset:\\nYou will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their attributes from FIFA. This will be fixed overtime as the crawling algorithm is being improved. The dataset will also be expanded to include international games, national cups, Champion\\'s League and Europa League. Please ask me if you\\'re after a specific tournament.\\nPlease get in touch with me if you want to help improve this dataset.\\nCLICK HERE TO ACCESS THE PROJECT GITHUB\\nImportant note for people interested in using the crawlers: since I first wrote the crawling scripts (in python), it appears sofifa.com has changed its design and with it comes new requirements for the scripts. The existing script to crawl players (\\'Player Spider\\') will not work until i\\'ve updated it.\\nExploring the data:\\nNow that\\'s the fun part, there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! Here are some ideas for you:\\nThe Holy Grail... ... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win, Draw, Away Win). They get it right about 53% of the time. This is also what I\\'ve achieved so far using my own SVM. Though it may sound high for such a random sport game, you\\'ve got to know that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision.\\nProbabilities vs Odds\\nWhen running a multi-class classifier like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had very different predictions.\\nExplore and visualize features\\nWith access to players and teams attributes, team formations and in-game events you should be able to produce some interesting insights into The Beautiful Game . Who knows, Guardiola himself may hire one of you some day!',\n",
       " \"Background\\nWhat can we say about the success of a movie before it is released? Are there certain companies (Pixar?) that have found a consistent formula? Given that major films costing over $100 million to produce can still flop, this question is more important than ever to the industry. Film aficionados might have different interests. Can we predict which films will be highly rated, whether or not they are a commercial success?\\nThis is a great place to start digging in to those questions, with data on the plot, cast, crew, budget, and revenues of several thousand films.\\nData Source Transfer Summary\\nWe (Kaggle) have removed the original version of this dataset per a DMCA takedown request from IMDB. In order to minimize the impact, we're replacing it with a similar set of films and data fields from The Movie Database (TMDb) in accordance with their terms of use. The bad news is that kernels built on the old dataset will most likely no longer work.\\nThe good news is that:\\nYou can port your existing kernels over with a bit of editing. This kernel offers functions and examples for doing so. You can also find a general introduction to the new format here.\\nThe new dataset contains full credits for both the cast and the crew, rather than just the first three actors.\\nActor and actresses are now listed in the order they appear in the credits. It's unclear what ordering the original dataset used; for the movies I spot checked it didn't line up with either the credits order or IMDB's stars order.\\nThe revenues appear to be more current. For example, IMDB's figures for Avatar seem to be from 2010 and understate the film's global revenues by over $2 billion.\\nSome of the movies that we weren't able to port over (a couple of hundred) were just bad entries. For example, this IMDB entry has basically no accurate information at all. It lists Star Wars Episode VII as a documentary.\\nData Source Transfer Details\\nSeveral of the new columns contain json. You can save a bit of time by porting the load data functions from this kernel.\\nEven in simple fields like runtime may not be consistent across versions. For example, previous dataset shows the duration for Avatar's extended cut while TMDB shows the time for the original version.\\nThere's now a separate file containing the full credits for both the cast and crew.\\nAll fields are filled out by users so don't expect them to agree on keywords, genres, ratings, or the like.\\nYour existing kernels will continue to render normally until they are re-run.\\nIf you are curious about how this dataset was prepared, the code to access TMDb's API is posted here.\\nNew columns:\\nhomepage\\nid\\noriginal_title\\noverview\\npopularity\\nproduction_companies\\nproduction_countries\\nrelease_date\\nspoken_languages\\nstatus\\ntagline\\nvote_average\\nLost columns:\\nactor_1_facebook_likes\\nactor_2_facebook_likes\\nactor_3_facebook_likes\\naspect_ratio\\ncast_total_facebook_likes\\ncolor\\ncontent_rating\\ndirector_facebook_likes\\nfacenumber_in_poster\\nmovie_facebook_likes\\nmovie_imdb_link\\nnum_critic_for_reviews\\nnum_user_for_reviews\\nOpen Questions About the Data\\nThere are some things we haven't had a chance to confirm about the new dataset. If you have any insights, please let us know in the forums!\\nAre the budgets and revenues all in US dollars? Do they consistently show the global revenues?\\nThis dataset hasn't yet gone through a data quality analysis. Can you find any obvious corrections? For example, in the IMDb version it was necessary to treat values of zero in the budget field as missing. Similar findings would be very helpful to your fellow Kagglers! (It's probably a good idea to keep treating zeros as missing, with the caveat that missing budgets much more likely to have been from small budget films in the first place).\\nInspiration\\nCan you categorize the films by type, such as animated or not? We don't have explicit labels for this, but it should be possible to build them from the crew's job titles.\\nHow sharp is the divide between major film studios and the independents? Do those two groups fall naturally out of a clustering analysis or is something more complicated going on?\\nAcknowledgements\\nThis dataset was generated from The Movie Database API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here.\",\n",
       " 'Context\\nInformation on more than 170,000 Terrorist Attacks\\nThe Global Terrorism Database (GTD) is an open-source database including information on terrorist attacks around the world from 1970 through 2016 (with annual updates planned for the future). The GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 170,000 cases. The database is maintained by researchers at the National Consortium for the Study of Terrorism and Responses to Terrorism (START), headquartered at the University of Maryland. More Information\\nContent\\nGeography: Worldwide\\nTime period: 1970-2016, except 1993 (2017 in progress, publication expected June 2018)\\nUnit of analysis: Attack\\nVariables: >100 variables on location, tactics, perpetrators, targets, and outcomes\\nSources: Unclassified media articles (Note: Please interpret changes over time with caution. Global patterns are driven by diverse trends in particular regions, and data collection is influenced by fluctuations in access to media coverage over both time and place.)\\nDefinition of terrorism:\\n\"The threatened or actual use of illegal force and violence by a non-state actor to attain a political, economic, religious, or social goal through fear, coercion, or intimidation.\"\\nSee the GTD Codebook for important details on data collection methodology, definitions, and coding schema.\\nAcknowledgements\\nThe Global Terrorism Database is funded through START, by the US Department of State (Contract Number: SAQMMA12M1292) and the US Department of Homeland Security Science and Technology Directorate’s Office of University Programs (Award Number 2012-ST-061-CS0001, CSTAB 3.1). The coding decisions and classifications contained in the database are determined independently by START researchers and should not be interpreted as necessarily representing the official views or policies of the United States Government.\\nGTD Team\\nPublications\\nThe GTD has been leveraged extensively in scholarly publications, reports, and media articles. Putting Terrorism in Context: Lessons from the Global Terrorism Database, by GTD principal investigators LaFree, Dugan, and Miller investigates patterns of terrorism and provides perspective on the challenges of data collection and analysis. The GTD\\'s data collection manager, Michael Jensen, discusses important Benefits and Drawbacks of Methodological Advancements in Data Collection and Coding.\\nTerms of Use\\nUse of the data signifies your agreement to the following terms and conditions.\\nDefinitions: Within this section: \"GTD\" will refer to the Global Terrorism Database produced by the National Consortium for the Study of Terrorism and Responses to Terrorism. This includes the data and codebook, any auxiliary materials present, and the World Wide Web interface by which the data are presented. \"START\" will refer to the National Consortium for the Study of Terrorism and Responses to Terrorism, a United States Department of Homeland Security Center of Excellence based at the University of Maryland. \"USER\" denotes the individual or set of individuals who access the GTD, i.e. the data, codebook, any auxiliary materials, and the World Wide Web interface by which the data are presented. \"GTD representatives\" denotes any senior management staff of START, and any employee or representative of said organization whom senior management staff designate to represent START in dealings with the USER.\\nUsage Rights: Pursuant to this agreement, START grants the USER the non-exclusive, non-guaranteed right to search, browse, and view all contents of the GTD World Wide Web interface.\\nAuthorship: All contents of the GTD were assembled by representatives of START and do not purport to reflect the official position or data collections of the Department of Homeland Security or any other agency of the United States government.\\nAcknowledgement: All information sourced from the GTD should be acknowledged by the USER and cited as follows: \"National Consortium for the Study of Terrorism and Responses to Terrorism (START). (2017). Global Terrorism Database [Data file]. Retrieved from https://www.kaggle.com/START-UMD/gtd\"\\nUnauthorized Publication of the Data: No part of the GTD may be republished on any website or accessible for public download in any format without the express permission of a GTD staff member. In addition, no part of the GTD may be distributed for any commercial purpose, nor with the intent that the data be used in any commercial enterprise, without the express permission of a GTD staff member. START reserves the right to withhold this permission.\\nPenalties: Penalties for failure to comply with the terms of this agreement may result in loss of access to the GTD and the forfeiture of user privileges, in addition to any other appropriate legal remedies.\\nLimitation of Liability: Although every reasonable effort has been made to check sources and verify facts, START cannot guarantee that accounts reported in the open literature are complete and accurate. START shall not be held liable for any loss or damage caused by errors or omissions or resulting from any use, misuse, or alteration of GTD data by the USER. The USER should not infer any additional actions or results beyond what is presented in a GTD entry and specifically, the USER should not infer an individual associated with a particular incident was tried and convicted of terrorism or any other criminal offense. If new documentation about an event becomes available, an entry may be modified, as necessary and appropriate.\\nTermination of Rights: The GTD developers reserve the right to remove access to the GTD website from any particular IP address or set of IP addresses, or to remove the database entirely from public access, at their discretion. In such an event, all USER rights granted in this document are terminated.\\nTraining\\nSTART has released the first in a series of training modules designed to equip GTD users with the knowledge and tools to best leverage the database. This training module provides a general overview of the GTD, including the data collection process, uses of the GTD, and patterns of global terrorism. Participants will learn basic data handling and how to generate summary statistics from the GTD using PivotTables in Microsoft Excel.\\nQuestions?\\nFind answers to Frequently Asked Questions.\\nContact the GTD staff at gtd@start.umd.edu.',\n",
       " 'Context\\nBitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus \"chained\" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining!\\nContent\\ncoincheckJPY_1-min_data_2014-10-31_to_2018-01-08.csv\\nbitflyerJPY_1-min_data_2017-07-04_to_2018-01-08.csv\\ncoinbaseUSD_1-min_data_2014-12-01_to_2018-01-08.csv\\nbitstampUSD_1-min_data_2012-01-01_to_2018-01-08.csv\\nCSV files for select bitcoin exchanges for the time period of Jan 2012 to Jan 2018, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. Timestamps are in Unix time. Timestamps without any trades or activity have their data fields populated with NaNs. If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk.\\nAcknowledgements and Inspiration\\nThe various exchange APIs, for making it difficult or unintuitive enough to get OHLC and volume data at 1-min intervals that I set out on this data scraping project. Satoshi Nakamoto and the novel core concept of the blockchain, as well as its first execution via the bitcoin protocol. I\\'d also like to thank viewers like you! Can\\'t wait to see what code or insights you all have to share.\\nI am a lowly Ph.D. student who did this for fun in my meager spare time. If you find this data interesting and you can spare a coffee to fuel my science, send it my way and I\\'d be immensely grateful!\\n1kmWmcQa8qN9ZrdGfdkw8EHKBgugKBRcF',\n",
       " 'Context\\nFor the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.\\nTo share some of the initial insights from the survey, we’ve worked with the folks from The Pudding to put together this interactive report. They’ve shared all of the kernels used in the report here.\\nContent\\nThe data includes 5 files:\\nschema.csv: a CSV file with survey schema. This schema includes the questions that correspond to each column name in both the multipleChoiceResponses.csv and freeformResponses.csv.\\nmultipleChoiceResponses.csv: Respondents\\' answers to multiple choice and ranking questions. These are non-randomized and thus a single row does correspond to all of a single user\\'s answers. -freeformResponses.csv: Respondents\\' freeform answers to Kaggle\\'s survey questions. These responses are randomized within a column, so that reading across a single row does not give a single user\\'s answers.\\nconversionRates.csv: Currency conversion rates (to USD) as accessed from the R package \"quantmod\" on September 14, 2017\\nRespondentTypeREADME.txt: This is a schema for decoding the responses in the \"Asked\" column of the schema.csv file.\\nKernel Awards in November\\nIn the month of November, we’re awarding $1000 a week for code and analyses shared on this dataset via Kaggle Kernels. Read more about this month’s Kaggle Kernels Awards and help us advance the state of machine learning and data science by exploring this one of a kind dataset.\\nMethodology\\nThis survey received 16,716 usable respondents from 171 countries and territories. If a country or territory received less than 50 respondents, we grouped them into a group named “Other” for anonymity.\\nWe excluded respondents who were flagged by our survey system as “Spam” or who did not answer the question regarding their employment status (this question was the first required question, so not answering it indicates that the respondent did not proceed past the 5th question in our survey).\\nMost of our respondents were found primarily through Kaggle channels, like our email list, discussion forums and social media channels.\\nThe survey was live from August 7th to August 25th. The median response time for those who participated in the survey was 16.4 minutes. We allowed respondents to complete the survey at any time during that window.\\nWe received salary data by first asking respondents for their day-to-day currency, and then asking them to write in either their total compensation.\\nWe’ve provided a csv with an exchange rate to USD for you to calculate the salary in US dollars on your own.\\nThe question was optional\\nNot every question was shown to every respondent. In an attempt to ask relevant questions to each respondent, we generally asked work related questions to employed data scientists and learning related questions to students. There is a column in the schema.csv file called \"Asked\" that describes who saw each question. You can learn more about the different segments we used in the schema.csv file and RespondentTypeREADME.txt in the data tab.\\nTo protect the respondents’ identity, the answers to multiple choice questions have been separated into a separate data file from the open-ended responses. We do not provide a key to match up the multiple choice and free form responses. Further, the free form responses have been randomized column-wise such that the responses that appear on the same row did not necessarily come from the same survey-taker.',\n",
       " \"The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\\nThe columns in this dataset are:\\nId\\nSepalLengthCm\\nSepalWidthCm\\nPetalLengthCm\\nPetalWidthCm\\nSpecies\",\n",
       " \"The World Development Indicators from the World Bank contain over a thousand annual indicators of economic development from hundreds of countries around the world.\\nHere's a list of the available indicators along with a list of the available countries.\\nFor example, this data includes the life expectancy at birth from many countries around the world:\\nThe dataset hosted here is a slightly transformed verion of the raw files available here to facilitate analytics.\",\n",
       " 'Actually, I prepare this dataset for students on my Deep Learning and NLP course.\\nBut I am also very happy to see kagglers play around with it.\\nHave fun!\\nDescription:\\nThere are two channels of data provided in this dataset:\\nNews data: I crawled historical news headlines from Reddit WorldNews Channel (/r/worldnews). They are ranked by reddit users\\' votes, and only the top 25 headlines are considered for a single date. (Range: 2008-06-08 to 2016-07-01)\\nStock data: Dow Jones Industrial Average (DJIA) is used to \"prove the concept\". (Range: 2008-08-08 to 2016-07-01)\\nI provided three data files in .csv format:\\nRedditNews.csv: two columns The first column is the \"date\", and second column is the \"news headlines\". All news are ranked from top to bottom based on how hot they are. Hence, there are 25 lines for each date.\\nDJIA_table.csv: Downloaded directly from Yahoo Finance: check out the web page for more info.\\nCombined_News_DJIA.csv: To make things easier for my students, I provide this combined dataset with 27 columns. The first column is \"Date\", the second is \"Label\", and the following ones are news headlines ranging from \"Top1\" to \"Top25\".\\n=========================================\\nTo my students:\\nI made this a binary classification task. Hence, there are only two labels:\\n\"1\" when DJIA Adj Close value rose or stayed as the same;\\n\"0\" when DJIA Adj Close value decreased.\\nFor task evaluation, please use data from 2008-08-08 to 2014-12-31 as Training Set, and Test Set is then the following two years data (from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split.\\nAnd, of course, use AUC as the evaluation metric.\\n=========================================\\n+++++++++++++++++++++++++++++++++++++++++\\nTo all kagglers:\\nPlease upvote this dataset if you like this idea for market prediction.\\nIf you think you coded an amazing trading algorithm,\\nfriendly advice\\ndo play safe with your own money :)\\n+++++++++++++++++++++++++++++++++++++++++\\nFeel free to contact me if there is any question~\\nAnd, remember me when you become a millionaire :P',\n",
       " \"This data set includes 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. It has been of great use when teaching statistics to kids. With certain types you can also give a geeky introduction to machine learning.\\nThis are the raw attributes that are used for calculating how much damage an attack will do in the games. This dataset is about the pokemon games (NOT pokemon cards or Pokemon Go).\\nThe data as described by Myles O'Neill is:\\n#: ID for each pokemon\\nName: Name of each pokemon\\nType 1: Each pokemon has a type, this determines weakness/resistance to attacks\\nType 2: Some pokemon are dual type and have 2\\nTotal: sum of all stats that come after this, a general guide to how strong a pokemon is\\nHP: hit points, or health, defines how much damage a pokemon can withstand before fainting\\nAttack: the base modifier for normal attacks (eg. Scratch, Punch)\\nDefense: the base damage resistance against normal attacks\\nSP Atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)\\nSP Def: the base damage resistance against special attacks\\nSpeed: determines which pokemon attacks first each round\\nThe data for this table has been acquired from several different sites, including:\\npokemon.com\\npokemondb\\nbulbapeida\\nOne question has been answered with this database: The type of a pokemon cannot be inferred only by it's Attack and Deffence. It would be worthy to find which two variables can define the type of a pokemon, if any. Two variables can be plotted in a 2D space, and used as an example for machine learning. This could mean the creation of a visual example any geeky Machine Learning class would love.\",\n",
       " 'These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k',\n",
       " \"Context\\nAfter watching Somm (a documentary on master sommeliers) I wondered how I could create a predictive model to identify wines through blind tasting like a master sommelier would. The first step in this journey was gathering some data to train a model. I plan to use deep learning to predict the wine variety using words in the description/review. The model still won't be able to taste the wine, but theoretically it could identify the wine based on a description that a sommelier could give. If anyone has any ideas on how to accomplish this, please post them!\\nContent\\nThe data consists of 10 fields:\\nPoints: the number of points WineEnthusiast rated the wine on a scale of 1-100 (though they say they only post reviews for wines that score >=80)\\nTitle: the title of the wine review, which often contains the vintage if you're interested in extracting that feature\\nVariety: the type of grapes used to make the wine (ie Pinot Noir)\\nDescription: a few sentences from a sommelier describing the wine's taste, smell, look, feel, etc.\\nCountry: the country that the wine is from\\nProvince: the province or state that the wine is from\\nRegion 1: the wine growing area in a province or state (ie Napa)\\nRegion 2: sometimes there are more specific regions specified within a wine growing area (ie Rutherford inside the Napa Valley), but this value can sometimes be blank\\nWinery: the winery that made the wine\\nDesignation: the vineyard within the winery where the grapes that made the wine are from\\nPrice: the cost for a bottle of the wine\\nTaster Name: name of the person who tasted and reviewed the wine\\nTaster Twitter Handle: Twitter handle for the person who tasted and reviewed the wine\\nAcknowledgements\\nThe data was scraped from WineEnthusiast during the week of June 15th, 2017. The code for the scraper can be found here if you have any more specific questions about data collection that I didn't address.\\nUPDATE 11/24/2017 After feedback from users of the dataset I scraped the reviews again on November 22nd, 2017. This time around I collected the title of each review, which you can parse the year out of, the tasters name, and the taster's Twitter handle. This should also fix the duplicate entry issue.\\nInspiration\\nI think that this dataset offers some great opportunities for sentiment analysis and other text related predictive models. My overall goal is to create a model that can identify the variety, winery, and location of a wine based on a description. If anyone has any ideas, breakthroughs, or other interesting insights/models please post them.\",\n",
       " 'Some say climate change is the biggest threat of our age while others say it’s a myth based on dodgy science. We are turning some of the data over to you so you can form your own view.\\nEven more than with other data sets that Kaggle has featured, there’s a huge amount of data cleaning and preparation that goes into putting together a long-time study of climate trends. Early data was collected by technicians using mercury thermometers, where any variation in the visit time impacted measurements. In the 1940s, the construction of airports caused many weather stations to be moved. In the 1980s, there was a move to electronic thermometers that are said to have a cooling bias.\\nGiven this complexity, there are a range of organizations that collate climate trends data. The three most cited land and ocean temperature data sets are NOAA’s MLOST, NASA’s GISTEMP and the UK’s HadCrut.\\nWe have repackaged the data from a newer compilation put together by the Berkeley Earth, which is affiliated with Lawrence Berkeley National Laboratory. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives. It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away.\\nIn this dataset, we have include several files:\\nGlobal Land and Ocean-and-Land Temperatures (GlobalTemperatures.csv):\\nDate: starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures\\nLandAverageTemperature: global average land temperature in celsius\\nLandAverageTemperatureUncertainty: the 95% confidence interval around the average\\nLandMaxTemperature: global average maximum land temperature in celsius\\nLandMaxTemperatureUncertainty: the 95% confidence interval around the maximum land temperature\\nLandMinTemperature: global average minimum land temperature in celsius\\nLandMinTemperatureUncertainty: the 95% confidence interval around the minimum land temperature\\nLandAndOceanAverageTemperature: global average land and ocean temperature in celsius\\nLandAndOceanAverageTemperatureUncertainty: the 95% confidence interval around the global average land and ocean temperature\\nOther files include:\\nGlobal Average Land Temperature by Country (GlobalLandTemperaturesByCountry.csv)\\nGlobal Average Land Temperature by State (GlobalLandTemperaturesByState.csv)\\nGlobal Land Temperatures By Major City (GlobalLandTemperaturesByMajorCity.csv)\\nGlobal Land Temperatures By City (GlobalLandTemperaturesByCity.csv)\\nThe raw data comes from the Berkeley Earth data page.',\n",
       " 'A food products database\\nOpen Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of information we can find on product labels.\\nMade by everyone\\nOpen Food Facts is a non-profit association of volunteers. 5000+ contributors like you have added 100 000+ products from 150 countries using our Android, iPhone or Windows Phone app or their camera to scan barcodes and upload pictures of products and their labels.\\nFor everyone\\nData about food is of public interest and has to be open. The complete database is published as open data and can be reused by anyone and for any use. Check-out the cool reuses or make your own!\\nDataset structure\\nThe dataset contains a single table, FoodFacts, in CSV form in FoodFacts.csv and in SQLite form in database.sqlite.\\nThe columns in Open Food Facts are as follows:\\ncode (text)\\nurl (text)\\ncreator (text)\\ncreated_t (text)\\ncreated_datetime (text)\\nlast_modified_t (text)\\nlast_modified_datetime (text)\\nproduct_name (text)\\ngeneric_name (text)\\nquantity (text)\\npackaging (text)\\npackaging_tags (text)\\nbrands (text)\\nbrands_tags (text)\\ncategories (text)\\ncategories_tags (text)\\ncategories_en (text)\\norigins (text)\\norigins_tags (text)\\nmanufacturing_places (text)\\nmanufacturing_places_tags (text)\\nlabels (text)\\nlabels_tags (text)\\nlabels_en (text)\\nemb_codes (text)\\nemb_codes_tags (text)\\nfirst_packaging_code_geo (text)\\ncities (text)\\ncities_tags (text)\\npurchase_places (text)\\nstores (text)\\ncountries (text)\\ncountries_tags (text)\\ncountries_en (text)\\ningredients_text (text)\\nallergens (text)\\nallergens_en (text)\\ntraces (text)\\ntraces_tags (text)\\ntraces_en (text)\\nserving_size (text)\\nno_nutriments (numeric)\\nadditives_n (numeric)\\nadditives (text)\\nadditives_tags (text)\\nadditives_en (text)\\ningredients_from_palm_oil_n (numeric)\\ningredients_from_palm_oil (numeric)\\ningredients_from_palm_oil_tags (text)\\ningredients_that_may_be_from_palm_oil_n (numeric)\\ningredients_that_may_be_from_palm_oil (numeric)\\ningredients_that_may_be_from_palm_oil_tags (text)\\nnutrition_grade_uk (numeric)\\nnutrition_grade_fr (text)\\npnns_groups_1 (text)\\npnns_groups_2 (text)\\nstates (text)\\nstates_tags (text)\\nstates_en (text)\\nmain_category (text)\\nmain_category_en (text)\\nimage_url (text)\\nimage_small_url (text)\\nenergy_100g (numeric)\\nenergy_from_fat_100g (numeric)\\nfat_100g (numeric)\\nsaturated_fat_100g (numeric)\\nbutyric_acid_100g (numeric)\\ncaproic_acid_100g (numeric)\\ncaprylic_acid_100g (numeric)\\ncapric_acid_100g (numeric)\\nlauric_acid_100g (numeric)\\nmyristic_acid_100g (numeric)\\npalmitic_acid_100g (numeric)\\nstearic_acid_100g (numeric)\\narachidic_acid_100g (numeric)\\nbehenic_acid_100g (numeric)\\nlignoceric_acid_100g (numeric)\\ncerotic_acid_100g (numeric)\\nmontanic_acid_100g (numeric)\\nmelissic_acid_100g (numeric)\\nmonounsaturated_fat_100g (numeric)\\npolyunsaturated_fat_100g (numeric)\\nomega_3_fat_100g (numeric)\\nalpha_linolenic_acid_100g (numeric)\\neicosapentaenoic_acid_100g (numeric)\\ndocosahexaenoic_acid_100g (numeric)\\nomega_6_fat_100g (numeric)\\nlinoleic_acid_100g (numeric)\\narachidonic_acid_100g (numeric)\\ngamma_linolenic_acid_100g (numeric)\\ndihomo_gamma_linolenic_acid_100g (numeric)\\nomega_9_fat_100g (numeric)\\noleic_acid_100g (numeric)\\nelaidic_acid_100g (numeric)\\ngondoic_acid_100g (numeric)\\nmead_acid_100g (numeric)\\nerucic_acid_100g (numeric)\\nnervonic_acid_100g (numeric)\\ntrans_fat_100g (numeric)\\ncholesterol_100g (numeric)\\ncarbohydrates_100g (numeric)\\nsugars_100g (numeric)\\nsucrose_100g (numeric)\\nglucose_100g (numeric)\\nfructose_100g (numeric)\\nlactose_100g (numeric)\\nmaltose_100g (numeric)\\nmaltodextrins_100g (numeric)\\nstarch_100g (numeric)\\npolyols_100g (numeric)\\nfiber_100g (numeric)\\nproteins_100g (numeric)\\ncasein_100g (numeric)\\nserum_proteins_100g (numeric)\\nnucleotides_100g (numeric)\\nsalt_100g (numeric)\\nsodium_100g (numeric)\\nalcohol_100g (numeric)\\nvitamin_a_100g (numeric)\\nbeta_carotene_100g (numeric)\\nvitamin_d_100g (numeric)\\nvitamin_e_100g (numeric)\\nvitamin_k_100g (numeric)\\nvitamin_c_100g (numeric)\\nvitamin_b1_100g (numeric)\\nvitamin_b2_100g (numeric)\\nvitamin_pp_100g (numeric)\\nvitamin_b6_100g (numeric)\\nvitamin_b9_100g (numeric)\\nvitamin_b12_100g (numeric)\\nbiotin_100g (numeric)\\npantothenic_acid_100g (numeric)\\nsilica_100g (numeric)\\nbicarbonate_100g (numeric)\\npotassium_100g (numeric)\\nchloride_100g (numeric)\\ncalcium_100g (numeric)\\nphosphorus_100g (numeric)\\niron_100g (numeric)\\nmagnesium_100g (numeric)\\nzinc_100g (numeric)\\ncopper_100g (numeric)\\nmanganese_100g (numeric)\\nfluoride_100g (numeric)\\nselenium_100g (numeric)\\nchromium_100g (numeric)\\nmolybdenum_100g (numeric)\\niodine_100g (numeric)\\ncaffeine_100g (numeric)\\ntaurine_100g (numeric)\\nph_100g (numeric)\\nfruits_vegetables_nuts_100g (numeric)\\ncollagen_meat_protein_ratio_100g (numeric)\\ncocoa_100g (numeric)\\nchlorophyl_100g (numeric)\\ncarbon_footprint_100g (numeric)\\nnutrition_score_fr_100g (numeric)\\nnutrition_score_uk_100g (numeric)',\n",
       " \"Salary Increase By Type of College\\nParty school? Liberal Arts college? State School? You already know your starting salary will be different depending on what type of school you attend. But, increased earning power shows less disparity. Ten years out, graduates of Ivy League schools earned 99% more than they did at graduation. Party school graduates saw an 85% increase. Engineering school graduates fared worst, earning 76% more 10 years out of school. See where your school ranks.\\nSalaries By Region\\nAttending college in the Midwest leads to the lowest salary both at graduation and at mid-career, according to the PayScale Inc. survey. Graduates of schools in the Northeast and California fared best.\\nSalary Increase By Major\\nYour parents might have worried when you chose Philosophy or International Relations as a major. But a year-long survey of 1.2 million people with only a bachelor's degree by PayScale Inc. shows that graduates in these subjects earned 103.5% and 97.8% more, respectively, about 10 years post-commencement. Majors that didn't show as much salary growth include Nursing and Information Technology.\\nAll data was obtained from the Wall Street Journal based on data from Payscale, Inc:\\nSalaries for Colleges by Type\\nSalaries for Colleges by Region\\nDegrees that Pay you Back\",\n",
       " 'Context\\nAlthough this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago, mushroom hunting (otherwise known as \"shrooming\") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be?\\nContent\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be\\'\\' for Poisonous Oak and Ivy.\\nTime period: Donated to UCI ML 27 April 1987\\nInspiration\\nWhat types of machine learning models perform best on this dataset?\\nWhich features are most indicative of a poisonous mushroom?\\nAcknowledgements\\nThis dataset was originally donated to the UCI Machine Learning repository. You can learn more about past research using the data here.\\nStart a new kernel',\n",
       " 'Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\\nThis database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/\\nAlso can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\\nAttribute Information:\\n1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\\nTen real-valued features are computed for each cell nucleus:\\na) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\\nAll feature values are recoded with four significant digits.\\nMissing attribute values: none\\nClass distribution: 357 benign, 212 malignant',\n",
       " \"Context\\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\\nContents\\nReviews.csv: Pulled from the corresponding SQLite table named Reviews in database.sqlite\\ndatabase.sqlite: Contains the table 'Reviews'\\n\\nData includes:\\n- Reviews from Oct 1999 - Oct 2012\\n- 568,454 reviews\\n- 256,059 users\\n- 74,258 products\\n- 260 users with > 50 reviews\\nAcknowledgements\\nSee this SQLite query for a quick sample of the dataset.\\nIf you publish articles based on this dataset, please cite the following paper:\\nJ. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013.\",\n",
       " 'Context\\nThe World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields – economics, psychology, survey analysis, national statistics, health, public policy and more – describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.\\nContent\\nThe happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll. This question, known as the Cantril ladder, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. The scores are from nationally representative samples for the years 2013-2016 and use the Gallup weights to make the estimates representative. The columns following the happiness score estimate the extent to which each of six factors – economic production, social support, life expectancy, freedom, absence of corruption, and generosity – contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world’s lowest national averages for each of the six factors. They have no impact on the total score reported for each country, but they do explain why some countries rank higher than others.\\nInspiration\\nWhat countries or regions rank the highest in overall happiness and each of the six factors contributing to happiness? How did country ranks or scores change between the 2015 and 2016 as well as the 2016 and 2017 reports? Did any country experience a significant increase or decrease in happiness?\\nWhat is Dystopia?\\nDystopia is an imaginary country that has the world’s least-happy people. The purpose in establishing Dystopia is to have a benchmark against which all countries can be favorably compared (no country performs more poorly than Dystopia) in terms of each of the six key variables, thus allowing each sub-bar to be of positive width. The lowest scores observed for the six key variables, therefore, characterize Dystopia. Since life would be very unpleasant in a country with the world’s lowest incomes, lowest life expectancy, lowest generosity, most corruption, least freedom and least social support, it is referred to as “Dystopia,” in contrast to Utopia.\\nWhat are the residuals?\\nThe residuals, or unexplained components, differ for each country, reflecting the extent to which the six variables either over- or under-explain average 2014-2016 life evaluations. These residuals have an average value of approximately zero over the whole set of countries. Figure 2.2 shows the average residual for each country when the equation in Table 2.1 is applied to average 2014- 2016 data for the six variables in that country. We combine these residuals with the estimate for life evaluations in Dystopia so that the combined bar will always have positive values. As can be seen in Figure 2.2, although some life evaluation residuals are quite large, occasionally exceeding one point on the scale from 0 to 10, they are always much smaller than the calculated value in Dystopia, where the average life is rated at 1.85 on the 0 to 10 scale.\\nWhat do the columns succeeding the Happiness Score(like Family, Generosity, etc.) describe?\\nThe following columns: GDP per Capita, Family, Life Expectancy, Freedom, Generosity, Trust Government Corruption describe the extent to which these factors contribute in evaluating the happiness in each country. The Dystopia Residual metric actually is the Dystopia Happiness Score(1.85) + the Residual value or the unexplained value for each country as stated in the previous answer.\\nIf you add all these factors up, you get the happiness score so it might be un-reliable to model them to predict Happiness Scores.\\nStart a new kernel',\n",
       " 'Context\\nFashion-MNIST is a dataset of Zalando\\'s article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn\\'t work on MNIST, it won\\'t work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\\nZalando seeks to replace the original MNIST dataset\\nContent\\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\\nTo locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\\n\\nLabels\\nEach training and test example is assigned to one of the following labels:\\n0 T-shirt/top\\n1 Trouser\\n2 Pullover\\n3 Dress\\n4 Coat\\n5 Sandal\\n6 Shirt\\n7 Sneaker\\n8 Bag\\n9 Ankle boot\\n\\nTL;DR\\nEach row is a separate image\\nColumn 1 is the class label.\\nRemaining columns are pixel numbers (784 total).\\nEach value is the darkness of the pixel (1 to 255)\\nAcknowledgements\\nOriginal dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist\\nDataset was converted to CSV with this script: https://pjreddie.com/projects/mnist-in-csv/\\nLicense\\nThe MIT License (MIT) Copyright © [2017] Zalando SE, https://tech.zalando.com\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.',\n",
       " 'This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.\\nExploration Ideas\\nWhat candidates within the Republican party have results that are the most anti-correlated?\\nWhich Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?\\nWhat insights can you discover by mapping this data?\\nDo you have answers or other exploration ideas? Add your ideas to this forum post and share your insights through Kaggle Scripts!\\nDo you think that we should augment this dataset with more data sources? Submit a pull request to this repo, or let us know here!\\nData Description\\nThe 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the \"Download Data\" link at the top of the page, or interact with the data in Kaggle Scripts through the ../input directory.\\nOriginal Data Sources\\nPrimary Results from CNN\\nNew Hampshire County-Level Results\\nCounty Shapefiles\\nCounty QuickFacts',\n",
       " 'Overview\\nGame of Thrones is a hit fantasy tv show based on the equally famous book series \"A Song of Fire and Ice\" by George RR Martin. The show is well known for its vastly complicated political landscape, large number of characters, and its frequent character deaths.\\nData Sources\\nThis dataset combines three sources of data, all of which are based on information from the book series.\\nFirstly, there is battles.csv which contains Chris Albon\\'s \"The War of the Five Kings\" Dataset, which can be found here: https://github.com/chrisalbon/war_of_the_five_kings_dataset . Its a great collection of all of the battles in the series.\\nSecondly we have character-deaths.csv from Erin Pierce and Ben Kahle. This dataset was created as a part of their Bayesian Survival Analysis which can be found here: http://allendowney.blogspot.com/2015/03/bayesian-survival-analysis-for-game-of.html\\nFinally we have a more comprehensive character dataset with character-predictions.csv. This comes from the team at A Song of Ice and Data who scraped it from http://awoiaf.westeros.org/ . It also includes their predictions on which character will die, the methodology of which can be found here: https://got.show/machine-learning-algorithm-predicts-death-game-of-thrones\\nWhat insights about the complicated political landscape of this fantasy world can you find in this data?\\nOf course, it goes without saying that this dataset contains spoilers ;)',\n",
       " 'What influences love at first sight? (Or, at least, love in the first four minutes?) This dataset was compiled by Columbia Business School professors Ray Fisman and Sheena Iyengar for their paper Gender Differences in Mate Selection: Evidence From a Speed Dating Experiment.\\nData was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four minute \"first date\" with every other participant of the opposite sex. At the end of their four minutes, participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes: Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests.\\nThe dataset also includes questionnaire data gathered from participants at different points in the process. These fields include: demographics, dating habits, self-perception across key attributes, beliefs on what others find valuable in a mate, and lifestyle information. See the Speed Dating Data Key document below for details.\\nFor more analysis from Iyengar and Fisman, read Racial Preferences in Dating.\\nData Exploration Ideas\\nWhat are the least desirable attributes in a male partner? Does this differ for female partners?\\nHow important do people think attractiveness is in potential mate selection vs. its real impact?\\nAre shared interests more important than a shared racial background?\\nCan people accurately predict their own perceived value in the dating market?\\nIn terms of getting a second date, is it better to be someone\\'s first speed date of the night or their last?',\n",
       " \"Context\\nMost publicly available football (soccer) statistics are limited to aggregated data such as Goals, Shots, Fouls, Cards. When assessing performance or building predictive models, this simple aggregation, without any context, can be misleading. For example, a team that produced 10 shots on target from long range has a lower chance of scoring than a club that produced the same amount of shots from inside the box. However, metrics derived from this simple count of shots will similarly asses the two teams.\\nA football game generates much more events and it is very important and interesting to take into account the context in which those events were generated. This dataset should keep sports analytics enthusiasts awake for long hours as the number of questions that can be asked is huge.\\nContent\\nThis dataset is a result of a very tiresome effort of webscraping and integrating different data sources. The central element is the text commentary. All the events were derived by reverse engineering the text commentary, using regex. Using this, I was able to derive 11 types of events, as well as the main player and secondary player involved in those events and many other statistics. In case I've missed extracting some useful information, you are gladly invited to do so and share your findings. The dataset provides a granular view of 9,074 games, totaling 941,009 events from the biggest 5 European football (soccer) leagues: England, Spain, Germany, Italy, France from 2011/2012 season to 2016/2017 season as of 25.01.2017. There are games that have been played during these seasons for which I could not collect detailed data. Overall, over 90% of the played games during these seasons have event data.\\nThe dataset is organized in 3 files:\\nevents.csv contains event data about each game. Text commentary was scraped from: bbc.com, espn.com and onefootball.com\\nginf.csv - contains metadata and market odds about each game. odds were collected from oddsportal.com\\ndictionary.txt contains a dictionary with the textual description of each categorical variable coded with integers\\nPast Research\\nI have used this data to:\\ncreate predictive models for football games in order to bet on football outcomes.\\nmake visualizations about upcoming games\\nbuild expected goals models and compare players\\nInspiration\\nThere are tons of interesting questions a sports enthusiast can answer with this dataset. For example:\\nWhat is the value of a shot? Or what is the probability of a shot being a goal given it's location, shooter, league, assist method, gamestate, number of players on the pitch, time - known as expected goals (xG) models\\nWhen are teams more likely to score?\\nWhich teams are the best or sloppiest at holding the lead?\\nWhich teams or players make the best use of set pieces?\\nIn which leagues is the referee more likely to give a card?\\nHow do players compare when they shoot with their week foot versus strong foot? Or which players are ambidextrous?\\nIdentify different styles of plays (shooting from long range vs shooting from the box, crossing the ball vs passing the ball, use of headers)\\nWhich teams have a bias for attacking on a particular flank?\\nAnd many many more...\",\n",
       " \"Context\\nZillow's Economic Research Team collects, cleans and publishes housing and economic data from a variety of public and proprietary sources. Public property record data filed with local municipalities -- including deeds, property facts, parcel information and transactional histories -- forms the backbone of our data products, and is fleshed out with proprietary data derived from property listings and user behavior on Zillow.\\nThe large majority of Zillow's aggregated housing market and economic data is made available for free download at zillow.com/data.\\nContent\\nVariable Availability:\\nZillow Home Value Index (ZHVI): A smoothed seasonally adjusted measure of the median estimated home value across a given region and housing type. A dollar denominated alternative to repeat-sales indices. Find a more detailed methodology here: http://www.zillow.com/research/zhvi-methodology-6032/\\nZillow Rent Index (ZRI): A smoothed seasonally adjusted measure of the median estimated market rate rent across a given region and housing type. A dollar denominated alternative to repeat-rent indices. Find a more detailed methodology here: http://www.zillow.com/research/zillow-rent-index-methodology-2393/\\nFor-Sale Listing/Inventory Metrics: Zillow provides many variables capturing current and historical for-sale listings availability, generally from 2012 to current. These variables include median list prices and inventory counts, both by various property types. Variables capturing for-sale market competitiveness including share of listings with a price cut, median price cut size, age of inventory, and the days a listing spend on Zillow before the sale is final.\\nHome Sales Metrics: Zillow provides data on sold homes including median sale price by various housing types, sale counts (methodology here: http://www.zillow.com/research/home-sales-methodology-7733/), and a normalized view of sale volume referred to as turnover. The prevalence of foreclosures is also provided as ratio of the housing stock and the share of all sales in which the home was previously foreclosed upon.\\nFor-Rent Listing Metrics: Zillow provides median rents prices and median rent price per square foot by property type and bedroom count.\\nHousing type definitions:\\nAll Homes: Zillow defines all homes as single-family, condominium and co-operative homes with a county record. Unless specified, all series cover this segment of the housing stock.\\nCondo/Co-op: Condominium and co-operative homes.\\nMultifamily 5+ units: Units in buildings with 5 or more housing units, that are not a condominiums or co-ops.\\nDuplex/Triplex: Housing units in buildings with 2 or 3 housing units.\\nTiers: By metro, we determine price tier cutoffs that divide the all homes housing stock into thirds using the full distribution of estimated home values. We then estimate real estate metrics within the property sets, Bottom, Middle, and Top, defined by these cutoffs. When reported at the national level, all Bottom Tier homes defined at the metro level are pooled together to form the national bottom tier. The same holds for Middle and Top Tier homes.\\nRegional Availability:\\nZillow metrics are reported for common US geographies including Nation, State, Metro (2013 Census Defined CBSAs), County, City, ZIP code, and Neighborhood.\\nWe provide a crosswalk between colloquial Zillow region names and federally defined region names and linking variables such as County FIPS codes and CBSA codes. Cities and Neighborhoods do not match standard jurisdictional boundaries. Zillow city boundaries reflect mailing address conventions and so are often visually similar to collections of ZIP codes. Zillow neighborhood boundaries can be found here.\\nSuppression Rules: To ensure reliability of reported values the Zillow Economic Research team applies suppression rules triggered by low sample sizes and excessive volatility. These rules are customized to the metric and region type and explain most missingness found in the provided datasets.\\nAdditional Data Products\\nThe following data products and more are available for free download exclusively at Zillow.com/Data:\\nZillow Home Value Forecast\\nZillow Rent Forecast\\nNegative Equity (the share of mortgaged properties worth less than mortgage balance)\\nZillow Home Price Expectations Survey\\nZillow Housing Aspirations Report\\nZillow Rising Sea Levels Research\\nCash Buyers Time Series\\nBuy vs. Rent Breakeven Horizon\\nMortgage Affordability, Rental Affordability, Price-to-Income Ratio\\nConventional 30-year Fixed Mortgage Rate, Weekly Time Series\\nJumbo 30-year Fixed Mortgage Rates, Weekly Time Series\\nAcknowledgements\\nThe mission of the Zillow Economic Research Team is to be the most open, authoritative source for timely and accurate housing data and unbiased insight. We aim to empower consumers, industry professionals, policy makers and researchers looking to better understand the housing market.\\nTo see more of our mission in action, we invite you to learn more about us and to check out our collection of research briefs, stories, data tools and past presentations at https://www.zillow.com/research/\\nInspiration\\nZillow, and the Zillow Economic Research Team, firmly believe that not only do data want to be free, data are going to be free. Instead of simply publishing raw data, we believe in the power of pushing data up the ladder from raw data bits, to actionable information and finally to unique insight. We aim to answer questions of all kinds, even questions our users may not have known they had before coming to us. When done right, we firmly believe this process of turning data into insight can be transformational in people's lives.\\nPlease join us on this journey, and we're excited to see what insights you can discover hidden amongst our data!\",\n",
       " \"This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\\nIt's a great dataset for evaluating simple regression models.\",\n",
       " \"Context\\nThese files contain metadata for all 45,000 movies listed in the Full MovieLens Dataset. The dataset consists of movies released on or before July 2017. Data points include cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts and vote averages.\\nThis dataset also has files containing 26 million ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website.\\nContent\\nThis dataset consists of the following files:\\nmovies_metadata.csv: The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies.\\nkeywords.csv: Contains the movie plot keywords for our MovieLens movies. Available in the form of a stringified JSON Object.\\ncredits.csv: Consists of Cast and Crew Information for all our movies. Available in the form of a stringified JSON Object.\\nlinks.csv: The file that contains the TMDB and IMDB IDs of all the movies featured in the Full MovieLens dataset.\\nlinks_small.csv: Contains the TMDB and IMDB IDs of a small subset of 9,000 movies of the Full Dataset.\\nratings_small.csv: The subset of 100,000 ratings from 700 users on 9,000 movies.\\nThe Full MovieLens Dataset consisting of 26 million ratings and 750,000 tag applications from 270,000 users on all the 45,000 movies in this dataset can be accessed here\\nAcknowledgements\\nThis dataset is an ensemble of data collected from TMDB and GroupLens. The Movie Details, Credits and Keywords have been collected from the TMDB Open API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here.\\nThe Movie Links and Ratings have been obtained from the Official GroupLens website. The files are a part of the dataset available here\\nInspiration\\nThis dataset was assembled as part of my second Capstone Project for Springboard's Data Science Career Track. I wanted to perform an extensive EDA on Movie Data to narrate the history and the story of Cinema and use this metadata in combination with MovieLens ratings to build various types of Recommender Systems.\\nBoth my notebooks are available as kernels with this dataset: The Story of Film and Movie Recommender Systems\\nSome of the things you can do with this dataset: Predicting movie revenue and/or movie success based on a certain metric. What movies tend to get higher vote counts and vote averages on TMDB? Building Content Based and Collaborative Filtering Based Recommendation Engines.\",\n",
       " \"Context\\nThis dataset is a playground for fundamental and technical analysis. It is said that 30% of traffic on stocks is already generated by machines, can trading be fully automated? If not, there is still a lot to learn from historical data.\\nContent\\nDataset consists of following files:\\nprices.csv: raw, as-is daily prices. Most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time, this set doesn't account for that.\\nprices-split-adjusted.csv: same as prices, but there have been added adjustments for splits.\\nsecurities.csv: general description of each company with division on sectors\\nfundamentals.csv: metrics extracted from annual SEC 10K fillings (2012-2016), should be enough to derive most of popular fundamental indicators.\\nAcknowledgements\\nPrices were fetched from Yahoo Finance, fundamentals are from Nasdaq Financials, extended by some fields from EDGAR SEC databases.\\nMining\\nHere is couple of things one could try out with this data:\\nTechnical\\nOne day ahead prediction: Rolling Linear Regression, ARIMA, Neural Networks, LSTM\\nMomentum/Mean-Reversion Strategies\\nSecurity clustering, portfolio construction/hedging\\nFundamental\\nWhich company has biggest chance of being bankrupt? Which one is undervalued (how prices behaved afterwards), what is Return on Investment?\",\n",
       " 'Context\\nIn the last few days, I have been hearing a lot of buzz around cryptocurrencies. Things like Block chain, Bitcoin, Bitcoin cash, Ethereum, Ripple etc are constantly coming in the news articles I read. So I wanted to understand more about it and this post helped me get started. Once the basics are done, the DS guy sleeping inside me (always lazy.!) woke up and started raising questions like\\nHow many such cryptocurrencies are there and what are their prices and valuations?\\nWhy is there a sudden surge in the interest in recent days? Is it due to the increase in the price in the last few days? etc.\\nFor getting answers to all these questions (and if possible to predict the future prices ;)), I started getting the data from coinmarketcap about the cryptocurrencies.\\nUpdate : Bitcoin dataset\\nSo what next.? Now that we have the price data, I wanted to dig a little more about the factors affecting the price of coins. I started of with Bitcoin and there are quite a few parameters which affect the price of Bitcoin. Thanks to Blockchain Info, I was able to get quite a few parameters on once in two day basis.\\nThis will help understand the other factors related to Bitcoin price and also help one make future predictions in a better way than just using the historical price.\\nUpdate2: Ethereum Dataset:\\nThis dataset has features related to Ethereum. This is very similar to the bitcoin dataset and is available on a daily basis. Data is taken from Etherscan and the credits go to them for allowing us to use.\\nContent\\nThis dataset has the historical price information of some of the top cryptocurrencies by market capitalization. The currencies included are\\nBitcoin\\nEthereum\\nRipple\\nBitcoin cash\\nBitconnect\\nDash\\nEthereum Classic\\nIota\\nLitecoin\\nMonero\\nNem\\nNeo\\nNumeraire\\nStratis\\nWaves\\nIn case if you are interested in the prices of some other currencies, please post in comments section and I will try to add them in the next version. I am planning to revise it once in a week.\\nDataset has one csv file for each currency. Price history is available on a daily basis from April 28, 2013. The columns in the csv file are\\nDate : date of observation\\nOpen : Opening price on the given day\\nHigh : Highest price on the given day\\nLow : Lowest price on the given day\\nClose : Closing price on the given day\\nVolume : Volume of transactions on the given day\\nMarket Cap : Market capitalization in USD\\nBitcoin Dataset (bitcoin_dataset.csv) :\\nThis dataset has the following features.\\nDate : Date of observation\\nbtc_market_price : Average USD market price across major bitcoin exchanges.\\nbtc_total_bitcoins : The total number of bitcoins that have already been mined.\\nbtc_market_cap : The total USD value of bitcoin supply in circulation.\\nbtc_trade_volume : The total USD value of trading volume on major bitcoin exchanges.\\nbtc_blocks_size : The total size of all block headers and transactions.\\nbtc_avg_block_size : The average block size in MB.\\nbtc_n_orphaned_blocks : The total number of blocks mined but ultimately not attached to the main Bitcoin blockchain.\\nbtc_n_transactions_per_block : The average number of transactions per block.\\nbtc_median_confirmation_time : The median time for a transaction to be accepted into a mined block.\\nbtc_hash_rate : The estimated number of tera hashes per second the Bitcoin network is performing.\\nbtc_difficulty : A relative measure of how difficult it is to find a new block.\\nbtc_miners_revenue : Total value of coinbase block rewards and transaction fees paid to miners.\\nbtc_transaction_fees : The total value of all transaction fees paid to miners.\\nbtc_cost_per_transaction_percent : miners revenue as percentage of the transaction volume.\\nbtc_cost_per_transaction : miners revenue divided by the number of transactions.\\nbtc_n_unique_addresses : The total number of unique addresses used on the Bitcoin blockchain.\\nbtc_n_transactions : The number of daily confirmed Bitcoin transactions.\\nbtc_n_transactions_total : Total number of transactions.\\nbtc_n_transactions_excluding_popular : The total number of Bitcoin transactions, excluding the 100 most popular addresses.\\nbtc_n_transactions_excluding_chains_longer_than_100 : The total number of Bitcoin transactions per day excluding long transaction chains.\\nbtc_output_volume : The total value of all transaction outputs per day.\\nbtc_estimated_transaction_volume : The total estimated value of transactions on the Bitcoin blockchain.\\nbtc_estimated_transaction_volume_usd : The estimated transaction value in USD value.\\nEthereum Dataset (ethereum_dataset.csv):\\nThis dataset has the following features\\nDate(UTC) : Date of transaction\\nUnixTimeStamp : unix timestamp\\neth_etherprice : price of ethereum\\neth_tx : number of transactions per day\\neth_address : Cumulative address growth\\neth_supply : Number of ethers in supply\\neth_marketcap : Market cap in USD\\neth_hashrate : hash rate in GH/s\\neth_difficulty : Difficulty level in TH\\neth_blocks : number of blocks per day\\neth_uncles : number of uncles per day\\neth_blocksize : average block size in bytes\\neth_blocktime : average block time in seconds\\neth_gasprice : Average gas price in Wei\\neth_gaslimit : Gas limit per day\\neth_gasused : total gas used per day\\neth_ethersupply : new ether supply per day\\neth_chaindatasize : chain data size in bytes\\neth_ens_register : Ethereal Name Service (ENS) registrations per day\\nAcknowledgements\\nThis data is taken from coinmarketcap and it is free to use the data.\\nBitcoin dataset is obtained from Blockchain Info.\\nEthereum dataset is obtained from Etherscan.\\nCover Image : Photo by Thomas Malama on Unsplash\\nInspiration\\nSome of the questions which could be inferred from this dataset are:\\nHow did the historical prices / market capitalizations of various currencies change over time?\\nPredicting the future price of the currencies\\nWhich currencies are more volatile and which ones are more stable?\\nHow does the price fluctuations of currencies correlate with each other?\\nSeasonal trend in the price fluctuations\\nBitcoin / Ethereum dataset could be used to look at the following:\\nFactors affecting the bitcoin / ether price.\\nDirectional prediction of bitcoin / ether price. (refer this paper for more inspiration)\\nActual bitcoin price prediction.',\n",
       " \"Context\\nThese datasets contain information about all audio-video recordings of TED Talks uploaded to the official TED.com website until September 21st, 2017. The TED main dataset contains information about all talks including number of views, number of comments, descriptions, speakers and titles. The TED transcripts dataset contains the transcripts for all talks available on TED.com.\\nContent (for the CSV files)\\nTED Main Dataset\\nname: The official name of the TED Talk. Includes the title and the speaker.\\ntitle: The title of the talk\\ndescription: A blurb of what the talk is about.\\nmain_speaker: The first named speaker of the talk.\\nspeaker_occupation: The occupation of the main speaker.\\nnum_speaker: The number of speakers in the talk.\\nduration: The duration of the talk in seconds.\\nevent: The TED/TEDx event where the talk took place.\\nfilm_date: The Unix timestamp of the filming.\\npublished_date: The Unix timestamp for the publication of the talk on TED.com\\ncomments: The number of first level comments made on the talk.\\ntags: The themes associated with the talk.\\nlanguages: The number of languages in which the talk is available.\\nratings: A stringified dictionary of the various ratings given to the talk (inspiring, fascinating, jaw dropping, etc.)\\nrelated_talks: A list of dictionaries of recommended talks to watch next.\\nurl: The URL of the talk.\\nviews: The number of views on the talk.\\nTED Transcripts Dataset\\nurl: The URL of the talk\\ntranscript: The official English transcript of the talk.\\nAcknowledgements\\nThe data has been scraped from the official TED Website and is available under the Creative Commons License.\\nInspiration\\nI've always been fascinated by TED Talks and the immense diversity of content that it provides for free. I was also thoroughly inspired by a TED Talk that visually explored TED Talks stats and I was motivated to do the same thing, albeit on a much less grander scale.\\nSome of the questions that can be answered with this dataset: 1. How is each TED Talk related to every other TED Talk? 2. Which are the most viewed and most favorited Talks of all time? Are they mostly the same? What does this tell us? 3. What kind of topics attract the maximum discussion and debate (in the form of comments)? 4. Which months are most popular among TED and TEDx chapters? 5. Which themes are most popular amongst TEDsters?\",\n",
       " 'The Health Insurance Marketplace Public Use Files contain data on health and dental plans offered to individuals and small businesses through the US Health Insurance Marketplace.\\nExploration Ideas\\nTo help get you started, here are some data exploration ideas:\\nHow do plan rates and benefits vary across states?\\nHow do plan benefits relate to plan rates?\\nHow do plan rates vary by age?\\nHow do plans vary across insurance network providers?\\nSee this forum thread for more ideas, and post there if you want to add your own ideas or answer some of the open questions!\\nData Description\\nThis data was originally prepared and released by the Centers for Medicare & Medicaid Services (CMS). Please read the CMS Disclaimer-User Agreement before using this data.\\nHere, we\\'ve processed the data to facilitate analytics. This processed version has three components:\\n1. Original versions of the data\\nThe original versions of the 2014, 2015, 2016 data are available in the \"raw\" directory of the download and \"../input/raw\" on Kaggle Scripts. Search for \"dictionaries\" on this page to find the data dictionaries describing the individual raw files.\\n2. Combined CSV files that contain\\nIn the top level directory of the download (\"../input\" on Kaggle Scripts), there are six CSV files that contain the combined at across all years:\\nBenefitsCostSharing.csv\\nBusinessRules.csv\\nNetwork.csv\\nPlanAttributes.csv\\nRate.csv\\nServiceArea.csv\\nAdditionally, there are two CSV files that facilitate joining data across years:\\nCrosswalk2015.csv - joining 2014 and 2015 data\\nCrosswalk2016.csv - joining 2015 and 2016 data\\n3. SQLite database\\nThe \"database.sqlite\" file contains tables corresponding to each of the processed CSV files.\\nThe code to create the processed version of this data is available on GitHub.',\n",
       " 'Context\\nThe Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\\nIntroversion (I) – Extroversion (E)\\nIntuition (N) – Sensing (S)\\nThinking (T) – Feeling (F)\\nJudging (J) – Perceiving (P)\\n(More can be learned about what these mean here)\\nSo for example, someone who prefers introversion, intuition, thinking and perceiving would be labelled an INTP in the MBTI system, and there are lots of personality based components that would model or describe this person’s preferences or behaviour based on the label.\\nIt is one of, if not the, the most popular personality test in the world. It is used in businesses, online, for fun, for research and lots more. A simple google search reveals all of the different ways the test has been used over time. It’s safe to say that this test is still very relevant in the world in terms of its use.\\nFrom scientific or psychological perspective it is based on the work done on cognitive functions by Carl Jung i.e. Jungian Typology. This was a model of 8 distinct functions, thought processes or ways of thinking that were suggested to be present in the mind. Later this work was transformed into several different personality systems to make it more accessible, the most popular of which is of course the MBTI.\\nRecently, its use/validity has come into question because of unreliability in experiments surrounding it, among other reasons. But it is still clung to as being a very useful tool in a lot of areas, and the purpose of this dataset is to help see if any patterns can be detected in specific types and their style of writing, which overall explores the validity of the test in analysing, predicting or categorising behaviour.\\nContent\\nThis dataset contains over 8600 rows of data, on each row is a person’s:\\nType (This persons 4 letter MBTI code/type)\\nA section of each of the last 50 things they have posted (Each entry separated by \"|||\" (3 pipe characters))\\nAcknowledgements\\nThis data was collected through the PersonalityCafe forum, as it provides a large selection of people and their MBTI personality type, as well as what they have written.\\nInspiration\\nSome basic uses could include:\\nUse machine learning to evaluate the MBTIs validity and ability to predict language styles and behaviour online.\\nProduction of a machine learning algorithm that can attempt to determine a person’s personality type based on some text they have written.',\n",
       " \"Of all the universities in the world, which are the best?\\nRanking universities is a difficult, political, and controversial practice. There are hundreds of different national and international university ranking systems, many of which disagree with each other. This dataset contains three global university rankings from very different places.\\nUniversity Ranking Data\\nThe Times Higher Education World University Ranking is widely regarded as one of the most influential and widely observed university measures. Founded in the United Kingdom in 2010, it has been criticized for its commercialization and for undermining non-English-instructing institutions.\\nThe Academic Ranking of World Universities, also known as the Shanghai Ranking, is an equally influential ranking. It was founded in China in 2003 and has been criticized for focusing on raw research power and for undermining humanities and quality of instruction.\\nThe Center for World University Rankings, is a less well know listing that comes from Saudi Arabia, it was founded in 2012.\\nHow do these rankings compare to each other?\\nAre the various criticisms levied against these rankings fair or not?\\nHow does your alma mater fare against the world?\\nSupplementary Data\\nTo further extend your analyses, we've also included two sets of supplementary data.\\nThe first of these is a set of data on educational attainment around the world. It comes from The World Data Bank and comprises information from the UNESCO Institute for Statistics and the Barro-Lee Dataset. How does national educational attainment relate to the quality of each nation's universities?\\nThe second supplementary dataset contains information about public and private direct expenditure on education across nations. This data comes from the National Center for Education Statistics. It represents expenditure as a percentage of gross domestic product. Does spending more on education lead to better international university rankings?\",\n",
       " 'Dataset Information\\nThis dataset is from a 2014 survey that measures attitudes towards mental health and frequency of mental health disorders in the tech workplace. You are also encouraged to analyze data from the ongoing 2016 survey found here.\\nContent\\nThis dataset contains the following data:\\nTimestamp\\nAge\\nGender\\nCountry\\nstate: If you live in the United States, which state or territory do you live in?\\nself_employed: Are you self-employed?\\nfamily_history: Do you have a family history of mental illness?\\ntreatment: Have you sought treatment for a mental health condition?\\nwork_interfere: If you have a mental health condition, do you feel that it interferes with your work?\\nno_employees: How many employees does your company or organization have?\\nremote_work: Do you work remotely (outside of an office) at least 50% of the time?\\ntech_company: Is your employer primarily a tech company/organization?\\nbenefits: Does your employer provide mental health benefits?\\ncare_options: Do you know the options for mental health care your employer provides?\\nwellness_program: Has your employer ever discussed mental health as part of an employee wellness program?\\nseek_help: Does your employer provide resources to learn more about mental health issues and how to seek help?\\nanonymity: Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources?\\nleave: How easy is it for you to take medical leave for a mental health condition?\\nmental_health_consequence: Do you think that discussing a mental health issue with your employer would have negative consequences?\\nphys_health_consequence: Do you think that discussing a physical health issue with your employer would have negative consequences?\\ncoworkers: Would you be willing to discuss a mental health issue with your coworkers?\\nsupervisor: Would you be willing to discuss a mental health issue with your direct supervisor(s)?\\nmental_health_interview: Would you bring up a mental health issue with a potential employer in an interview?\\nphys_health_interview: Would you bring up a physical health issue with a potential employer in an interview?\\nmental_vs_physical: Do you feel that your employer takes mental health as seriously as physical health?\\nobs_consequence: Have you heard of or observed negative consequences for coworkers with mental health conditions in your workplace?\\ncomments: Any additional notes or comments\\nInspiration\\nSome questions worth exploring:\\nHow does the frequency of mental health illness and attitudes towards mental health vary by geographic location?\\nWhat are the strongest predictors of mental health illness or certain attitudes towards mental health in the workplace?\\nAcknowledgements\\nThe original dataset is from Open Sourcing Mental Illness and can be downloaded here.',\n",
       " \"Context\\nHigh-quality financial data is expensive to acquire and is therefore rarely shared for free. Here I provide the full historical daily price and volume data for all U.S.-based stocks and ETFs trading on the NYSE, NASDAQ, and NYSE MKT. It's one of the best datasets of its kind you can obtain.\\nContent\\nThe data (last updated 11/10/2017) is presented in CSV format as follows: Date, Open, High, Low, Close, Volume, OpenInt. Note that prices have been adjusted for dividends and splits.\\nAcknowledgements\\nThis dataset belongs to me. I’m sharing it here for free. You may do with it as you wish.\\nInspiration\\nMany have tried, but most have failed, to predict the stock market's ups and downs. Can you do any better?\",\n",
       " 'Context\\nA person makes a doctor appointment, receives all the instructions and no-show. Who to blame? If this is help, don´t forget to upvote :) Greatings!\\nContent\\n300k medical appointments and its 15 variables (characteristics) of each. The most important one if the patient show-up or no-show the appointment. Variable names are self-explanatory, if you have doubts, just let me know!\\nscholarship variable means this concept = https://en.wikipedia.org/wiki/Bolsa_Fam%C3%ADlia\\nData Dictionary\\nPatientId - Identification of a patient AppointmentID - Identification of each appointment Gender = Male or Female . Female is the greater proportion, woman takes way more care of they health in comparison to man. DataMarcacaoConsulta = The day of the actuall appointment, when they have to visit the doctor. DataAgendamento = The day someone called or registered the appointment, this is before appointment of course. Age = How old is the patient. Neighbourhood = Where the appointment takes place. Scholarship = Ture of False . Observation, this is a broad topic, consider reading this article https://en.wikipedia.org/wiki/Bolsa_Fam%C3%ADlia Hipertension = True or False Diabetes = True or False Alcoholism = True or False Handcap = True or False SMS_received = 1 or more messages sent to the patient. No-show = True or False.\\nInspiration\\nWhat if that possible to predict someone to no-show an appointment?',\n",
       " 'This data originally came from Crowdflower\\'s Data for Everyone library.\\nAs the original source says,\\nA sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\\nThe data we\\'re providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is available on GitHub\\nFor example, it contains whether the sentiment of the tweets in this set was positive, neutral, or negative for six US airlines:',\n",
       " \"Voice Gender\\nGender Recognition by Voice and Speech Analysis\\nThis database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz (human vocal range).\\nThe Dataset\\nThe following acoustic properties of each voice are measured and included within the CSV:\\nmeanfreq: mean frequency (in kHz)\\nsd: standard deviation of frequency\\nmedian: median frequency (in kHz)\\nQ25: first quantile (in kHz)\\nQ75: third quantile (in kHz)\\nIQR: interquantile range (in kHz)\\nskew: skewness (see note in specprop description)\\nkurt: kurtosis (see note in specprop description)\\nsp.ent: spectral entropy\\nsfm: spectral flatness\\nmode: mode frequency\\ncentroid: frequency centroid (see specprop)\\npeakf: peak frequency (frequency with highest energy)\\nmeanfun: average of fundamental frequency measured across acoustic signal\\nminfun: minimum fundamental frequency measured across acoustic signal\\nmaxfun: maximum fundamental frequency measured across acoustic signal\\nmeandom: average of dominant frequency measured across acoustic signal\\nmindom: minimum of dominant frequency measured across acoustic signal\\nmaxdom: maximum of dominant frequency measured across acoustic signal\\ndfrange: range of dominant frequency measured across acoustic signal\\nmodindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\\nlabel: male or female\\nAccuracy\\nBaseline (always predict male)\\n50% / 50%\\nLogistic Regression\\n97% / 98%\\nCART\\n96% / 97%\\nRandom Forest\\n100% / 98%\\nSVM\\n100% / 99%\\nXGBoost\\n100% / 99%\\nResearch Questions\\nAn original analysis of the data-set can be found in the following article:\\nIdentifying the Gender of a Voice using Machine Learning\\nThe best model achieves 99% accuracy on the test set. According to a CART model, it appears that looking at the mean fundamental frequency might be enough to accurately classify a voice. However, some male voices use a higher frequency, even though their resonance differs from female voices, and may be incorrectly classified as female. To the human ear, there is apparently more than simple frequency, that determines a voice's gender.\\nQuestions\\nWhat other features differ between male and female voices?\\nCan we find a difference in resonance between male and female voices?\\nCan we identify falsetto from regular voices? (separate data-set likely needed for this)\\nAre there other interesting features in the data?\\nCART Diagram\\nMean fundamental frequency appears to be an indicator of voice gender, with a threshold of 140hz separating male from female classifications.\\nReferences\\nThe Harvard-Haskins Database of Regularly-Timed Speech\\nTelecommunications & Signal Processing Laboratory (TSP) Speech Database at McGill University, Home\\nVoxForge Speech Corpus, Home\\nFestvox CMU_ARCTIC Speech Database at Carnegie Mellon University\",\n",
       " 'Cryptocurrency Market Data\\nHistorical Cryptocurrency Prices For ALL Tokens!\\nSummary\\n> Observations: 649,051\\n> Variables: 13  \\n> Crypto Tokens: 1,382  \\n> Start Date: 28/04/2017  \\n> End Date: 03/01/2018  \\nDescription\\nAll historic open, high, low, close, trading volume and market cap info for all cryptocurrencies.\\nI\\'ve had to go over the code with a fine tooth comb to get it compatible with CRAN so there have been significant enhancements to how some of the field conversions have been undertaken and the data being cleaned. This should eliminate a few issues around number formatting or unexpected handling of scientific notations.\\nData Structure\\nObservations: 649,051    \\nVariables: 13    \\n$ slug        <chr> \"bitcoin\", \"bitcoin\", \"bitcoin\", \"bitcoin\"...        \\n$ symbol      <chr> \"BTC\", \"BTC\", \"BTC\", \"BTC\", \"BTC\", \"BTC\", ...    \\n$ name        <chr> \"Bitcoin\", \"Bitcoin\", \"Bitcoin\", \"Bitcoin\"...    \\n$ date        <date> 2013-04-28, 2013-04-29, 2013-04-30, 2013-...    \\n$ ranknow     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...    \\n$ open        <dbl> 135.30, 134.44, 144.00, 139.00, 116.38, 10...    \\n$ high        <dbl> 135.98, 147.49, 146.93, 139.89, 125.60, 10...    \\n$ low         <dbl> 132.10, 134.00, 134.05, 107.72, 92.28, 79...    \\n$ close       <dbl> 134.21, 144.54, 139.00, 116.99, 105.21, 97...    \\n$ volume      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    \\n$ market      <dbl> 1500520000, 1491160000, 1597780000, 154282...    \\n$ close_ratio <dbl> 0.5438, 0.7813, 0.3843, 0.2882, 0.3881, 0...    \\n$ spread      <dbl> 3.88, 13.49, 12.88, 32.17, 33.32, 29.03, 2...    \\nBuilt With :heart: R\\nI\\'ve ported my original kernel to generate this data, across into a R package which is awaiting being published on CRAN. Run the below to go scrape all the historical tables of all the different cryptocurrencies listed on CoinMarketCap and turn it into a data frame.\\nYou can install it via the github link below, or:\\ndevtools::install_github(\"jessevent/crypto\")  \\nlibrary(crypto)  \\nwill_i_get_rich <- getCoins()  \\nAuthors\\nJesse Vent - Package Author - jessevent\\nAcknowledgments\\nGithub - View my github repository for the full package.\\nCoinSpot - Invest $AUD into Crypto today!\\nCoinMarketCap - Providing amazing data @CoinMarketCap\\nIf this helps you become rich please consider making a donation!\\n    ERC-20: 0x375923Bf82F0b728d23A5704261a6e16341fd860\\n    XRP: rK59semLsuJZEWftxBFhWuNE6uhznjz2bK\\n    LTC: LWpiZMd2cEyqCdrZrs9TjsouTLWbFFxwCj',\n",
       " \"Context:\\nThe data were obtained in a survey of students math and portuguese language courses in secondary school. It contains a lot of interesting social, gender and study information about students. You can use it for some EDA or try to predict students final grade.\\nContent:\\nAttributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:\\nschool - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\\nsex - student's sex (binary: 'F' - female or 'M' - male)\\nage - student's age (numeric: from 15 to 22)\\naddress - student's home address type (binary: 'U' - urban or 'R' - rural)\\nfamsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\\nPstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\\nMedu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\\nFedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\\nMjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\\nFjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\\nreason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\\nguardian - student's guardian (nominal: 'mother', 'father' or 'other')\\ntraveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\\nstudytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\\nfailures - number of past class failures (numeric: n if 1<=n<3, else 4)\\nschoolsup - extra educational support (binary: yes or no)\\nfamsup - family educational support (binary: yes or no)\\npaid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\\nactivities - extra-curricular activities (binary: yes or no)\\nnursery - attended nursery school (binary: yes or no)\\nhigher - wants to take higher education (binary: yes or no)\\ninternet - Internet access at home (binary: yes or no)\\nromantic - with a romantic relationship (binary: yes or no)\\nfamrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\\nfreetime - free time after school (numeric: from 1 - very low to 5 - very high)\\ngoout - going out with friends (numeric: from 1 - very low to 5 - very high)\\nDalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\\nWalc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\\nhealth - current health status (numeric: from 1 - very bad to 5 - very good)\\nabsences - number of school absences (numeric: from 0 to 93)\\nThese grades are related with the course subject, Math or Portuguese:\\nG1 - first period grade (numeric: from 0 to 20)\\nG2 - second period grade (numeric: from 0 to 20)\\nG3 - final grade (numeric: from 0 to 20, output target)\\nAdditional note: there are several (382) students that belong to both datasets . These students can be identified by searching for identical attributes that characterize each student, as shown in the annexed R file.\\nSource Information\\nP. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\\nFabio Pagnotta, Hossain Mohammad Amran. Email:fabio.pagnotta@studenti.unicam.it, mohammadamra.hossain '@' studenti.unicam.it University Of Camerino\\nhttps://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION\",\n",
       " \"Context\\nH-1B visas are a category of employment-based, non-immigrant visas for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, a US employer must offer them a job and submit a petition for a H-1B visa to the US immigration department. This is also the most common visa status applied for and held by international students once they complete college or higher education and begin working in a full-time position.\\nThe following articles contain more information about the H-1B visa process:\\nWhat is H1B LCA ? Why file it ? Salary, Processing times – DOL\\nH1B Application Process: Step by Step Guide\\nContent\\nThis dataset contains five year's worth of H-1B petition data, with approximately 3 million records overall. The columns in the dataset include case status, employer name, worksite coordinates, job title, prevailing wage, occupation code, and year filed.\\nFor more information on individual columns, refer to the column metadata. A detailed description of the underlying raw dataset is available in an official data dictionary.\\nAcknowledgements\\nThe Office of Foreign Labor Certification (OFLC) generates program data, including data about H1-B visas. The disclosure data updated annually and is available online.\\nThe raw data available is messy and not immediately suitable analysis. A set of data transformations were performed making the data more accessible for quick exploration. To learn more, refer to this blog post and to the complimentary R Notebook.\\nInspiration\\nIs the number of petitions with Data Engineer job title increasing over time?\\nWhich part of the US has the most Hardware Engineer jobs?\\nWhich industry has the most number of Data Scientist positions?\\nWhich employers file the most petitions each year?\",\n",
       " \"Introduction\\nIn 2013, students of the Statistics class at FSEV UK were asked to invite their friends to participate in this survey.\\nThe data file (responses.csv) consists of 1010 rows and 150 columns (139 integer and 11 categorical).\\nFor convenience, the original variable names were shortened in the data file. See the columns.csv file if you want to match the data with the original names.\\nThe data contain missing values.\\nThe survey was presented to participants in both electronic and written form.\\nThe original questionnaire was in Slovak language and was later translated into English.\\nAll participants were of Slovakian nationality, aged between 15-30.\\nThe variables can be split into the following groups:\\nMusic preferences (19 items)\\nMovie preferences (12 items)\\nHobbies & interests (32 items)\\nPhobias (10 items)\\nHealth habits (3 items)\\nPersonality traits, views on life, & opinions (57 items)\\nSpending habits (7 items)\\nDemographics (10 items)\\nResearch questions\\nMany different techniques can be used to answer many questions, e.g.\\nClustering: Given the music preferences, do people make up any clusters of similar behavior?\\nHypothesis testing: Do women fear certain phenomena significantly more than men? Do the left handed people have different interests than right handed?\\nPredictive modeling: Can we predict spending habits of a person from his/her interests and movie or music preferences?\\nDimension reduction: Can we describe a large number of human interests by a smaller number of latent concepts?\\nCorrelation analysis: Are there any connections between music and movie preferences?\\nVisualization: How to effectively visualize a lot of variables in order to gain some meaningful insights from the data?\\n(Multivariate) Outlier detection: Small number of participants often cheats and randomly answers the questions. Can you identify them? Hint: Local outlier factor may help.\\nMissing values analysis: Are there any patterns in missing responses? What is the optimal way of imputing the values in surveys?\\nRecommendations: If some of user's interests are known, can we predict the other? Or, if we know what a person listen, can we predict which kind of movies he/she might like?\\nPast research\\n(in slovak) Sleziak, P. - Sabo, M.: Gender differences in the prevalence of specific phobias. Forum Statisticum Slovacum. 2014, Vol. 10, No. 6. [Differences (gender + whether people lived in village/town) in the prevalence of phobias.]\\nSabo, Miroslav. Multivariate Statistical Methods with Applications. Diss. Slovak University of Technology in Bratislava, 2014. [Clustering of variables (music preferences, movie preferences, phobias) + Clustering of people w.r.t. their interests.]\\nQuestionnaire\\nMUSIC PREFERENCES\\nI enjoy listening to music.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI prefer.: Slow paced music 1-2-3-4-5 Fast paced music (integer)\\nDance, Disco, Funk: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nFolk music: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nCountry: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nClassical: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nMusicals: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nPop: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nRock: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nMetal, Hard rock: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nPunk: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nHip hop, Rap: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nReggae, Ska: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nSwing, Jazz: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nRock n Roll: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nAlternative music: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nLatin: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nTechno, Trance: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nOpera: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nMOVIE PREFERENCES\\nI really enjoy watching movies.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nHorror movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nThriller movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nComedies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nRomantic movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nSci-fi movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nWar movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nTales: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nCartoons: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nDocumentaries: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nWestern movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nAction movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\\nHOBBIES & INTERESTS\\nHistory: Not interested 1-2-3-4-5 Very interested (integer)\\nPsychology: Not interested 1-2-3-4-5 Very interested (integer)\\nPolitics: Not interested 1-2-3-4-5 Very interested (integer)\\nMathematics: Not interested 1-2-3-4-5 Very interested (integer)\\nPhysics: Not interested 1-2-3-4-5 Very interested (integer)\\nInternet: Not interested 1-2-3-4-5 Very interested (integer)\\nPC Software, Hardware: Not interested 1-2-3-4-5 Very interested (integer)\\nEconomy, Management: Not interested 1-2-3-4-5 Very interested (integer)\\nBiology: Not interested 1-2-3-4-5 Very interested (integer)\\nChemistry: Not interested 1-2-3-4-5 Very interested (integer)\\nPoetry reading: Not interested 1-2-3-4-5 Very interested (integer)\\nGeography: Not interested 1-2-3-4-5 Very interested (integer)\\nForeign languages: Not interested 1-2-3-4-5 Very interested (integer)\\nMedicine: Not interested 1-2-3-4-5 Very interested (integer)\\nLaw: Not interested 1-2-3-4-5 Very interested (integer)\\nCars: Not interested 1-2-3-4-5 Very interested (integer)\\nArt: Not interested 1-2-3-4-5 Very interested (integer)\\nReligion: Not interested 1-2-3-4-5 Very interested (integer)\\nOutdoor activities: Not interested 1-2-3-4-5 Very interested (integer)\\nDancing: Not interested 1-2-3-4-5 Very interested (integer)\\nPlaying musical instruments: Not interested 1-2-3-4-5 Very interested (integer)\\nPoetry writing: Not interested 1-2-3-4-5 Very interested (integer)\\nSport and leisure activities: Not interested 1-2-3-4-5 Very interested (integer)\\nSport at competitive level: Not interested 1-2-3-4-5 Very interested (integer)\\nGardening: Not interested 1-2-3-4-5 Very interested (integer)\\nCelebrity lifestyle: Not interested 1-2-3-4-5 Very interested (integer)\\nShopping: Not interested 1-2-3-4-5 Very interested (integer)\\nScience and technology: Not interested 1-2-3-4-5 Very interested (integer)\\nTheatre: Not interested 1-2-3-4-5 Very interested (integer)\\nSocializing: Not interested 1-2-3-4-5 Very interested (integer)\\nAdrenaline sports: Not interested 1-2-3-4-5 Very interested (integer)\\nPets: Not interested 1-2-3-4-5 Very interested (integer)\\nPHOBIAS\\nFlying: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nThunder, lightning: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nDarkness: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nHeights: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nSpiders: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nSnakes: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nRats, mice: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nAgeing: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nDangerous dogs: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nPublic speaking: Not afraid at all 1-2-3-4-5 Very afraid of (integer)\\nHEALTH HABITS\\nSmoking habits: Never smoked - Tried smoking - Former smoker - Current smoker (categorical)\\nDrinking: Never - Social drinker - Drink a lot (categorical)\\nI live a very healthy lifestyle.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nPERSONALITY TRAITS, VIEWS ON LIFE & OPINIONS\\nI take notice of what goes on around me.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI try to do tasks as soon as possible and not leave them until last minute.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always make a list so I don't forget anything.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI often study or work even in my spare time.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI look at things from all different angles before I go ahead.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI believe that bad people will suffer one day and good people will be rewarded.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI am reliable at work and always complete all tasks given to me.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always keep my promises.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI can fall for someone very quickly and then completely lose interest.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI would rather have lots of friends than lots of money.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always try to be the funniest one.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI can be two faced sometimes.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI damaged things in the past when angry.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI take my time to make decisions.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always try to vote in elections.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI often think about and regret the decisions I make.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI can tell if people listen to me or not when I talk to them.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI am a hypochondriac.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI am emphatetic person.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI eat because I have to. I don't enjoy food and eat as fast as I can.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI try to give as much as I can to other people at Christmas.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI don't like seeing animals suffering.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI look after things I have borrowed from others.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI feel lonely in life.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI used to cheat at school.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI worry about my health.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI wish I could change the past because of the things I have done.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI believe in God.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always have good dreams.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always give to charity.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI have lots of friends.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nTimekeeping.: I am often early. - I am always on time. - I am often running late. (categorical)\\nDo you lie to others?: Never. - Only to avoid hurting someone. - Sometimes. - Everytime it suits me. (categorical)\\nI am very patient.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI can quickly adapt to a new environment.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nMy moods change quickly.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI am well mannered and I look after my appearance.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI enjoy meeting new people.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always let other people know about my achievements.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI think carefully before answering any important letters.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI enjoy childrens' company.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI am not afraid to give my opinion if I feel strongly about something.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI can get angry very easily.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always make sure I connect with the right people.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI have to be well prepared before public speaking.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI will find a fault in myself if people don't like me.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI cry when I feel down or things don't go the right way.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI am 100% happy with my life.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI am always full of life and energy.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI prefer big dangerous dogs to smaller, calmer dogs.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI believe all my personality traits are positive.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nIf I find something the doesn't belong to me I will hand it in.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI find it very difficult to get up in the morning.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI have many different hobbies and interests.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI always listen to my parents' advice.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI enjoy taking part in surveys.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nHow much time do you spend online?: No time at all - Less than an hour a day - Few hours a day - Most of the day (categorical)\\nSPENDING HABITS\\nI save all the money I can.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI enjoy going to large shopping centres.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI prefer branded clothing to non branded.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI spend a lot of money on partying and socializing.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI spend a lot of money on my appearance.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI spend a lot of money on gadgets.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nI will hapilly pay more money for good, quality or healthy food.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\\nDEMOGRAPHICS\\nAge: (integer)\\nHeight: (integer)\\nWeight: (integer)\\nHow many siblings do you have?: (integer)\\nGender: Female - Male (categorical)\\nI am: Left handed - Right handed (categorical)\\nHighest education achieved: Currently a Primary school pupil - Primary school - Secondary school - College/Bachelor degree (categorical)\\nI am the only child: No - Yes (categorical)\\nI spent most of my childhood in a: City - village (categorical)\\nI lived most of my childhood in a: house/bungalow - block of flats (categorical)\",\n",
       " 'Context\\nYouTube (the world-famous video sharing website) maintains a list of the top trending videos on the platform. According to Variety magazine, “To determine the year’s top-trending videos, YouTube uses a combination of factors including measuring users interactions (number of views, shares, comments and likes). Note that they’re not the most-viewed videos overall for the calendar year”. Top performers on the YouTube trending list are music videos (such as the famously virile “Gangam Style”), celebrity and/or reality TV performances, and the random dude-with-a-camera viral videos that YouTube is well-known for.\\nThis dataset is a daily record of the top trending YouTube videos.\\nNote that this dataset is a structurally improved version of this dataset.\\nContent\\nThis dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US, GB, DE, CA, and FR regions (USA, Great Britain, Germany, Canada, and France, respectively), with up to 200 listed trending videos per day.\\nEach region’s data is in a separate file. Data includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count.\\nThe data also includes a category_id field, which varies between regions. To retrieve the categories for a specific video, find it in the associated JSON. One such file is included for each of the five regions in the dataset.\\nFor more information on specific columns in the dataset refer to the column metadata.\\nAcknowledgements\\nThis dataset was collected using the YouTube API.\\nInspiration\\nPossible uses for this dataset could include:\\nSentiment analysis in a variety of forms\\nCategorising YouTube videos based on their comments and statistics.\\nTraining ML algorithms like RNNs to generate their own YouTube comments.\\nAnalysing what factors affect how popular a YouTube video will be.\\nStatistical analysis over time .\\nFor further inspiration, see the kernels on this dataset!',\n",
       " \"This dataset contains a list of video games with sales greater than 100,000 copies. It was generated by a scrape of vgchartz.com.\\nFields include\\nRank - Ranking of overall sales\\nName - The games name\\nPlatform - Platform of the games release (i.e. PC,PS4, etc.)\\nYear - Year of the game's release\\nGenre - Genre of the game\\nPublisher - Publisher of the game\\nNA_Sales - Sales in North America (in millions)\\nEU_Sales - Sales in Europe (in millions)\\nJP_Sales - Sales in Japan (in millions)\\nOther_Sales - Sales in the rest of the world (in millions)\\nGlobal_Sales - Total worldwide sales.\\nThe script to scrape the data is available at https://github.com/GregorUT/vgchartzScrape. It is based on BeautifulSoup using Python. There are 16,598 records. 2 records were dropped due to incomplete information.\",\n",
       " \"This dataset is the result of a crawl I did on http://ign.com/games/reviews .\\nIt contains 18625 lines with the fields like the release date, it's platform and IGN's score. All the lines are fully filled.\\nIn 20 years, the gaming industry has grown and sophisticated. By exploring this dataset, one is able to find trends about the industry, compare consoles against eachother, search through the most popular genres and more.\\nThe dataset can also be a great place for beginners to start using Python modules such as Pandas and Seaborn.\\nYou can find the crawl I used for the retrieval here\",\n",
       " \"Throughout 2015, Hillary Clinton has been embroiled in controversy over the use of personal email accounts on non-government servers during her time as the United States Secretary of State. Some political experts and opponents maintain that Clinton's use of personal email accounts to conduct Secretary of State affairs is in violation of protocols and federal laws that ensure appropriate recordkeeping of government activity. Hillary's campaign has provided their own four sentence summary of her email use here.\\nThere have been a number of Freedom of Information lawsuits filed over the State Department's failure to fully release the emails sent and received on Clinton's private accounts. On Monday, August 31, the State Department released nearly 7,000 pages of Clinton's heavily redacted emails (its biggest release of emails to date).\\nThe documents were released by the State Department as PDFs. We've cleaned and normalized the released documents and are hosting them for public analysis. Kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent.\\nHere's the code that creates this data release.\",\n",
       " \"Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.\\nEducation 1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'\\nEnvironmentSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\\nJobInvolvement\\n1 'Low' 2 'Medium' 3 'High' 4 'Very High'\\nJobSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\\nPerformanceRating\\n1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'\\nRelationshipSatisfaction\\n1 'Low' 2 'Medium' 3 'High' 4 'Very High'\\nWorkLifeBalance 1 'Bad' 2 'Good' 3 'Better' 4 'Best'\",\n",
       " 'Context\\nChocolate is one of the most popular candies in the world. Each year, residents of the United States collectively eat more than 2.8 billions pounds. However, not all chocolate bars are created equal! This dataset contains expert ratings of over 1,700 individual chocolate bars, along with information on their regional origin, percentage of cocoa, the variety of chocolate bean used and where the beans were grown.\\nFlavors of Cacao Rating System:\\n5= Elite (Transcending beyond the ordinary limits)\\n4= Premium (Superior flavor development, character and style)\\n3= Satisfactory(3.0) to praiseworthy(3.75) (well made with special qualities)\\n2= Disappointing (Passable but contains at least one significant flaw)\\n1= Unpleasant (mostly unpalatable)\\nEach chocolate is evaluated from a combination of both objective qualities and subjective interpretation. A rating here only represents an experience with one bar from one batch. Batch numbers, vintages and review dates are included in the database when known.\\nThe database is narrowly focused on plain dark chocolate with an aim of appreciating the flavors of the cacao when made into chocolate. The ratings do not reflect health benefits, social missions, or organic status.\\nFlavor is the most important component of the Flavors of Cacao ratings. Diversity, balance, intensity and purity of flavors are all considered. It is possible for a straight forward single note chocolate to rate as high as a complex flavor profile that changes throughout. Genetics, terroir, post harvest techniques, processing and storage can all be discussed when considering the flavor component.\\nTexture has a great impact on the overall experience and it is also possible for texture related issues to impact flavor. It is a good way to evaluate the makers vision, attention to detail and level of proficiency.\\nAftermelt is the experience after the chocolate has melted. Higher quality chocolate will linger and be long lasting and enjoyable. Since the aftermelt is the last impression you get from the chocolate, it receives equal importance in the overall rating.\\nOverall Opinion is really where the ratings reflect a subjective opinion. Ideally it is my evaluation of whether or not the components above worked together and an opinion on the flavor development, character and style. It is also here where each chocolate can usually be summarized by the most prominent impressions that you would remember about each chocolate.\\nAcknowledgements\\nThese ratings were compiled by Brady Brelinski, Founding Member of the Manhattan Chocolate Society. For up-to-date information, as well as additional content (including interviews with craft chocolate makers), please see his website: Flavors of Cacao\\nInspiration\\nWhere are the best cocoa beans grown?\\nWhich countries produce the highest-rated bars?\\nWhat’s the relationship between cocoa solids percentage and rating?',\n",
       " \"Uber TLC FOIL Response\\nThis directory contains data on over 4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015. Trip-level data on 10 other for-hire vehicle (FHV) companies, as well as aggregated data for 329 FHV companies, is also included. All the files are as they were received on August 3, Sept. 15 and Sept. 22, 2015.\\nFiveThirtyEight obtained the data from the NYC Taxi & Limousine Commission (TLC) by submitting a Freedom of Information Law request on July 20, 2015. The TLC has sent us the data in batches as it continues to review trip data Uber and other HFV companies have submitted to it. The TLC's correspondence with FiveThirtyEight is included in the files TLC_letter.pdf, TLC_letter2.pdf and TLC_letter3.pdf. TLC records requests can be made here.\\nThis data was used for four FiveThirtyEight stories: Uber Is Serving New York’s Outer Boroughs More Than Taxis Are, Public Transit Should Be Uber’s New Best Friend, Uber Is Taking Millions Of Manhattan Rides Away From Taxis, and Is Uber Making NYC Rush-Hour Traffic Worse?.\\nThe Data\\nThe dataset contains, roughly, four groups of files:\\nUber trip data from 2014 (April - September), separated by month, with detailed location information\\nUber trip data from 2015 (January - June), with less fine-grained location information\\nnon-Uber FHV (For-Hire Vehicle) trips. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.\\naggregate ride and vehicle statistics for all FHV companies (and, occasionally, for taxi companies)\\nUber trip data from 2014\\nThere are six files of raw data on Uber pickups in New York City from April to September 2014. The files are separated by month and each has the following columns:\\nDate/Time : The date and time of the Uber pickup\\nLat : The latitude of the Uber pickup\\nLon : The longitude of the Uber pickup\\nBase : The TLC base company code affiliated with the Uber pickup\\nThese files are named:\\nuber-raw-data-apr14.csv\\nuber-raw-data-aug14.csv\\nuber-raw-data-jul14.csv\\nuber-raw-data-jun14.csv\\nuber-raw-data-may14.csv\\nuber-raw-data-sep14.csv\\nUber trip data from 2015\\nAlso included is the file uber-raw-data-janjune-15.csv This file has the following columns:\\nDispatching_base_num : The TLC base company code of the base that dispatched the Uber\\nPickup_date : The date and time of the Uber pickup\\nAffiliated_base_num : The TLC base company code affiliated with the Uber pickup\\nlocationID : The pickup location ID affiliated with the Uber pickup\\nThe Base codes are for the following Uber bases:\\nB02512 : Unter B02598 : Hinter B02617 : Weiter B02682 : Schmecken B02764 : Danach-NY B02765 : Grun B02835 : Dreist B02836 : Drinnen\\nFor coarse-grained location information from these pickups, the file taxi-zone-lookup.csv shows the taxi Zone (essentially, neighborhood) and Borough for each locationID.\\nNon-Uber FLV trips\\nThe dataset also contains 10 files of raw data on pickups from 10 for-hire vehicle (FHV) companies. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.\\nThese files are named:\\nAmerican_B01362.csv\\nDiplo_B01196.csv\\nHighclass_B01717.csv\\nSkyline_B00111.csv\\nCarmel_B00256.csv\\nFederal_02216.csv\\nLyft_B02510.csv\\nDial7_B00887.csv\\nFirstclass_B01536.csv\\nPrestige_B01338.csv\\nAggregate Statistics\\nThere is also a file other-FHV-data-jan-aug-2015.csv containing daily pickup data for 329 FHV companies from January 2015 through August 2015.\\nThe file Uber-Jan-Feb-FOIL.csv contains aggregated daily Uber trip statistics in January and February 2015.\",\n",
       " 'The latest hot topic in the news is fake news and many are wondering what data scientists can do to detect it and stymie its viral spread. This dataset is only a first step in understanding and tackling this problem. It contains text and metadata scraped from 244 websites tagged as \"bullshit\" by the BS Detector Chrome Extension by Daniel Sieradski.\\nWarning: I did not modify the list of news sources from the BS Detector so as not to introduce my (useless) layer of bias; I\\'m not an authority on fake news. There may be sources whose inclusion you disagree with. It\\'s up to you to decide how to work with the data and how you might contribute to \"improving it\". The labels of \"bs\" and \"junksci\", etc. do not constitute capital \"t\" Truth. If there are other sources you would like to include, start a discussion. If there are sources you believe should not be included, start a discussion or write a kernel analyzing the data. Or take the data and do something else productive with it. Kaggle\\'s choice to host this dataset is not meant to express any particular political affiliation or intent.\\nContents\\nThe dataset contains text and metadata from 244 websites and represents 12,999 posts in total from the past 30 days. The data was pulled using the webhose.io API; because it\\'s coming from their crawler, not all websites identified by the BS Detector are present in this dataset. Each website was labeled according to the BS Detector as documented here. Data sources that were missing a label were simply assigned a label of \"bs\". There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don\\'t trust anything you read.\\nFake news in the news\\nFor inspiration, I\\'ve included some (presumably non-fake) recent stories covering fake news in the news. This is a sensitive, nuanced topic and if there are other resources you\\'d like to see included here, please leave a suggestion. From defining fake, biased, and misleading news in the first place to deciding how to take action (a blacklist is not a good answer), there\\'s a lot of information to consider beyond what can be neatly arranged in a CSV file.\\nHow Fake News Spreads (NYT)\\nWe Tracked Down A Fake-News Creator In The Suburbs. Here\\'s What We Learned (NPR)\\nDoes Facebook Generate Over Half of its Revenue from Fake News? (Forbes)\\nFake News is Not the Only Problem (Points - Medium)\\nWashington Post Disgracefully Promotes a McCarthyite Blacklist From a New, Hidden, and Very Shady Group (The Intercept)\\nImprovements\\nIf you have suggestions for improvements or would like to contribute, please let me know. The most obvious extensions are to include data from \"real\" news sites and to address the bias in the current list. I\\'d be happy to include any contributions in future versions of the dataset.\\nAcknowledgements\\nThanks to Anthony for pointing me to Daniel Sieradski\\'s BS Detector. Thank you to Daniel Nouri for encouraging me to add a disclaimer to the dataset\\'s page.',\n",
       " 'One way to understand how a city government works is by looking at who it employs and how its employees are compensated. This data contains the names, job title, and compensation for San Francisco city employees on an annual basis from 2011 to 2014.\\nExploration Ideas\\nTo help get you started, here are some data exploration ideas:\\nHow have salaries changed over time between different groups of people?\\nHow are base pay, overtime pay, and benefits allocated between different groups?\\nIs there any evidence of pay discrimination based on gender in this dataset?\\nHow is budget allocated based on different groups and responsibilities?\\nHave other ideas you\\'re curious for someone else to explore? Post them in this forum thread.\\nData Description\\nsf-salaries-release-*.zip (downloadable via the \"Download Data\" link in the header above) contains a CSV table and a SQLite database (with the same data as the CSV file). Here\\'s the code that creates this data release.\\nThe original source for this data is here. We\\'ve taken the raw files here and combined/normalized them into a single CSV file as well as a SQLite database with an equivalently-defined table.',\n",
       " 'Context\\nThis dataset contains over 80,000 reports of UFO sightings over the last century.\\nContent\\nThere are two versions of this dataset: scrubbed and complete. The complete data includes entries where the location of the sighting was not found or blank (0.8146%) or have an erroneous or blank time (8.0237%). Since the reports date back to the 20th century, some older data might be obscured. Data contains city, state, time, description, and duration of each sighting.\\nInspiration\\nWhat areas of the country are most likely to have UFO sightings?\\nAre there any trends in UFO sightings over time? Do they tend to be clustered or seasonal?\\nDo clusters of UFO sightings correlate with landmarks, such as airports or government research centers?\\nWhat are the most common UFO descriptions?\\nAcknowledgement\\nThis dataset was scraped, geolocated, and time standardized from NUFORC data by Sigmond Axel here.',\n",
       " 'Every year the CDC releases the country’s most detailed report on death in the United States under the National Vital Statistics Systems. This mortality dataset is a record of every death in the country for 2005 through 2015, including detailed information about causes of death and the demographic background of the deceased.\\nIt\\'s been said that \"statistics are human beings with the tears wiped off.\" This is especially true with this dataset. Each death record represents somebody\\'s loved one, often connected with a lifetime of memories and sometimes tragically too short.\\nPutting the sensitive nature of the topic aside, analyzing mortality data is essential to understanding the complex circumstances of death across the country. The US Government uses this data to determine life expectancy and understand how death in the U.S. differs from the rest of the world. Whether you’re looking for macro trends or analyzing unique circumstances, we challenge you to use this dataset to find your own answers to one of life’s great mysteries.\\nOverview\\nThis dataset is a collection of CSV files each containing one year\\'s worth of data and paired JSON files containing the code mappings, plus an ICD 10 code set. The CSVs were reformatted from their original fixed-width file formats using information extracted from the CDC\\'s PDF manuals using this script. Please note that this process may have introduced errors as the text extracted from the pdf is not a perfect match. If you have any questions or find errors in the preparation process, please leave a note in the forums. We hope to publish additional years of data using this method soon.\\nA more detailed overview of the data can be found here. You\\'ll find that the fields are consistent within this time window, but some of data codes change every few years. For example, the 113_cause_recode entry 069 only covers ICD codes (I10,I12) in 2005, but by 2015 it covers (I10,I12,I15). When I post data from years prior to 2005, expect some of the fields themselves to change as well.\\nAll data comes from the CDC’s National Vital Statistics Systems, with the exception of the Icd10Code, which are sourced from the World Health Organization.\\nProject ideas\\nThe CDC\\'s mortality data was the basis of a widely publicized paper, by Anne Case and Nobel prize winner Angus Deaton, arguing that middle-aged whites are dying at elevated rates. One of the criticisms against the paper is that it failed to properly account for the exact ages within the broad bins available through the CDC\\'s WONDER tool. What do these results look like with exact/not-binned age data?\\nSimilarly, how sensitive are the mortality trends being discussed in the news to the choice of bin-widths?\\nAs noted above, the data preparation process could have introduced errors. Can you find any discrepancies compared to the aggregate metrics on WONDER? If so, please let me know in the forums!\\nWONDER is cited in numerous economics, sociology, and public health research papers. Can you find any papers whose conclusions would be altered if they used the exact data available here rather than binned data from Wonder?\\nDifferences from the first version of the dataset\\nThis version of the dataset was prepared in a completely different many. This has allowed us to provide a much larger volume of data and ensure that codes are available for every field.\\nWe\\'ve replaced the batch of sql files with a single JSON per year. Kaggle\\'s platform currently offer\\'s better support for JSON files, and this keeps the number of files manageable.\\nA tutorial kernel providing a quick introduction to the new format is available here.\\nLastly, I apologize if the transition has interrupted anyone\\'s work! If need be, you can still download v1.',\n",
       " 'This is the ball by ball data of all the IPL cricket matches till season 9.\\nSource: http://cricsheet.org/ (data is available on this website in the YAML format. This is converted to CSV format by the contributors)\\nThe dataset contains 2 files: deliveries.csv and matches.csv. matches.csv contains details related to the match such as location, contesting teams, umpires, results, etc. deliveries.csv is the ball-by-ball data of all the IPL matches including data of the batting team, batsman, bowler, non-striker, runs scored, etc.\\nResearch scope: Predicting the winner of the next season of IPL based on past data, Visualizations, Perspectives, etc.',\n",
       " \"Context\\n45 episodes across 4 seasons of Monty Python's Flying Circus - all of the scripts broken down into reusable bits.\\nContent\\nThe data attempts to create a structure around the Flying Circus scripts by breaking actions down into Dialogue (someone is speaking) and Direction (instructions for the actors). Along with each action I have tried to allocate the episode number, episode name, recording date, air date, segment name, name of character and name of actor playing the character.\\nAcknowledgements\\nThe scripts are hosted in HTML at http://www.ibras.dk/montypython/justthewords.htm I have loaded all of the code that I wrote to scrape and process the data (warning: very messy) at https://github.com/allank/monty-python\\nInspiration\\nI scraped the data because I was looking at data sources for doing RNN to generate text based on an existing corpus. While the amount of data available in the Flying Circus scripts is probably not sufficient, there might be some interesting things to do with the data. For example, some Markov chain generated dialogue lines:\\nRemember, buy Whizzo butter and this dead crab. Yeah, er, I, I personally think this is getting too silly. I don't like the sound of two bricks being bashed together.\",\n",
       " 'Try your hand at automatically separating normal heartbeats from abnormal heartbeats and heart murmur with this machine learning challenge by Peter Bentley et al\\nThe Data\\nHere\\'s a brief overview of the format of this dataset as uploaded to Kaggle. For a more detailed description, look at the Description section below.\\nThe dataset is split into two sources, A and B: A was collected from the general public via an iPhone app, and B was collected from a clinical trial in hospitals using a digital stethoscope.\\nThe goal of the task is to first (1) identify the locations of heart sounds from the audio, and (2) to classify the heart sounds into one of several categories (normal v. various non-normal heartbeat sounds).\\nThe CSV files provided are: set_a.csv\\nset_b.csv set_a_timing.csv\\nThe fields for set_a and set_b are as follows:\\ndataset: a or b\\nfname: the audio file\\nlabel: either \"normal\", blank (for unlabelled data), or one of various categories of abnormal heartbeats\\nsublabel: in set_b, some recordings are categorized as noisy, meaning they contain non-heart background noise; this field holds information on whether something is e.g. \"noisynormal\" or \"noisymurmur\"\\nThe file set_a_timing.csv contains gold-standard timing information for the \"normal\" recordings from Set A. This file contains the following fields:\\nfname: the audio file\\ncycle: anywhere from 1 to 19; the heartbeat cycle that the time observation refers to\\nsound: either S1 or S2; see below for what these mean\\nlocation: the time location of this sound, in audio samples\\nDescription\\nThe task, as described by the original authors\\nTask Overview\\nData has been gathered from two sources: (A) from the general public via the iStethoscope Pro iPhone app, provided in Dataset A, and (B) from a clinic trial in hospitals using the digital stethoscope DigiScope, provided in Dataset B.\\nCHALLENGE 1 - Heart Sound Segmentation\\nThe first challenge is to produce a method that can locate S1(lub) and S2(dub) sounds within audio data, segmenting the Normal audio files in both datasets. To enable your machine learning method to learn we provide the exact location of S1 and S2 sounds for some of the audio files. You need to use them to identify and locate the S1 and S2 sounds of all the heartbeats in the unlabelled group. The locations of sounds are measured in audio samples for better precision. Your method must use the same unit.\\nCHALLENGE 2 - Heart Sound Classification\\nThe task is to produce a method that can classify real heart audio (also known as “beat classification”) into one of four categories for Dataset A:\\nNormal\\nMurmur\\nExtra Heart Sound\\nArtifact\\nand three classes for Dataset B:\\nNormal\\nMurmur\\nExtrasystole\\nYou may tackle either or both of these challenges. If you can solve the first challenge, the second will be considerably easier! The winner of each challenge will be the method best able to segment and/or classify two sets of unlabelled data into the correct categories after training on both datasets provided below.\\n[Obviously no longer applicable -- ed.]: The creator of the winning method will receive a WiFi 32Gb iPad as the prize, awarded at a workshop at AISTATS 2012.\\nThe audio files are of varying lengths, between 1 second and 30 seconds (some have been clipped to reduce excessive noise and provide the salient fragment of the sound). Most information in heart sounds is contained in the low frequency components, with noise in the higher frequencies. It is common to apply a low-pass filter at 195 Hz. Fast Fourier transforms are also likely to provide useful information about volume and frequency over time. More domain-specific knowledge about the difference between the categories of sounds is provided below.\\nNormal Category\\nIn the Normal category there are normal, healthy heart sounds. These may contain noise in the final second of the recording as the device is removed from the body. They may contain a variety of background noises (from traffic to radios). They may also contain occasional random noise corresponding to breathing, or brushing the microphone against clothing or skin. A normal heart sound has a clear “lub dub, lub dub” pattern, with the time from “lub” to “dub” shorter than the time from “dub” to the next “lub” (when the heart rate is less than 140 beats per minute). Note the temporal description of “lub” and “dub” locations over time in the following illustration:\\n…lub……….dub……………. lub……….dub……………. lub……….dub……………. lub……….dub…\\nIn medicine we call the lub sound \"S1\" and the dub sound \"S2\". Most normal heart rates at rest will be between about 60 and 100 beats (‘lub dub’s) per minute. However, note that since the data may have been collected from children or adults in calm or excited states, the heart rates in the data may vary from 40 to 140 beats or higher per minute. Dataset B also contains noisy_normal data - normal data which includes a substantial amount of background noise or distortion. You may choose to use this or ignore it, however the test set will include some equally noisy examples.\\nMurmur Category\\nHeart murmurs sound as though there is a “whooshing, roaring, rumbling, or turbulent fluid” noise in one of two temporal locations: (1) between “lub” and “dub”, or (2) between “dub” and “lub”. They can be a symptom of many heart disorders, some serious. There will still be a “lub” and a “dub”. One of the things that confuses non-medically trained people is that murmurs happen between lub and dub or between dub and lub; not on lub and not on dub. Below, you can find an asterisk* at the locations a murmur may be.\\n…lub..*...dub……………. lub..*..dub ……………. lub..*..dub ……………. lub..*..dub … or …lub……….dub…*….lub………. dub…*….lub ………. dub…**….lub ……….dub…\\nDataset B also contains noisy_murmur data - murmur data which includes a substantial amount of background noise or distortion. You may choose to use this or ignore it, however the test set will include some equally noisy examples\\nExtra Heart Sound Category (Dataset A)\\nExtra heart sounds can be identified because there is an additional sound, e.g. a “lub-lub dub” or a “lub dub-dub”. An extra heart sound may not be a sign of disease. However, in some situations it is an important sign of disease, which if detected early could help a person. The extra heart sound is important to be able to detect as it cannot be detected by ultrasound very well. Below, note the temporal description of the extra heart sounds:\\n…lub.lub……….dub………..………. lub. lub……….dub…………….lub.lub……..…….dub……. or …lub………. dub.dub………………….lub.……….dub.dub………………….lub……..…….dub. dub……\\nArtifact Category (Dataset A)\\nIn the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise. There are usually no discernable heart sounds, and thus little or no temporal periodicity at frequencies below 195 Hz. This category is the most different from the others. It is important to be able to distinguish this category from the other three categories, so that someone gathering the data can be instructed to try again.\\nExtrasystole Category (Dataset B)\\nExtrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats, e.g. a “lub-lub dub” or a “lub dub-dub”. (This is not the same as an extra heart sound as the event is not regularly occuring.) An extrasystole may not be a sign of disease. It can happen normally in an adult and can be very common in children. However, in some situations extrasystoles can be caused by heart diseases. If these diseases are detected earlier, then treatment is likely to be more effective. Below, note the temporal description of the extra heart sounds: …........lub……….dub………..………. lub. ………..……….dub…………….lub.lub……..…….dub……. or …lub………. dub......………………….lub.…………………dub.dub………………….lub……..…….dub.……\\nAcknowledgments\\nPlease use the following citation if the data is used:\\n@misc{pascal-chsc-2011, author = \"Bentley, P. and Nordehn, G. and Coimbra, M. and Mannor, S.\", title = \"The {PASCAL} {C}lassifying {H}eart {S}ounds {C}hallenge 2011 {(CHSC2011)} {R}esults\", howpublished = \"http://www.peterjbentley.com/heartchallenge/index.html\"}',\n",
       " 'Live Feed\\nPlease comment on the Discussion above \"Live Feed\" and I will share details with you in a message. Hope we won\\'t exceed server limitations.\\nContext\\nI have been recording available different types of data on soccer matches since 2012, live 24/7. The whole database contains more than 350,000 soccer matches held all around the world from over 27,000 teams of more than 180 countries. An all-in-one package including servers, algorithms and its database are now under the \"Analyst Masters\" research platform. The app is also free for everyone to get its predictions on Android Play Store . How could it become useful for a data scientist?\\nDid you know that,\\nmore than 1000 soccer matches are played in a week?\\nthe average profit of the stock market from its beginning to now has been less than 10% a year? but you can earn at least 10% on a single match in 2 hours and get your profit in cash\\nIt is one of the very rare datasets that you do not need to prove to other companies your method is the most accurate one and get the prize :) . On the other hand you do not have to classify every data point to be rewarded. Just tune or focus to correctly classify only 1% of matches and there you go! Let me give you a simple hint how easily it can become a classification problem rather than a time series prediction:\\nExample 1: Who wins based on the number of wins in a head 2 head history?\\nQ) Consider two teams Midtjylland and Randers from Denmark. They have played against each other for very long time. Midtjyland has won Randers over 8 times in the past 10 matches in a 4 year time span. Forget any other complicated algorithm and simply predict who wins this match?\\nA) That is easy! However, I am also gathering a lot more information than just their history. You can check their head-to-head history and the odds you could get for predicting this match is \"1.73\" check here.\\nExample 2: Number of Goals based on their history?\\nQ) Consider two teams \"San Martin S.J.\" and \"Rosario Central\" from Argentina. Their odds for wining \"Team 1 (Home)\", \"Draw\" and \"Team 2 (away)\" is [3.16, 3.2, 2.25] respectively. They rank 22 and 13 in their league. They have recently won 45%,35% of their matches in their past 14 matches. Their average head to head goals in their last 7 matches were 1.3 full time (F) and 0.3 until half-time (HT). How many goals do you think they score in their match? (Note that a safe side of number of goals in soccer betting is Over 0.5 goals in HT, Under 1.5 goals in HT, Over 1.5 goals in FT and Under 3.5 goals in FT). Which one do you choose?\\nA) For sure under 1.5 goals in HT (you get 35%) and under 3.5 goals in FT (you get 30%) . Bingo you get 65% in a single match in 2 hours\\nExample 3: Based on the money placed for betting on teams who wins the match?\\nQ) \"Memmingen\" and \"Munich 1860\" are well known in Germany. One of our reliable sources of data is the ratio of money placed on betting from 10 hours before the match until it starts. Assume that the ratio of bets on \"Munich 1860\" to \"Memmingen\" are recorded every hour as below, which team do you think will win?\\n[bets in $ on Munich 1860]/[bets in $ on Memmingen] : {1.01, 1.02, 1.04, 1.1, 1.2, 1.4, 1.58, 2.3, 2.6, 2.8}\\nA) in 10 hours the amount of money placed on wining Munich 1860 Vs Memmingen increased from 1.01 to 2.8, who is the winner? Easy again, Munich 1860 that gives you 160% as stated here.\\nTry the dataset and inspect every strategy you may come up with, as I gave you three reliable examples above. Just perform well enough to predict 15 matches correctly in a row, start with $1000 and you are a millionaire. If you can\\'t be that accurate use the Kelly Criterion to divide your whole money into smaller stakes.\\nLet me do the math for you, if you can only get 90% accuracy on 1% of data points (10 out of 1000 matches a week) and your average profit on each match is only 20%. You earn (9*20% = 180%) and lose 100% for your error in 10 predictions. Your net profit would be 80% in a week or approximately 12% in a day. if you risk only 33% of your whole money on each match then the daily net profit becomes 4%. I guess you can easily calculate how fast you can progress @ 4% daily accumulative profit.\\nFor sure one needs a live data feed to predict the outcome before the match. If everything goes well and enough users are interested I will open the live feed of data for you in a shared folder of Dropbox saved in CSV.\\nContent\\nHere is what the dataset contains for \\'n\\' matches:\\nnames6.csv\\n: team names as the order of \"home-away\" separated using \"/\" ; size : (n x 1)\\nresults6.csv*\\n: Scores recorded during the match, every 2 rows show scores for one match ; size : (2n x 14)\\nfresults6.csv*\\n: Final scores after full-time ; size : (n x 2)\\nodds6.csv\\n: odds in the order of: Home-Draw-Away ; size : (n x 3)\\ndollars6.csv*\\n: Ratio of the money spent on teams at 15 minutes intervals ; size : (n x 76)\\nranks6.csv\\n: their ranks in the league at the day of the match Irrespectively ; size : (n x 2)\\nwinrate6.csv\\n: their winrate In the last (maximum 14) matches in 2017 Irrespectively ; size : (n x 2)\\ncountry6.csv\\n: their country as some countries are difficult to analyze e.g. Belarussia ; size : (n x 1)\\nwins6.csv\\n: number of wins in their last (maximum 6) head to head matches ; size : (n x 1)\\nFT_HT6.csv\\n: average of total goals in their last (maximum 6) head to head matches FT and HT ; size : (n x 2)\\n*. recorded every 15 minutes\\nTry to predict the match as examples above using the given data in the zip file. I will upload the respective data from Mid of August to Mid September later on.\\nAcknowledgements\\nI developed various data scrappers and classifiers running on multiple servers worldwide and never published a paper due to their sensitivity. You may refer to this database by mentioning the \"Analyst Masters\" research package.\\nInspiration\\n10 years ago I invented the world\\'s first home-size cooking robot in my father\\'s basement but in the end after cooking for us for 2 years it ended up in nothing. So, you as a data scientist can earn money using this live data stream for yourself if you can perform accurately without outperforming others in the competition just get an acceptable accuracy and you are good to go :)\\nFor more information on the overall platform and its live, pre-match and in-play analysis read at www.analystmasters.com or download the app for FREE to get easy predictions at 5% profit per week. More details on how the app operates is available at https://youtu.be/fqlu0YEyqc0',\n",
       " \"Context\\nThis dataset is a subset of Yelp's businesses, reviews, and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp's data and share their discoveries. In the dataset you'll find information about businesses across 11 metropolitan areas in four countries.\\nContent\\nThis dataset contains seven CSV files. The original JSON files can be found in yelp_academic_dataset.zip.\\n\\nYou may find this documentation helpful:\\nhttps://www.yelp.com/dataset/documentation/json\\nIn total, there are :\\n5,200,000 user reviews\\nInformation on 174,000 businesses\\nThe data spans 11 metropolitan areas\\nAcknowledgements\\nThe dataset was converted from JSON to CSV format and we thank the team of the Yelp dataset challenge for creating this dataset.\\nBy downloading this dataset, you agree to the Yelp Dataset Terms of Use.\\nInspiration\\nNatural Language Processing & Sentiment Analysis\\nWhat's in a review? Is it positive or negative? Yelp's reviews contain a lot of metadata that can be mined and used to infer meaning, business attributes, and sentiment.\\nGraph Mining\\nWe recently launched our Local Graph but can you take the graph further? How do user's relationships define their usage patterns? Where are the trend setters eating before it becomes popular?\",\n",
       " 'If you want to download and experiment with the Scrapy script, you can do so from forum data science This page is a forum for data scientist I started, in hope , that you will participate and maybe even improve the scrapy script.\\nOver 370000 used cars scraped with Scrapy from Ebay-Kleinanzeigen. The content of the data is in german, so one has to translate it first if one can not speak german. Those fields are included: autos.csv:\\ndateCrawled : when this ad was first crawled, all field-values are taken from this date\\nname : \"name\" of the car\\nseller : private or dealer\\nofferType\\nprice : the price on the ad to sell the car\\nabtest\\nvehicleType\\nyearOfRegistration : at which year the car was first registered\\ngearbox\\npowerPS : power of the car in PS\\nmodel\\nkilometer : how many kilometers the car has driven\\nmonthOfRegistration : at which month the car was first registered\\nfuelType\\nbrand\\nnotRepairedDamage : if the car has a damage which is not repaired yet\\ndateCreated : the date for which the ad at ebay was created\\nnrOfPictures : number of pictures in the ad (unfortunately this field contains everywhere a 0 and is thus useless (bug in crawler!) )\\npostalCode\\nlastSeenOnline : when the crawler saw this ad last online\\nThe fields lastSeen and dateCreated could be used to estimate how long a car will be at least online before it is sold.\\nbrought to you by Orges Leka\\nRegression on average Price per Year based on this dataset\\nTable of value loss of an average used car per year\\nThe second file is produced in MySQL from the first one through the query:\\nselect \\n count(*) as count, \\n kilometer, \\n yearOfRegistration, \\n20*round(powerPS/20) as powerPS, \\nmin(price) as minprice, \\nmax(price) as maxPrice, \\navg(price) as avgPreis, \\nsqrt(variance(price)) as sdPreis from items where \\n     yearOfRegistration > 1990 and yearOfRegistration < 2016 \\n    and price > 100 and price < 100000 \\n    and powerPS < 600 and powerPS > 0 \\n group by yearOfRegistration, round(powerPS/20),kilometer \\nhaving count > 10 \\ninto outfile \\'/tmp/cnt_km_year_powerPS_minPrice_maxPrice_avgPrice_sdPrice.csv\\' \\nfields terminated by \\',\\' lines terminated by \\'\\\\n\\';\\nHappy Coding!',\n",
       " \"Context\\nMotivated by Gregory Smith's web scrape of VGChartz Video Games Sales, this data set simply extends the number of variables with another web scrape from Metacritic. Unfortunately, there are missing observations as Metacritic only covers a subset of the platforms. Also, a game may not have all the observations of the additional variables discussed below. Complete cases are ~ 6,900\\nContent\\nAlongside the fields: Name, Platform, Year_of_Release, Genre, Publisher, NA_Sales, EU_Sales, JP_Sales, Other_Sales, Global_Sales, we have:-\\nCritic_score - Aggregate score compiled by Metacritic staff\\nCritic_count - The number of critics used in coming up with the Critic_score\\nUser_score - Score by Metacritic's subscribers\\nUser_count - Number of users who gave the user_score\\nDeveloper - Party responsible for creating the game\\nRating - The ESRB ratings\\nAcknowledgements\\nThis repository, https://github.com/wtamu-cisresearch/scraper, after a few adjustments worked extremely well!\\nInspiration\\nIt would be interesting to see any machine learning techniques or continued data visualizations applied on this data set.\",\n",
       " 'US Social Security applications are a great way to track trends in how babies born in the US are named.\\nData.gov releases two datasets that are helplful for this: one at the national level and another at the state level. Note that only names with at least 5 babies born in the same year (/ state) are included in this dataset for privacy.\\nI\\'ve taken the raw files here and combined/normalized them into two CSV files (one for each dataset) as well as a SQLite database with two equivalently-defined tables. The code that did these transformations is available here.\\nNew to data exploration in R? Take the free, interactive DataCamp course, \"Data Exploration With Kaggle Scripts,\" to learn the basics of visualizing data with ggplot. You\\'ll also create your first Kaggle Scripts along the way.',\n",
       " \"Content\\nThe Murder Accountability Project is the most complete database of homicides in the United States currently available. This dataset includes murders from the FBI's Supplementary Homicide Report from 1976 to the present and Freedom of Information Act data on more than 22,000 homicides that were not reported to the Justice Department. This dataset includes the age, race, sex, ethnicity of victims and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used.\\nAcknowledgements\\nThe data was compiled and made available by the Murder Accountability Project, founded by Thomas Hargrove.\",\n",
       " 'Overview\\nThis dataset contains 50000 ranked ladder matches from the Dota 2 data dump created by Opendota. It was inspired by the Dota 2 Matches data published here by Joe Ramir. This is an update and improved version of that dataset. I have kept the same image and a similar title.\\nDota 2 is a popular MOBA available as free to play, and can take up thousands of hours of your life. The number of games in this dataset are played about every hour. If you like the data there are an additional 2-3 million matches easily available for download.\\nThe aim of this dataset is to enable the exploration of player behavior, skill estimation, or anything you find interesting. The intent is to create an accessible, and easy to use resource, which can be expanded and modified if needed. As such I am open to a wide variety of suggestions as to what additions or changes to make.\\nHelp getting started\\nIf there is some aspect of this data you would like to explore but seems difficult to get figure out how to work with please feel free to request some starter code in one of the following two Kernels discussion section. I usually check kaggle every day or so. If you post a request about the current data I will try to get something working.\\nPython https://www.kaggle.com/devinanzelmo/d/devinanzelmo/dota-2-matches/misc-howtos-dota-requests-welcome/\\nR https://www.kaggle.com/devinanzelmo/d/devinanzelmo/dota-2-matches/howtos-request-welcome/\\nWhats Currently Available\\nSee https://github.com/odota/core/wiki/JSON-Data-Dump for documentaion on data. I have found a few undocumented areas in the data, including the objectives information. player_slot can be used to combine most of the data, and it is available in most of the tables. Additionally all tables include match_id, and some have account_id to make it easier to look at an individual players matches. match_id, and account_id have been reencoded to save a little space. I can upload tables to allow conversion if needed.\\nmatches: contains top level information about each match. see https://wiki.teamfortress.com/wiki/WebAPI/GetMatchDetails#Tower_Status%22tower_status_dire%22:%202047) for interpreting tower and barracks status. Cluster can link matches to geographic region.\\nplayers: Individual players are identified by account_id but there is an option to play anonymously and roughly one third of the account_id are not available. Anonymous users have the value of 0 for account_id. Contains totals for kills, deaths, denies, etc. Player action counts are available, and are indicated by variable names beginning with unit_order_. Counts for reasons for acquiring or losing gold, and gaining experience, have prefixes gold_, and xp_.\\nplayer_time: Contains last hits, experience, and gold sampled at one minute interval for all players in all matches. The column names indicate the player_slot. For instance xp_t_1 indicates that this column has experience sums for the player in slot one.\\nteamfights: Start and stop time of teamfights, as well as last death time. Teamfights appear to be all battles with three or more deaths. As such this does not include all battles for the entire match.\\nteamfights_players : Additional information provided for each player in each teamfight. player_slot can be used to link this back to players.csv\\nobjectives: Gives information on all the objectives completed, by which player and at what time.\\nchat: All chat for the 50k matches. There is plenty of profanity, and good natured trolling.\\ntest_labels: match_id and radiant_win(as integer 1 or 0)\\ntest_player: full player and match table with hero_id, player_slot, match_id, and account_id\\nNov 5th Update\\nAdded several additional tables. None of the previously uploaded data was altered. I plan to add several Kernels in the next week going over how to use the data, and performing some EDA. Many improvements to the player rating method I used are possible for those interested in MMR.\\nplayer_ratings contains match counts, win counts, and TrueSkill rating, calculated on 900k matches which occurred prior to other uploaded data. trueskill ratings have two components, mu, which can be interpreted as the skill, with higher value being better, and sigma which is the uncertainty of the rating.\\nmatch_outcomes data for ~900k matches used to calculate player ratings. Use this to improve on the ratings I uploaded.\\npurchase_log item purchase times\\nability_upgrade ability upgrade times and levels\\ncluster_region allows the mapping cluster found in match.csv to geographic region.\\npatch_dates release dates for various patches, use start_time from match.csv to determine which patch a match was played in.\\nability_ids use with ability_upgrades.csv to get the names of upgraded abilities\\nitem_ids use with purchase_log.csv to get the names of purchased items\\nKernel showing how player skill was computed: Contains several resources on trueskill rating system.\\nPast Research\\nThere seem to be some efforts to establish indicators for skillfull play based on specific parts of gameplay. Opendota has many statistics, and some analysis for specific benchmarks at different times in the game. Dotabuff has a lot of information I have not explored it deeply. This is an area to gather more information.\\nSome possible directions of investigation\\nInsight from domain experts would also be useful to help clarify what problems are interesting to work on. Some initial task ideas\\nPredict match outcomes based on aggregates for individual players using only account_id as prior information\\nAdd hero id to this and see if there is a differences in performance\\nEstimate player skill based on a sample of in game play(this might need an external mmr source or different definition skill)\\nCreate improved indicators of skillful play based game actions to help players target areas for improvement\\nAll of these areas have been worked on, but I am not aware of the most up to date research on dota2 gameplay.\\nI plan on setting up several different predictive tasks in the upcoming weeks. A test set of an additional 50 to 100 thousand matches with just hero_id, and account_id included along with outcome of the match.\\nThe current dataset seems pretty small for modeling individual players. I would prefer to have a wide range of features instead of a larger dataset for the moment.\\nDataset idea for anyone interested in creating their own Dota 2 dataset. It would be useful to have a few full matches available to work on. They would need to be extracted from the .dem replay file to something easily parsed by R and Python as available in kernels. Given the size of a full match data only a few matches would be needed. There are files available from opendota\\' s website(check for replays). Looking at fine grained match details would potentially allow for the creation of better high level parsed data. I think it would be a lot of work just to get a handle on working with full match data so a sample would be good to have.\\nAcknowledgements\\nOrginal kaggle dataset on dota2 matches by Joe Ramir I also borrowed the image and some of the content for these acknowledgements from the above, thanks!.\\nimage source\\nData download source created by yasp\\nDescription of original dataset creation: https://github.com/yasp-dota/yasp/issues/924\\nyasp\\'s license\\n\"License: CC BY-SA 4.0\"\\n\"Terms: We ask that you attribute yasp.co if you create or publish anything related to our data. Also, please seed for as long as possible.\"\\nYasp is now known as opendota here are links to their website and github page\\nhttps://www.opendota.com/ the data is used to for this site and its a easy way to get familier with it\\nhttps://github.com/odota/core check here for info especially this wiki page which gives details on the schema.',\n",
       " 'Analysis of the public dataset: \"Airplane Crashes and Fatalities Since 1908\" (Full history of airplane crashes throughout the world, from 1908-present) hosted by Open Data by Socrata available at:\\nhttps://opendata.socrata.com/Government/Airplane-Crashes-and-Fatalities-Since-1908/q2te-8cvq\\nQuestions\\nYearly how many planes crashed? how many people were on board? how many survived? how many died?\\nHighest number of crashes by operator and Type of aircrafts.\\n‘Summary’ field has the details about the crashes. Find the reasons of the crash and categorize them in different clusters i.e Fire, shot down, weather (for the ‘Blanks’ in the data category can be UNKNOWN) you are open to make clusters of your choice but they should not exceed 7.\\nFind the number of crashed aircrafts and number of deaths against each category from above step.\\nFind any interesting trends/behaviors that you encounter when you analyze the dataset.\\nMy solution\\nThe following bar charts display the answers requested by point 1. of the assignment, in particular:\\nthe planes crashed per year\\npeople aboard per year during crashes\\npeople dead per year during crashes\\npeople survived per year during crashes\\nThe following answers regard point 2 of the assignment\\nHighest number of crashes by operator: Aeroflot with 179 crashes\\nBy Type of aircraft: Douglas DC-3 with 334 crashes\\nI have identified 7 clusters using k-means clustering technique on a matrix obtained by a text corpus created by using Text Analysis (plain text, remove punctuation, to lower, etc.) The following table summarize for each cluster the number of crashes and death.\\nCluster 1: 258 crashes, 6368 deaths\\nCluster 2: 500 crashes, 9408 deaths\\nCluster 3: 211 crashes, 3513 deaths\\nCluster 4: 1014 crashes, 14790 deaths\\nCluster 5: 2749 crashes, 58826 deaths\\nCluster 6: 195 crashes, 4439 deaths\\nCluster 7: 341 crashes, 8135 deaths\\nThe following picture shows clusters using the first 2 principal components:\\nFor each clusters I will summarize the most used words and I will try to identify the causes of the crash\\nCluster 1 (258) aircraft, crashed, plane, shortly, taking. No many information about this cluster can be deducted using Text Analysis\\nCluster 2 (500) aircraft, airport, altitude, crashed, crew, due, engine, failed, failure, fire, flight, landing, lost, pilot, plane, runway, takeoff, taking. Engine failure on the runway after landing or takeoff\\nCluster 3 (211): aircraft, crashed, fog Crash caused by fog\\nCluster 4 (1014): aircraft, airport, attempting, cargo, crashed, fire, land, landing, miles, pilot, plane, route, runway, struck, takeoff Struck a cargo during landing or takeoff\\nCluster 5 (2749): accident, aircraft, airport, altitude, approach, attempting, cargo, conditions, control, crashed, crew, due, engine, failed, failure, feet, fire, flight, flying, fog, ground, killed, land, landing, lost, low, miles, mountain, pilot. plane, poor, route, runway, short, shortly, struck, takeoff, taking, weather\\nStruck a cargo due to engine failure or bad weather conditions mainly fog\\nCluster 6 (195): aircraft, crashed, engine, failure, fire, flight, left, pilot, plane, runway\\nEngine failure on the runway\\nCluster 7 (341): accident, aircraft, altitude, cargo, control, crashed, crew, due, engine, failure, flight, landing, loss, lost, pilot, plane, takeoff\\nEngine failure during landing or takeoff\\nBetter solutions are welcome. Thanks.',\n",
       " \"Context\\nMass Shootings in the United States of America (1966-2017) The US has witnessed 398 mass shootings in last 50 years that resulted in 1,996 deaths and 2,488 injured. The latest and the worst mass shooting of October 2, 2017 killed 58 and injured 515 so far. The number of people injured in this attack is more than the number of people injured in all mass shootings of 2015 and 2016 combined. The average number of mass shootings per year is 7 for the last 50 years that would claim 39 lives and 48 injured per year.\\nContent\\nGeography: United States of America\\nTime period: 1966-2017\\nUnit of analysis: Mass Shooting Attack\\nDataset: The dataset contains detailed information of 398 mass shootings in the United States of America that killed 1996 and injured 2488 people.\\nVariables: The dataset contains Serial No, Title, Location, Date, Summary, Fatalities, Injured, Total Victims, Mental Health Issue, Race, Gender, and Lat-Long information.\\nAcknowledgements\\nI’ve consulted several public datasets and web pages to compile this data. Some of the major data sources include Wikipedia, Mother Jones, Stanford, USA Today and other web sources.\\nInspiration\\nWith a broken heart, I like to call the attention of my fellow Kagglers to use Machine Learning and Data Sciences to help me explore these ideas:\\n• How many people got killed and injured per year?\\n• Visualize mass shootings on the U.S map\\n• Is there any correlation between shooter and his/her race, gender\\n• Any correlation with calendar dates? Do we have more deadly days, weeks or months on average\\n• What cities and states are more prone to such attacks\\n• Can you find and combine any other external datasets to enrich the analysis, for example, gun ownership by state\\n• Any other pattern you see that can help in prediction, crowd safety or in-depth analysis of the event\\n• How many shooters have some kind of mental health problem? Can we compare that shooter with general population with same condition\\nMass Shootings Dataset Ver 3\\nThis is the new Version of Mass Shootings Dataset. I've added eight new variables:\\nIncident Area (where the incident took place),\\nOpen/Close Location (Inside a building or open space)\\nTarget (possible target audience or company),\\nCause (Terrorism, Hate Crime, Fun (for no obvious reason etc.)\\nPoliceman Killed (how many on duty officers got killed)\\nAge (age of the shooter)\\nEmployed (Y/N)\\nEmployed at (Employer Name)\\nAge, Employed and Employed at (3 variables) contain shooter details\\nMass Shootings Dataset Ver 4\\nQuite a few missing values have been added\\nMass Shootings Dataset Ver 5\\nThree more recent mass shootings have been added including the Texas Church shooting of November 5, 2017\\nI hope it will help create more visualization and extract patterns.\\nKeep Coding!\",\n",
       " 'Context\\nPart backorders is a common supply chain problem. Working to identify parts at risk of backorder before the event occurs so the business has time to react.\\nContent\\nTraining data file contains the historical data for the 8 weeks prior to the week we are trying to predict. The data was taken as weekly snapshots at the start of each week. Columns are defined as follows:\\nsku - Random ID for the product\\nnational_inv - Current inventory level for the part\\nlead_time - Transit time for product (if available)\\nin_transit_qty - Amount of product in transit from source\\nforecast_3_month - Forecast sales for the next 3 months\\nforecast_6_month - Forecast sales for the next 6 months\\nforecast_9_month - Forecast sales for the next 9 months\\nsales_1_month - Sales quantity for the prior 1 month time period\\nsales_3_month - Sales quantity for the prior 3 month time period\\nsales_6_month - Sales quantity for the prior 6 month time period\\nsales_9_month - Sales quantity for the prior 9 month time period\\nmin_bank - Minimum recommend amount to stock\\npotential_issue - Source issue for part identified\\npieces_past_due - Parts overdue from source\\nperf_6_month_avg - Source performance for prior 6 month period\\nperf_12_month_avg - Source performance for prior 12 month period\\nlocal_bo_qty - Amount of stock orders overdue\\ndeck_risk - Part risk flag\\noe_constraint - Part risk flag\\nppap_risk - Part risk flag\\nstop_auto_buy - Part risk flag\\nrev_stop - Part risk flag\\nwent_on_backorder - Product actually went on backorder. This is the target value.',\n",
       " 'Context\\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\\nContent\\nThe files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\\nAcknowledgements\\nThe original dataset can be found here. The creators would like to note that in case you find the dataset useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\\nInspiration\\nCan you use this dataset to build a prediction model that will accurately classify which texts are spam?',\n",
       " \"Context\\nThis dataset deals with pollution in the U.S. Pollution in the U.S. has been well documented by the U.S. EPA but it is a pain to download all the data and arrange them in a format that interests data scientists. Hence I gathered four major pollutants (Nitrogen Dioxide, Sulphur Dioxide, Carbon Monoxide and Ozone) for every day from 2000 - 2016 and place them neatly in a CSV file.\\nContent\\nThere is a total of 28 fields. The four pollutants (NO2, O3, SO2 and O3) each has 5 specific columns. Observations totaled to over 1.4 million. This kernel provides a good introduction to this dataset!\\nFor observations on specific columns visit the Column Metadata on the Data tab.\\nAcknowledgements\\nAll the data is scraped from the database of U.S. EPA : https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html\\nInspiration\\nI did a related project with some of my friends in college, and decided to open source our dataset so that data scientists don't need to re-scrape the U.S. EPA site for historical pollution data.\",\n",
       " \"Context\\nThe U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled, and diverted flights is published in DOT's monthly Air Travel Consumer Report and in this dataset of 2015 flight delays and cancellations.\\nAcknowledgements\\nThe flight delay and cancellation data was collected and published by the DOT's Bureau of Transportation Statistics.\",\n",
       " \"The Dataset you can play with.\\nContext\\nDataset for people who love data science and have grown up playing FIFA.\\nContent\\nEvery player featuring in FIFA 18\\n70+ attributes\\nPlayer and Flag Images\\nPlaying Position Data\\nAttributes based on actual data of the latest EA's FIFA 18 game\\nAttributes include on all player style statistics like Dribbling, Aggression, GK Skills etc.\\nPlayer personal data like Nationality, Photo, Club, Age, Wage, Salary etc.\\nUpcoming Update will Include :\\nTeam (National and Club) Data\\nPlayer Images in Zip folder\\nBetting Odds\\nThe dataset contains all the statistics and playing attributes of all the players in the Full version of FIFA 18.\\nData Source\\nThe data is scraped from the website https://sofifa.com by extracting the Player personal data and Player Ids and then the playing and style statistics.\\nGithub Project\\nPossible Explorations\\nMake your dream team\\nAnalyse which Club or National Team has the best-rated players\\nAssess the strength of a team at a particular position\\nAnalyse the team with the best dribbling speed\\nCo-relate between Age and Overall rating\\nCo-relate between Age and Nationality\\nCo-relate between Age and Potential\\nCould prove of immense value to Fantasy Premier League enthusiasts.\\nThese are just basic examples, sky is the limit.\\nAcknowledgements\\nThe data has been crawled from the https://sofifa.com website.\\nInspiration\\nSeveral insights and correlations between player value, wage, age, and performance can be derived from the dataset. Furthermore, how do the players in this dataset compare against themselves in last year's dataset?\\nContributing\\nChanges and Improvement suggestions are welcome. Feel free to comment new additions that you think are useful or drop a PR on the github project.\",\n",
       " \"We aren't saying this dataset is the Rosetta Stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity.\\nStrategizing to become a Master? Wondering who, where, and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.\\nThis data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below.\\nNote that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. Over time, we'll add more of the tables that we can release publicly to it.\",\n",
       " \"The dataset for people who double on Fifa and Data Science\\nContent\\n17,000+ players\\n50+ attributes per player ranging from ball skills aggression etc.\\nPlayer's attributes sourced from EA Sports' FIFA video game series, including the weekly updates\\nPlayers from all around the globe\\nURLs to their homepage\\nClub logos\\nPlayer images male and female\\nNational and club team data\\nWeekly Updates would include :\\nReal life data (Match events etc.)\\nThe fifa generated player dataset\\nBetting odds\\nGrowth\\nData Source\\nData was scraped from https://www.fifaindex.com/ first by getting player profile url set (as stored in PlayerNames.csv) and then scraping the individual pages for their attributes\\nImprovements\\nYou may have noticed that for a lot of players, their national details are absent (Team and kit number) even though the nationality is listed. This may be attributed to the missing data on fifa sites.\\nGITHUB PROJECT\\nThere is much more than just 50 attributes by which fifa decides what happens to players over time, how they perform under pressure, how they grow etc. This data obviously would be well hidden by the organisation and thus would be tough to find\\nImportant note for people interested in using the scraping: The site is not uniform and thus the scraping script requires considering a lot of corner cases (i.e. interchanged position of different attributes). Also the script contains proxy preferences which may be removed if not required.\\nExploring the data\\nFor starters you can become a scout:\\nCreate attribute dependent or overall best teams\\nCreate the fastest/slowest teams\\nSee which areas of the world provide which attributes (like Africa : Stamina, Pace)\\nSee which players are the best at each position\\nSee which outfield players can play a better role at some other position\\nSee which youngsters have attributes which can be developed\\nAnd that is just the beginning. This is the playground.. literally!\\nData description\\nThe file FullData.csv contains attributes describing the in game play style and also some of the real statistics such as Nationality etc.\\nThe file PlayerNames.csv contains URLs for different players from their profiles on fifaindex.com. Append the URLs after the base url fifaindex.com.\\nThe compressed file Pictures.zip contains pictures for top 1000 players in Fifa 17.\\nThe compressed file Pictures_f.zip contains pictures for top 139 female players in Fifa 17.\\nThe compressed file ClubPictures.zip contains pictures for emblems of some major clubs in Fifa 17.\\nInspiration\\nI am a huge FIFA fanatic. While playing career mode I realised that I picked great young players early on every single time and since a lot of digital learning relies on how our brain works, I thought scouting great qualities in players would be something that can be worked on. Since then I started working on scraping the website and here is the data. I hope we can build something on it.\\nWith access to players attributes you can become the best scout in the world. Go for it!\",\n",
       " 'Context\\nNetflix held the Netflix Prize open competition for the best algorithm to predict user ratings for films. The grand prize was $1,000,000 and was won by BellKor\\'s Pragmatic Chaos team. This is the dataset that was used in that competition.\\nContent\\nThis comes directly from the README:\\nTRAINING DATASET FILE DESCRIPTION\\nThe file \"training_set.tar\" is a tar of a directory containing 17770 files, one per movie. The first line of each file contains the movie id followed by a colon. Each subsequent line in the file corresponds to a rating from a customer and its date in the following format:\\nCustomerID,Rating,Date\\nMovieIDs range from 1 to 17770 sequentially.\\nCustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\\nRatings are on a five star (integral) scale from 1 to 5.\\nDates have the format YYYY-MM-DD.\\nMOVIES FILE DESCRIPTION\\nMovie information in \"movie_titles.txt\" is in the following format:\\nMovieID,YearOfRelease,Title\\nMovieID do not correspond to actual Netflix movie ids or IMDB movie ids.\\nYearOfRelease can range from 1890 to 2005 and may correspond to the release of corresponding DVD, not necessarily its theaterical release.\\nTitle is the Netflix movie title and may not correspond to titles used on other sites. Titles are in English.\\nQUALIFYING AND PREDICTION DATASET FILE DESCRIPTION\\nThe qualifying dataset for the Netflix Prize is contained in the text file \"qualifying.txt\". It consists of lines indicating a movie id, followed by a colon, and then customer ids and rating dates, one per line for that movie id. The movie and customer ids are contained in the training set. Of course the ratings are withheld. There are no empty lines in the file.\\nMovieID1:\\nCustomerID11,Date11\\nCustomerID12,Date12\\n...\\nMovieID2:\\nCustomerID21,Date21\\nCustomerID22,Date22\\nFor the Netflix Prize, your program must predict the all ratings the customers gave the movies in the qualifying dataset based on the information in the training dataset.\\nThe format of your submitted prediction file follows the movie and customer id, date order of the qualifying dataset. However, your predicted rating takes the place of the corresponding customer id (and date), one per line.\\nFor example, if the qualifying dataset looked like:\\n111:\\n3245,2005-12-19\\n5666,2005-12-23\\n6789,2005-03-14\\n225:\\n1234,2005-05-26\\n3456,2005-11-07\\nthen a prediction file should look something like:\\n111:\\n3.0\\n3.4\\n4.0\\n225:\\n1.0\\n2.0\\nwhich predicts that customer 3245 would have rated movie 111 3.0 stars on the 19th of Decemeber, 2005, that customer 5666 would have rated it slightly higher at 3.4 stars on the 23rd of Decemeber, 2005, etc.\\nYou must make predictions for all customers for all movies in the qualifying dataset.\\nTHE PROBE DATASET FILE DESCRIPTION\\nTo allow you to test your system before you submit a prediction set based on the qualifying dataset, we have provided a probe dataset in the file \"probe.txt\". This text file contains lines indicating a movie id, followed by a colon, and then customer ids, one per line for that movie id.\\nMovieID1:\\nCustomerID11\\nCustomerID12\\n...\\nMovieID2:\\nCustomerID21\\nCustomerID22\\nLike the qualifying dataset, the movie and customer id pairs are contained in the training set. However, unlike the qualifying dataset, the ratings (and dates) for each pair are contained in the training dataset.\\nIf you wish, you may calculate the RMSE of your predictions against those ratings and compare your RMSE against the Cinematch RMSE on the same data. See http://www.netflixprize.com/faq#probe for that value.\\nAcknowledgements\\nThe training data came in 17,000+ files. In the interest of keeping files together and file sizes as low as possible, I combined them into four text files: combined_data_(1,2,3,4).txt\\nThe contest was originally hosted at http://netflixprize.com/index.html\\nThe dataset was downloaded from https://archive.org/download/nf_prize_dataset.tar\\nInspiration\\nThis is a fun dataset to work with. You can read about the winning algorithm by BellKor\\'s Pragmatic Chaos here',\n",
       " \"The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.\\nThis is the May 7, 2015 Version of dataset, as published at https://www.cs.cmu.edu/~./enron/\",\n",
       " \"Data on shots taken during the 2014-2015 season, who took the shot, where on the floor was the shot taken from, who was the nearest defender, how far away was the nearest defender, time on the shot clock, and much more. The column titles are generally self-explanatory.\\nUseful for evaluating who the best shooter is, who the best defender is, the hot-hand hypothesis, etc.\\nScraped from NBA's REST API.\",\n",
       " 'Context\\nThere is a lack of public available datasets on financial services and specially in the emerging mobile money transactions domain. Financial datasets are important to many researchers and in particular to us performing research in the domain of fraud detection. Part of the problem is the intrinsically private nature of financial transactions, that leads to no publicly available datasets.\\nWe present a synthetic dataset generated using the simulator called PaySim as an approach to such a problem. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods.\\nContent\\nPaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world.\\nThis synthetic dataset is scaled down 1/4 of the original dataset and it is created just for Kaggle.\\nHeaders\\nThis is a sample of 1 row with headers explanation:\\n1,PAYMENT,1060.31,C429214117,1089.0,28.69,M1591654462,0.0,0.0,0,0\\nstep - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).\\ntype - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\\namount - amount of the transaction in local currency.\\nnameOrig - customer who started the transaction\\noldbalanceOrg - initial balance before the transaction\\nnewbalanceOrig - new balance after the transaction\\nnameDest - customer who is the recipient of the transaction\\noldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\\nnewbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).\\nisFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\\nisFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.\\nPast Research\\nThere are 5 similar files that contain the run of 5 different scenarios. These files are better explained at my PhD thesis chapter 7 (PhD Thesis Available here http://urn.kb.se/resolve?urn=urn:nbn:se:bth-12932).\\nWe ran PaySim several times using random seeds for 744 steps, representing each hour of one month of real time, which matches the original logs. Each run took around 45 minutes on an i7 intel processor with 16GB of RAM. The final result of a run contains approximately 24 million of financial records divided into the 5 types of categories: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\\nAcknowledgements\\nThis work is part of the research project ”Scalable resource-efficient systems for big data analytics” funded by the Knowledge Foundation (grant: 20140032) in Sweden.\\nPlease refer to this dataset using the following citations:\\nPaySim first paper of the simulator:\\nE. A. Lopez-Rojas , A. Elmir, and S. Axelsson. \"PaySim: A financial mobile money simulator for fraud detection\". In: The 28th European Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus. 2016',\n",
       " \"YouTube Faces Dataset with Facial Keypoints\\nThis dataset is a processed version of the YouTube Faces Dataset, that basically contained short videos of celebrities that are publicly available and were downloaded from YouTube. There are multiple videos of each celebrity (up to 6 videos per celebrity). I've cropped the original videos around the faces, plus kept only consecutive frames of up to 240 frames for each original video. This is done also for reasons of disk space, but mainly to make the dataset easier to use.\\nAdditionally, for this kaggle version of the dataset I've extracted facial keypoints for each frame of each video using this amazing 2D and 3D Face alignment library that was recently published. please check out this video demonstrating the library. It's performance is really amazing, and I feel I'm quite qualified to say that after manually curating many thousands of individual frames and their corresponding keypoints. I removed all videos with extremely bad keypoints labeling. The end result of my curation process is approximately 2800 videos. Right now only 1293 of those videos are uploaded due to dataset size limitations (10GB), but since overall this totals into 155,560 single image frames, I think this is more than enough to do a lot of interesting kernels as well as potentially very interesting research.\\nContext\\nKaggle datasets platform and its integration with kernels is really amazing, but it's yet to have a videos dataset (at least that I'm aware of). Videos are special in the fact that they contain rich spatial patterns (in this case images of human faces) and rich temporal patterns (in this case how the faces move in time).\\nI was also inspired by the Face Images with Marked Landmark Points dataset uploaded by DrGuillermo and decided to create and share a dataset that would be similar but would also add something extra.\\nAcknowledgements\\nIf you use The YouTube Faces Dataset, or refer to its results, please cite the following paper:\\nLior Wolf, Tal Hassner and Itay Maoz\\nFace Recognition in Unconstrained Videos with Matched Background Similarity.\\nIEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2011. (pdf)\\nif you use the 2D or 3D keypoints, or refer to its results, please cite the following paper:\\nAdrian Bulat and Georgios Tzimiropoulos.\\nHow far are we from solving the 2D & 3D Face Alignment problem?\\n(and a dataset of 230,000 3D facial landmarks), arxiv, 2017. (pdf)\\nAlso, I would like to thank Gil Levi for pointing out YouTube Faces to me a few years back.\\nInspiration\\nThe YouTube Faces Dataset was originally intended to be used for face recognition across videos, i.e. given two videos, are those videos of the same person or not?\\nI think it can be used to serve many additional goals, especially when combined with the keypoints information. For example, can we build a face movement model and predict what facial expression will come next?\\nThis dataset can also be used to test transfer learning between other face datasets (like Face Images with Marked Landmark Points that I mentioned earlier), or even other types of faces like cat or dog faces (like here or here). Also, using the pre-trained Keras models might be useful (example kernel).\\nHave Fun!\",\n",
       " 'Dataset Information\\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\\nContent\\nThere are 25 variables:\\nID: ID of each client\\nLIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\\nSEX: Gender (1=male, 2=female)\\nEDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\\nMARRIAGE: Marital status (1=married, 2=single, 3=others)\\nAGE: Age in years\\nPAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\\nPAY_2: Repayment status in August, 2005 (scale same as above)\\nPAY_3: Repayment status in July, 2005 (scale same as above)\\nPAY_4: Repayment status in June, 2005 (scale same as above)\\nPAY_5: Repayment status in May, 2005 (scale same as above)\\nPAY_6: Repayment status in April, 2005 (scale same as above)\\nBILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\\nBILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\\nBILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\\nBILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\\nBILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\\nBILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\\nPAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\\nPAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\\nPAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\\nPAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\\nPAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\\nPAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\\ndefault.payment.next.month: Default payment (1=yes, 0=no)\\nInspiration\\nSome ideas for exploration:\\nHow does the probability of default payment vary by categories of different demographic variables?\\nWhich variables are the strongest predictors of default payment?\\nAcknowledgements\\nAny publications based on this dataset should acknowledge the following:\\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\\nThe original dataset can be found here at the UCI Machine Learning Repository.',\n",
       " 'Emergency (911) Calls: Fire, Traffic, EMS for Montgomery County, PA\\nYou can get a quick introduction to this Dataset with this kernel: Dataset Walk-through\\nAcknowledgements: Data provided by montcoalert.org',\n",
       " 'This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\\nDataset information\\nSeveral constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\\nRelevant papers\\nSmith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.',\n",
       " \"The Search for New Earths\\nGitHub\\nThe data describe the change in flux (light intensity) of several thousand stars. Each star has a binary label of 2 or 1. 2 indicated that that the star is confirmed to have at least one exoplanet in orbit; some observations are in fact multi-planet systems.\\nAs you can imagine, planets themselves do not emit light, but the stars that they orbit do. If said star is watched over several months or years, there may be a regular 'dimming' of the flux (the light intensity). This is evidence that there may be an orbiting body around the star; such a star could be considered to be a 'candidate' system. Further study of our candidate system, for example by a satellite that captures light at a different wavelength, could solidify the belief that the candidate can in fact be 'confirmed'.\\nIn the above diagram, a star is orbited by a blue planet. At t = 1, the starlight intensity drops because it is partially obscured by the planet, given our position. The starlight rises back to its original value at t = 2. The graph in each box shows the measured flux (light intensity) at each time interval.\\nDescription\\nTrainset:\\n5087 rows or observations.\\n3198 columns or features.\\nColumn 1 is the label vector. Columns 2 - 3198 are the flux values over time.\\n37 confirmed exoplanet-stars and 5050 non-exoplanet-stars.\\nTestset:\\n570 rows or observations.\\n3198 columns or features.\\nColumn 1 is the label vector. Columns 2 - 3198 are the flux values over time.\\n5 confirmed exoplanet-stars and 565 non-exoplanet-stars.\\nAcknowledgements\\nThe data presented here are cleaned and are derived from observations made by the NASA Kepler space telescope. The Mission is ongoing - for instance data from Campaign 12 was released on 8th March 2017. Over 99% of this dataset originates from Campaign 3. To boost the number of exoplanet-stars in the dataset, confirmed exoplanets from other campaigns were also included.\\nTo be clear, all observations from Campaign 3 are included. And in addition to this, confirmed exoplanet-stars from other campaigns are also included.\\nThe datasets were prepared late-summer 2016.\\nCampaign 3 was used because 'it was felt' that this Campaign is unlikely to contain any undiscovered (i.e. wrongly labelled) exoplanets.\\nNASA open-sources the original Kepler Mission data and it is hosted at the Mikulski Archive. After being beamed down to Earth, NASA applies de-noising algorithms to remove artefacts generated by the telescope. The data - in the .fits format - is stored online. And with the help of a seasoned astrophysicist, anyone with an internet connection can embark on a search to find and retrieve the datafiles from the Archive.\\nThe cover image is copyright © 2011 by Dan Lessmann\",\n",
       " \"Recently Reddit released an enormous dataset containing all ~1.7 billion of their publicly available comments. The full dataset is an unwieldy 1+ terabyte uncompressed, so we've decided to host a small portion of the comments here for Kagglers to explore. (You don't even need to leave your browser!)\\nYou can find all the comments from May 2015 on scripts for your natural language processing pleasure. What had redditors laughing, bickering, and NSFW-ing this spring?\\nWho knows? Top visualizations may just end up on Reddit.\\nData Description\\nThe database has one table, May2015, with the following fields:\\ncreated_utc\\nups\\nsubreddit_id\\nlink_id\\nname\\nscore_hidden\\nauthor_flair_css_class\\nauthor_flair_text\\nsubreddit\\nid\\nremoval_reason\\ngilded\\ndowns\\narchived\\nauthor\\nscore\\nretrieved_on\\nbody\\ndistinguished\\nedited\\ncontroversiality\\nparent_id\",\n",
       " 'Context\\nBlockchain technology, first implemented by Satoshi Nakamoto in 2009 as a core component of Bitcoin, is a distributed, public ledger recording transactions. Its usage allows secure peer-to-peer communication by linking blocks containing hash pointers to a previous block, a timestamp, and transaction data. Bitcoin is a decentralized digital currency (cryptocurrency) which leverages the Blockchain to store transactions in a distributed manner in order to mitigate against flaws in the financial industry.\\nNearly ten years after its inception, Bitcoin and other cryptocurrencies experienced an explosion in popular awareness. The value of Bitcoin, on the other hand, has experienced more volatility. Meanwhile, as use cases of Bitcoin and Blockchain grow, mature, and expand, hype and controversy have swirled.\\nContent\\nIn this dataset, you will have access to information about blockchain blocks and transactions. All historical data are in the bigquery-public-data:bitcoin_blockchain dataset. It’s updated it every 10 minutes. The data can be joined with historical prices in kernels. See available similar datasets here: https://www.kaggle.com/datasets?search=bitcoin.\\nQuerying BigQuery tables\\nYou can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.bitcoin_blockchain.[TABLENAME]. Fork this kernel to get started.\\nMethod & Acknowledgements\\nAllen Day (Twitter | Medium), Google Cloud Developer Advocate & Colin Bookman, Google Cloud Customer Engineer retrieve data from the Bitcoin network using a custom client available on GitHub that they built with the bitcoinj Java library. Historical data from the origin block to 2018-01-31 were loaded in bulk to two BigQuery tables, blocks_raw and transactions. These tables contain fresh data, as they are now appended when new blocks are broadcast to the Bitcoin network. For additional information visit the Google Cloud Big Data and Machine Learning Blog post \"Bitcoin in BigQuery: Blockchain analytics on public data\".\\nPhoto by Andre Francois on Unsplash.\\nInspiration\\nHow many bitcoins are sent each day?\\nHow many addresses receive bitcoin each day?\\nCompare transaction volume to historical prices by joining with other available data sources',\n",
       " 'Every year, Stack Overflow conducts a massive survey of people on the site, covering all sorts of information like programming languages, salary, code style and various other information. This year, they amassed more than 64,000 responses fielded from 213 countries.\\nData\\nThe data is made up of two files:\\n1. survey_results_public.csv - CSV file with main survey results, one respondent per row and one column per answer\\n2. survey_results_schema.csv - CSV file with survey schema, i.e., the questions that correspond to each column name m\\nAcknowledgements\\nData is directly taken from StackOverflow and licensed under the ODbL license.',\n",
       " 'It\\'s no secret that US university students often graduate with debt repayment obligations that far outstrip their employment and income prospects. While it\\'s understood that students from elite colleges tend to earn more than graduates from less prestigious universities, the finer relationships between future income and university attendance are quite murky. In an effort to make educational investments less speculative, the US Department of Education has matched information from the student financial aid system with federal tax returns to create the College Scorecard dataset.\\nKaggle is hosting the College Scorecard dataset in order to facilitate shared learning and collaboration. Insights from this dataset can help make the returns on higher education more transparent and, in turn, more fair.\\nData Description\\nHere\\'s a script showing an exploratory overview of some of the data.\\ncollege-scorecard-release-*.zip contains a compressed version of the same data available through Kaggle Scripts.\\nIt consists of three components:\\nAll the raw data files released in version 1.40 of the college scorecard data\\nScorecard.csv, a single CSV file with all the years data combined. In it, we\\'ve converted categorical variables represented by integer keys in the original data to their labels and added a Year column\\ndatabase.sqlite, a SQLite database containing a single Scorecard table that contains the same information as Scorecard.csv\\nNew to data exploration in R? Take the free, interactive DataCamp course, \"Data Exploration With Kaggle Scripts,\" to learn the basics of visualizing data with ggplot. You\\'ll also create your first Kaggle Scripts along the way.',\n",
       " 'Context\\nThis data set contains information on user preference data from 73,516 users on 12,294 anime. Each user is able to add anime to their completed list and give it a rating and this data set is a compilation of those ratings.\\nContent\\nAnime.csv\\nanime_id - myanimelist.net\\'s unique id identifying an anime.\\nname - full name of anime.\\ngenre - comma separated list of genres for this anime.\\ntype - movie, TV, OVA, etc.\\nepisodes - how many episodes in this show. (1 if movie).\\nrating - average rating out of 10 for this anime.\\nmembers - number of community members that are in this anime\\'s \"group\".\\nRating.csv\\nuser_id - non identifiable randomly generated user id.\\nanime_id - the anime that this user has rated.\\nrating - rating out of 10 this user has assigned (-1 if the user watched it but didn\\'t assign a rating).\\nAcknowledgements\\nThanks to myanimelist.net API for providing anime data and user ratings.\\nInspiration\\nBuilding a better anime recommendation system based only on user viewing history.',\n",
       " 'About\\nA dataset containing the attributes and the ratings for around 94,000 among board games and expansions as get from BoardGameGeek.\\nResources\\nThe same data and the scripts used to crawl it from BoardGameGeek are available in the form of R package on Github.',\n",
       " 'Description\\nWe collected EEG signal data from 10 college students while they watched MOOC video clips. We extracted online education videos that are assumed not to be confusing for college students, such as videos of the introduction of basic algebra or geometry. We also prepare videos that are expected to confuse a typical college student if a student is not familiar with the video topics like Quantum Mechanics, and Stem Cell Research. We prepared 20 videos, 10 in each category. Each video was about 2 minutes long. We chopped the two-minute clip in the middle of a topic to make the videos more confusing. The students wore a single-channel wireless MindSet that measured activity over the frontal lobe. The MindSet measures the voltage between an electrode resting on the forehead and two electrodes (one ground and one reference) each in contact with an ear. After each session, the student rated his/her confusion level on a scale of 1-7, where one corresponded to the least confusing and seven corresponded to the most confusing. These labels if further normalized into labels of whether the students are confused or not. This label is offered as self-labelled confusion in addition to our predefined label of confusion.\\nData information:\\n-----data.csv\\nColumn 1: Subject ID\\nColumn 2: Video ID\\nColumn 3: Attention (Proprietary measure of mental focus)\\nColumn 4: Mediation (Proprietary measure of calmness)\\nColumn 5: Raw (Raw EEG signal)\\nColumn 6: Delta (1-3 Hz of power spectrum)\\nColumn 7: Theta (4-7 Hz of power spectrum)\\nColumn 8: Alpha 1 (Lower 8-11 Hz of power spectrum)\\nColumn 9: Alpha 2 (Higher 8-11 Hz of power spectrum)\\nColumn 10: Beta 1 (Lower 12-29 Hz of power spectrum)\\nColumn 11: Beta 2 (Higher 12-29 Hz of power spectrum)\\nColumn 12: Gamma 1 (Lower 30-100 Hz of power spectrum)\\nColumn 13: Gamma 2 (Higher 30-100 Hz of power spectrum)\\nColumn 14: predefined label (whether the subject is expected to be confused)\\nColumn 15: user-defined label (whether the subject is actually confused)\\n-----subject demographic\\nColumn 1: Subject ID\\nColumn 2: Age\\nColumn 3: Ethnicity (Categorized according to https://en.wikipedia.org/wiki/List_of_contemporary_ethnic_groups)\\nColumn 4: Gender\\n-----video data\\nEach video lasts roughly two-minute long, we remove the first 30 seconds and last 30 seconds, only collect the EEG data during the middle 1 minute.\\nFormat\\nThese data are collected from ten students, each watching ten videos.\\nTherefore, it can be seen as only 100 data points for these 12000+ rows. If you look at this way, then each data point consists of 120+ rows, which is sampled every 0.5 seconds (so each data point is a one minute video). Signals with higher frequency are reported as the mean value during each 0.5 second.\\nReference:\\nWang, H., Li, Y., Hu, X., Yang, Y., Meng, Z., & Chang, K. M. (2013, June). Using EEG to Improve Massive Open Online Courses Feedback Interaction. In AIED Workshops. [PDF]\\nData Collection\\nThe data is collected from a software that we implemented ourselves. Check HaohanWang/Bioimaging for the source code.\\nInspiration\\nThis dataset is an extremely challenging data set to perform binary classification. 65% of prediction accuracy is quite decent according to our experience.\\nIt is an interesting data set to carry out the variable selection (causal inference) task that may help further research. Past research has indicated that Theta signal is correlated with confusion level.\\nIt is also an interesting data set for confounding factors correction model because we offer two labels (subject id and video id) that could profoundly confound the results.\\nWarning\\nThe data for subject 3 might be corrupted.\\nOther Resources\\nPromotion Video\\nSource Code of Data Collection Software\\nContact\\nHaohan Wang',\n",
       " 'Baffled why your team traded for that 34-year-old pitcher? Convinced you can create a new and improved version of WAR? Wondering what made the 1907 Cubs great and if can they do it again?\\nThe History of Baseball is a reformatted version of the famous Lahman’s Baseball Database. It contains Major League Baseball’s complete batting and pitching statistics from 1871 to 2015, plus fielding statistics, standings, team stats, park stats, player demographics, managerial records, awards, post-season data, and more.\\nScripts, Kaggle’s free, in-browser analytics tool, makes it easy to share detailed sabermetrics, predict the next hall of fame inductee, illustrate how speed scores runs, or publish a definitive analysis on why the Los Angeles Dodgers will never win another World Series.\\nWe have more ideas for analysis than games in a season, but here are a few we’d really love to see:\\nIs there a most error-prone position?\\nWhen do players at different positions peak?\\nAre the best performers selected for all-star game?\\nHow many walks does it take for a starting pitcher to get pulled?\\nDo players with a high ground into double play (GIDP) have a lower batting average?\\nWhich players are the most likely to choke during the post-season?\\nWhy should or shouldn’t the National League adopt the designated hitter rule?\\nSee the full SQLite schema.',\n",
       " 'Context\\nThis dataset contains all noise complaints calls that were received by the city police with complaint type \"Loud music/Party\" in 2016. The data contains the time of the call, time of the police response, coordinates and part of the city.\\nThis data should help match taxi rides from \"New York City Taxi Trip Duration\" competition to the night rides of partygoers.\\nContent\\nThe New York city hotline receives non-urgent community concerns, which are made public by the city through NYC Open Data portal. The full dataset contains a variety of complaints ranging from illegal parking to customer complaints. This dataset focuses on Noise complaints that were collected in 2016 and indicate ongoing party in a given neighborhood.\\nparties_in_nyc.csv:\\nColumns:\\nCreated Date - time of the call\\nClosed Date - time when ticket was closed by police\\nLocation Type - type of the location\\nIncident Zip - zip code of the location\\nCity - name of the city (almost the same as the Borough field)\\nBorough - administrative division of the city\\nLatitude - latitude of the location\\nLongitude - longitude of the location\\n\\ntest_parties and train_parties:\\nColumns:\\nid - id of the ride\\nnum_complaints - number of noise complaints about ongoing parties within ~500 meters and within 2 hours of pickup place and time\\nAcknowledgements\\nhttps://opendata.cityofnewyork.us/ - NYC Open Data portal contains many other interesting datasets Photo by Yvette de Wit on Unsplash\\nInspiration\\nAfter a fun night out in the city majority of people are too exhausted to travel by public transport, so they catch a cab to their home. I hope this data will help the community to find the patterns in the data that will lead to better solutions.',\n",
       " 'Context: This data set contains published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). It contains expression values for ~12.000 proteins for each sample, with missing values present when a given protein could not be quantified in a given sample.\\nContent:\\nFile: 77_cancer_proteomes_CPTAC_itraq.csv\\nRefSeq_accession_number: RefSeq protein ID (each protein has a unique ID in a RefSeq database)\\ngene_symbol: a symbol unique to each gene (every protein is encoded by some gene)\\ngene_name: a full name of that gene Remaining columns: log2 iTRAQ ratios for each sample (protein expression data, most important), three last columns are from healthy individuals\\nFile: clinical_data_breast_cancer.csv\\nFirst column \"Complete TCGA ID\" is used to match the sample IDs in the main cancer proteomes file (see example script). All other columns have self-explanatory names, contain data about the cancer classification of a given sample using different methods. \\'PAM50 mRNA\\' classification is being used in the example script.\\nFile: PAM50_proteins.csv\\nContains the list of genes and proteins used by the PAM50 classification system. The column RefSeqProteinID contains the protein IDs that can be matched with the IDs in the main protein expression data set.\\nPast Research: The original study: http://www.nature.com/nature/journal/v534/n7605/full/nature18003.html (paywall warning)\\nIn brief: the data were used to assess how the mutations in the DNA are affecting the protein expression landscape in breast cancer. Genes in our DNA are first transcribed into RNA molecules which then are translated into proteins. Changing the information content of DNA has impact on the behavior of the proteome, which is the main functional unit of cells, taking care of cell division, DNA repair, enzymatic reactions and signaling etc. They performed K-means clustering on the protein data to divide the breast cancer patients into sub-types, each having unique protein expression signature. They found that the best clustering was achieved using 3 clusters (original PAM50 gene set yields four different subtypes using RNA data).\\nInspiration:\\nThis is an interesting study and I myself wanted to use this breast cancer proteome data set for other types of analyses using machine learning that I am performing as a part of my PhD. However, I though that the Kaggle community (or at least that part with biomedical interests) would enjoy playing with it. I added a simple K-means clustering example for that data with some comments, the same approach as used in the original paper. One thing is that there is a panel of genes, the PAM50 which is used to classify breast cancers into subtypes. This panel was originally based on the RNA expression data which is (in my opinion) not as robust as the measurement of mRNA\\'s final product, the protein. Perhaps using this data set, someone could find a different set of proteins (they all have unique NP_/XP_ identifiers) that would divide the data set even more robustly? Perhaps into a higher numbers of clusters with very distinct protein expression signatures?\\nExample K-means analysis script: http://pastebin.com/A0Wj41DP',\n",
       " \"Context\\nThese are the lyrics for 57650 songs. They can be used for Natural Language Processing purposes, such as clustering of the words with similar meanings or predicting artist by the song. The dataset can be expanded with some more features for more advanced research like sentiment analysis. The data is not modified, only slightly cleaned, which gives a lot of freedom to devise your own applications.\\nMining\\nI have mined this dataset as a corpus for my NLP studies. However, before performing any transformation to bag-of-words or bag-of-N-grams, I decided to share the data. The data has been acquired from LyricsFreak through scraping. Then I did some very basic work on removing inconvenient data: non-English lyrics, extremely short and extremely long lyrics, lyrics with non-ASCII symbols. However, there's still work to be done in terms of data preparation.\\nContent\\nThe dataset contains 4 columns:\\nArtist\\nSong Name\\nLink to a webpage with the song (for reference). This is to be concatenated with http://www.lyricsfreak.com to form a real URL.\\nLyrics of the song, unmodified.\\nAcknowledgements\\nI would like to acknowledge LyricsFreak, which is the direct source of the data.\",\n",
       " \"Context\\nI'm a crowdfunding enthusiast and i'm watching kickstarter since its early days. Right now I just collect data and the only app i've made is this twitter bot which tweet any project reaching some milestone: @bloomwatcher . I have a lot of other ideas, but sadly not enough time to develop them... But I hope you can!\\nContent\\nYou'll find most useful data for project analysis. Columns are self explanatory except:\\nusd_pledged: conversion in US dollars of the pledged column (conversion done by kickstarter).\\nusd pledge real: conversion in US dollars of the pledged column (conversion from Fixer.io API).\\nusd goal real: conversion in US dollars of the goal column (conversion from Fixer.io API).\\nAcknowledgements\\nData are collected from Kickstarter Platform\\nusd conversion (usd_pledged_real and usd_goal_real columns) were generated from convert ks pledges to usd script done by tonyplaysguitar\\nInspiration\\nI hope to see great projects, and why not a model to predict if a project will be successful before it is released? :)\",\n",
       " \"Students' Academic Performance Dataset (xAPI-Edu-Data)\\nData Set Characteristics: Multivariate\\nNumber of Instances: 480\\nArea: E-learning, Education, Predictive models, Educational Data Mining\\nAttribute Characteristics: Integer/Categorical\\nNumber of Attributes: 16\\nDate: 2016-11-8\\nAssociated Tasks: Classification\\nMissing Values? No\\nFile formats: xAPI-Edu-Data.csv\\nSource:\\nElaf Abu Amrieh, Thair Hamtini, and Ibrahim Aljarah, The University of Jordan, Amman, Jordan, http://www.Ibrahimaljarah.com www.ju.edu.jo\\nDataset Information:\\nThis is an educational data set which is collected from learning management system (LMS) called Kalboard 360. Kalboard 360 is a multi-agent LMS, which has been designed to facilitate learning through the use of leading-edge technology. Such system provides users with a synchronous access to educational resources from any device with Internet connection.\\nThe data is collected using a learner activity tracker tool, which called experience API (xAPI). The xAPI is a component of the training and learning architecture (TLA) that enables to monitor learning progress and learner’s actions like reading an article or watching a training video. The experience API helps the learning activity providers to determine the learner, activity and objects that describe a learning experience. The dataset consists of 480 student records and 16 features. The features are classified into three major categories: (1) Demographic features such as gender and nationality. (2) Academic background features such as educational stage, grade Level and section. (3) Behavioral features such as raised hand on class, opening resources, answering survey by parents, and school satisfaction.\\nThe dataset consists of 305 males and 175 females. The students come from different origins such as 179 students are from Kuwait, 172 students are from Jordan, 28 students from Palestine, 22 students are from Iraq, 17 students from Lebanon, 12 students from Tunis, 11 students from Saudi Arabia, 9 students from Egypt, 7 students from Syria, 6 students from USA, Iran and Libya, 4 students from Morocco and one student from Venezuela.\\nThe dataset is collected through two educational semesters: 245 student records are collected during the first semester and 235 student records are collected during the second semester.\\nThe data set includes also the school attendance feature such as the students are classified into two categories based on their absence days: 191 students exceed 7 absence days and 289 students their absence days under 7.\\nThis dataset includes also a new category of features; this feature is parent parturition in the educational process. Parent participation feature have two sub features: Parent Answering Survey and Parent School Satisfaction. There are 270 of the parents answered survey and 210 are not, 292 of the parents are satisfied from the school and 188 are not.\\n(See the related papers for more details).\\nAttributes\\n1 Gender - student's gender (nominal: 'Male' or 'Female’)\\n2 Nationality- student's nationality (nominal:’ Kuwait’,’ Lebanon’,’ Egypt’,’ SaudiArabia’,’ USA’,’ Jordan’,’ Venezuela’,’ Iran’,’ Tunis’,’ Morocco’,’ Syria’,’ Palestine’,’ Iraq’,’ Lybia’)\\n3 Place of birth- student's Place of birth (nominal:’ Kuwait’,’ Lebanon’,’ Egypt’,’ SaudiArabia’,’ USA’,’ Jordan’,’ Venezuela’,’ Iran’,’ Tunis’,’ Morocco’,’ Syria’,’ Palestine’,’ Iraq’,’ Lybia’)\\n4 Educational Stages- educational level student belongs (nominal: ‘lowerlevel’,’MiddleSchool’,’HighSchool’)\\n5 Grade Levels- grade student belongs (nominal: ‘G-01’, ‘G-02’, ‘G-03’, ‘G-04’, ‘G-05’, ‘G-06’, ‘G-07’, ‘G-08’, ‘G-09’, ‘G-10’, ‘G-11’, ‘G-12 ‘)\\n6 Section ID- classroom student belongs (nominal:’A’,’B’,’C’)\\n7 Topic- course topic (nominal:’ English’,’ Spanish’, ‘French’,’ Arabic’,’ IT’,’ Math’,’ Chemistry’, ‘Biology’, ‘Science’,’ History’,’ Quran’,’ Geology’)\\n8 Semester- school year semester (nominal:’ First’,’ Second’)\\n9 Parent responsible for student (nominal:’mom’,’father’)\\n10 Raised hand- how many times the student raises his/her hand on classroom (numeric:0-100)\\n11- Visited resources- how many times the student visits a course content(numeric:0-100)\\n12 Viewing announcements-how many times the student checks the new announcements(numeric:0-100)\\n13 Discussion groups- how many times the student participate on discussion groups (numeric:0-100)\\n14 Parent Answering Survey- parent answered the surveys which are provided from school or not (nominal:’Yes’,’No’)\\n15 Parent School Satisfaction- the Degree of parent satisfaction from school(nominal:’Yes’,’No’)\\n16 Student Absence Days-the number of absence days for each student (nominal: above-7, under-7)\\nThe students are classified into three numerical intervals based on their total grade/mark:\\nLow-Level: interval includes values from 0 to 69,\\nMiddle-Level: interval includes values from 70 to 89,\\nHigh-Level: interval includes values from 90-100.\\nRelevant Papers:\\nAmrieh, E. A., Hamtini, T., & Aljarah, I. (2016). Mining Educational Data to Predict Student’s academic Performance using Ensemble Methods. International Journal of Database Theory and Application, 9(8), 119-136.\\nAmrieh, E. A., Hamtini, T., & Aljarah, I. (2015, November). Preprocessing and analyzing educational data set using X-API for improving student's performance. In Applied Electrical Engineering and Computing Technologies (AEECT), 2015 IEEE Jordan Conference on (pp. 1-5). IEEE.\\nCitation Request:\\nPlease include these citations if you plan to use this dataset:\\nAmrieh, E. A., Hamtini, T., & Aljarah, I. (2016). Mining Educational Data to Predict Student’s academic Performance using Ensemble Methods. International Journal of Database Theory and Application, 9(8), 119-136.\\nAmrieh, E. A., Hamtini, T., & Aljarah, I. (2015, November). Preprocessing and analyzing educational data set using X-API for improving student's performance. In Applied Electrical Engineering and Computing Technologies (AEECT), 2015 IEEE Jordan Conference on (pp. 1-5). IEEE.\",\n",
       " \"Context\\nStock market data can be interesting to analyze and as a further incentive, strong predictive models can have large financial payoff. The amount of financial data on the web is seemingly endless. A large and well structured dataset on a wide array of companies can be hard to come by. Here I provide a dataset with historical stock prices (last 5 years) for all companies currently found on the S&P 500 index.\\nThe script I used to acquire all of these .csv files can be found in this GitHub repository In the future if you wish for a more up to date dataset, this can be used to acquire new versions of the .csv files.\\nFeb 2018 note: I have just updated the dataset to include data up to Feb 2018. I have also accounted for changes in the stocks on the S&P 500 index (RIP whole foods etc. etc.).\\nContent\\nThe data is presented in a couple of formats to suit different individual's needs or computational limitations. I have included files containing 5 years of stock data (in the all_stocks_5yr.csv and corresponding folder).\\nThe folder individual_stocks_5yr contains files of data for individual stocks, labelled by their stock ticker name. The all_stocks_5yr.csv contains the same data, presented in a merged .csv file. Depending on the intended use (graphing, modelling etc.) the user may prefer one of these given formats.\\nAll the files have the following columns: Date - in format: yy-mm-dd\\nOpen - price of the stock at market open (this is NYSE data so all in USD)\\nHigh - Highest price reached in the day\\nLow Close - Lowest price reached in the day\\nVolume - Number of shares traded\\nName - the stock's ticker name\\nAcknowledgements\\nDue to volatility in google finance, for the newest version I have switched over to acquiring the data from The Investor's Exchange api, the simple script I use to do this is found here. Special thanks to Kaggle, Github, pandas_datareader and The Market.\\nInspiration\\nThis dataset lends itself to a some very interesting visualizations. One can look at simple things like how prices change over time, graph an compare multiple stocks at once, or generate and graph new metrics from the data provided. From these data informative stock stats such as volatility and moving averages can be easily calculated. The million dollar question is: can you develop a model that can beat the market and allow you to make statistically informed trades!\",\n",
       " 'Cervical Cancer Risk Factors for Biopsy: This Dataset is Obtained from UCI Repository and kindly acknowledged!\\nThis file contains a List of Risk Factors for Cervical Cancer leading to a Biopsy Examination!\\nAbout 11,000 new cases of invasive cervical cancer are diagnosed each year in the U.S. However, the number of new cervical cancer cases has been declining steadily over the past decades. Although it is the most preventable type of cancer, each year cervical cancer kills about 4,000 women in the U.S. and about 300,000 women worldwide. In the United States, cervical cancer mortality rates plunged by 74% from 1955 - 1992 thanks to increased screening and early detection with the Pap test. AGE Fifty percent of cervical cancer diagnoses occur in women ages 35 - 54, and about 20% occur in women over 65 years of age. The median age of diagnosis is 48 years. About 15% of women develop cervical cancer between the ages of 20 - 30. Cervical cancer is extremely rare in women younger than age 20. However, many young women become infected with multiple types of human papilloma virus, which then can increase their risk of getting cervical cancer in the future. Young women with early abnormal changes who do not have regular examinations are at high risk for localized cancer by the time they are age 40, and for invasive cancer by age 50. SOCIOECONOMIC AND ETHNIC FACTORS Although the rate of cervical cancer has declined among both Caucasian and African-American women over the past decades, it remains much more prevalent in African-Americans -- whose death rates are twice as high as Caucasian women. Hispanic American women have more than twice the risk of invasive cervical cancer as Caucasian women, also due to a lower rate of screening. These differences, however, are almost certainly due to social and economic differences. Numerous studies report that high poverty levels are linked with low screening rates. In addition, lack of health insurance, limited transportation, and language difficulties hinder a poor woman’s access to screening services. HIGH SEXUAL ACTIVITY Human papilloma virus (HPV) is the main risk factor for cervical cancer. In adults, the most important risk factor for HPV is sexual activity with an infected person. Women most at risk for cervical cancer are those with a history of multiple sexual partners, sexual intercourse at age 17 years or younger, or both. A woman who has never been sexually active has a very low risk for developing cervical cancer. Sexual activity with multiple partners increases the likelihood of many other sexually transmitted infections (chlamydia, gonorrhea, syphilis).Studies have found an association between chlamydia and cervical cancer risk, including the possibility that chlamydia may prolong HPV infection. FAMILY HISTORY Women have a higher risk of cervical cancer if they have a first-degree relative (mother, sister) who has had cervical cancer. USE OF ORAL CONTRACEPTIVES Studies have reported a strong association between cervical cancer and long-term use of oral contraception (OC). Women who take birth control pills for more than 5 - 10 years appear to have a much higher risk HPV infection (up to four times higher) than those who do not use OCs. (Women taking OCs for fewer than 5 years do not have a significantly higher risk.) The reasons for this risk from OC use are not entirely clear. Women who use OCs may be less likely to use a diaphragm, condoms, or other methods that offer some protection against sexual transmitted diseases, including HPV. Some research also suggests that the hormones in OCs might help the virus enter the genetic material of cervical cells. HAVING MANY CHILDREN Studies indicate that having many children increases the risk for developing cervical cancer, particularly in women infected with HPV. SMOKING Smoking is associated with a higher risk for precancerous changes (dysplasia) in the cervix and for progression to invasive cervical cancer, especially for women infected with HPV. IMMUNOSUPPRESSION Women with weak immune systems, (such as those with HIV / AIDS), are more susceptible to acquiring HPV. Immunocompromised patients are also at higher risk for having cervical precancer develop rapidly into invasive cancer. DIETHYLSTILBESTROL (DES) From 1938 - 1971, diethylstilbestrol (DES), an estrogen-related drug, was widely prescribed to pregnant women to help prevent miscarriages. The daughters of these women face a higher risk for cervical cancer. DES is no longer prsecribed.',\n",
       " 'This dataset contains US stocks fundamental data, such as income statement, balance sheet and cash flows.\\n12,129 companies\\n8,526 unique indicators\\n~20 indicators comparable across most companies\\nFive years of data, yearly\\nThe data is provided by http://usfundamentals.com.',\n",
       " \"Context\\nRay Kroc wanted to build a restaurant system that would be famous for providing food of consistently high quality and uniform methods of preparation. He wanted to serve burgers, buns, fries and beverages that tasted just the same in Alaska as they did in Alabama. To achieve this, he chose a unique path: persuading both franchisees and suppliers to buy into his vision, working not for McDonald’s but for themselves, together with McDonald’s. Many of McDonald’s most famous menu items – like the Big Mac, Filet-O-Fish, and Egg McMuffin – were created by franchisees.\\nContent\\nThis dataset provides a nutrition analysis of every menu item on the US McDonald's menu, including breakfast, beef burgers, chicken and fish sandwiches, fries, salads, soda, coffee and tea, milkshakes, and desserts.\\nAcknowledgements\\nThe menu items and nutrition facts were scraped from the McDonald's website.\\nInspiration\\nHow many calories does the average McDonald's value meal contain? How much do beverages, like soda or coffee, contribute to the overall caloric intake? Does ordered grilled chicken instead of crispy increase a sandwich's nutritional value? What about ordering egg whites instead of whole eggs? What is the least number of items could you order from the menu to meet one day's nutritional requirements?\\nStart a new kernel\",\n",
       " 'Crime Data for Philadelphia\\nTo get started quickly, take a look at Philly Data Crime Walk-through.\\nData was provided by OpenDataPhilly',\n",
       " 'The Human Activity Recognition database was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed.\\nDescription of experiment\\nThe experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\\nAttribute information\\nFor each record in the dataset the following is provided:\\nTriaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\\nTriaxial Angular velocity from the gyroscope.\\nA 561-feature vector with time and frequency domain variables.\\nIts activity label.\\nAn identifier of the subject who carried out the experiment.\\nRelevant papers\\nDavide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012\\nDavide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L. Reyes-Ortiz. Energy Efficient Smartphone-Based Activity Recognition using Fixed-Point Arithmetic. Journal of Universal Computer Science. Special Issue in Ambient Assisted Living: Home Care. Volume 19, Issue 9. May 2013\\nDavide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. 4th International Workshop of Ambient Assited Living, IWAAL 2012, Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture Notes in Computer Science 2012, pp 216-223.\\nJorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas, Davide Anguita, Joan Cabestany, Andreu Català. Human Activity and Motion Disorder Recognition: Towards Smarter Interactive Cognitive Environments. 21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\\nCitation\\nDavide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.',\n",
       " \"Context\\nThe Ethereum blockchain gives a revolutionary way of decentralized applications and provides its own cryptocurrency. Ethereum is a decentralized platform that runs smart contracts: applications that run exactly as programmed without any possibility of downtime, censorship, fraud or third party interference. These apps run on a custom built blockchain, an enormously powerful shared global infrastructure that can move value around and represent the ownership of property. This enables developers to create markets, store registries of debts or promises, move funds in accordance with instructions given long in the past (like a will or a futures contract) and many other things that have not been invented yet, all without a middle man or counterparty risk.\\nContent\\nWhat you may see in the CSVs are just numbers, but there is more to this. Numbers make machine learning easy. I've labeled each column, the first in all of them is the day; it may look weird but it makes sense if you look closely.\\nNote:\\nTIMESTAMP FORMAT\\nHow to convert timestamp in python:\\nimport datetime as dt\\n# The (would-be) timestamp value is below\\ntimestamp = 1339521878.04 \\n# Technechly you would iterate through and change them all if you were graphing\\ntimeValue = dt.datetime.fromtimestamp(timestamp)\\n#Year, month, day, hour, minute, second\\nprint(timeValue.strftime('%Y-%m-%d %H:%M:%S'))\\nAcknowledgements\\nMR. Vitalik Buterin. co-founder of Ethereum and as a co-founder of Bitcoin Magazine.\\nHit a brother up\\n0x767e8b211f70c5b8b4caa38c2efe05bf8eac0da7\\nWill be updating every month with new Ethereum history!\",\n",
       " 'Context\\nTypically e-commerce datasets are proprietary and consequently hard to find among publicly available data. However, The UCI Machine Learning Repository has made this dataset containing actual transactions from 2010 and 2011. The dataset is maintained on their site, where it can be found by the title \"Online Retail\".\\nContent\\n\"This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\"\\nAcknowledgements\\nPer the UCI Machine Learning Repository, this data was made available by Dr Daqing Chen, Director: Public Analytics group. chend \\'@\\' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\\nImage from stocksnap.io.\\nInspiration\\nAnalyses for this dataset could include time series, clustering, classification and more.',\n",
       " 'Introduction\\nData and code behind the stories and interactives at FiveThirtyEight. There are 80 Datasets here, included in one place. This allows you to make comparisons and utilize multiple Datasets.\\nairline-safety\\nalcohol-consumption\\navengers\\nbad-drivers\\nbechdel\\nbiopics\\nbirths\\nbob-ross\\nbuster-posey-mvp\\nclassic-rock\\ncollege-majors\\ncomic-characters\\ncomma-survey-data\\ncongress-age\\ncousin-marriage\\n... For the complete list, see the zip files listed here\\nPython\\nIf you\\'re planning to use Python, you might want to take a look at How to read datasets, which helps navigate the zipped Datasets. Again, there are multiple, 80 Datasets, in this Dataset.\\nR\\nIf you\\'re planning to use R, it\\'s included in the kernels. No need to unzip files. For example, if you wanted to load \"bechdel\", you could use the following commands.\\nlibrary(fivethirtyeight)\\ndata(package = \"fivethirtyeight\")\\nhead(bechdel) \\nReference R Quick Start, for an example.\\nReferences\\nThe following is FiveThirtyEight\\'s public repository on Github.\\nhttps://github.com/fivethirtyeight/data\\nThe CRAN package is maintained at the following link:\\nhttps://github.com/rudeboybert/fivethirtyeight\\nLicense\\nCopyright (c) 2014 ESPN Internet Ventures\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.',\n",
       " \"GitHub is how people build software and is home to the largest community of open source developers in the world, with over 12 million people contributing to 31 million projects on GitHub since 2008.\\nThis 3TB+ dataset comprises the largest released source of GitHub activity to date. It contains a full snapshot of the content of more than 2.8 million open source GitHub repositories including more than 145 million unique commits, over 2 billion different file paths, and the contents of the latest revision for 163 million files, all of which are searchable with regular expressions.\\nQuerying BigQuery tables\\nYou can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.github_repos.[TABLENAME]. Fork this kernel to get started to learn how to safely manage analyzing large BigQuery datasets.\\nAcknowledgements\\nThis dataset was made available per GitHub's terms of service.\\nInspiration\\nThis is the perfect dataset for fighting language wars.\\nCan you identify any signals that predict which packages or languages will become popular, in advance of their mass adoption?\",\n",
       " 'This dataset contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes, dating back to 1989.\\nInspiration and credit for gathering the data goes to Todd Schneider:\\nhttp://toddwschneider.com/posts/the-simpsons-by-the-data/\\nhttps://github.com/toddwschneider/flim-springfield',\n",
       " 'Context\\nIn November, the United States will elect a new president. Before then, three presidential debates will take place between Hillary Clinton and Donald Trump as well as one vice presidential debate between Tim Kaine and Mike Pence. While you can watch the debates live, why not also read deeply into the candidates\\' responses with text analytics?\\nYou can now answer any questions you have about the platforms of our presidential hopefuls or their speaking skills.\\nWhich candidate is the most given to loquaciousness?\\nHow many times does Clinton get interrupted?\\nWho gets the most audience applause?\\nWhen is positive sentiment at its highest during the candidates\\' word play?\\nContent & Acknowledgements\\nFor consistency, full transcripts of the debates were all taken from The Washington Post who made annotated transcripts available following each debate:\\nFirst debate taking place September 26th, 2016 was obtained from The Washington Post.\\nThe vice presidential debate from October 4th, 2016 was similarly obtained here.\\nThe \"town hall\" presidential debate on October 9th is found here.\\nThe final presidential debate taking place on October 19th is found here.\\nPlease make any dataset suggestions or requests on the forum. Word cloud from Debate Visualization by Jane Yu.',\n",
       " \"310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)\\nLower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spine. Typical sources of low back pain include:\\nThe large nerve roots in the low back that go to the legs may be irritated\\nThe smaller nerves that supply the low back may be irritated\\nThe large paired lower back muscles (erector spinae) may be strained\\nThe bones, ligaments or joints may be damaged\\nAn intervertebral disc may be degenerating\\nAn irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. Many lower back problems also cause back muscle spasms, which don't sound like much but can cause severe pain and disability.\\nWhile lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. A simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.\\nThis data set is about to identify a person is abnormal or normal using collected physical spine details/data.\",\n",
       " 'Content\\nThe data-set contains aggregate individual statistics for 67 NBA seasons. from basic box-score attributes such as points, assists, rebounds etc., to more advanced money-ball like features such as Value Over Replacement.\\nAcknowledgements\\nThe data was scraped from Basketball-reference Take a look in their glossary for a detailed column description Glossary',\n",
       " \"Welcome to Weedle's cave. Will you be able to predict the outcome of future matches?\\nTo do it you will have the pokemon characteristics and the results of previous combats.\\nThree files are available. The first one contains the pokemon characteristics (the first column being the id of the pokemon). The second one contains information about previous combats. The first two columns contain the ids of the combatants and the third one the id of the winner. Important: The pokemon in the first columns attacks first.\\nThe goal is to develop a Machine Learning model able to predict the result of future pokemon combats.\\nIf you have any questions, please email: t7pokemonchallenge@intelygenz.com\\nDISCLAIMER\\nIn Intelygenz we are against animal abuse. No animal real or imaginary should be forced to fight against other. Freedom for the pokemons !\",\n",
       " 'This dataset contains a list of 2410 US craft beers and 510 US breweries. The beers and breweries are linked together with an \"id\". This data was collected in January 2017 on CraftCans.com. The dataset is an a tidy format and values have been cleaned up for your enjoyment.\\nIf you are interested in learning more about how this dataset was acquired, I wrote an extensive blogpost about it (http://www.jeannicholashould.com/python-web-scraping-tutorial-for-craft-beers.html).\\nEnjoy!',\n",
       " 'Context\\nStarbucks started as a roaster and retailer of whole bean and ground coffee, tea and spices with a single store in Seattle’s Pike Place Market in 1971. The company now operates more than 24,000 retail stores in 70 countries.\\nContent\\nThis dataset includes a record for every Starbucks or subsidiary store location currently in operation as of February 2017.\\nAcknowledgements\\nThis data was scraped from the Starbucks store locator webpage by Github user chrismeller.\\nInspiration\\nWhat city or country has the highest number of Starbucks stores per capita? What two Starbucks locations are the closest in proximity to one another? What location on Earth is farthest from a Starbucks? How has Starbucks expanded overseas?',\n",
       " 'Context\\nThe Holy Quran is the central text for 1.5 billion Muslims around the world. It literally means \"The Recitation.\" It is undoubtedly the finest work in Arabic literature and revealed by Allah (God) to His Messenger Prophet Muhammed (Peace Be Upon Him) through angel Gabriel. It was revealed verbally from December 22, 609 (AD) to 632 AD (when Prophet Muhammed (Peace Be Upon Him) died)\\nThe book is divided into 30 parts, 114 Chapters and 6,000+ verses.\\nThere has been a lot of questions and comments on the text of this holy book given the contemporary Geo-political situation of the world, wars in the Middle East and Afghanistan and the ongoing terrorism.\\nI have put this dataset together to call my fellow data scientists to run their NLP algorithms and Kernels to find and explore the sacred text by them selves.\\nContent\\nThe data contains complete Holy Quran in following 21 languages (so data scientists from different parts of the world can work with it). The original text was revealed in Arabic. Other 20 files are the translations of the original text.\\nArabic (Original Book by God)\\nEnglish (Transalation by Yusuf Ali)\\nPersian (Makarim Sheerazi)\\nUrdu (Jalandhari)\\nTurkish (Y. N. Ozturk)\\nPortuguese (El. Hayek)\\nDutch (Keyzer)\\nNorwegian (Einar Berg)\\nItalian (Piccardo)\\nFrench (Hamidullah)\\nGerman (Zaidan)\\nSwedish (Rashad Kalifa)\\nIndonesia (Bhasha Indoenisan)\\nBangla\\nChinese/Madarin\\nJapanese\\nMalay\\nMalayalam\\nRussian\\nTamil\\nUzbek\\nInspiration\\nHere are some ideas to explore:\\nCan we make a word cloud for each chapter\\nCan we make a word cloud of the whole book and find out the frequency of each word\\nCan we describe or annotate subjects in each chapter and verse\\nCan we find how many times The Quran has mentioned Humans, Women, Humility, Heaven or Hell\\nCan we compare the text with other famous books and see the correlation\\nCan we compare the text with laws in multiple countries to see the resemblance\\nAny other ideas you can think of\\nI am looking forward to see your work and ideas and will keep adding more ideas to explore\\nWelcome on board to learn the finest text on earth with Data Sciences and Machine Learning!\\nUpdates\\nComplete Verse-By-Verse Dataset has been shared. A good contribution by Zohaib Ali - https://www.kaggle.com/zohaib1111 (Nov 20, 2017)',\n",
       " \"What's In The Deep-NLP Dataset?\\nSheet_1.csv contains 80 user responses, in the response_text column, to a therapy chatbot. Bot said: 'Describe a time when you have acted as a resource for someone else'.  User responded. If a response is 'not flagged', the user can continue talking to the bot. If it is 'flagged', the user is referred to help.\\nSheet_2.csv contains 125 resumes, in the resume_text column. Resumes were queried from Indeed.com with keyword 'data scientist', location 'Vermont'. If a resume is 'not flagged', the applicant can submit a modified resume version at a later date. If it is 'flagged', the applicant is invited to interview.\\nWhat Do I Do With This?\\nClassify new resumes/responses as flagged or not flagged.\\nThere are two sets of data here - resumes and responses. Split the data into a train set and a test set to test the accuracy of your classifier. Bonus points for using the same classifier for both problems.\\nGood luck.\\nAcknowledgements\\nThank you to Parsa Ghaffari (Aylien), without whom these visuals (cover photo is in Parsa Ghaffari's excellent LinkedIn article on English, Spanish and German postive v. negative sentiment analysis) would not exist.\\nThere Is A 'deep natural language processing' Kernel. I will update it. I Hope You Find It Useful.\\nYou can use any of the code in that kernel anywhere, on or off Kaggle. Ping me at @_samputnam for questions.\",\n",
       " \"Because of memory limitations,data format change csv -> db (sqlite format)\\nThis dataset includes yearly and monthly versions of Japan's international trading data (segmented by country , the type of good and local custom ).\\nJapan trade statistics is searchable here\",\n",
       " 'Context\\nThis dataset contains all stories and comments from Hacker News from its launch in 2006. Each story contains a story id, the author that made the post, when it was written, and the number of points the story received. Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham\\'s investment fund and startup incubator, Y Combinator. In general, content that can be submitted is defined as \"anything that gratifies one\\'s intellectual curiosity\".\\nContent\\nEach story contains a story ID, the author that made the post, when it was written, and the number of points the story received.\\nPlease note that the text field includes profanity. All texts are the author’s own, do not necessarily reflect the positions of Kaggle or Hacker News, and are presented without endorsement.\\nQuerying BigQuery tables\\nYou can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.hacker_news.[TABLENAME]. Fork this kernel to get started.\\nAcknowledgements\\nThis dataset was kindly made publicly available by Hacker News under the MIT license.\\nInspiration\\nRecent studies have found that many forums tend to be dominated by a very small fraction of users. Is this true of Hacker News?\\nHacker News has received complaints that the site is biased towards Y Combinator startups. Do the data support this?\\nIs the amount of coverage by Hacker News predictive of a startup’s success?',\n",
       " \"An outbreak of the Zika virus, an infection transmitted mostly by the Aedes species mosquito (Ae. aegypti and Ae. albopictus), has been sweeping across the Americas and the Pacific since mid-2015. Although first isolated in 1947 in Uganda, a lack of previous research has challenged the scientific community to quickly understand its devastating effects as the epidemic continues to spread.\\nAll Countries & Territories with Active Zika Virus Transmission\\nThe data\\nThis dataset shares publicly available data related to the ongoing Zika epidemic. It is being provided as a resource to the scientific community engaged in the public health response. The data provided here is not official and should be considered provisional and non-exhaustive. The data in reports may change over time, reflecting delays in reporting or changes in classifications. And while accurate representation of the reported data is the objective in the machine readable files shared here, that accuracy is not guaranteed. Before using any of these data, it is advisable to review the original reports and sources, which are provided whenever possible along with further information on the CDC Zika epidemic GitHub repo.\\nThe dataset includes the following fields:\\nreport_date - The report date is the date that the report was published. The date should be specified in standard ISO format (YYYY-MM-DD).\\nlocation - A location is specified for each observation following the specific names specified in the country place name database. This may be any place with a 'location_type' as listed below, e.g. city, state, country, etc. It should be specified at up to three hierarchical levels in the following format: [country]-[state/province]-[county/municipality/city], always beginning with the country name. If the data is for a particular city, e.g. Salvador, it should be specified: Brazil-Bahia-Salvador.\\nlocation_type - A location code is included indicating: city, district, municipality, county, state, province, or country. If there is need for an additional 'location_type', open an Issue to create a new 'location_type'.\\ndata_field - The data field is a short description of what data is represented in the row and is related to a specific definition defined by the report from which it comes.\\ndata_field_code - This code is defined in the country data guide. It includes a two letter country code (ISO-3166 alpha-2, list), followed by a 4-digit number corresponding to a specific report type and data type.\\ntime_period - Optional. If the data pertains to a specific period of time, for example an epidemiological week, that number should be indicated here and the type of time period in the 'time_period_type', otherwise it should be NA.\\ntime_period_type - Required only if 'time_period' is specified. Types will also be specified in the country data guide. Otherwise should be NA.\\nvalue - The observation indicated for the specific 'report_date', 'location', 'data_field' and when appropriate, 'time_period'.\\nunit - The unit of measurement for the 'data_field'. This should conform to the 'data_field' unit options as described in the country-specific data guide.\\nIf you find the data useful, please support data sharing by referencing this dataset and the original data source. If you're interested in contributing to the Zika project from GitHub, you can read more here. The source for the Zika virus structure is available here.\",\n",
       " \"Update 28/11/2017 - Last few weeks clearance levels starting to decrease (I may just be seeing a pattern I want to see.. maybe I'm just evil). Anyway, can any of you magicians make any sense of it?\\nMelbourne is currently experiencing a housing bubble (some experts say it may burst soon). Maybe someone can find a trend or give a prediction? Which suburbs are the best to buy in? Which ones are value for money? Where's the expensive side of town? And more importantly where should I buy a 2 bedroom unit?\\nContent & Acknowledgements\\nThis data was scraped from publicly available results posted every week from Domain.com.au, I've cleaned it as best I can, now it's up to you to make data analysis magic. The dataset includes Address, Type of Real estate, Suburb, Method of Selling, Rooms, Price, Real Estate Agent, Date of Sale and distance from C.B.D.\\n....Now with extra data including including property size, land size and council area, you may need to change your code!\\nSome Key Details\\nSuburb: Suburb\\nAddress: Address\\nRooms: Number of rooms\\nPrice: Price in dollars\\nMethod: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available.\\nType: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.\\nSellerG: Real Estate Agent\\nDate: Date sold\\nDistance: Distance from CBD\\nRegionname: General Region (West, North West, North, North east ...etc)\\nPropertycount: Number of properties that exist in the suburb.\\nBedroom2 : Scraped # of Bedrooms (from different source)\\nBathroom: Number of Bathrooms\\nCar: Number of carspots\\nLandsize: Land Size\\nBuildingArea: Building Size\\nYearBuilt: Year the house was built\\nCouncilArea: Governing council for the area\\nLattitude: Self explanitory\\nLongtitude: Self explanitory\",\n",
       " 'Context\\nThis dataset reflects reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the City of Chicago from 2001 to present, minus the most recent seven days. Data is extracted from the Chicago Police Department\\'s CLEAR (Citizen Law Enforcement Analysis and Reporting) system. In order to protect the privacy of crime victims, addresses are shown at the block level only and specific locations are not identified. Should you have questions about this dataset, you may contact the Research & Development Division of the Chicago Police Department at 312.745.6071 or RDAnalysis@chicagopolice.org. Disclaimer: These crimes may be based upon preliminary information supplied to the Police Department by the reporting parties that have not been verified. The preliminary crime classifications may be changed at a later date based upon additional investigation and there is always the possibility of mechanical or human error. Therefore, the Chicago Police Department does not guarantee (either expressed or implied) the accuracy, completeness, timeliness, or correct sequencing of the information and the information should not be used for comparison purposes over time. The Chicago Police Department will not be responsible for any error or omission, or for the use of, or the results obtained from the use of this information. All data visualizations on maps should be considered approximate and attempts to derive specific addresses are strictly prohibited. The Chicago Police Department is not responsible for the content of any off-site pages that are referenced by or that reference this web page other than an official City of Chicago or Chicago Police Department web page. The user specifically acknowledges that the Chicago Police Department is not responsible for any defamatory, offensive, misleading, or illegal conduct of other users, links, or third parties and that the risk of injury from the foregoing rests entirely with the user. The unauthorized use of the words \"Chicago Police Department,\" \"Chicago Police,\" or any colorable imitation of these words or the unauthorized use of the Chicago Police Department logo is unlawful. This web page does not, in any way, authorize such use. Data are updated daily. The dataset contains more than 6,000,000 records/rows of data and cannot be viewed in full in Microsoft Excel. To access a list of Chicago Police Department - Illinois Uniform Crime Reporting (IUCR) codes, go to http://data.cityofchicago.org/Public-Safety/Chicago-Police-Department-Illinois-Uniform-Crime-R/c7ck-438e\\nContent\\nID - Unique identifier for the record.\\nCase Number - The Chicago Police Department RD Number (Records Division Number), which is unique to the incident.\\nDate - Date when the incident occurred. this is sometimes a best estimate.\\nBlock - The partially redacted address where the incident occurred, placing it on the same block as the actual address.\\nIUCR - The Illinois Unifrom Crime Reporting code. This is directly linked to the Primary Type and Description. See the list of IUCR codes at https://data.cityofchicago.org/d/c7ck-438e.\\nPrimary Type - The primary description of the IUCR code.\\nDescription - The secondary description of the IUCR code, a subcategory of the primary description.\\nLocation Description - Description of the location where the incident occurred.\\nArrest - Indicates whether an arrest was made.\\nDomestic - Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act.\\nBeat - Indicates the beat where the incident occurred. A beat is the smallest police geographic area – each beat has a dedicated police beat car. Three to five beats make up a police sector, and three sectors make up a police district. The Chicago Police Department has 22 police districts. See the beats at https://data.cityofchicago.org/d/aerh-rz74.\\nDistrict - Indicates the police district where the incident occurred. See the districts at https://data.cityofchicago.org/d/fthy-xz3r.\\nWard - The ward (City Council district) where the incident occurred. See the wards at https://data.cityofchicago.org/d/sp34-6z76.\\nCommunity Area - Indicates the community area where the incident occurred. Chicago has 77 community areas. See the community areas at https://data.cityofchicago.org/d/cauq-8yn6.\\nFBI Code - Indicates the crime classification as outlined in the FBI\\'s National Incident-Based Reporting System (NIBRS). See the Chicago Police Department listing of these classifications at http://gis.chicagopolice.org/clearmap_crime_sums/crime_types.html.\\nX Coordinate - The x coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\\nY Coordinate - The y coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\\nYear - Year the incident occurred.\\nUpdated On - Date and time the record was last updated.\\nLatitude - The latitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\\nLongitude - The longitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\\nLocation - The location where the incident occurred in a format that allows for creation of maps and other geographic operations on this data portal. This location is shifted from the actual location for partial redaction but falls on the same block.\\nAcknowledgements\\nI really want to say thank you to the City of Chicago and the Chicago Police Department for making this comprehensive data set available to everyone!\\nInspiration\\nHow has crime changed over the years? Is it possible to predict where or when a crime will be committed? Which areas of the city have evolved over this time span?',\n",
       " 'Overview\\nPokemonGo is a mobile augmented reality game developed by Niantic inc. for iOS, Android, and Apple Watch devices. It was initially released in selected countries in July 2016. In the game, players use a mobile device\\'s GPS capability to locate, capture, battle, and train virtual creatures, called Pokémon, who appear on the screen as if they were in the same real-world location as the player.\\nDataset\\nDataset consists of roughly 293,000 pokemon sightings (historical appearances of Pokemon), having coordinates, time, weather, population density, distance to pokestops/ gyms etc. as features. The target is to train a machine learning algorithm so that it can predict where pokemon appear in future. So, can you predict\\'em all?)\\nFeature description\\npokemonId - the identifier of a pokemon, should be deleted to not affect predictions. (numeric; ranges between 1 and 151)\\nlatitude, longitude - coordinates of a sighting (numeric)\\nappearedLocalTime - exact time of a sighting in format yyyy-mm-dd\\'T\\'hh-mm-ss.ms\\'Z\\' (nominal)\\ncellId 90-5850m - geographic position projected on a S2 Cell, with cell sizes ranging from 90 to 5850m (numeric)\\nappearedTimeOfDay - time of the day of a sighting (night, evening, afternoon, morning)\\nappearedHour/appearedMinute - local hour/minute of a sighting (numeric)\\nappearedDayOfWeek - week day of a sighting (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday)\\nappearedDay/appearedMonth/appearedYear - day/month/year of a sighting (numeric)\\nterrainType - terrain where pokemon appeared described with help of GLCF Modis Land Cover (numeric)\\ncloseToWater - did pokemon appear close (100m or less) to water (Boolean, same source as above)\\ncity - the city of a sighting (nominal)\\ncontinent (not always parsed right) - the continent of a sighting (nominal)\\nweather - weather type during a sighting (Foggy Clear, PartlyCloudy, MostlyCloudy, Overcast, Rain, BreezyandOvercast, LightRain, Drizzle, BreezyandPartlyCloudy, HeavyRain, BreezyandMostlyCloudy, Breezy, Windy, WindyandFoggy, Humid, Dry, WindyandPartlyCloudy, DryandMostlyCloudy, DryandPartlyCloudy, DrizzleandBreezy, LightRainandBreezy, HumidandPartlyCloudy, HumidandOvercast, RainandWindy) // Source for all weather features\\ntemperature - temperature in celsius at the location of a sighting (numeric)\\nwindSpeed - speed of the wind in km/h at the location of a sighting (numeric)\\nwindBearing - wind direction (numeric)\\npressure - atmospheric pressure in bar at the location of a sighting (numeric)\\nweatherIcon - a compact representation of the weather at the location of a sighting (fog, clear-night, partly-cloudy-night, partly-cloudy-day, cloudy, clear-day, rain, wind)\\nsunriseMinutesMidnight-sunsetMinutesBefore - time of appearance relatively to sunrise/sunset Source\\npopulation density - what is the population density per square km of a sighting (numeric, Source)\\nurban-rural - how urban is location where pokemon appeared (Boolean, built on Population density, <200 for rural, >=200 and <400 for midUrban, >=400 and <800 for subUrban, >800 for urban)\\ngymDistanceKm, pokestopDistanceKm - how far is the nearest gym/pokestop in km from a sighting? (numeric, extracted from this dataset)\\ngymIn100m-pokestopIn5000m - is there a gym/pokestop in 100/200/etc meters? (Boolean)\\ncooc 1-cooc 151 - co-occurrence with any other pokemon (pokemon ids range between 1 and 151) within 100m distance and within the last 24 hours (Boolean)\\nclass - says which pokemonId it is, to be predicted.\\nData dump\\nAll pokemon sightings (in JSON file, without features) can be found in Discussion \"Datadump\"',\n",
       " 'In the mid-eighteenth to nineteenth centuries, navigating the open ocean was an imprecise and often dangerous feat. In order to calculate their daily progress and avoid running/sailing into the unknown, a ship\\'s crew kept a detailed logbook with data on winds, waves, and any remarkable weather.\\nHandwritten in archived logbooks, these rich datasets were nearly impossible to study until the European Union funded their digitization in 2001. You can visit the EU project website for detailed information on the countries and ships included.\\nWe\\'re hosting the full 1750-1850 dataset on Kaggle to promote the exploration of this unique and invaluable climatology resource.\\nData Description\\nThis data comes from the Climatological Database for the World\\'s Oceans 1750-1850 (CLIWOC), version 1.5 data release.\\nThe primary data file is CLIWOC15.csv. The columns in this table are described on this page (scroll down to the table that starts with \"Field abbreviation\"). It includes 280,280 observational records of ship locations weather data, and other associated information.\\nThe ancillary data files are described on the above site.',\n",
       " 'Context:\\nThis data publication contains a spatial database of wildfires that occurred in the United States from 1992 to 2015. It is the third update of a publication originally generated to support the national Fire Program Analysis (FPA) system. The wildfire records were acquired from the reporting systems of federal, state, and local fire organizations. The following core data elements were required for records to be included in this data publication: discovery date, final fire size, and a point location at least as precise as Public Land Survey System (PLSS) section (1-square mile grid). The data were transformed to conform, when possible, to the data standards of the National Wildfire Coordinating Group (NWCG). Basic error-checking was performed and redundant records were identified and removed, to the degree possible. The resulting product, referred to as the Fire Program Analysis fire-occurrence database (FPA FOD), includes 1.88 million geo-referenced wildfire records, representing a total of 140 million acres burned during the 24-year period.\\nContent:\\nThis dataset is an SQLite database that contains the following information:\\nFires: Table including wildfire data for the period of 1992-2015 compiled from US federal, state, and local reporting systems.\\nFOD_ID = Global unique identifier.\\nFPA_ID = Unique identifier that contains information necessary to track back to the original record in the source dataset.\\nSOURCE_SYSTEM_TYPE = Type of source database or system that the record was drawn from (federal, nonfederal, or interagency).\\nSOURCE_SYSTEM = Name of or other identifier for source database or system that the record was drawn from. See Table 1 in Short (2014), or \\\\Supplements\\\\FPA_FOD_source_list.pdf, for a list of sources and their identifier.\\nNWCG_REPORTING_AGENCY = Active National Wildlife Coordinating Group (NWCG) Unit Identifier for the agency preparing the fire report (BIA = Bureau of Indian Affairs, BLM = Bureau of Land Management, BOR = Bureau of Reclamation, DOD = Department of Defense, DOE = Department of Energy, FS = Forest Service, FWS = Fish and Wildlife Service, IA = Interagency Organization, NPS = National Park Service, ST/C&L = State, County, or Local Organization, and TRIBE = Tribal Organization).\\nNWCG_REPORTING_UNIT_ID = Active NWCG Unit Identifier for the unit preparing the fire report.\\nNWCG_REPORTING_UNIT_NAME = Active NWCG Unit Name for the unit preparing the fire report.\\nSOURCE_REPORTING_UNIT = Code for the agency unit preparing the fire report, based on code/name in the source dataset.\\nSOURCE_REPORTING_UNIT_NAME = Name of reporting agency unit preparing the fire report, based on code/name in the source dataset.\\nLOCAL_FIRE_REPORT_ID = Number or code that uniquely identifies an incident report for a particular reporting unit and a particular calendar year.\\nLOCAL_INCIDENT_ID = Number or code that uniquely identifies an incident for a particular local fire management organization within a particular calendar year.\\nFIRE_CODE = Code used within the interagency wildland fire community to track and compile cost information for emergency fire suppression (https://www.firecode.gov/).\\nFIRE_NAME = Name of the incident, from the fire report (primary) or ICS-209 report (secondary).\\nICS_209_INCIDENT_NUMBER = Incident (event) identifier, from the ICS-209 report.\\nICS_209_NAME = Name of the incident, from the ICS-209 report.\\nMTBS_ID = Incident identifier, from the MTBS perimeter dataset.\\nMTBS_FIRE_NAME = Name of the incident, from the MTBS perimeter dataset.\\nCOMPLEX_NAME = Name of the complex under which the fire was ultimately managed, when discernible.\\nFIRE_YEAR = Calendar year in which the fire was discovered or confirmed to exist.\\nDISCOVERY_DATE = Date on which the fire was discovered or confirmed to exist.\\nDISCOVERY_DOY = Day of year on which the fire was discovered or confirmed to exist.\\nDISCOVERY_TIME = Time of day that the fire was discovered or confirmed to exist.\\nSTAT_CAUSE_CODE = Code for the (statistical) cause of the fire.\\nSTAT_CAUSE_DESCR = Description of the (statistical) cause of the fire.\\nCONT_DATE = Date on which the fire was declared contained or otherwise controlled (mm/dd/yyyy where mm=month, dd=day, and yyyy=year).\\nCONT_DOY = Day of year on which the fire was declared contained or otherwise controlled.\\nCONT_TIME = Time of day that the fire was declared contained or otherwise controlled (hhmm where hh=hour, mm=minutes).\\nFIRE_SIZE = Estimate of acres within the final perimeter of the fire.\\nFIRE_SIZE_CLASS = Code for fire size based on the number of acres within the final fire perimeter expenditures (A=greater than 0 but less than or equal to 0.25 acres, B=0.26-9.9 acres, C=10.0-99.9 acres, D=100-299 acres, E=300 to 999 acres, F=1000 to 4999 acres, and G=5000+ acres).\\nLATITUDE = Latitude (NAD83) for point location of the fire (decimal degrees).\\nLONGITUDE = Longitude (NAD83) for point location of the fire (decimal degrees).\\nOWNER_CODE = Code for primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident.\\nOWNER_DESCR = Name of primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident.\\nSTATE = Two-letter alphabetic code for the state in which the fire burned (or originated), based on the nominal designation in the fire report.\\nCOUNTY = County, or equivalent, in which the fire burned (or originated), based on nominal designation in the fire report.\\nFIPS_CODE = Three-digit code from the Federal Information Process Standards (FIPS) publication 6-4 for representation of counties and equivalent entities.\\nFIPS_NAME = County name from the FIPS publication 6-4 for representation of counties and equivalent entities.\\nNWCG_UnitIDActive_20170109: Look-up table containing all NWCG identifiers for agency units that were active (i.e., valid) as of 9 January 2017, when the list was downloaded from https://www.nifc.blm.gov/unit_id/Publish.html and used as the source of values available to populate the following fields in the Fires table: NWCG_REPORTING_AGENCY, NWCG_REPORTING_UNIT_ID, and NWCG_REPORTING_UNIT_NAME.\\nUnitId = NWCG Unit ID.\\nGeographicArea = Two-letter code for the geographic area in which the unit is located (NA=National, IN=International, AK=Alaska, CA=California, EA=Eastern Area, GB=Great Basin, NR=Northern Rockies, NW=Northwest, RM=Rocky Mountain, SA=Southern Area, and SW=Southwest).\\nGacc = Seven or eight-letter code for the Geographic Area Coordination Center in which the unit is located or primarily affiliated with (CAMBCIFC=Canadian Interagency Forest Fire Centre, USAKCC=Alaska Interagency Coordination Center, USCAONCC=Northern California Area Coordination Center, USCAOSCC=Southern California Coordination Center, USCORMCC=Rocky Mountain Area Coordination Center, USGASAC=Southern Area Coordination Center, USIDNIC=National Interagency Coordination Center, USMTNRC=Northern Rockies Coordination Center, USNMSWC=Southwest Area Coordination Center, USORNWC=Northwest Area Coordination Center, USUTGBC=Western Great Basin Coordination Center, USWIEACC=Eastern Area Coordination Center).\\nWildlandRole = Role of the unit within the wildland fire community.\\nUnitType = Type of unit (e.g., federal, state, local).\\nDepartment = Department (or state/territory) to which the unit belongs (AK=Alaska, AL=Alabama, AR=Arkansas, AZ=Arizona, CA=California, CO=Colorado, CT=Connecticut, DE=Delaware, DHS=Department of Homeland Security, DOC= Department of Commerce, DOD=Department of Defense, DOE=Department of Energy, DOI= Department of Interior, DOL=Department of Labor, FL=Florida, GA=Georgia, IA=Iowa, IA/GC=Non-Departmental Agencies, ID=Idaho, IL=Illinois, IN=Indiana, KS=Kansas, KY=Kentucky, LA=Louisiana, MA=Massachusetts, MD=Maryland, ME=Maine, MI=Michigan, MN=Minnesota, MO=Missouri, MS=Mississippi, MT=Montana, NC=North Carolina, NE=Nebraska, NG=Non-Government, NH=New Hampshire, NJ=New Jersey, NM=New Mexico, NV=Nevada, NY=New York, OH=Ohio, OK=Oklahoma, OR=Oregon, PA=Pennsylvania, PR=Puerto Rico, RI=Rhode Island, SC=South Carolina, SD=South Dakota, ST/L=State or Local Government, TN=Tennessee, Tribe=Tribe, TX=Texas, USDA=Department of Agriculture, UT=Utah, VA=Virginia, VI=U. S. Virgin Islands, VT=Vermont, WA=Washington, WI=Wisconsin, WV=West Virginia, WY=Wyoming).\\nAgency = Agency or bureau to which the unit belongs (AG=Air Guard, ANC=Alaska Native Corporation, BIA=Bureau of Indian Affairs, BLM=Bureau of Land Management, BOEM=Bureau of Ocean Energy Management, BOR=Bureau of Reclamation, BSEE=Bureau of Safety and Environmental Enforcement, C&L=County & Local, CDF=California Department of Forestry & Fire Protection, DC=Department of Corrections, DFE=Division of Forest Environment, DFF=Division of Forestry Fire & State Lands, DFL=Division of Forests and Land, DFR=Division of Forest Resources, DL=Department of Lands, DNR=Department of Natural Resources, DNRC=Department of Natural Resources and Conservation, DNRF=Department of Natural Resources Forest Service, DOA=Department of Agriculture, DOC=Department of Conservation, DOE=Department of Energy, DOF=Department of Forestry, DVF=Division of Forestry, DWF=Division of Wildland Fire, EPA=Environmental Protection Agency, FC=Forestry Commission, FEMA=Federal Emergency Management Agency, FFC=Bureau of Forest Fire Control, FFP=Forest Fire Protection, FFS=Forest Fire Service, FR=Forest Rangers, FS=Forest Service, FWS=Fish & Wildlife Service, HQ=Headquarters, JC=Job Corps, NBC=National Business Center, NG=National Guard, NNSA=National Nuclear Security Administration, NPS=National Park Service, NWS=National Weather Service, OES=Office of Emergency Services, PRI=Private, SF=State Forestry, SFS=State Forest Service, SP=State Parks, TNC=The Nature Conservancy, USA=United States Army, USACE=United States Army Corps of Engineers, USAF=United States Air Force, USGS=United States Geological Survey, USN=United States Navy).\\nParent = Agency subgroup to which the unit belongs (A concatenation of State and Unit from this report - https://www.nifc.blm.gov/unit_id/publish/UnitIdReport.rtf).\\nCountry = Country in which the unit is located (e.g. US = United States).\\nState = Two-letter code for the state in which the unit is located (or primarily affiliated).\\nCode = Unit code (follows state code to create UnitId).\\nName = Unit name.\\nAcknowledgements:\\nThese data were collected using funding from the U.S. Government and can be used without additional permissions or fees. If you use these data in a publication, presentation, or other research product please use the following citation:\\nShort, Karen C. 2017. Spatial wildfire occurrence data for the United States, 1992-2015 [FPA_FOD_20170508]. 4th Edition. Fort Collins, CO: Forest Service Research Data Archive. https://doi.org/10.2737/RDS-2013-0009.4\\nInspiration:\\nHave wildfires become more or less frequent over time?\\nWhat counties are the most and least fire-prone?\\nGiven the size, location and date, can you predict the cause of a fire wildfire?',\n",
       " \"UPDATED: https://www.kaggle.com/datasnaek/youtube-new\\nPlan\\nData collected from the (up to) 200 listed trending YouTube videos every day in the US and the UK.\\nDescription\\nThe dataset includes data gathered from videos on YouTube that are contained within the trending category each day.\\nThere are two kinds of data files, one includes comments and one includes video statistics. They are linked by the unique video_id field.\\nThe headers in the video file are:\\nvideo_id (Common id field to both comment and video csv files)\\ntitle\\nchannel_title\\ncategory_id (Can be looked up using the included JSON files, but varies per region so use the appropriate JSON file for the CSV file's country)\\ntags (Separated by | character, [none] is displayed if there are no tags)\\nviews\\nlikes\\ndislikes\\nthumbnail_link\\ndate (Formatted like so: [day].[month])\\nThe headers in the comments file are:\\nvideo_id (Common id field to both comment and video csv files)\\ncomment_text\\nlikes\\nreplies\\nExtra info: The YouTube API is not effective at formatting comments by relevance, although it claims to do so. As a result, the most relevant comments do not align with the top comments at all, they aren't even sorted by likes or replies.\\nInspiration\\nPossible uses for this dataset could include:\\nSentiment analysis in a variety of forms\\nCategorising YouTube videos based on their comments and statistics.\\nTraining ML algorithms to generate their own YouTube comments.\\nAnalysing what factors affect how popular a YouTube video will be.\\nAlthough there are likely many more possibilities, including analysis of changes over time etc.\",\n",
       " 'The American Community Survey is an ongoing survey from the US Census Bureau. In this survey, approximately 3.5 million households per year are asked detailed questions about who they are and how they live. Many topics are covered, including ancestry, education, work, transportation, internet use, and residency.\\nThe responses reveal a fascinating, granular snapshot into the lives of many Americans.\\nWe\\'\\'re publishing this data on scripts to make it easy for you to explore this rich dataset, share your work, and collaborate with other data scientists. No data download or local environment needed! We\\'\\'ve also added shapefiles to simplify publishing maps.\\nWhat surprising insights can you find in this data? We look forward to seeing and sharing what you discover on scripts!\\nData Description\\nHere\\'\\'s a data dictionary.\\nThere are two types of survey data provided, housing and population.\\nFor the housing data, each row is a housing unit, and the characteristics are properties like rented vs. owned, age of home, etc.\\nFor the population data, each row is a person and the characteristics are properties like age, gender, whether they work, method/length of commute, etc.\\nEach data set is divided in two pieces, \"a\" and \"b\" (where \"a\" contains states 1 to 25 and \"b\" contains states 26 to 50).\\nBoth data sets have weights associated with them. Weights are included to account for the fact that individuals are not sampled with equal probably (people who have a greater chance of being sampled have a lower weight to reflect this).\\nWeight variable for the housing data: WGTP\\nWeight variable for the population data: PWGTP\\nIn Kaggle Scripts, these files can be accessed at:\\n../input/pums/ss13husa.csv (housing, a)\\n../input/pums/ss13husb.csv (housing, b)\\n../input/pums/ss13pusa.csv (population, a)\\n../input/pums/ss13pusb.csv (population, b)\\nYou can download the data from the census website:\\nhousing\\npopulation\\nIn scripts, they are accessed at:\\n../input/shapefiles/pums/tl_2013_[state]_puma10.[extension].\\nThe shapefiles can also be downloaded here.\\nDataCamp and Kaggle have teamed up to bring you the basics of Data Exploration With Kaggle Scripts. Take the free, interactive course here and start building your data science portfolio.',\n",
       " 'We scraped over 17,000 tweets from 100+ pro-ISIS fanboys from all over the world since the November 2015 Paris Attacks. We are working with content producers and influencers to develop effective counter-messaging measures against violent extremists at home and abroad. In order to maximize our impact, we need assistance in quickly analyzing message frames.\\nThe dataset includes the following:\\nName\\nUsername\\nDescription\\nLocation\\nNumber of followers at the time the tweet was downloaded\\nNumber of statuses by the user when the tweet was downloaded\\nDate and timestamp of the tweet\\nThe tweet itself\\nBased on this data, here are some useful ways of deriving insights and analysis:\\nSocial Network Cluster Analysis: Who are the major players in the pro-ISIS twitter network? Ideally, we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers.\\nKeyword Analysis: Which keywords derived from the name, username, description, location, and tweets were the most commonly used by ISIS fanboys? Examples include: \"baqiyah\", \"dabiq\", \"wilayat\", \"amaq\"\\nData Categorization of Links: Which websites are pro-ISIS fanboys linking to? Categories include: Mainstream Media, Altermedia, Jihadist Websites, Image Upload, Video Upload,\\nSentiment Analysis: Which clergy do pro-ISIS fanboys quote the most and which ones do they hate the most? Search the tweets for names of prominent clergy and classify the tweet as positive, negative, or neutral and if negative, include the reasons why. Examples of clergy they like the most: \"Anwar Awlaki\", \"Ahmad Jibril\", \"Ibn Taymiyyah\", \"Abdul Wahhab\". Examples of clergy that they hate the most: \"Hamza Yusuf\", \"Suhaib Webb\", \"Yaser Qadhi\", \"Nouman Ali Khan\", \"Yaqoubi\".\\nTimeline View: Visualize all the tweets over a timeline and identify peak moments\\nFurther Reading: \"ISIS Has a Twitter Strategy and It is Terrifying [Infographic]\"\\nAbout Fifth Tribe\\nFifth Tribe is a digital agency based out of DC that serves businesses, non-profits, and government agencies. We provide our clients with product development, branding, web/mobile development, and digital marketing services. Our client list includes Oxfam, Ernst and Young, Kaiser Permanente, Aetna Innovation Health, the U.S. Air Force, and the U.S. Peace Corps. Along with Goldman Sachs International and IBM, we serve on the Private Sector Committee of the Board of the Global Community Engagement and Resilience Fund (GCERF), the first global effort to support local, community-level initiatives aimed at strengthening resilience against violent extremism. In December 2014, we won the anti-ISIS \"Hedaya Hack\" organized by Affinis Labs and hosted at the \"Global Countering Violent Extremism (CVE) Expo \" in Abu Dhabi. Since then, we\\'ve been actively involved in working with the open-source community and community content producers in developing counter-messaging campaigns and tools.',\n",
       " 'Context\\nI created this dataset to investigate the claim that 2016 had an unnaturally large number of celebrity deaths.\\nContent\\nPoints listed by Name, Age, Cause of death and Reason for fame\\nAcknowledgements\\nLifted from: https://en.wikipedia.org/wiki/Deaths_in_2016 for all years',\n",
       " 'Variation of hospital charges in the various hospitals in the US for the top 100 diagnoses.\\nThe dataset is owned by the US government. It is freely available on data.gov The dataset keeps getting updated periodically here\\nThis dataset will show you how price for the same diagnosis and the same treatment and in the same city can vary differently across different providers. It might help you or your loved one find a better hospital for your treatment. You can also analyze to detect fraud among providers.',\n",
       " 'Crowdfunding has become one of the main sources of initial capital for small businesses and start-up companies that are looking to launch their first products. Websites like Kickstarter and Indiegogo provide a platform for millions of creators to present their innovative ideas to the public. This is a win-win situation where creators could accumulate initial fund while the public get access to cutting-edge prototypical products that are not available in the market yet.\\nAt any given point, Indiegogo has around 10,000 live campaigns while Kickstarter has 6,000. It has become increasingly difficult for projects to stand out of the crowd. Of course, advertisements via various channels are by far the most important factor to a successful campaign. However, for creators with a smaller budget, this leaves them wonder,\\n\"How do we increase the probability of success of our campaign starting from the very moment we create our project on these websites?\"\\nData Sources\\nAll of my raw data are scraped from Kickstarter.com.\\nFirst 4000 live projects that are currently campaigning on Kickstarter (live.csv)\\nLast updated: 2016-10-29 5pm PDT\\namt.pledged: amount pledged (float)\\nblurb: project blurb (string)\\nby: project creator (string)\\ncountry: abbreviated country code (string of length 2)\\ncurrency: currency type of amt.pledged (string of length 3)\\nend.time: campaign end time (string \"YYYY-MM-DDThh:mm:ss-TZD\")\\nlocation: mostly city (string)\\npecentage.funded: unit % (int)\\nstate: mostly US states (string of length 2) and others (string)\\ntitle: project title (string)\\ntype: type of location (string: County/Island/LocalAdmin/Suburb/Town/Zip)\\nurl: project url after domain (string)\\nTop 4000 most backed projects ever on Kickstarter (most_backed.csv)\\nLast updated: 2016-10-30 10pm PDT\\namt.pledged\\nblurb\\nby\\ncategory: project category (string)\\ncurrency\\ngoal: original pledge goal (float)\\nlocation\\nnum.backers: total number of backers (int)\\nnum.backers.tier: number of backers corresponds to the pledge amount in pledge.tier (int[len(pledge.tier)])\\npledge.tier: pledge tiers in USD (float[])\\ntitle\\nurl\\nSee more at http://datapolymath.paperplane.io/',\n",
       " \"The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled and diverted flights appears in DOT's monthly Air Travel Consumer Report, published about 30 days after the month's end, as well as in summary tables posted on this website. BTS began collecting details on the causes of flight delays in June 2003. Summary statistics and raw data are made available to the public at the time the Air Travel Consumer Report is released.\\nThis version of the dataset was compiled from the Statistical Computing Statistical Graphics 2009 Data Expo and is also available here.\",\n",
       " 'League of Legends competitive matches between 2015-2017. The matches include the NALCS, EULCS, LCK, LMS, and CBLoL leagues as well as the World Championship and Mid-Season Invitational tournaments.',\n",
       " 'Context\\nThis contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.)\\nSite: http://www.abc.net.au/\\nPrepared by Rohit Kulkarni\\nContent\\nFormat: CSV Rows: 1,103,665\\nColumn 1: publish_date (yyyyMMdd format)\\nColumn 2: headline_text (ascii, lowercase)\\nStart Date: 2003-02-19 End Date: 2017-12-31\\nAcknowledgements\\nSpecial thanks to the java jsoup library.\\nThis dataset is free to use with citation:\\nRohit Kulkarni (2017), A Million News Headlines [CSV Data file], doi:10.7910/DVN/SYBGZL, Retrieved from: [this url]\\nInspiration\\nI look at this news dataset as a summarised historical record of noteworthy events in the globe from early-2003 to end-2017 with a more granular focus on Australia.\\nThis includes the entire corpus of articles published by the ABC website in the given time range. With a volume of 200 articles per day and a good focus on international news, we can be fairly certain that every event of significance has been captured here.\\nDigging into the keywords, one can see all the important episodes shaping the last decade and how they evolved over time. Ex: financial crisis, iraq war, multiple US elections, ecological disasters, terrorism, famous people, Australian crimes etc.\\nSimilar Work\\nYour kernals can be reused with minimal changes across all these datasets\\n3M Clickbait Headlines for 6 years: Examine the Examiner\\n1.3M Global Headlines from 20K sources over 1 week: Global News Week\\n2.6M News Headlines from India from 2001-2017: Headlines of India',\n",
       " 'Context\\nSudoku is a popular number puzzle that requires you to fill blanks in a 9X9 grid with digits so that each column, each row, and each of the nine 3×3 subgrids contains all of the digits from 1 to 9. Sudoku-solving has gained much attention from various fields. As a deep learning researcher, I was inclined to investigate the possibilities of neural networks solving Sudoku. This dataset was prepared for that.\\nContent\\nThere are dozens of source codes to generate Sudoku games available. I picked one of them, and ran the code. It took approximately 6 hours to generate 1 million games ( + solutions).\\nA Sudoku puzzle is represented as a 9x9 Python numpy array. The blanks were replaced with 0\\'s. You can easily load and explore the data by running this.\\nimport numpy as np\\nquizzes = np.load(\\'sudoku_quizzes.npy\\') # shape = (1000000, 9, 9)\\nsolutions = np.load(\\'sudoku_solutions.npy\\') # shape = (1000000, 9, 9)\\nfor quiz, solution in zip(quizzes[:10], solutions[:10]):\\n    print(quiz)\\n    print(solution)\\n** Updates for Version 3. **\\nI converted NumPy arrays to csv so they are easily accessible, irrespective of language. In each line, a Sudoku quiz and its corresponding solution are separated by a comma. You can restore the csv file content to Numpy arrays if needed as follows:\\nimport numpy as np\\nquizzes = np.zeros((1000000, 81), np.int32)\\nsolutions = np.zeros((1000000, 81), np.int32)\\nfor i, line in enumerate(open(\\'sudoku.csv\\', \\'r\\').read().splitlines()[1:]):\\n    quiz, solution = line.split(\",\")\\n    for j, q_s in enumerate(zip(quiz, solution)):\\n        q, s = q_s\\n        quizzes[i, j] = q\\n        solutions[i, j] = s\\nquizzes = quizzes.reshape((-1, 9, 9))\\nsolutions = solutions.reshape((-1, 9, 9))\\nAcknowledgements\\nI\\'m grateful to Arel Cordero, who wrote and shared this great Sudoku generation code. https://www.ocf.berkeley.edu/~arel/sudoku/main.html.\\nInspiration\\nCheck https://github.com/Kyubyong/sudoku to see if CNNs can crack Sudoku puzzles.\\nAlso, reinforcement learning can be a promising alternative to this task.\\nFeel free to challenge Sudoku puzzles.',\n",
       " 'U.S. Opiate Prescriptions\\nAccidental death by fatal drug overdose is a rising trend in the United States. What can you do to help?\\nThis dataset contains summaries of prescription records for 250 common opioid and non-opioid drugs written by 25,000 unique licensed medical professionals in 2014 in the United States for citizens covered under Class D Medicare as well as some metadata about the doctors themselves. This is a small subset of data that was sourced from cms.gov. The full dataset contains almost 24 million prescription instances in long format. I have cleaned and compiled this data here in a format with 1 row per prescriber and limited the approximately 1 million total unique prescribers down to 25,000 to keep it manageable. If you are interested in more data, you can get the script I used to assemble the dataset here and run it yourself. The main data is in prescriber-info.csv. There is also opioids.csv that contains the names of all opioid drugs included in the data and overdoses.csv that contains information on opioid related drug overdose fatalities.\\nThe increase in overdose fatalities is a well-known problem, and the search for possible solutions is an ongoing effort. My primary interest in this dataset is detecting sources of significant quantities of opiate prescriptions. However, there is plenty of other studies to perform, and I am interested to see what other Kagglers will come up with, or if they can improve the model I have already built.\\nThe data consists of the following characteristics for each prescriber\\nNPI – unique National Provider Identifier number\\nGender - (M/F)\\nState - U.S. State by abbreviation\\nCredentials - set of initials indicative of medical degree\\nSpecialty - description of type of medicinal practice\\nA long list of drugs with numeric values indicating the total number of prescriptions written for the year by that individual\\nOpioid.Prescriber - a boolean label indicating whether or not that individual prescribed opiate drugs more than 10 times in the year',\n",
       " 'Each week the CFPB sends thousands of consumers’ complaints about financial products and services to companies for response. Those complaints are published here after the company responds or after 15 days, whichever comes first. By adding their voice, consumers help improve the financial marketplace.',\n",
       " 'Context\\nThe demonetization of ₹500 and ₹1000 banknotes was a step taken by the Government of India on 8 November 2016, ceasing the usage of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as a form of legal tender in India from 9 November 2016.\\nThe announcement was made by the Prime Minister of India Narendra Modi in an unscheduled live televised address to the nation at 20:15 Indian Standard Time (IST) the same day. In the announcement, Modi declared circulation of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as invalid and announced the issuance of new ₹500 and ₹2000 banknotes of the Mahatma Gandhi New Series in exchange for the old banknotes.\\nContent\\nThe data contains 6000 most recent tweets on #demonetization. There are 6000 rows(one for each tweet) and 14 columns.\\nMetadata:\\nText (Tweets)\\nfavorited\\nfavoriteCount\\nreplyToSN\\ncreated\\ntruncated\\nreplyToSID\\nid\\nreplyToUID\\nstatusSource\\nscreenName\\nretweetCount\\nisRetweet\\nretweeted\\nAcknowledgement\\nThe data was collected using the \"twitteR\" package in R using the twitter API.\\nPast Research\\nI have performed my own analysis on the data. I only did a sentiment analysis and formed a word cloud.\\nClick here to see the analysis on GitHub\\nInspiration\\nWhat percentage of tweets are negative, positive or neutral ?\\nWhat are the most famous/re-tweeted tweets ?',\n",
       " 'Our first glimpse at planets outside of the solar system we call home came in 1992 when several terrestrial-mass planets were detected orbiting the pulsar PSR B1257+12. In this dataset, you can become a space explorer too by analyzing the characteristics of all discovered exoplanets (plus some familiar faces like Mars, Saturn, and even Earth). Data fields include planet and host star attributes, discovery methods, and (of course) date of discovery.\\nData was originally collected and continues to be updated by Hanno Rein at the Open Exoplanet Catalogue Github repository. If you discover any new exoplanets, please submit a pull request there.\\nConstants\\nJupiter mass: 1.8991766e+27 kg\\nSolar mass: 1.9891e+30 kg\\nJupiter radius: 69911000 m\\nSolar radius: 6.96e+08 m\\nLicense\\nThe database is licensed under an MIT license. If you use it for a scientific publication, please include a reference to the Open Exoplanet Catalogue on GitHub or to this arXiv paper.',\n",
       " \"Context\\nThis classic dataset contains the prices and other attributes of almost 54,000 diamonds. It's a great dataset for beginners learning to work with data analysis and visualization.\\nContent\\nprice price in US dollars (\\\\$326--\\\\$18,823)\\ncarat weight of the diamond (0.2--5.01)\\ncut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\\ncolor diamond colour, from J (worst) to D (best)\\nclarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\\nx length in mm (0--10.74)\\ny width in mm (0--58.9)\\nz depth in mm (0--31.8)\\ndepth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\\ntable width of top of diamond relative to widest point (43--95)\",\n",
       " 'Context\\nThe two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\\nThese datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\\nThis dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so, I will take this down if requested.)\\nContent\\nFor more information, read [Cortez et al., 2009].\\nInput variables (based on physicochemical tests):\\n1 - fixed acidity\\n2 - volatile acidity\\n3 - citric acid\\n4 - residual sugar\\n5 - chlorides\\n6 - free sulfur dioxide\\n7 - total sulfur dioxide\\n8 - density\\n9 - pH\\n10 - sulphates\\n11 - alcohol\\nOutput variable (based on sensory data):\\n12 - quality (score between 0 and 10)\\nTips\\nWhat might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as \\'good/1\\' and the remainder as \\'not good/0\\'. This allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value. Without doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm)\\nKNIME is a great tool (GUI) that can be used for this.\\n1 - File Reader (for csv) to linear correlation node and to interactive histogram for basic EDA.\\n2- File Reader to \\'Rule Engine Node\\' to turn the 10 point scale to dichtome variable (good wine and rest), the code to put in the rule engine is something like this:\\n- $quality$ > 6.5 => \"good\"\\n- TRUE => \"bad\"\\n3- Rule Engine Node output to input of Column Filter node to filter out your original 10point feature (this prevent leaking)\\n4- Column Filter Node output to input of Partitioning Node (your standard train/tes split, e.g. 75%/25%, choose \\'random\\' or \\'stratified\\')\\n5- Partitioning Node train data split output to input of Train data split to input Decision Tree Learner node and\\n6- Partitioning Node test data split output to input Decision Tree predictor Node\\n7- Decision Tree learner Node output to input Decision Tree Node input\\n8- Decision Tree output to input ROC Node.. (here you can evaluate your model base on AUC value)\\nInspiration\\nUse machine learning to determine which physiochemical properties make a wine \\'good\\'!\\nAcknowledgements\\nThis dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (I am mistaken and the public license type disallowed me from doing so, I will take this down at first request. I am not the owner of this dataset.\\nPlease include this citation if you plan to use this database: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\\nRelevant publication\\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.',\n",
       " \"Context\\nKaggle has more and more computer vision challenges. Although Kernel resources were increased recently we still can not train useful CNNs without GPU. The other main problem is that Kernels can't use network connection to download pretrained keras model weights. This dataset helps you to apply your favorite pretrained model in the Kaggle Kernel environment.\\nHappy data exploration and transfer learning!\\nContent\\nModel (Top-1 Accuracy | Top -5 Accuracy)\\nXception (0.790 | 0.945)\\nVGG16 (0.715 | 0.901)\\nVGG19 (0.727 | 0.910)\\nResNet50 (0.759 | 0.929)\\nInceptionV3 (0.788 | 0.944)\\nInceptionResNetV2 (0.804 | 0.953) (could not upload due to 500 MB limit)\\nFor more information see https://keras.io/applications/\\nAcknowledgements\\nThanks to François Chollet for collecting these models and for the awesome keras.\",\n",
       " 'Context\\nThe National Earthquake Information Center (NEIC) determines the location and size of all significant earthquakes that occur worldwide and disseminates this information immediately to national and international agencies, scientists, critical facilities, and the general public. The NEIC compiles and provides to scientists and to the public an extensive seismic database that serves as a foundation for scientific research through the operation of modern digital national and global seismograph networks and cooperative international agreements. The NEIC is the national data center and archive for earthquake information.\\nContent\\nThis dataset includes a record of the date, time, location, depth, magnitude, and source of every earthquake with a reported magnitude 5.5 or higher since 1965.\\nStart a new kernel',\n",
       " \"Context\\nFree Code Camp is an open source community where you learn to code and build projects for nonprofits. CodeNewbie.org is the most supportive community of people learning to code. Together, we surveyed more than 15,000 people who are actively learning to code. We reached them through the twitter accounts and email lists of various organizations that help people learn to code. Our goal was to understand these people's motivations in learning to code, how they're learning to code, their demographics, and their socioeconomic background. We've written in depth about this dataset.\\nIn May 2017 we just released an even bigger open dataset with our 2017 survey results.\",\n",
       " 'This data set was used to train a CrowdFlower AI gender predictor. You can read all about the project here. Contributors were asked to simply view a Twitter profile and judge whether the user was a male, a female, or a brand (non-individual). The dataset contains 20,000 rows, each with a user name, a random tweet, account profile and image, location, and even link and sidebar color.\\nInspiration\\nHere are a few questions you might try to answer with this dataset:\\nhow well do words in tweets and profiles predict user gender?\\nwhat are the words that strongly predict male or female gender?\\nhow well do stylistic factors (like link color and sidebar color) predict user gender?\\nAcknowledgments\\nData was provided by the Data For Everyone Library on Crowdflower.\\nOur Data for Everyone library is a collection of our favorite open data jobs that have come through our platform. They\\'re available free of charge for the community, forever.\\nThe Data\\nThe dataset contains the following fields:\\n_unit_id: a unique id for user\\n_golden: whether the user was included in the gold standard for the model; TRUE or FALSE\\n_unit_state: state of the observation; one of finalized (for contributor-judged) or golden (for gold standard observations)\\n_trusted_judgments: number of trusted judgments (int); always 3 for non-golden, and what may be a unique id for gold standard observations\\n_last_judgment_at: date and time of last contributor judgment; blank for gold standard observations\\ngender: one of male, female, or brand (for non-human profiles)\\ngender:confidence: a float representing confidence in the provided gender\\nprofile_yn: \"no\" here seems to mean that the profile was meant to be part of the dataset but was not available when contributors went to judge it\\nprofile_yn:confidence: confidence in the existence/non-existence of the profile\\ncreated: date and time when the profile was created\\ndescription: the user\\'s profile description\\nfav_number: number of tweets the user has favorited\\ngender_gold: if the profile is golden, what is the gender?\\nlink_color: the link color on the profile, as a hex value\\nname: the user\\'s name\\nprofile_yn_gold: whether the profile y/n value is golden\\nprofileimage: a link to the profile image\\nretweet_count: number of times the user has retweeted (or possibly, been retweeted)\\nsidebar_color: color of the profile sidebar, as a hex value\\ntext: text of a random one of the user\\'s tweets\\ntweet_coord: if the user has location turned on, the coordinates as a string with the format \"[latitude, longitude]\"\\ntweet_count: number of tweets that the user has posted\\ntweet_created: when the random tweet (in the text column) was created\\ntweet_id: the tweet id of the random tweet\\ntweet_location: location of the tweet; seems to not be particularly normalized\\nuser_timezone: the timezone of the user',\n",
       " 'Overview\\nThe dataset is designed to allow for different methods to be tested for examining the trends in CT image data associated with using contrast and patient age. The basic idea is to identify image textures, statistical patterns and features correlating strongly with these traits and possibly build simple tools for automatically classifying these images when they have been misclassified (or finding outliers which could be suspicious cases, bad measurements, or poorly calibrated machines)\\nData\\nThe data are a tiny subset of images from the cancer imaging archive. They consist of the middle slice of all CT images taken where valid age, modality, and contrast tags could be found. This results in 475 series from 69 different patients.\\nTCIA Archive Link - https://wiki.cancerimagingarchive.net/display/Public/TCGA-LUAD\\nLicense\\nhttp://creativecommons.org/licenses/by/3.0/\\nAfter the publication embargo period ends these collections are freely available to browse, download, and use for commercial, scientific and educational purposes as outlined in the Creative Commons Attribution 3.0 Unported License. Questions may be directed to help@cancerimagingarchive.net. Please be sure to acknowledge both this data set and TCIA in publications by including the following citations in your work:\\nData Citation\\nAlbertina, B., Watson, M., Holback, C., Jarosz, R., Kirk, S., Lee, Y., … Lemmerman, J. (2016). Radiology Data from The Cancer Genome Atlas Lung Adenocarcinoma [TCGA-LUAD] collection. The Cancer Imaging Archive. http://doi.org/10.7937/K9/TCIA.2016.JGNIHEP5\\nTCIA Citation\\nClark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F. The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045-1057. (paper)',\n",
       " \"Context\\nWell, basically what happened was I was looking for a semi-definite easy to read list of international football matches and couldn't find anything decent. So I took it upon myself to collect it for my own use. I might as well share it.\\nContent\\nThis dataset includes 38,759 results of international football matches starting from the very first official match in 1972 up to 2018. The matches range from World Cup to Baltic Cup to regular friendly matches. The matches are strictly men's full internationals and the data does not include Olympic Games or matches where at least one of the teams was the nation's B-team, U-23 or a league select team.\\nresults.csv includes the following columns:\\ndate\\nhome_team\\naway_team\\nhome_score\\naway_score\\ntournament\\ncity\\ncountry\\nAcknowledgements\\nThe data is gathered from several sources including but not limited to wikipedia, fifa.com, rsssf.com and individual football associations' websites.\\nInspiration\\nSome directions to take when exploring the data:\\nWhich teams dominated different eras of football\\nWho is the best team of all time\\nWhat trends have there been in international football throughout the ages - home advantage, total goals scored, distribution of teams' strength etc\\nCan we say anything about geopolitics from football fixtures - how has the number of countries changed, which teams like to play each other\\nWhich countries host the most matches where they themselves are not participating in\\nHow much, if at all, does hosting a major tournament help a country's chances in said tournament\\nWhich teams are the most active in playing friendlies and friendly tournaments - does it help or harm them\\nDo you dare to make any predictions for 2018 World Cup based on this data?\\nand so on...\\nThe world's your oyster, my friend.\",\n",
       " 'Context\\nMusic streaming is ubiquitous. Currently, Spotify plays an important part on that. This dataset enable us to explore how artists and songs\\' popularity varies in time.\\nContent\\nThis dataset contains the daily ranking of the 200 most listened songs in 53 countries from 2017 and 2018 by Spotify users. It contains more than 2 million rows, which comprises 6629 artists, 18598 songs for a total count of one hundred five billion streams count.\\nThe data spans from 1st January 2017 to 9th January 2018 and will be kept up-to-date on following versions. It has been collected from Spotify\\'s regional chart data.\\nInspiration\\nCan you predict what is the rank position or the number of streams a song will have in the future?\\nHow long does songs \"resist\" on the top 3, 5, 10, 20 ranking?\\nWhat are the signs of a song that gets into the top rank to stay?\\nDo continents share same top ranking artists or songs?\\nAre people listening to the very same top ranking songs on countries far away from each other?\\nHow long time does a top ranking song takes to get into the ranking of neighbor countries?\\nExample\\nTo start out, you can take a look into a simple Kernel I have made in order to read the data, filter data from a song, plot is temporal tendency per country than make a simple forecast of the its streams count here.\\nCrawler\\nThe crawler used to collect this data can be found here.',\n",
       " \"Context\\nTo better follow the energy consumption, the government wants energy suppliers to install smart meters in every home in England, Wales and Scotland. There are more than 26 million homes for the energy suppliers to get to, with the goal of every home having a smart meter by 2020.\\nThis roll out of meter is lead by the European Union who asked all member governments to look at smart meters as part of measures to upgrade our energy supply and tackle climate change. After an initial study, the British government decided to adopt smart meters as part of their plan to update our ageing energy system.\\nIn this dataset, you will find a refactorised version of the data from the London data store, that contains the energy consumption readings for a sample of 5,567 London Households that took part in the UK Power Networks led Low Carbon London project between November 2011 and February 2014. The data from the smart meters seems associated only to the electrical consumption.\\nThere is infomations on the ACORN classification details that you can find in this report or the website of CACI.\\nI added weather data for London area, I used the darksky api to collect this data.\\nContent\\nThere is 19 files in this dataset :\\ninformations_households.csv : this file that contains all the information on the households in the panel (their acorn group, their tariff) and in which block.csv.gz file their data are stored\\nhalfhourly_dataset.zip: Zip file that contains the block files with the half-hourly smart meter measurement\\ndaily_dataset.zip: Zip file that contains the block files with the daily information like the number of measures, minimum, maximum, mean, median, sum and std.\\nacorn_details.csv : Details on the acorn groups and their profile of the people in the group, it's come from this xlsx spreadsheet.The first three columns are the attributes studied, the ACORN-X is the index of the attribute. At a national scale, the index is 100 if for one column the value is 150 it means that there are 1.5 times more people with this attribute in the ACORN group than at the national scale. You can find an explanation on the CACI website\\nweather_daily_darksky.csv : that contains the daily data from darksky api. You can find more details about the parameters in the documentation of the api\\nweather_hourly_darksky.csv : that contains the hourly data from darksky api. You can find more details about the parameters in the documentation of the api\\nAcknowledgements\\nAll the big work of data collection has been done by the UK power networks for the smart meter data.\\nThe details related at the acorn group are provided by the CACI.\\nThe weather data are from darksky.\\nInspiration\\nFor me some ideas to analyze the data:\\nSegmentation of the consumption daily pattern\\nDisaggregation of the electricity load curve\\nCross the consumption result and the acorn information\\nForecast the electricity consumption of a household, I wrote an article on this subject\\nWhat if I add electrical heating system ? an EV battery system ?\\nForecast at a global scale (London consumption)\",\n",
       " 'Context:\\nLEGO is a popular brand of toy building bricks. They are often sold in sets with in order to build a specific object. Each set contains a number of parts in different shapes, sizes and colors. This database contains information on which parts are included in different LEGO sets. It was originally compiled to help people who owned some LEGO sets already figure out what other sets they could build with the pieces they had.\\nContent:\\nThis dataset contains the LEGO Parts/Sets/Colors and Inventories of every official LEGO set in the Rebrickable database. These files are current as of July 2017. If you need it to be more recent data, you can use Rebrickable’s API which provides up to date data, and additional features.\\nAcknowledgements:\\nThis dataset was compiled by Rebrickable, which is a website to help identify what LEGO sets can be built given bricks and pieces from other LEGO sets. You can use these files for any purpose.\\nInspiration:\\nThis is a very rich dataset that offers lots of rooms for exploration, especially since the “sets” file includes the year in which a set was first released.\\nHow have the size of sets changed over time?\\nWhat colors are associated with witch themes? Could you predict which theme a set is from just by the bricks it contains?\\nWhat sets have the most-used pieces in them? What sets have the rarest pieces in them?\\nHave the colors of LEGOs included in sets changed over time?',\n",
       " 'Context\\nThe UK government amassed traffic data from 2000 and 2016, recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. It\\'s a huge picture of a country undergoing change.\\nNote that all the contained accident data comes from police reports, so this data does not include minor incidents.\\nContent\\nukTrafficAADF.csv tracks how much traffic there was on all major roads in the given time period (2000 through 2016). AADT, the core statistic included in this file, stands for \"Average Annual Daily Flow\", and is a measure of how activity a road segment based on how many vehicle trips traverse it. The AADT page on Wikipedia is a good reference on the subject.\\nAccidents data is split across three CSV files: accidents_2005_to_2007.csv, accidents_2009_to_2011.csv, and accidents_2012_to_2014.csv. These three files together constitute 1.6 million traffic accidents. The total time period is 2005 through 2014, but 2008 is missing.\\nA data dictionary for the raw dataset at large is available from the UK Department of Transport website here. For descriptions of individual columns, see the column metadata.\\nAcknowledgements\\nThe license for this dataset is the Open Givernment Licence used by all data on data.gov.uk (here). The raw datasets are available from the UK Department of Transport website here.\\nInspiration\\nHow has changing traffic flow impacted accidents?\\nCan we predict accident rates over time? What might improve accident rates?\\nPlot interactive maps of changing trends, e.g. How has London has changed for cyclists? Busiest roads in the nation?\\nWhich areas never change and why? Identify infrastructure needs, failings and successes.\\nHow have Rural and Urban areas differed (see RoadCategory)? How about the differences between England, Scotland, and Wales?\\nThe UK government also like to look at miles driven. You can do this by multiplying the AADF by the corresponding length of road (link length) and by the number of days in the years. What does this tell you about UK roads?',\n",
       " 'This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year.\\nDescription of fnlwgt (final weight)\\nThe weights on the Current Population Survey (CPS) files are controlled to independent estimates of the civilian noninstitutional population of the US. These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are:\\nA single cell estimate of the population 16+ for each state.\\nControls for Hispanic Origin by age and sex.\\nControls by Race, age and sex.\\nWe use all three sets of controls in our weighting program and \"rake\" through them 6 times so that by the end we come back to all the controls we used. The term estimate refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. There is one important caveat to remember about this statement. That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.\\nRelevant papers\\nRon Kohavi, \"Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\", Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996. (PDF)',\n",
       " 'Context: Daily horse racing (thoroughbred) information that has(is) being actively collected and aggregated from a variety of sources. Years covered are just 2016, country is irrelevant to the dataset.\\nAcknowledgements: This data has(is) being actively collected and aggregated from a variety of sources, all in the public domain.\\nPast Research: None of merit, data is used currently to influence some betting decisions but no solid machine learning model(s) have been developed.\\nHave thrown various versions of the data into:\\nGoogle Prediction\\nAmazon Machine Learning\\nAzure Machine Learning\\nWatson Analytics\\nas a way to learn how these systems work.\\nInspiration: Probably one of the hardest things to do is pick stocks and horses. I have been involved in the stocks and horses industry for many years and through publishing previous libraries and software I have met many interesting people and also one of my long term clients/friends.\\nI am currently trying enhance my software development skills by learning data science / machine learning.\\nI have a done a few tutorials and I am hoping that by publishing this data I can learn and collaborate with members of the Kaggle Community.\\nContent:\\nmarkets.csv\\nid\\nstart_time\\nwhat time did the race start, datetime in UTC\\nvenue_id\\nrace_number\\ndistance(m)\\ncondition_id\\ntrack condition, see conditions.csv\\nweather_id\\nweather on day, see weathers.csv\\ntotal_pool_win_one\\nrough $ amount wagered across all runners for win market\\ntotal_pool_place_one\\nrough $ amount wagered across all runners for place market\\ntotal_pool_win_two\\ntotal_pool_place_two\\ntotal_pool_win_three\\ntotal_pool_place_three\\nrunners.csv\\nid\\ncollected\\nwhat time was this row created/data collected, datetime in UTC\\nmarket_id\\nposition\\nTHIS IS THE FIELD WE WANT TO PREDICT!!!!\\nWill either be 1,2,3,4,5,6 etc or 0/null if the horse was scratched or failed to finish\\nIf all positions for a market_id are null it means we were unable to match up the positional data for this market\\nplace_paid\\nWill either be 1/0 or null\\nIf you see a race that only has 2 booleans of 1 it means that the race only paid out places on the first two positions\\nmargin\\nIf the runner didnt win, how many lengths behind the 1st place was it\\nhorse_id\\nsee horses.csv\\ntrainer_id\\nrider_id\\nsee riders.csv\\nhandicap_weight\\nnumber\\nbarrier\\nblinkers\\nemergency\\ndid it come into the race at the last minute\\nform_rating_one\\nform_rating_two\\nform_rating_three\\nlast_five_starts\\nfavourite_odds_win\\nfrom one of the odds sources, will it win - true/false\\nfavourite_odds_place\\nfrom one of the odds sources, will it win - true/false\\nfavourite_pool_win\\nfavourite_pool_place\\ntip_one_win\\nfrom a tipster, will it win - true/false\\ntip_one_place\\nfrom a tipster, will it place - true/false\\ntip_two_win\\ntip_two_place\\ntip_three_win\\ntip_three_place\\ntip_four_win\\ntip_four_place\\ntip_five_win\\ntip_five_place\\ntip_six_win\\ntip_six_place\\ntip_seven_win\\ntip_seven_place\\ntip_eight_win\\ntip_eight_place\\ntip_nine_win\\ntip_nine_place\\nodds.csv (collected for every runner 10 minutes out from race start until race starts)\\nrunner_id\\ncollected\\nwhat time was this row created/data collected, datetime in UTC\\nodds_one_win\\nfrom odds source, win odds\\nodds_one_win_wagered\\nfrom odds source, rough $ amount wagered on win\\nodds_one_place\\nfrom odds source, place odds\\nodds_one_place_wagered\\nfrom odds source, rough $ amount wagered on place\\nodds_two_win\\nodds_two_win_wagered\\nodds_two_place\\nodds_two_place_wagered\\nodds_three_win\\nodds_three_win_wagered\\nodds_three_place\\nodds_three_place_wagered\\nodds_four_win\\nodds_four_win_wagered\\nodds_four_place\\nodds_four_place_wagered\\nforms.csv\\ncollected\\nwhat time was this row created/data collected, datetime in UTC\\nmarket_id\\nhorse_id\\nrunner_number\\nlast_twenty_starts\\ne.g. f9x726x753x92222x35\\nf = failed to finish, 7 = finished 7th, 6 = finished 6th, 7 = finished 7th, x = runner was scratched\\nclass_level_id\\n1 = eq (in same class as other horses)\\n2 = up (up in class)\\n3 = dn (down in class)\\nfield_strength\\ndays_since_last_run\\nruns_since_spell\\noverall_starts\\noverall_wins\\noverall_places\\ntrack_starts\\ntrack_wins\\ntrack_places\\nfirm_starts\\nfirm_wins\\nfirm_places\\ngood_starts\\ngood_wins\\ngood_places\\ndead_starts\\ndead_wins\\ndead_places\\nslow_starts\\nslow_wins\\nslow_places\\nsoft_starts\\nsoft_wins\\nsoft_places\\nheavy_starts\\nheavy_wins\\nheavy_places\\ndistance_starts\\ndistance_wins\\ndistance_places\\nclass_same_starts\\nclass_same_wins\\nclass_same_places\\nclass_stronger_starts\\nclass_stronger_wins\\nclass_stronger_places\\nfirst_up_starts\\nfirst_up_wins\\nfirst_up_places\\nsecond_up_starts\\nsecond_up_wins\\nsecond_up_places\\ntrack_distance_starts\\ntrack_distance_wins\\ntrack_distance_places\\nconditions.csv\\nid\\nname\\nweathers.csv\\nid\\nname\\nriders.csv (jockeys)\\nid\\nsex\\nhorses.csv\\nid\\nage\\nsex_id\\nsee horse_sexes.csv\\nsire_id\\nnot related to horses.id, there is another table called horse_sires that is not present here\\ndam_id\\nnot related to horses.id, there is another table called horse_dams that is not present here\\nprize_money\\ntotal aggregate prize money\\nhorse_sexes.csv\\nid\\nname',\n",
       " 'Outline\\nIt was reported that an estimated 4292,000 new cancer cases and 2814,000 cancer deaths would occur in China in 2015. Chen, W., etc. (2016), Cancer statistics in China, 2015.\\nSmall molecules play an non-trivial role in cancer chemotherapy. Here I focus on inhibitors of 8 protein kinases(name: abbr):\\nCyclin-dependent kinase 2: cdk2\\nEpidermal growth factor receptor erbB1: egfr_erbB1\\nGlycogen synthase kinase-3 beta: gsk3b\\nHepatocyte growth factor receptor: hgfr\\nMAP kinase p38 alpha: map_k_p38a\\nTyrosine-protein kinase LCK: tpk_lck\\nTyrosine-protein kinase SRC: tpk_src\\nVascular endothelial growth factor receptor 2: vegfr2\\nFor each protein kinase, several thousand inhibitors are collected from chembl database, in which molecules with IC50 lower than 10 uM are usually considered as inhibitors, otherwise non-inhibitors.\\nChallenge\\nBased on those labeled molecules, build your model, and try to make the right prediction.\\nAdditionally, more than 70,000 small molecules are generated from pubchem database. And you can screen these molecules to find out potential inhibitors. P.S. the majority of these molecules are non-inhibitors.\\nDataSets(hdf5 version)\\nThere are 8 protein kinase files and 1 pubchem negative samples file. Taking \"cdk2.h5\" as an example:\\nimport h5py\\nfrom scipy import sparse\\nhf = h5py.File(\"../input/cdk2.h5\", \"r\")\\nids = hf[\"chembl_id\"].value # the name of each molecules\\nap = sparse.csr_matrix((hf[\"ap\"][\"data\"], hf[\"ap\"][\"indices\"], hf[\"ap\"][\"indptr\"]), shape=[len(hf[\"ap\"][\"indptr\"]) - 1, 2039])\\nmg = sparse.csr_matrix((hf[\"mg\"][\"data\"], hf[\"mg\"][\"indices\"], hf[\"mg\"][\"indptr\"]), shape=[len(hf[\"mg\"][\"indptr\"]) - 1, 2039])\\ntt = sparse.csr_matrix((hf[\"tt\"][\"data\"], hf[\"tt\"][\"indices\"], hf[\"tt\"][\"indptr\"]), shape=[len(hf[\"tt\"][\"indptr\"]) - 1, 2039])\\nfeatures = sparse.hstack([ap, mg, tt]).toarray() # the samples\\' features, each row is a sample, and each sample has 3*2039 features\\nlabels = hf[\"label\"].value # the label of each molecule',\n",
       " \"Food choices and preferences of college students\\nThis dataset includes information on food choices, nutrition, preferences, childhood favorites, and other information from college students. There are 126 responses from students. Data is raw and uncleaned. Cleaning is in the process and as soon as that is done, additional versions of the data will be posted. Acknowledgements\\nThank you to all the students of Mercyhurst University who agreed to participate in this survey.\\nInspiration\\nHow important is nutrition information for today's college kids? Is their taste in food defined by their food preferences when they were children? Are kids of parents who cook more likely to make better food choices than others? Are these kids likely to have a different taste compared to others? There a number of open ended questions included in this dataset such as: What is your favorite comfort food? What is your favorite cuisine? that could work well for natural language processing\",\n",
       " \"Context\\nThis dataset is an extension of that found here. It contains several extra fields and is pre-cleaned to a much greater extent. After talking with the creator of the original dataset, he and I agreed that merging our work would require making breaking changes to the original, and that this should be published as a new dataset.\\nContent\\n185 fields for every player in FIFA 18.\\nPlayer info such as age, club, league, nationality, salary and physical attributes\\nAll playing attributes, such as finishing and dribbling\\nSpecial attributes like skill moves and international reputation\\nTraits and specialities\\nOverall, potential, and ratings for each position\\nDifferences\\nHere are the columns in this dataset that aren't in the original:\\nbirth_date\\neur_release_clause\\nheight_cm\\nweight_kg\\nbody_type\\nreal_face\\nleague\\nHeadline attributes: pac, sho, pas, dri, def, and phy. These are what appear on Ultimate Team cards\\ninternational_reputation\\nskill_moves\\nweak_foot\\nwork_rate_att\\nwork_rate_def\\npreferred_foot\\nall traits and specialities as dummy variables\\nall position preferences as dummy variables\\nAcknowledgements\\nCredit goes to Aman Shrivastava for building the original dataset. And thanks of course to https://sofifa.com for not banning my IP when I scraped over 18000 pages to get this data.\\nInspiration\\nWhat insights can this data give us, not only into FIFA 18 but into real-world football? The kernels on last year's dataset are a good place to find ideas.\\nContributing\\nContributions to the GitHub project are more than welcome. Do let me know if you think of ways to improve either the code or the dataset!\",\n",
       " \"This data originally came from Crowdflower's Data for Everyone library.\\nAs the original source says,\\nWe looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from the uploaded dataset.\\nThe data we're providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is available on GitHub\",\n",
       " \"Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.\\nThis year, Kaggle is hosting the NIPS 2015 paper dataset to facilitate and showcase exploratory analytics on the NIPS data. We've extracted the paper text from the raw PDF files and are releasing that both in CSV files and as a SQLite database. Here's a quick script that gives an overview of what's included in the data.\\nWe encourage you to explore this data and share what you find through Kaggle Scripts!\\nData Description\\nOverview of the data in Kaggle Scripts.\\nnips-2015-papers-release-*.zip (downloadable from the link above) contains the below files/folders. All this data's available through Kaggle Scripts as well, and you can create a new script to immediately start exploring the data in R, Python, Julia, or SQLite.\\nThis dataset is available in two formats: three CSV files and a single SQLite database (consisting of three tables with content identical to the CSV files).\\nYou can see the code used to create this dataset on Github.\\nPapers.csv\\nThis file contains one row for each of the 403 NIPS papers from this year's conference. It includes the following fields\\nId - unique identifier for the paper (equivalent to the one in NIPS's system)\\nTitle - title of the paper\\nEventType - whether it's a poster, oral, or spotlight presentation\\nPdfName - filename for the PDF document\\nAbstract - text for the abstract (scraped from the NIPS website)\\nPaperText - raw text from the PDF document (created using the tool pdftotext)\\nAuthors.csv\\nThis file contains id's and names for each of the authors on this year's NIPS papers.\\nId - unique identifier for the author (equivalent to the one in NIPS's system)\\nName - author's name\\nPaperAuthors.csv\\nThis file links papers to their corresponding authors.\\nId - unique identifier\\nPaperId - id for the paper\\nAuthorId - id for the author\\ndatabase.sqlite\\nThis SQLite database contains the tables with equivalent data and formatting as the Papers.csv, Authors.csv, and PaperAuthors.csv files.\\npdfs\\nThis folder contains the raw pdf files for each of the papers.\",\n",
       " 'This dataset contains data for the entire collection of cards for Hearthstone, the popular online card game by Blizzard. Launching to the public on March 11, 2011 after being under development for almost 5 years, Hearthstone has gained popularity as a freemium game, launching into eSports across the globe, and the source of many Twitch channels.\\nThe data in this dataset was extracted from hearthstonejson.com, and the documentation for all the data can be found on the cards.json documentation page.\\nThe original data was extracted from the actual card data files used in the game, so all of the data should be here, enabling explorations like:\\nCard strengths and weaknesses\\nCard strengths relative to cost and rarity\\nComparisons across player classes, bosses, and sets\\nWhether a set of optimal cards can be determined per class\\nThe cards can be explored in one of four ways:\\ncards.json: The raw JSON pulled from hearthstonejson.com\\ncards_flat.csv: A flat CSV containing a row for each card, and any n:m data stored as arrays in single fields\\ndatabase.sqlite: A SQLite database containing relational data of the cards\\ncards.csv, mechanics.csv, dust_costs.csv, play_requirements.csv, and entourages.csv: the normalized data in CSV format.\\nThis dataset will be updated as new releases and expansions are made to Hearthstone.\\nCurrently, any localized string values are in en-us, but I may look into adding other languages if the demand seems to be there.',\n",
       " 'Introduction\\nThe lack of publicly available National Football League (NFL) data sources has been a major obstacle in the creation of modern, reproducible research in football analytics. While clean play-by-play data is available via open-source software packages in other sports (e.g. nhlscrapr for hockey; PitchF/x data in baseball; the NBA API for basketball), the equivalent datasets are not freely available for researchers interested in the statistical analysis of the NFL. To solve this issue, a group of Carnegie Mellon University statistical researchers led by recent graduate, Maksim Horowitz, built and released nflscrapR an R package which uses an API maintained by the NFL to scrape, clean, parse, and output clean datasets at the individual play, player, game, and season levels. These datasets allow for the advancement of NFL research in the public domain by allowing analysts to develop from a common source in order to create reproducible NFL research, similar to what is being done currently in other professional sports.\\n2015 NFL Play-by-Play Dataset\\nThe dataset made available on Kaggle contains all the regular season plays from the 2015-2016 NFL season. The dataset contain 46,129 rows and 63 columns. Each play is broken down into great detail containing information on; game situation, players involved and results. Detailed information about the dataset can be found in the nflscrapR documentation.\\nDownloading and Installing nflscrapR:\\nUse the following code in your R console:\\n# Must install the devtools package using the below code\\ninstall.packages(\\'devtools\\')\\nlibrary(devtools)\\n# For now you must install nflscrapR from github\\nif (!is.element(\"nflscrapR\", installed.packages())) {\\n    # Print Installing nflscrapR\\n    devtools::install_github(repo = \"maksimhorowitz/nflscrapR\")\\n}\\n\\nlibrary(nflscrapR)',\n",
       " 'Context\\nWith the rise of the popularity of machine learning, this is a good opportunity to share a wide database of the even more popular video-game Pokémon by Nintendo, Game freak, and Creatures, originally released in 1996.\\nPokémon started as a Role Playing Game (RPG), but due to its increasing popularity, its owners ended up producing many TV series, manga comics, and so on, as well as other types of video-games (like the famous Pokémon Go!).\\nThis dataset is focused on the stats and features of the Pokémon in the RPGs. Until now (08/01/2017) seven generations of Pokémon have been published. All in all, this dataset does not include the data corresponding to the last generation, since 1) I created the databased when the seventh generation was not released yet, and 2) this database is a modification+extension of the database \"721 Pokemon with stats\" by Alberto Barradas (https://www.kaggle.com/abcsds/pokemon), which does not include (of course) the latest generation either.\\nContent\\nThis database includes 21 variables per each of the 721 Pokémon of the first six generations, plus the Pokémon ID and its name. These variables are briefly described next:\\nNumber. Pokémon ID in the Pokédex.\\nName. Name of the Pokémon.\\nType_1. Primary type.\\nType_2. Second type, in case the Pokémon has it.\\nTotal. Sum of all the base stats (Health Points, Attack, Defense, Special Attack, Special Defense, and Speed).\\nHP. Base Health Points.\\nAttack. Base Attack.\\nDefense. Base Defense.\\nSp_Atk. Base Special Attack.\\nSp_Def. Base Special Defense.\\nSpeed. Base Speed.\\nGeneration. Number of the generation when the Pokémon was introduced.\\nisLegendary. Boolean that indicates whether the Pokémon is Legendary or not.\\nColor. Color of the Pokémon according to the Pokédex.\\nhasGender. Boolean that indicates if the Pokémon can be classified as female or male.\\nPr_male. In case the Pokémon has Gender, the probability of its being male. The probability of being female is, of course, 1 minus this value.\\nEgg_Group_1. Egg Group of the Pokémon.\\nEgg_Group_2. Second Egg Group of the Pokémon, in case it has two.\\nhasMegaEvolution. Boolean that indicates whether the Pokémon is able to Mega-evolve or not.\\nHeight_m. Height of the Pokémon, in meters.\\nWeight_kg. Weight of the Pokémon, in kilograms.\\nCatch_Rate. Catch Rate.\\nBody_Style. Body Style of the Pokémon according to the Pokédex.\\nNotes\\nPlease note that many Pokémon are multi-form, and also some of them can Mega-evolve. I wanted to keep the structure of the dataset as simple and general as possible, as well as the Number variable (the ID of the Pokémon) unique. Hence, in the cases of the multi-form Pokémon, or the ones capable of Mega-evolve, I just chose one of the forms, the one I (and my brother) considered the standard and/or the most common. The specific choice for each of this Pokémon are shown below:\\nMega-Evolutions are not considered as Pokémon.\\nKyogre, Groudon. Primal forms not considered.\\nDeoxis. Only normal form considered.\\nWormadam. Only plant form considered.\\nRotom. Only normal form considered, the one with types Electric and Ghost.\\nGiratina. Origin form considered.\\nShaymin. Land form considered.\\nDarmanitan. Standard mode considered.\\nTornadus, Thundurus, Landorus. Incarnate form considered.\\nKyurem. Normal form considered, not white or black forms.\\nMeloetta. Aria form considered.\\nMewstic. Both female and male forms are equal in the considered variables.\\nAegislash. Shield form considered.\\nPumpkaboo, Gourgeist. Average size considered.\\nZygarde. 50% form considered.\\nHoopa. Confined form considered.\\nAcknowledgements\\nAs said at the beginning, this database was based on the Kaggle database \"721 Pokemon with stats\" by Alberto Barradas (https://www.kaggle.com/abcsds/pokemon). The other resources I mainly used are listed below:\\nWikiDex (http://es.pokemon.wikia.com/wiki/WikiDex).\\nBulbapedia, the community driven Pokémon encyclopedia (http://bulbapedia.bulbagarden.net/wiki/Main_Page).\\nSmogon University (http://www.smogon.com/).\\nPossible future work\\nThis dataset can be used with different objectives, such as, Pokémon clustering, trying to find relations or dependencies between the variables, and also for supervised classification purposes, where the class could be the Primary Type, but also many of the other variables.\\nAuthor\\nAsier López Zorrilla',\n",
       " 'Context\\nSatellite imagery provides unique insights into various markets, including agriculture, defense and intelligence, energy, and finance. New commercial imagery providers, such as Planet and BlackSky, are using constellations of small satellites to exponentially increase the amount of images of the earth captured every day.\\nThis flood of new imagery is outgrowing the ability for organizations to manually look at each image that gets captured, and there is a need for machine learning and computer vision algorithms to help automate the analysis process.\\nThe aim of this dataset is to help address the difficult task of detecting the location of large ships in satellite images. Automating this process can be applied to many issues including monitoring port activity levels and supply chain analysis.\\nContinusouly updates will be made to this dataset as new Planet imagery released. Current images were collected as late as September 2017.\\nContent\\nThe dataset consists of image chips extracted from Planet satellite imagery collected over the San Franciso Bay area. It includes 2800 80x80 RGB images labeled with either a \"ship\" or \"no-ship\" classification. Image chips were derived from PlanetScope full-frame visual scene products, which are orthorectified to a 3 meter pixel size.\\nProvided is a zipped directory shipsnet.7z that contains the entire dataset as .png image chips. Each individual image filename follows a specific format: {label} __ {scene id} __ {longitude} _ {latitude}.png\\nlabel: Valued 1 or 0, representing the \"ship\" class and \"no-ship\" class, respectively.\\nscene id: The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene.\\nlongitude_latitude: The longitude and latitude coordinates of the image center point, with values separated by a single underscore.\\nThe dataset is also distributed as a JSON formatted text file shipsnet.json. The loaded object contains data, label, scene_ids, and location lists.\\nThe pixel value data for each 80x80 RGB image is stored as a list of 19200 integers within the data list. The first 6400 entries contain the red channel values, the next 6400 the green, and the final 6400 the blue. The image is stored in row-major order, so that the first 80 entries of the array are the red channel values of the first row of the image.\\nThe list values at index i in labels, scene_ids, and locations each correspond to the i-th image in the data list.\\nClass Labels\\nThe \"ship\" class includes 700 images. Images in this class are near-centered on the body of a single ship. Ships of different ship sizes, orientations, and atmospheric collection conditions are included. Example images from this class are shown below.\\nThe \"no-ship\" class includes 2100 images. A third of these are a random sampling of different landcover features - water, vegetion, bare earth, buildings, etc. - that do not include any portion of an ship. The next third are \"partial ships\" that contain only a portion of an ship, but not enough to meet the full definition of the \"ship\" class. The last third are images that have previously been mislabeled by machine learning models, typically caused by bright pixels or strong linear features. Example images from this class are shown below.\\nAcknowledgements\\nSatellite imagery used to build this dataset is made available through Planet\\'s Open California dataset, which is openly licensed. As such, this dataset is also available under the same CC-BY-SA license. Users can sign up for a free Planet account to search, view, and download thier imagery and gain access to their API.',\n",
       " 'The AQS Data Mart is a database containing all of the information from AQS. It has every measured value the EPA has collected via the national ambient air monitoring program. It also includes the associated aggregate values calculated by EPA (8-hour, daily, annual, etc.). The AQS Data Mart is a copy of AQS made once per week and made accessible to the public through web-based applications. The intended users of the Data Mart are air quality data analysts in the regulatory, academic, and health research communities. It is intended for those who need to download large volumes of detailed technical data stored at EPA and does not provide any interactive analytical tools. It serves as the back-end database for several Agency interactive tools that could not fully function without it: AirData, AirCompare, The Remote Sensing Information Gateway, the Map Monitoring Sites KML page, etc.\\nAQS must maintain constant readiness to accept data and meet high data integrity requirements, thus is limited in the number of users and queries to which it can respond. The Data Mart, as a read only copy, can allow wider access.\\nThe most commonly requested aggregation levels of data (and key metrics in each) are:\\nSample Values (2.4 billion values back as far as 1957, national consistency begins in 1980, data for 500 substances routinely collected) The sample value converted to standard units of measure (generally 1-hour averages as reported to EPA, sometimes 24-hour averages) Local Standard Time (LST) and GMT timestamps Measurement method Measurement uncertainty, where known Any exceptional events affecting the data NAAQS Averages NAAQS average values (8-hour averages for ozone and CO, 24-hour averages for PM2.5) Daily Summary Values (each monitor has the following calculated each day) Observation count Observation per cent (of expected observations) Arithmetic mean of observations Max observation and time of max AQI (air quality index) where applicable Number of observations > Standard where applicable Annual Summary Values (each monitor has the following calculated each year) Observation count and per cent Valid days Required observation count Null observation count Exceptional values count Arithmetic Mean and Standard Deviation 1st - 4th maximum (highest) observations Percentiles (99, 98, 95, 90, 75, 50) Number of observations > Standard Site and Monitor Information FIPS State Code (the first 5 items on this list make up the AQS Monitor Identifier) FIPS County Code Site Number (unique within the county) Parameter Code (what is measured) POC (Parameter Occurrence Code) to distinguish from different samplers at the same site Latitude Longitude Measurement method information Owner / operator / data-submitter information Monitoring Network to which the monitor belongs Exemptions from regulatory requirements Operational dates City and CBSA where the monitor is located Quality Assurance Information Various data fields related to the 19 different QA assessments possible\\nQuerying BigQuery tables\\nYou can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.epa_historical_air_quality.[TABLENAME]. Fork this kernel to get started.\\nAcknowledgements\\nData provided by the US Environmental Protection Agency Air Quality System Data Mart.',\n",
       " \"Context\\nQuora's first public dataset is related to the problem of identifying duplicate questions. At Quora, an important product principle is that there should be a single question page for each logically distinct question. For example, the queries “What is the most populous state in the USA?” and “Which state in the United States has the most people?” should not exist separately on Quora because the intent behind both is identical. Having a canonical page for each logically distinct query makes knowledge-sharing more efficient in many ways: for example, knowledge seekers can access all the answers to a question in a single location, and writers can reach a larger readership than if that audience was divided amongst several pages.\\nThe dataset is based on actual data from Quora and will give anyone the opportunity to train and test models of semantic equivalence.\\nContent\\nThere are over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair.\\nAcknowledgements\\nFor more information on this dataset, check out Quora's first dataset release page.\\nLicense\\nThis data is subject to Quora's Terms of Service, allowing for non-commercial use.\",\n",
       " 'Version 5 Description\\nTLDR\\nThe directory 1-1-16_5-31-17_Weather contains 1663 files (one for each of the 1663 stations in Japan)\\nAs its name implies, the data is from the same date window as the competition\\'s date_info file\\nIf a station file\\'s name ends with four underscores and a date, the date indicates when the station was terminated\\nPlease, read on...\\nContext\\nThis dataset contains the dataset from the Recruit Restaurant Visitor Forecasting competition (active from 11-28-17 to 2-6-18).\\nThe focus is on using data about reservations made at various restaurants throughout Japan, along with restaurant location and genre information to predict the actual number of visitors a restaurant will have on a given day.\\nThis dataset augments the above with the addition of information about the weather at various locations in Japan over time to produce an exciting, multi-faceted dataset that deals with time, geography, weather, and delicious food.\\nThank you for your interest in this dataset! Please let me know if you have any suggestions, questions or problems!\\nContent\\nThe core of the dataset comprises the files in the following directory:\\n1-1-16_5-31-17_Weather (1663 .csv files):\\nThis directory contains translated weather data for the time period denoted by the directory’s name (from 1-1-16 through 5-31-17).\\nEach .csv file in this directory is of shape (517, 15), and is named according to the id values in the below weather_stations file.\\nThere are a few reasons why there may seem to be a lot of null values:\\nThe primary reason is that different types of stations/sensors are used, and some just don\\'t capture as much data as others\\nQuestionable data is sometimes removed by the Agency\\nIf the station was terminated, its values are null\\nThese are the features for all translated weather files (I won’t hazard a description for the features that aren’t already self-explanatory because I’m no meteorologist, and I’d hate for you to get that impression):\\ncalendar_date - the observation date, formatted thusly \"yyyy-mm-dd\"\\navg_temperature\\nhigh_temperature\\nlow_temperature\\nprecipitation\\nhours_sunlight\\nsolar_radiation\\ndeepest_snowfall\\ntotal_snowfall\\navg_wind_speed\\navg_vapor_pressure\\navg_local_pressure\\navg_humidity\\navg_sea_pressure\\ncloud_cover\\nThis dataset adds the following .csv files regarding weather stations, and their relations to the competition data:\\nweather_stations.csv (1663, 8):\\nThis file contains the location and termination dates for 1,663 weather stations in Japan.\\nid - the join of a station’s prefecture, first_name, and second_name, with \"__\" (double underscores)\\nNote: If date_terminated is not null, id will end with four underscores and the date_terminated\\nprefecture - the prefecture in which this station is located (see note 1)\\nfirst_name - the first name given to specify a location (see note 2)\\nsecond_name - the second name given to specify a location (see note 2)\\nlatitude - latitude of the station, converted from degrees, minutes, seconds to decimal degrees for consistency\\nlongitude - longitude of the station, converted from degrees, minutes, seconds to decimal degrees for consistency\\naltitude - altitude of the station\\ndate_terminated - If the station was terminated, the date of its termination (formatted thusly \"yyyy-mm-dd\") else null\\nnearby_active_stations.csv (62, 8):\\nThis file is a subset of weather_stations.csv (above) selected via the following criteria:\\n    1) the station was not terminated, and\\n    2) the station was the closest station to at least one store in air_store_info or hpg_store_info\\nAs you can see, there is a lot of overlap here, because while the weather stations seem to generally be scattered throughout Japan, the store locations tend to be clustered around several areas.\\nColumn names and descriptions are identical to those of weather_stations.\\nfeature_manifest.csv (1663, 15):\\nThis file contains information about each station\\'s \"coverage\" of each weather feature.\\nValues of 0.0 for any of the below features except id mean that station collected no data on that feature.\\nValues of 1.0 for any of the below features except id mean that station collected data on that feature for every day.\\nid - the id of this weather station\\navg_temperature - ratio of non-null values for this feature at this station\\nhigh_temperature - ratio of non-null values for this feature at this station\\nlow_temperature - ratio of non-null values for this feature at this station\\nprecipitation - ratio of non-null values for this feature at this station\\nhours_sunlight - ratio of non-null values for this feature at this station\\nsolar_radiation - ratio of non-null values for this feature at this station\\ndeepest_snowfall - ratio of non-null values for this feature at this station\\ntotal_snowfall - ratio of non-null values for this feature at this station\\navg_wind_speed - ratio of non-null values for this feature at this station\\navg_vapor_pressure - ratio of non-null values for this feature at this station\\navg_local_pressure - ratio of non-null values for this feature at this station\\navg_humidity - ratio of non-null values for this feature at this station\\navg_sea_pressure - ratio of non-null values for this feature at this station\\ncloud_cover - ratio of non-null values for this feature at this station\\nair_station_distances.csv (1663, 111):\\nThis file contains the Vincenty distance from every weather station to every unique latitude/longitude pair in the air system.\\nstation_id - the id of this weather station\\nstation_latitude - station latitude (in decimal degrees)\\nstation_longitude - station longitude (in decimal degrees)\\n<<UNIQUE AIR COORDINATE PAIRS>> - The remaining 108 columns are stringified versions of all unique air coordinate pairs\\nThey are formatted thusly: (##.###, ##.###), where the first float is the store latitude and the second is the store longitude (see note 3)\\nEX) The first of these columns is: (34.6951242, 135.1978525)\\nhpg_station_distances.csv (1663, 132):\\nThis file contains the Vincenty distance from every weather station to every unique latitude/longitude pair in the hpg system.\\nstation_id - the id of this weather station\\nstation_latitude - station latitude (in decimal degrees)\\nstation_longitude - station longitude (in decimal degrees)\\n<<UNIQUE HPG COORDINATE PAIRS>> - The remaining 129 columns are stringified versions of all unique hpg coordinate pairs\\nThey are formatted thusly: (##.###, ##.###), where the first float is the store latitude and the second is the store longitude (see note 3)\\nEX) The first of these columns is: (35.6436746642265, 139.668220854814)\\nair_store_info_with_nearest_active_station.csv (829, 12):\\nhpg_store_info_with_nearest_active_station.csv (4690, 12):\\nThese two files are supplemented versions of air_store_info and hpg_store_info, and they contain the original competition data for the store_info file specified in the file’s name, plus the following features:\\nlatitude_str - a stringified version of this store\\'s latitude for lookup in air_station_distances and hpg_station_distances\\nlongitude_str - a stringified version of this store\\'s longitude for lookup in air_station_distances and hpg_station_distances\\nstation_id - the id of the weather station nearest to this store\\nstation_latitude - the latitude (in decimal degrees) of the weather station nearest to this store\\nstation_longitude - the longitude (in decimal degrees) of the weather station nearest to this store\\nstation_vincenty - the Vincenty distance between this store and the station to which it is closest\\nstation_great_circle - the Great Circle distance between this store and the station to which it is closest\\nOther Available Data\\nDate ranges outside of the one used for this competition\\nRecords seem to go fairly far back, but of course the data we can actually get will be subject to the activation and termination dates of each individual station\\nObservation periods other than daily values\\nHourly, every “x” days, monthly, seasonal\\nOption to compare data with average from the past 30 years\\nOther features:\\nDay’s maximum instantaneous wind speed\\nDay’s maximum wind direction\\nDay’s minimum relative humidity\\nDay’s minimum sea pressure\\nOption to show the time at which the observation period’s maximum and minimum values occurred\\nAcknowledgements\\nAll weather data contained herein came from the Japan Meteorological Agency\\nAll data pertaining to restaurants, reservations, and visitors came from the Recruit Restaurant Visitor Forecasting competition\\nDisclaimers\\nThe site has one station (that I know of) that I did not include in the list of all stations. That station is located in the Antarctic, and I didn’t include it because the Antarctic is not located in Japan\\nI have not tested the feasibility of actually gathering and processing the “other available data” listed above\\nBelow is a list of my qualifications for processing weather data for Japan, from a site written in Japanese:\\n1\\n2\\n3\\nLook upon the list and see that it is empty. I’m just a guy that knows how to get stuff done and enjoys a challenge\\nAgain, read all translations with several grains of salt\\nPlans/Ideas to Expand This Dataset (Tell Me If These Interest You)\\nGetting data from previous years to get a better idea of how the weather should be on a given day (next version)\\nAveraging data from multiple stations to maybe create some representation of the weather for that prefecture/area\\nFor stores that are particularly far from their nearest station, doing a similar sort of averaging as above to attempt to minimize any risk posed by using weather data too far from the target store\\nNotes\\n1) The site separates the prefecture \"Hokkaido\" into its 14 subprefectures:\\nTherefore, when you might expect prefecture to be \"hokkaido\", you will actually see \"hokkaido_<subprefecture>\"\\ni.e., the concatenation of the three strings \"hokkaido\", \"_\" (single underscore), and the subprefecture name\\n2) Formatting of first_name and last_name:\\nSpaces were replaced by \"-\" (single dash)\\nIf a value was not provided or could not be translated, the value is \"NONE\"\\nI would suggest you don’t rely too heavily on these features, as they are the result of several different translation APIs, which were rarely in agreement on the translation. These features are provided more as a convenience, and in an effort to give you as much data as possible\\n3) Column formatting in air_station_distances.csv and hpg_station_distances.csv:\\nThese files show the coordinates with the precision shown in the original air_store_info.csv and hpg_store_info.csv files; however, Pandas, doesn\\'t read these values in with the same precision\\nIf you plan on using these files, you must use the latitude_str and longitude_str features in the appropriate ..._store_info_with_nearest_active_station file to look up coordinates\\nFor more information, see this discussion post',\n",
       " 'Context\\nI tried to gather as many lyrics as I could. I ran my code on a a free ec2 instance and ran out of storage space. I have attached the code below so if any one wants to try out it and get all lyrics, please do.\\nContent\\nThere are around 380,000+ lyrics in the data set from a lot of different artists from a lot of different genres arranged by year. Structure is artist/year/song. Every artist folder has a genre.txt that tells what is the genre of the musician. Find the crawler here.\\nAcknowledgements\\nI would like to thank Shruti Jasoria, SJasoria on GitHub for writing the multi-threaded version.\\nInspiration\\nI wanted to find out what genre and what artist abuses what substance. Do rapstars like cocaine or liquor? If liquor then what Liquor? Does Eminem prefer Hennesy over Jack Daniels? Do Rockstars love pot?',\n",
       " 'Context\\nThis data set includes customers who have paid off their loans, who have been past due and put into collection without paying back their loan and interests, and who have paid off only after they were put in collection. The financial product is a bullet loan that customers should pay off all of their loan debt in just one time by the end of the term, instead of an installment schedule. Of course, they could pay off earlier than their pay schedule.\\nContent\\nLoan_id A unique loan number assigned to each loan customers\\nLoan_status Whether a loan is paid off, in collection, new customer yet to payoff, or paid off after the collection efforts\\nPrincipal Basic principal loan amount at the origination\\nterms Can be weekly (7 days), biweekly, and monthly payoff schedule\\nEffective_date When the loan got originated and took effects\\nDue_date Since it’s one-time payoff schedule, each loan has one single due date\\nPaidoff_time The actual time a customer pays off the loan\\nPastdue_days How many days a loan has been past due\\nAge, education, gender A customer’s basic demographic information',\n",
       " 'Context\\nThis dataset contains information on all 802 Pokemon from all Seven Generations of Pokemon. The information contained in this dataset include Base Stats, Performance against Other Types, Height, Weight, Classification, Egg Steps, Experience Points, Abilities, etc. The information was scraped from http://serebii.net/\\nContent\\nname: The English name of the Pokemon\\njapanese_name: The Original Japanese name of the Pokemon\\npokedex_number: The entry number of the Pokemon in the National Pokedex\\npercentage_male: The percentage of the species that are male. Blank if the Pokemon is genderless.\\ntype1: The Primary Type of the Pokemon\\ntype2: The Secondary Type of the Pokemon\\nclassification: The Classification of the Pokemon as described by the Sun and Moon Pokedex\\nheight_m: Height of the Pokemon in metres\\nweight_kg: The Weight of the Pokemon in kilograms\\ncapture_rate: Capture Rate of the Pokemon\\nbase_egg_steps: The number of steps required to hatch an egg of the Pokemon\\nabilities: A stringified list of abilities that the Pokemon is capable of having\\nexperience_growth: The Experience Growth of the Pokemon\\nbase_happiness: Base Happiness of the Pokemon\\nagainst_?: Eighteen features that denote the amount of damage taken against an attack of a particular type\\nhp: The Base HP of the Pokemon\\nattack: The Base Attack of the Pokemon\\ndefense: The Base Defense of the Pokemon\\nsp_attack: The Base Special Attack of the Pokemon\\nsp_defense: The Base Special Defense of the Pokemon\\nspeed: The Base Speed of the Pokemon\\ngeneration: The numbered generation which the Pokemon was first introduced\\nis_legendary: Denotes if the Pokemon is legendary.\\nAcknowledgements\\nThe data was scraped from http://serebii.net/.\\nInspiration\\nPokemon holds a very special place in my heart as it is probably the only video game I have judiciously followed for more than 10 years. With this dataset, I wanted to be able to answer the following questions:\\nIs it possible to build a classifier to identify legendary Pokemon?\\nHow does height and weight of a Pokemon correlate with its various base stats?\\nWhat factors influence the Experience Growth and Egg Steps? Are these quantities correlated?\\nWhich type is the strongest overall? Which is the weakest?\\nWhich type is the most likely to be a legendary Pokemon?\\nCan you build a Pokemon dream team? A team of 6 Pokemon that inflicts the most damage while remaining relatively impervious to any other team of 6 Pokemon.',\n",
       " \"This dataset consists of a few million Amazon customer reviews (input text) and star ratings (output labels) for learning how to train fastText for sentiment analysis.\\nThe idea here is a dataset is more than a toy - real business data on a reasonable scale - but can be trained in minutes on a modest laptop.\\nContent\\nThe fastText supervised learning tutorial requires data in the following format:\\n__label__<X> __label__<Y> ... <Text>\\nwhere X and Y are the class names. No quotes, all on one line.\\nIn this case, the classes are __label__1 and __label__2, and there is only one class per row.\\n__label__1 corresponds to 1- and 2-star reviews, and __label__2 corresponds to 4- and 5-star reviews.\\n(3-star reviews i.e. reviews with neutral sentiment were not included in the original),\\nThe review titles, followed by ':' and a space, are prepended to the text.\\nMost of the reviews are in English, but there are a few in other languages, like Spanish.\\nSource\\nThe data was lifted from Xiang Zhang's Google Drive dir, but it was in .csv format, not suitable for fastText.\\nTraining and Testing\\nFollow the basic instructions at fastText supervised learning tutorial to set up the directory.\\nTo train:\\n./fasttext supervised -input train.ft.txt -output model_amzn\\nThis should take a few minutes.\\nTo test:\\n./fasttext test model_amzn.bin test.ft.txt\\nExpect precision and recall of 0.916 if all is in order.\\nYou can also train and test in Python, see Kernel.\",\n",
       " \"Context\\nCensus of India is a rich database which can tell stories of over a billion Indians. It is important not only for research point of view, but commercially as well for the organizations that want to understand India's complex yet strongly knitted heterogeneity. However, nowhere on the web, there exists a single database that combines the district- wise information of all the variables (most include no more than 4-5 out of over 50 variables!). Extracting and using data from Census of India 2001 is quite a laborious task since all data is made available in scattered PDFs district wise. Individual PDFs can be extracted from http://www.censusindia.gov.in/(S(ogvuk1y2e5sueoyc5eyc0g55))/Tables_Published/Basic_Data_Sheet.aspx.\\nContent\\nThis database has been extracted from Census of 2001 and includes data of 590 districts, having around 80 variables each.\\nIn case of confusion regarding the context of the variable, refer to the following PDF and you will be able to make sense out of it: http://censusindia.gov.in/Dist_File/datasheet-2923.pdf\\nAll the extraction work can be found @ https://github.com/preetskhalsa97/census2001auto The final CSV can be found at finalCSV/all.csv\\nThe subtle hack that was used to automate extraction to a great extent was the the URLs of all the PDFs were same except the four digits (that were respective state and district codes).\\nA few abbreviations used for states:\\nAN- Andaman and Nicobar CG- Chhattisgarh D_D- Daman and Diu D_N_H- Dadra and Nagar Haveli JK- Jammu and Kashmir MP- Madhya Pradesh TN- Tamil Nadu UP- Uttar Pradesh WB- West Bengal\\nA few variables for clarification: Growth..1991...2001- population growth from 1991 to 2001 X0..4 years- People in age group 0 to 4 years SC1- Scheduled Class with highest population\\nAcknowledgements\\nInspiration\\nThis is a massive dataset which can be used to explain the interplay between education, caste, development, gender and much more. It really can explain a lot about India and propel data driven research. Happy Number Crunching!\",\n",
       " 'Context\\nInterested in the Indian startup ecosystem just like me? Wanted to know what type of startups are getting funded in the last few years? Wanted to know who are the important investors? Wanted to know the hot fields that get a lot of funding these days? This dataset is a chance to explore the Indian start up scene. Deep dive into funding data and derive insights into the future!\\nContent\\nThis dataset has funding information of the Indian startups from January 2015 to August 2017. It includes columns with the date funded, the city the startup is based out of, the names of the funders, and the amount invested (in USD).\\nFor more information on the values of individual fields, check out the Column Metadata.\\nAcknowledgements\\nThanks to trak.in who are generous enough to share the data publicly for free.\\nInspiration\\nPossible questions which could be answered are:\\nHow does the funding ecosystem change with time?\\nDo cities play a major role in funding?\\nWhich industries are favored by investors for funding?\\nWho are the important investors in the Indian Ecosystem?\\nHow much funds does startups generally get in India?',\n",
       " 'Context\\nThe dataset consists of three files: a file with behaviour data (events.csv), a file with item properties (item_properties.сsv) and a file, which describes category tree (category_tree.сsv). The data has been collected from a real-world ecommerce website. It is raw data, i.e. without any content transformations, however, all values are hashed due to confidential issues. The purpose of publishing is to motivate researches in the field of recommender systems with implicit feedback.\\nContent\\nThe behaviour data, i.e. events like clicks, add to carts, transactions, represent interactions that were collected over a period of 4.5 months. A visitor can make three types of events, namely “view”, “addtocart” or “transaction”. In total there are 2 756 101 events including 2 664 312 views, 69 332 add to carts and 22 457 transactions produced by 1 407 580 unique visitors. For about 90% of events corresponding properties can be found in the “item_properties.csv” file.\\nFor example:\\n“1439694000000,1,view,100,” means visitorId = 1, clicked the item with id = 100 at 1439694000000 (Unix timestamp)\\n“1439694000000,2,transaction,1000,234” means visitorId = 2 purchased the item with id = 1000 in transaction with id = 234 at 1439694000000 (Unix timestamp)\\nThe file with item properties (item_properties.csv) includes 20 275 902 rows, i.e. different properties, describing 417 053 unique items. File is divided into 2 files due to file size limitations. Since the property of an item can vary in time (e.g., price changes over time), every row in the file has corresponding timestamp. In other words, the file consists of concatenated snapshots for every week in the file with the behaviour data. However, if a property of an item is constant over the observed period, only a single snapshot value will be present in the file. For example, we have three properties for single item and 4 weekly snapshots, like below:\\ntimestamp,itemid,property,value\\n1439694000000,1,100,1000\\n1439695000000,1,100,1000\\n1439696000000,1,100,1000\\n1439697000000,1,100,1000\\n1439694000000,1,200,1000\\n1439695000000,1,200,1100\\n1439696000000,1,200,1200\\n1439697000000,1,200,1300\\n1439694000000,1,300,1000\\n1439695000000,1,300,1000\\n1439696000000,1,300,1100\\n1439697000000,1,300,1100\\nAfter snapshot merge it would looks like:\\n1439694000000,1,100,1000\\n1439694000000,1,200,1000\\n1439695000000,1,200,1100\\n1439696000000,1,200,1200\\n1439697000000,1,200,1300\\n1439694000000,1,300,1000\\n1439696000000,1,300,1100\\nBecause property=100 is constant over time, property=200 has different values for all snapshots, property=300 has been changed once.\\nItem properties file contain timestamp column because all of them are time dependent, since properties may change over time, e.g. price, category, etc. Initially, this file consisted of snapshots for every week in the events file and contained over 200 millions rows. We have merged consecutive constant property values, so it\\'s changed from snapshot form to change log form. Thus, constant values would appear only once in the file. This action has significantly reduced the number of rows in 10 times.\\nAll values in the “item_properties.csv” file excluding \"categoryid\" and \"available\" properties were hashed. Value of the \"categoryid\" property contains item category identifier. Value of the \"available\" property contains availability of the item, i.e. 1 means the item was available, otherwise 0. All numerical values were marked with \"n\" char at the beginning, and have 3 digits precision after decimal point, e.g., \"5\" will become \"n5.000\", \"-3.67584\" will become \"n-3.675\". All words in text values were normalized (stemming procedure: https://en.wikipedia.org/wiki/Stemming) and hashed, numbers were processed as above, e.g. text \"Hello world 2017!\" will become \"24214 44214 n2017.000\"\\nThe category tree file has 1669 rows. Every row in the file specifies a child categoryId and the corresponding parent. For example:\\nLine “100,200” means that categoryid=1 has parent with categoryid=200\\nLine “300,” means that categoryid hasn’t parent in the tree\\nAcknowledgements\\nRetail Rocket (retailrocket.io) helps web shoppers make better shopping decisions by providing personalized real-time recommendations through multiple channels with over 100MM unique monthly users and 1000+ retail partners over the world.\\nInspiration\\nHow to use item properties and category tree data to improve collaborative filtering model?\\nRecurrent Neural Networks with Top-k Gains for Session-based Recommendations https://github.com/hidasib/GRU4Rec and paper https://arxiv.org/abs/1706.03847\\nhttps://www.researchgate.net/publication/280538158_Application_of_Kullback-Leibler_divergence_for_short-term_user_interest_detection\\nhttps://pdfs.semanticscholar.org/66dc/1724c4ed1e74fe6b22e636b52031a33c8ebe.pdf https://www.slideshare.net/LukasLerche/adaptation-and-evaluation-of-recommendationsfor-shortterm-shopping-goals Adaptation and Evaluation of Recommendations for Short-term Shopping Goals\\nTasks\\nTask 1\\nWhen a customer comes to an e-commerce site, he looks for a product with particular properties: price range, vendor, product type and etc. These properties are implicit, so it\\'s hard to determine them through clicks log.\\nTry to create an algorithm which predicts properties of items in \"addtocart\" event by using data from \"view\" events for any visitor in the published log.\\nTask 2\\nDescription:\\nProcess of analyzing ecommerce data include very important part of data cleaning. Researchers noticed that in some cases browsing data include up to 40% of abnormal traffic.\\nFirstly, abnormal users add a lot of noise into data and make recommendation system less effective. In order to increase efficiency of recommendation system, abnormal users should be removed from the raw data.\\nSecondly, abnormal users add bias to results of split tests, so this type of users should be removed also from split test data.\\nGoals:\\nThe main goal is to find abnormal users of e-shop.\\nSubgoals:\\nGenerate features\\nBuild a model\\nCreate a metric that helps to evaluate quality of the model',\n",
       " 'Context\\nThis data set contains combined on-court performance data for NBA players in the 2016-2017 season, alongside salary, Twitter engagement, and Wikipedia traffic data.\\nFurther information can be found in a series of articles for IBM Developerworks: \"Explore valuation and attendance using data science and machine learning\" and \"Exploring the individual NBA players\".\\nAcknowledgement\\nData sources include ESPN, Basketball-Reference, Twitter, Five-ThirtyEight, and Wikipedia. The source code for this dataset (in Python and R) can be found on GitHub. Links to more writing can be found at noahgift.com.\\nInspiration\\nDo NBA fans know more about who the best players are, or do owners?\\nWhat is the true worth of the social media presence of athletes in the NBA?',\n",
       " 'Context\\nA high-quality, dataset of images containing fruits. The following fruits are included: Apples - (different varieties: Golden, Golden-Red, Granny Smith, Red, Red Delicious), Apricot, Avocado, Avocado ripe, Banana (Yellow, Red), Cactus fruit, Carambula, Cherry, Clementine, Cocos, Dates, Granadilla, Grape (Pink, White, White2), Grapefruit (Pink, White), Guava, Huckleberry, Kiwi, Kaki, Kumsquats, Lemon (normal, Meyer), Lime, Litchi, Mandarine, Mango, Maracuja, Nectarine, Orange, Papaya, Passion fruit, Peach, Pepino, Pear (different varieties, Abate, Monster, Williams), Pineapple, Pitahaya Red, Plum, Pomegranate, Quince, Raspberry, Salak, Strawberry, Tamarillo, Tangelo.\\nDataset properties\\nTraining set size: 28736 images.\\nValidation set size: 9673 images.\\nNumber of classes: 60 (fruits).\\nImage size: 100x100 pixels.\\nFilename format: image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg (e.g. r_32_100.jpg). \"r\" stands for rotated fruit. \"100\" comes from image size (100x100 pixels).\\nDifferent varieties of the same fruit (apple for instance) are shown having different labels.\\nContent\\nFruits were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded.\\nA Logitech C920 camera was used for filming the fruits. This is one of the best webcams available.\\nBehind the fruits we placed a white sheet of paper as background.\\nHowever due to the variations in the lighting conditions, the background was not uniform and we wrote a dedicated algorithm which extract the fruit from the background. This algorithm is of flood fill type: we start from each edge of the image and we mark all pixels there, then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. We repeat the previous step until no more pixels can be marked.\\nAll marked pixels are considered as being background (which is then filled with white) and the rest of pixels are considered as belonging to the object.\\nThe maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie.\\nHow to cite\\nHorea Muresan, Mihai Oltean, Fruit recognition from images using deep learning, Technical Report, Babes-Bolyai University, 2017\\nAlternate download\\nThis dataset is also available for download from GitHub: Fruits-360 dataset\\nHistory\\nFruits were filmed at the dates given below:\\n2017.02.25 - Apple (golden).\\n2017.02.28 - Apple (red-yellow, red, golden2), Kiwi, Pear, Grapefruit, Lemon, Orange, Strawberry, Banana.\\n2017.03.05 - Apple (golden3, Braeburn, Granny Smith, red2).\\n2017.03.07 - Apple (red3).\\n2017.05.10 - Plum, Peach, Peach flat, Apricot, Nectarine, Pomegranate.\\n2017.05.27 - Avocado, Papaya, Grape, Cherrie.\\n2017.12.25 - Carambula, Cactus fruit, Granadilla, Kaki, Kumsquats, Passion fruit, Avocado ripe, Quince.\\n2017.12.28 - Clementine, Cocos, Mango, Lime, Litchi.\\n2017.12.31 - Apple Red Delicious, Pear Monster, Grape White.\\n2018.01.14 - Ananas, Grapefruit Pink, Mandarine, Pineapple, Tangelo.\\n2018.01.19 - Huckleberry, Raspberry.\\n2018.01.26 - Dates, Maracuja, Salak, Tamarillo.\\n2018.02.05 - Guava, Grape White 2, Lemon Meyer\\n2018.02.07 - Banana Red, Pepino, Pitahaya Red.\\n2018.02.08 - Pear Abate, Pear Williams.',\n",
       " 'Context\\nIn 2012, the Massachusetts Institute of Technology (MIT) and Harvard University launched open online courses on edX, a non-profit learning platform co-founded by the two institutions. Four years later, what have we learned about these online “classrooms” and the global community of learners who take them?\\nContent\\nThis report provides data on 290 Harvard and MIT online courses, 250 thousand certifications, 4.5 million participants, and 28 million participant hours on the edX platform since 2012.\\nAcknowledgements\\nIsaac Chuang, a professor at MIT, and Andrew Ho, a professor at Harvard University, published this data as an appendix to their paper \"HarvardX and MITx: Four Years of Open Online Courses\".',\n",
       " 'This version of the dataset is obsolete. It contains duplicate ratings (same user_id,book_id), as reported by Philipp Spachtholz in his illustrious notebook.\\nThe current version has duplicates removed, and more ratings (six million), sorted by time. Book and user IDs are the same.\\nIt is available at https://github.com/zygmuntz/goodbooks-10k.\\nThere have been good datasets for movies (Netflix, Movielens) and music (Million Songs) recommendation, but not for books. That is, until now.\\nThis dataset contains ratings for ten thousand popular books. As to the source, let\\'s say that these ratings were found on the internet. Generally, there are 100 reviews for each book, although some have less - fewer - ratings. Ratings go from one to five.\\nBoth book IDs and user IDs are contiguous. For books, they are 1-10000, for users, 1-53424. All users have made at least two ratings. Median number of ratings per user is 8.\\nThere are also books marked to read by the users, book metadata (author, year, etc.) and tags.\\nContents\\nratings.csv contains ratings and looks like that:\\nbook_id,user_id,rating\\n1,314,5\\n1,439,3\\n1,588,5\\n1,1169,4\\n1,1185,4\\nto_read.csv provides IDs of the books marked \"to read\" by each user, as user_id,book_id pairs.\\nbooks.csv has metadata for each book (goodreads IDs, authors, title, average rating, etc.).\\nThe metadata have been extracted from goodreads XML files, available in the third version of this dataset as books_xml.tar.gz. The archive contains 10000 XML files. One of them is available as sample_book.xml. To make the download smaller, these files are absent from the current version. Download version 3 if you want them.\\nbook_tags.csv contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs.\\ntags.csv translates tag IDs to names.\\nSee the notebook for some basic stats of the dataset.\\ngoodreads IDs\\nEach book may have many editions. goodreads_book_id and best_book_id generally point to the most popular edition of a given book, while goodreads work_id refers to the book in the abstract sense.\\nYou can use the goodreads book and work IDs to create URLs as follows:\\nhttps://www.goodreads.com/book/show/2767052\\nhttps://www.goodreads.com/work/editions/2792775',\n",
       " \"Context\\nI created this data set merging the census 2011 of Indian Cities with Population more than 1 Lac and City wise number of Graduates from the Census 2011, to create a visualization of where the future cities of India stands today, I will try to add more columns [ fertility rate, religion distribution, health standards, number of schools, Mortality rate ] in the future, hope people will contribute.\\nContent\\nData of 500 Cities with population more than 1 Lac by Census 2011\\n'name_of_city'                  : Name of the City \\n'state_code'                    : State Code of the City\\n'state_name'                    : State Name of the City\\n'dist_code'                     : District Code where the city belongs ( 99 means multiple district ) \\n'population_total'              : Total Population\\n'population_male'               : Male Population \\n'population_female'             : Female Population\\n'0-6_population_total'          : 0-6 Age Total Population\\n'0-6_population_male'           : 0-6 Age Male Population\\n'0-6_population_female'         : 0-6 Age Female Population\\n'literates_total'               : Total Literates\\n'literates_male'                : Male Literates\\n'literates_female'              : Female Literates \\n'sex_ratio'                     : Sex Ratio \\n'child_sex_ratio'               : Sex ratio in 0-6\\n'effective_literacy_rate_total' : Literacy rate over Age 7 \\n'effective_literacy_rate_male'  : Male Literacy rate over Age 7 \\n'effective_literacy_rate_female': Female Literacy rate over Age 7 \\n'location'                      : Lat,Lng\\n'total_graduates'               : Total Number of Graduates\\n'male_graduates'                : Male Graduates \\n'female_graduates'              : Female Graduates\\nAcknowledgements\\nCensus 2011\\nhttp://censusindia.gov.in/2011-prov-results/paper2/data_files/India2/Table_2_PR_Cities_1Lakh_and_Above.xls\\nGoogle Geocoder for Location Fetching.\\nGraduation Data Census 2011\\nhttp://www.censusindia.gov.in/2011census/C-series/DDWCT-0000C-08.xlsx\\nInspiration\\nWhat story do the top 500 cities of India tell to the world? I wrote a post in my blog about the dataset .\",\n",
       " 'Introduction\\nThe lack of publicly available National Football League (NFL) data sources has been a major obstacle in the creation of modern, reproducible research in football analytics. While clean play-by-play data is available via open-source software packages in other sports (e.g. nhlscrapr for hockey; PitchF/x data in baseball; the Basketball Reference for basketball), the equivalent datasets are not freely available for researchers interested in the statistical analysis of the NFL. To solve this issue, a group of Carnegie Mellon University statistical researchers including Maksim Horowitz, Ron Yurko, and Sam Ventura, built and released nflscrapR an R package which uses an API maintained by the NFL to scrape, clean, parse, and output clean datasets at the individual play, player, game, and season levels. Using the data outputted by the package, the trio went on to develop reproducible methods for building expected point and win probability models for the NFL. The outputs of these models are included in this dataset and can be accessed using the nflscrapR package.\\nContent\\nThe dataset made available on Kaggle contains all the regular season plays from the 2009-2016 NFL seasons. The dataset has 356,768 rows and 100 columns. Each play is broken down into great detail containing information on: game situation, players involved, results, and advanced metrics such as expected point and win probability values. Detailed information about the dataset can be found at the following web page, along with more NFL data: https://github.com/ryurko/nflscrapR-data.\\nAcknowledgements\\nThis dataset was compiled by Ron Yurko, Sam Ventura, and myself. Special shout-out to Ron for improving our current expected points and win probability models and compiling this dataset. All three of us are proud founders of the Carnegie Mellon Sports Analytics Club.\\nInspiration\\nThis dataset is meant to both grow and bring together the community of sports analytics by providing clean and easily accessible NFL data that has never been availabe on this scale for free.',\n",
       " 'Feel free to check and recommend my Medium post Part 1 on a classification model and Part 2 on a detection model (Faster R-CNN) about this dataset and what I am doing with it.\\nYou can also find the related GitHub repo here .\\nContext\\nAs a big Simpsons fan, I have watched a lot (and still watching) of The Simpson episodes -multiple times each- over the years. I wanted to build a neural network which can recognize characters\\nContent\\nI am still building this dataset (labeling pictures), I will upload new versions of this dataset. Please check the files there are descriptions and explanations.\\nFile simpson-set.tar.gz : This is an image dataset: 20 folders (one for each character) with 400-2000 pictures in each folder.\\nFile simpson-test-set.zip. : Preview of the image dataset\\nFile weights.best.h5 : Weights computed, in order to predict in Kernels.\\nFile annotation.txt : Annotation file for bounding boxes for each character\\nHelp me to build this dataset\\nIf someone wants to contribute and make this dataset bigger and more relevant, any help will be appreciated.\\nAcknowledgements\\nData is directly taken and labeled from TV show episodes.',\n",
       " 'Context\\nThis is a Glass Identification Data Set from UCI. It contains 10 attributes including id. The response is glass type(discrete 7 values)\\nContent\\nAttribute Information:\\nId number: 1 to 214 (removed from CSV file)\\nRI: refractive index\\nNa: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\\nMg: Magnesium\\nAl: Aluminum\\nSi: Silicon\\nK: Potassium\\nCa: Calcium\\nBa: Barium\\nFe: Iron\\nType of glass: (class attribute) -- 1 building_windows_float_processed -- 2 building_windows_non_float_processed -- 3 vehicle_windows_float_processed -- 4 vehicle_windows_non_float_processed (none in this database) -- 5 containers -- 6 tableware -- 7 headlamps\\nAcknowledgements\\nhttps://archive.ics.uci.edu/ml/datasets/Glass+Identification Source:\\nCreator: B. German Central Research Establishment Home Office Forensic Science Service Aldermaston, Reading, Berkshire RG7 4PN\\nDonor: Vina Spiehler, Ph.D., DABFT Diagnostic Products Corporation (213) 776-0180 (ext 3014)\\nInspiration\\nData exploration of this dataset reveals two important characteristics : 1) The variables are highly corelated with each other including the response variables: So which kind of ML algorithm is most suitable for this dataset Random Forest , KNN or other? Also since dataset is too small is there any chance of applying PCA or it should be completely avoided?\\n2) Highly Skewed Data: Is scaling sufficient or are there any other techniques which should be applied to normalize data? Like BOX-COX Power transformation?',\n",
       " 'Context\\nFull text of all questions and answers from Stack Overflow that are tagged with the python tag. Useful for natural language processing and community analysis. See also the dataset of R questions.\\nContent\\nThis dataset is organized as three tables:\\nQuestions contains the title, body, creation date, score, and owner ID for each Python question.\\nAnswers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.\\nTags contains the tags on each question besides the Python tag.\\nQuestions may be deleted by the user who posted them. They can also be closed by community vote, if the question is deemed off-topic for instance. Such questions are not included in this dataset.\\nThe dataset contains questions all questions asked between August 2, 2008 and Ocotober 19, 2016.\\nLicense\\nAll Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.',\n",
       " 'Context\\nThe National Park Service publishes a database of animal and plant species identified in individual national parks and verified by evidence — observations, vouchers, or reports that document the presence of a species in a park. All park species records are available to the public on the National Park Species portal; exceptions are made for sensitive, threatened, or endangered species when widespread distribution of information could pose a risk to the species in the park.\\nContent\\nNational Park species lists provide information on the presence and status of species in our national parks. These species lists are works in progress and the absence of a species from a list does not necessarily mean the species is absent from a park. The time and effort spent on species inventories varies from park to park, which may result in data gaps. Species taxonomy changes over time and reflects regional variations or preferences; therefore, records may be listed under a different species name.\\nEach park species record includes a species ID, park name, taxonomic information, scientific name, one or more common names, record status, occurrence (verification of species presence in park), nativeness (species native or foreign to park), abundance (presence and visibility of species in park), seasonality (season and nature of presence in park), and conservation status (species classification according to US Fish & Wildlife Service). Taxonomic classes have been translated from Latin to English for species categorization; order, family, and scientific name (genus, species, subspecies) are in Latin.\\nAcknowledgements\\nThe National Park Service species list database is managed and updated by staff at individual national parks and the systemwide Inventory and Monitoring department.',\n",
       " \"Context\\nFormula One (also Formula 1 or F1 and officially the FIA Formula One World Championship) is the highest class of single-seat auto racing that is sanctioned by the Fédération Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been one of the premier forms of racing around the world since its inaugural season in 1950.\\nContent\\nThis dataset contains data from 1950 all the way through the 2017 season, and consists of tables describing constructors, race drivers, lap times, pit stops and more.\\nAcknowledgements\\nThe data was downloaded from http://ergast.com/mrd/ at the conclusion of the 2017 season. The data was originally gathered and published to the public domain by Chris Newell.\\nInspiration\\nI think this dataset offers an exciting insight into a $ billion industry, enjoyed by hundreds of millions of viewers all over the world. So please, explore and enjoy!\",\n",
       " \"This is the dataset used in the Roam blog post Prescription-based prediction. It is derived from a variety of US open health datasets, but the bulk of the data points come from the Medicare Part D dataset and the National Provider Identifier dataset.\\nThe prescription vector for each doctor tells a rich story about that doctor's attributes, including specialty, gender, age, and region. There are 239,930 doctors in the dataset.\\nThe file is in JSONL format (one JSON record per line):\\n{\\n    'provider_variables': \\n        {\\n            'brand_name_rx_count': int,\\n            'gender': 'M' or 'F',\\n            'generic_rx_count': int,\\n            'region': 'South' or 'MidWest' or 'Northeast' or 'West',\\n            'settlement_type': 'non-urban' or 'urban'\\n            'specialty': str\\n            'years_practicing': int\\n        },\\n     'npi': str,\\n     'cms_prescription_counts':\\n        {\\n            `drug_name`: int, \\n            `drug_name`: int, \\n            ...\\n        }\\n}\\nThe brand/generic classifications behind brand_name_rx_count and generic_rx_count are defined heuristically. For more details, see the blog post or go directly to the associated code.\",\n",
       " 'The Meteoritical Society collects data on meteorites that have fallen to Earth from outer space. This dataset includes the location, mass, composition, and fall year for over 45,000 meteorites that have struck our planet.\\nNotes on missing or incorrect data points:\\na few entries here contain date information that was incorrectly parsed into the NASA database. As a spot check: any date that is before 860 CE or after 2016 are incorrect; these should actually be BCE years. There may be other errors and we are looking for a way to identify them.\\na few entries have latitude and longitude of 0N/0E (off the western coast of Africa, where it would be quite difficult to recover meteorites). Many of these were actually discovered in Antarctica, but exact coordinates were not given. 0N/0E locations should probably be treated as NA.\\nThe starter kernel for this dataset has a quick way to filter out these observations using dplyr in R, provided here for convenience:\\nmeteorites.geo <- meteorites.all %>%\\nfilter(year>=860 & year<=2016) %>% # filter out weird years\\nfilter(reclong<=180 & reclong>=-180 & (reclat!=0 | reclong!=0)) # filter out weird locations\\nThe Data\\nNote that a few column names start with \"rec\" (e.g., recclass, reclat, reclon). These are the recommended values of these variables, according to The Meteoritical Society. In some cases, there were historical reclassification of a meteorite, or small changes in the data on where it was recovered; this dataset gives the currently recommended values.\\nThe dataset contains the following variables:\\nname: the name of the meteorite (typically a location, often modified with a number, year, composition, etc)\\nid: a unique identifier for the meteorite\\nnametype: one of:\\n-- valid: a typical meteorite\\n-- relict: a meteorite that has been highly degraded by weather on Earth\\nrecclass: the class of the meteorite; one of a large number of classes based on physical, chemical, and other characteristics (see the Wikipedia article on meteorite classification for a primer)\\nmass: the mass of the meteorite, in grams\\nfall: whether the meteorite was seen falling, or was discovered after its impact; one of:\\n-- Fell: the meteorite\\'s fall was observed\\n-- Found: the meteorite\\'s fall was not observed\\nyear: the year the meteorite fell, or the year it was found (depending on the value of fell)\\nreclat: the latitude of the meteorite\\'s landing\\nreclong: the longitude of the meteorite\\'s landing\\nGeoLocation: a parentheses-enclose, comma-separated tuple that combines reclat and reclong\\nWhat can we do with this data?\\nHere are a couple of thoughts on questions to ask and ways to look at this data:\\nhow does the geographical distribution of observed falls differ from that of found meteorites? -- this would be great overlaid on a cartogram or alongside a high-resolution population density map\\nare there any geographical differences or differences over time in the class of meteorites that have fallen to Earth?\\nAcknowledgements\\nThis dataset was downloaded from NASA\\'s Data Portal, and is based on The Meteoritical Society\\'s Meteoritical Bulletin Database (this latter database provides additional information such as meteorite images, links to primary sources, etc.).',\n",
       " \"Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.\\nThis dataset includes the title, authors, abstracts, and extracted text for all NIPS papers to date (ranging from the first 1987 conference to the current 2016 conference). I've extracted the paper text from the raw PDF files and are releasing that both in CSV files and as a SQLite database. The code to scrape and create this dataset is on GitHub. Here's a quick RMarkdown exploratory overview of what's in the data. We encourage you to explore this data and share what you find through Kaggle Kernels!\",\n",
       " \"Context\\nI created this data set to help with the New York City Taxi Trip Duration playground. I used OSRM to extract information about the fastest routes for each data point.\\nI think it will be very useful for anyone doing work in that competition. Please try it out and tell me what you want me to add. I intend to improve and add features.\\nContent\\nstarting_street\\nThe street where the taxi-trip starts. In version 1 this field contained a lot of empty values. That has been dealt with.\\nend_street\\nThe street where the taxi-trip ends. In version 1 this field contained a lot of empty values. That has been dealt with.\\ntotal_distance\\nThe total distance is measured between the pickup coordinates and the drop-off coordinates in train.csv and test.csv. The unit is meters.\\ntotal_travel_time\\nThe total travel time for that data point in seconds\\nnumber_of_steps\\nThe number of steps on that trip. One step consists of some driving and an action the taxi needs to perform. It can be something like a turn or going on to a highway. See step_maneuvers for more information.\\nstreet_for_each_step\\nA list of streets where each step occurs. Multiple steps can be performed on the same street. Therefore there might the same street might occur multiple times.\\n(The values are stored as a string separated by '|')\\ndistance_per_step\\nThe distance for each step.\\n(The values are stored as a string separated by '|')\\ntravel_time_per_step\\nThe travel time for each step\\n(The values are stored as a string separated by '|')\\nstep_maneuvers\\nThe action (or maneuver) performed in each step. The possible maneuvers are:\\nturn: a basic turn\\nnew name: no turn is taken/possible, but the road name changes.\\ndepart: The trip starts\\narrive: The trip ends\\nmerge: Merge onto a street (e.g. getting on the highway from a ramp)\\non ramp: Entering a highway (direction given my modifier )\\noff ramp: Exiting a highway\\nfork: The road forks\\nend of road: The road ends in a T intersection\\ncontinue: Turn to stay on the same road\\nroundabout: A roundabout\\nrotary A traffic circle (a bigger roundabout)\\nroundabout turn: A small roundabout that can be treated as a regular turn\\n(The values are stored as a string separated by '|')\\nstep_direction\\nThe direction for each action (or maneuver)\\nstep_location_list\\nThe coordinates for each action (or maneuver)\",\n",
       " 'NIH Chest X-ray Dataset\\nNational Institutes of Health Chest X-Ray Dataset\\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, Openi was the largest publicly available source of chest X-ray images with 4,143 images available.\\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be >90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\\nLink to paper\\n\\nData limitations:\\nThe image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be >90%.\\nVery limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\\nChest x-ray radiology reports are not anticipated to be publicly shared. Parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later, maybe through manual annotation\\n\\nFile contents\\nImage format: 112,120 total images with size 1024 x 1024\\nimages_001.zip: Contains 4999 images\\nimages_002.zip: Contains 10,000 images\\nimages_003.zip: Contains 10,000 images\\nimages_004.zip: Contains 10,000 images\\nimages_005.zip: Contains 10,000 images\\nimages_006.zip: Contains 10,000 images\\nimages_007.zip: Contains 10,000 images\\nimages_008.zip: Contains 10,000 images\\nimages_009.zip: Contains 10,000 images\\nimages_010.zip: Contains 10,000 images\\nimages_011.zip: Contains 10,000 images\\nimages_012.zip: Contains 7,121 images\\nREADME_ChestXray.pdf: Original README file\\nBBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels\\nImage Index: File name\\nFinding Label: Disease type (Class label)\\nBbox x\\nBbox y\\nBbox w\\nBbox h\\nData_entry_2017.csv: Class labels and patient data for the entire dataset\\nImage Index: File name\\nFinding Labels: Disease type (Class label)\\nFollow-up #\\nPatient ID\\nPatient Age\\nPatient Gender\\nView Position: X-ray orientation\\nOriginalImageWidth\\nOriginalImageHeight\\nOriginalImagePixelSpacing_x\\nOriginalImagePixelSpacing_y\\n\\nClass descriptions\\nThere are 15 classes (14 diseases, and one for \"No findings\"). Images can be classified as \"No findings\" or one or more disease classes:\\nAtelectasis\\nConsolidation\\nInfiltration\\nPneumothorax\\nEdema\\nEmphysema\\nFibrosis\\nEffusion\\nPneumonia\\nPleural_thickening\\nCardiomegaly\\nNodule Mass\\nHernia\\n\\nFull Dataset Content\\nThere are 12 zip files in total and range from ~2 gb to 4 gb in size. Additionally, we randomly sampled 5% of these images and created a smaller dataset for use in Kernels. The random sample contains 5606 X-ray images and class labels.\\nSample: sample.zip\\n\\nModifications to original data\\nOriginal TAR archives were converted to ZIP archives to be compatible with the Kaggle platform\\nCSV headers slightly modified to be more explicit in comma separation and also to allow fields to be self-explanatory\\n\\nCitations\\nWang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf\\nNIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community\\nOriginal source files and documents: https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345\\n\\nAcknowledgements\\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov).',\n",
       " \"Context:\\nA permanent labor certification issued by the Department of Labor (DOL) allows an employer to hire a foreign worker to work permanently in the United States. In most instances, before the U.S. employer can submit an immigration petition to the Department of Homeland Security's U.S. Citizenship and Immigration Services (USCIS), the employer must obtain a certified labor certification application from the DOL's Employment and Training Administration (ETA). The DOL must certify to the USCIS that there are not sufficient U.S. workers able, willing, qualified and available to accept the job opportunity in the area of intended employment and that employment of the foreign worker will not adversely affect the wages and working conditions of similarly employed U.S. workers.\\nContent:\\nData covers 2012-2017 and includes information on employer, position, wage offered, job posting history, employee education and past visa history, associated lawyers, and final decision.\\nAcknowledgements:\\nThis data was collected and distributed by the US Department of Labor.\\nInspiration:\\nCan you predict visa decisions based on employee/employer/wage?\\nHow does this data compare to H1B decisions in this dataset?\",\n",
       " \"League of Legends Ranked Matches\\nData about 184070 League of Legends ranked solo games, spanning across several years\\nContent\\nMatches\\nPlayer and team stats\\nBans\\nAcknowledgements\\nI found this data on a SQL database and exported it to CSV. All data belongs ultimately to Riot Games and their data policies applies. These files are presented only as a simpler way to obtain a large dataset without stressing the Riot API and are in no way associated with Riot Games. The data is provided as-is without any warranty on its correctness. If your algorithm catches fire, don't blame me or Riot. If you are Rito and are opposed to sharing this data here, contact me and it will be removed immediately.\\nPossible questions\\nCan we predict the winner given the teams?\\nCan ranked matchmaking be assumed to be unbiased (or adjusted for red-side advantage)?\\nDoes the region affect significantly win rates?\\nCan we compare the data in relation to competitive data (also available on Kaggle)?\\nCan we assess information on the different metas?\",\n",
       " \"Context\\nSteam is the world's most popular PC Gaming hub, with over 6,000 games and a community of millions of gamers. With a massive collection that includes everything from AAA blockbusters to small indie titles, great discovery tools are a highly valuable asset for Steam. How can we make them better?\\nContent\\nThis dataset is a list of user behaviors, with columns: user-id, game-title, behavior-name, value. The behaviors included are 'purchase' and 'play'. The value indicates the degree to which the behavior was performed - in the case of 'purchase' the value is always 1, and in the case of 'play' the value represents the number of hours the user has played the game.\\nAcknowledgements\\nThis dataset is generated entirely from public Steam data, so we want to thank Steam for building such an awesome platform and community!\\nInspiration\\nThe dataset is formatted to be compatible with Tamber. Build a Tamber engine and take it for a spin!\\nCombine our collaborative filter's results with your favorite Machine Learning techniques with Ensemble Learning, or make Tamber do battle with something else you've built.\\nHave fun, The Tamber Team\",\n",
       " 'The Bay Area Bike Share enables quick, easy, and affordable bike trips around the San Francisco Bay Area. They make regular open data releases (this dataset is a transformed version of the data from this link), plus maintain a real-time API.\\nExploration Ideas\\nHow does weather impact bike trips?\\nHow do bike trip patterns vary by time of day and the day of the week?',\n",
       " \"500 actual SKUs from an outdoor apparel brand's product catalog. It's somewhat rare to get real item level data in a real-world format. Very useful for testing things like recommendation engines. In fact...maybe I'll publish some code along with this :)\",\n",
       " 'Context\\nStars mostly form in clusters and associations rather than in isolation. Milky Way star clusters are easily observable with small telescopes, and in some cases even with the naked eye. Depending on a variety of conditions, star clusters may dissolve quickly or be very long lived. The dynamical evolution of star clusters is a topic of very active research in astrophysics. Some popular models of star clusters are the so-called direct N-body simulations [1, 2], where every star is represented by a point particle that interacts gravitationally with every other particle. This kind of simulation is computationally expensive, as it scales as O(N^2) where N is the number of particles in the simulated cluster. In the following, the words \"particle\" and \"star\" are used interchangeably.\\nContent\\nThis dataset contains the positions and velocities of simulated stars (particles) in a direct N-body simulation of a star cluster. In the cluster there are initially 64000 stars distributed in position-velocity space according to a King model [3]. Each .csv file named c_xxxx.csv corresponds to a snapshot of the simulation at time t = xxxx. For example, c_0000.csv contains the initial conditions (positions and velocities of stars at time t=0). Times are measured in standard N-body units [4]. This is a system of units where G = M = −4E = 1 (G is the gravitational constant, M the total mass of the cluster, and E its total energy).\\nx, y, z Columns 1, 2, and 3 of each file are the x, y, z positions of the stars. They are also expressed in standard N-body units [4]. You can switch to units of the median radius of the cluster by finding the cluster center and calculating the median distance of stars from it, and then dividing x, y, and z by this number. In general, the median radius changes in time. The initial conditions are approximately spherically symmetric (you can check) so there is no particular physical meaning attached to the choice of x, y, and z.\\nvx, vy, vz Columns 4, 5, and 6 contain the x, y, and z velocity, also in N-body units. A scale velocity for the stars can be obtained by taking the standard deviation of velocity along one direction (e.g. z). You may check that the ratio between the typical radius (see above) and the typical velocity is of order unity.\\nm Column 7 is the mass of each star. For this simulation this is identically 1.5625e-05, i.e. 1/64000. The total mass of the cluster is initially 1. More realistic simulations (coming soon) have a spectrum of different masses and live stelar evolution, that results in changes in the mass of stars. This simulation is a pure N-body problem instead.\\nStar id number The id numbers of each particle are listed in the last column (8) of the files under the header \"id\". The ids are unique and can be used to trace the position and velocity of a star across all files. There are initially 64000 particles. At end of the simulation there are 63970. This is because some particles escape the cluster.\\nAcknowledgements\\nThis simulation was run on a Center for Galaxy Evolution Research (CGER) workstation at Yonsei University (Seoul, Korea), using the NBODY6 software (https://www.ast.cam.ac.uk/~sverre/web/pages/nbody.htm).\\nInspiration\\nSome stars hover around the center of the cluster, while some other get kicked out to the cluster outskirts or even leave the cluster altogether. Can we predict where a star will be at any given time based on its initial position and velocity? Can we predict its velocity?\\nHow correlated are the motions of stars? Can we predict the velocity of a given star based on the velocity of its neighbours?\\nThe size of the cluster can be measured by defining a center (see below) and finding the median distance of stars from it. This is called the three-dimensional effective radius. Can we predict how it evolves over time? What are its properties as a time series? What can we say about other quantiles of the radius?\\nHow to define the cluster center? Just as the mode of a KDE of the distribution of stars? How does it move over time and how to quantify the properties of its fluctuations? Is the cluster symmetric around this center?\\nSome stars leave the cluster: over time they exchange energy in close encounters with other stars and reach the escape velocity. This can be seen by comparing later snapshots with the initial one: some IDs are missing and there is overall a lower number of stars. Can we predict which stars are more likely to escape? When will a given star escape?\\nReferences\\n[1] Heggie, D., Hut, P. 2003, The Gravitational Million-Body Problem: A Multidisciplinary Approach to Star Cluster Dynamics ~ Cambridge University Press, 2003\\n[2] Aarseth, S.~J. 2003, Gravitational N-Body Simulations - Cambridge University Press, 2003\\n[3] King, I. 1966, AJ, 71, 64\\n[4] Heggie, D. C., Mathieu, R. D. 1986, Lecture Notes in Physics, Vol. 267, The Use of Supercomputers in Stellar Dynamics, Berlin, Springer',\n",
       " \"Context\\nThis is a table of shark attack incidents compiled by the Global Shark Attack File: Please see their website for more details on where this data comes from.\\nAcknowledgements\\nThis data was downloaded with permission from the Global Shark Attack File's website\",\n",
       " 'Context\\nPakistan Drone Attacks (2004-2016)\\nThe United States has targeted militants in the Federally Administered Tribal Areas [FATA] and the province of Khyber Pakhtunkhwa [KPK] in Pakistan via its Predator and Reaper drone strikes since year 2004. Pakistan Body Count (www.PakistanBodyCount.org) is the oldest and most accurate running tally of drone strikes in Pakistan. The given database (PakistanDroneAttacks.CSV) has been populated by using majority of the data from Pakistan Body Count, and building up on it by canvassing open source newspapers, media reports, think tank analyses, and personal contacts in media and law enforcement agencies. We provide a count of the people killed and injured in drone strikes, including the ones who died later in hospitals or homes due to injuries caused or aggravated by drone strikes, making it the most authentic source for drone related data in this region.\\nWe will keep releasing the updates every quarter at this page.\\nContent\\nGeography: Pakistan\\nTime period: 2004-2016\\nUnit of analysis: Attack\\nDataset: The dataset contains detailed information of 397 drone attacks in Pakistan that killed an estimated 3,558 and injured 1,333 people including 2,539 civilians.\\nVariables: The dataset contains Serial No, Incident Day & Date, Approximate Time of the attack, Specific Location, City, Province, Number of people killed who claimed to be from Al-Qaeeda, Number of people killed who claimed to be from Taliban, minimum and maximum count of foreigners killed, minimum and maximum count of civilians killed, minimum and maximum count of civilians injured, special mention (more details) and comments about the attack, longitude and latitude of the location. Sources: Unclassified media articles, hospital reports, think tank analysis and reports, and government official press releases.\\nAcknowledgements & References\\nPakistan Body Count has been leveraged extensively in scholarly publications, reports, media articles and books. The website and the dataset has been collected and curated by the founder Zeeshan-ul-hassan Usmani. Users are allowed to use, copy, distribute and cite the dataset as follows: “Zeeshan-ul-hassan Usmani, Pakistan Body Count, Drone Attacks Dataset, Kaggle Dataset Repository, Jan 25, 2017.”\\nPast Research\\nZeeshan-ul-hassan Usmani and Hira Bashir, “The Impact of Drone Strikes in Pakistan”, Cost of War Project, Brown University, December 16, 2014\\nInspiration\\nSome ideas worth exploring:\\n• How many people got killed and injured per year in last 12 years?\\n• How many attacks involved killing of actual terrorists from Al-Qaeeda and Taliban?\\n• How many attacks involved women and children?\\n• Visualize drone attacks on timeline\\n• Find out any correlation with number of drone attacks with specific date and time, for example, do we have more drone attacks in September?\\n• Find out any correlation with drone attacks and major global events (US funding to Pakistan and/or Afghanistan, Friendly talks with terrorist outfits by local or foreign government?)\\n• The number of drone attacks in Bush Vs Obama tenure?\\n• The number of drone attacks versus the global increase/decrease in terrorism?\\n• Correlation between number of drone strikes and suicide bombings in Pakistan\\nQuestions?\\nFor detailed visit www.PakistanBodyCount.org\\nOr contact Pakistan Body Count staff at info@pakistanbodycount.org',\n",
       " 'South Park cartoon lines\\n+70k lines, annotated with season, episode and speaker\\nIt is interesting to practice NLP with ML techniques in order to guess who is speaking. Later there will be file with pre-proccesed data to train',\n",
       " 'Quick Start\\nFor a quick introduction to this Dataset, take a look at the Kernel Traffic Fatalities Getting Started.\\nSee the Fatality Analysis Reporting System FARS User’s Manual for understanding the column abbreviations and possible values.\\nAlso, see the following reference\\nOriginal source of this data containing all files can be obtained here\\nBelow are the files released by the (NHTSA) National Highway Traffic Safety Administration, in their original format. Additional files can be found in the extra folder. Reference Traffic Fatalities Getting Started. for how to access this extra folder with contents.\\nData Compared to 2014\\nA few interesting notes about this data compared to 2014\\nPedalcyclist fatalities increased by 89 (12.2 percent)\\nMotorcyclist fatalities increased by 382 (8.3-percent increase)\\nAlcohol-impaired driving fatalities increased by 3.2 percent, from 9,943 in 2014 to 10,265 in 2015\\nVehicle miles traveled (VMT) increased by 3.5 percent from 2014 to 2015, the largest increase since 1992, nearly 25 years ago.\\nSee TRAFFIC SAFETY FACTS for more detail on the above findings.',\n",
       " 'Overview\\nThe World of Warcraft Avatar History Dataset is a collection of records that detail information about player characters in the game over time. It includes information about their character level, race, class, location, and social guild. The Kaggle version of this dataset includes only the information from 2008 (and the dataset in general only includes information from the \\'Horde\\' faction of players in the game from a single game server).\\nFull Dataset Source and Information: http://mmnet.iis.sinica.edu.tw/dl/wowah/\\nCode used to clean the data: https://github.com/myles-oneill/WoWAH-parser\\nIdeas for Using the Dataset\\nFrom the perspective of game system designers, players\\' behavior is one of the most important factors they must consider when designing game systems. To gain a fundamental understanding of the game play behavior of online gamers, exploring users\\' game play time provides a good starting point. This is because the concept of game play time is applicable to all genres of games and it enables us to model the system workload as well as the impact of system and network QoS on users\\' behavior. It can even help us predict players\\' loyalty to specific games.\\nOpen Questions\\nUnderstand user gameplay behavior (game sessions, movement, leveling)\\nUnderstand user interactions (guilds)\\nPredict players unsubscribing from the game based on activity\\nWhat are the most popular zones in WoW, what level players tend to inhabit each?\\nWrath of the Lich King\\nAn expansion to World of Warcraft, \"Wrath of the Lich King\" (Wotlk) was released on November 13, 2008. It introduced new zones for players to go to, a new character class (the death knight), and a new level cap of 80 (up from 70 previously). This event intersects nicely with the dataset and is probably interesting to investigate.\\nMap\\nThis dataset doesn\\'t include a shapefile (if you know of one that exists, let me know!) to show where the zones the dataset talks about are. Here is a list of zones an information from this version of the game, including their recommended levels: http://wowwiki.wikia.com/wiki/Zones_by_level_(original) .\\nUpdate (Version 3): dmi3kno has generously put together some supplementary zone information files which have now been included in this dataset. Some notes about the files:\\nNote that some zone names contain Chinese characters. Unicode names are preserved as a key to the original dataset. What this addition will allow is to understand properties of the zones a bit better - their relative location to each other, competititive properties, type of gameplay and, hopefully, their contribution to character leveling. Location coordinates contain some redundant (and possibly duplicate) records as they are collected from different sources. Working with uncleaned location coordinate data will allow users to demonstrate their data wrangling skills (both working with strings and spatial data).',\n",
       " \"Context\\nPitchfork is a music-centric online magazine. It was started in 1995 and grew out of independent music reviewing into a general publication format, but is still famed for its variety music reviews. I scraped over 18,000 Pitchfork reviews (going back to January 1999). Initially, this was done to satisfy a few of my own curiosities, but I bet Kagglers can come up with some really interesting analyses!\\nContent\\nThis dataset is provided as a sqlite database with the following tables: artists, content, genres, labels, reviews, years. For column-level information on specific tables, refer to the Metadata tab.\\nInspiration\\nDo review scores for individual artists generally improve over time, or go down?\\nHow has Pitchfork's review genre selection changed over time?\\nWho are the most highly rated artists? The least highly rated artists?\\nAcknowledgements\\nGotta love Beautiful Soup!\",\n",
       " 'Context\\nThe Challenge - One challenge of modeling retail data is the need to make decisions based on limited history. Holidays and select major events come once a year, and so does the chance to see how strategic decisions impacted the bottom line. In addition, markdowns are known to affect sales – the challenge is to predict which departments will be affected and to what extent.\\nContent\\nYou are provided with historical sales data for 45 stores located in different regions - each store contains a number of departments. The company also runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks.\\nWithin the Excel Sheet, there are 3 Tabs – Stores, Features and Sales\\nStores\\nAnonymized information about the 45 stores, indicating the type and size of store\\nFeatures\\nContains additional data related to the store, department, and regional activity for the given dates.\\nStore - the store number\\nDate - the week\\nTemperature - average temperature in the region\\nFuel_Price - cost of fuel in the region\\nMarkDown1-5 - anonymized data related to promotional markdowns. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA\\nCPI - the consumer price index\\nUnemployment - the unemployment rate\\nIsHoliday - whether the week is a special holiday week\\nSales\\nHistorical sales data, which covers to 2010-02-05 to 2012-11-01. Within this tab you will find the following fields:\\nStore - the store number\\nDept - the department number\\nDate - the week\\nWeekly_Sales -  sales for the given department in the given store\\nIsHoliday - whether the week is a special holiday week\\nThe Task\\nPredict the department-wide sales for each store for the following year\\nModel the effects of markdowns on holiday weeks\\nProvide recommended actions based on the insights drawn, with prioritization placed on largest business impact',\n",
       " 'Context\\nHealthStats provides key health, nutrition and population statistics gathered from a variety of international sources. Themes include population dynamics, nutrition, reproductive health, health financing, medical resources and usage, immunization, infectious diseases, HIV/AIDS, DALY, population projections and lending. HealthStats also includes health, nutrition and population statistics by wealth quintiles.\\nContent\\nThis dataset includes 345 indicators, such as immunization rates, malnutrition prevalence, and vitamin A supplementation rates across 263 countries around the world. Data was collected on a yearly basis from 1960-2016.\\nInspiration\\nIn your opinion, what are some of the more surprising indicators? Are there any you would consider adding?\\nIs there a relationship between condom use and rates of children born with HIV? How do these rates compare over time?\\nWhich countries have the highest consumption of iodized salt? Has this indicator changed over time, and if so, in which countries? Are there any other indicators that seem to correlate with this one?\\nAcknowledgements\\nData was acquired from the World Bank, and can be accessed in multiple formats here.',\n",
       " 'Context\\nThis dataset is meant to be used with other datasets that have features like country and city but no latitude/longitude. It is simply a list of cities in the world. Being able to put cities on a map will help people tell their stories more effectively. Another way to think about it is that you can use this make more pretty graphs!\\nContent\\nFields:\\ncity\\nregion\\ncountry\\npopulation\\nlatitude\\nlongitude\\nAcknowledgements\\nThese data come from Maxmind.com and have not been altered. The original source can be found by clicking here\\nAdditionally, the Maxmind sharing license has been included.\\nInspiration\\nI wanted to analyze a dataset and make a map, but I was only given a city name without any latitude or longotude coordinates. I found this dataset very helpful and I hoe you do too!',\n",
       " 'The Customer Support on Twitter dataset is a large, modern corpus of tweets and replies to aid innovation in natural language understanding and conversational models, and for study of modern customer support practices and impact.\\nContext\\nNatural language remains the densest encoding of human experience we have, and innovation in NLP has accelerated to power understanding of that data, but the datasets driving this innovation don\\'t match the real language in use today. The Customer Support on Twitter dataset offers a large corpus of modern English (mostly) conversations between consumers and customer support agents on Twitter, and has three important advantages over other conversational text datasets:\\nFocused - Consumers contact customer support to have a specific problem solved, and the manifold of problems to be discussed is relatively small, especially compared to unconstrained conversational datasets like the reddit Corpus.\\nNatural - Consumers in this dataset come from a much broader segment than those in the Ubuntu Dialogue Corpus and have much more natural and recent use of typed text than the Cornell Movie Dialogs Corpus.\\nSuccinct - Twitter\\'s brevity causes more natural responses from support agents (rather than scripted), and to-the-point descriptions of problems and solutions. Also, its convenient in allowing for a relatively low message limit size for recurrent nets.\\nInspiration\\nThe size and breadth of this dataset inspires many interesting questions:\\nCan we predict company responses? Given the bounded set of subjects handled by each company, the answer seems like yes!\\nDo requests get stale? How quickly do the best companies respond, compared to the worst?\\nCan we learn high quality dense embeddings or representations of similarity for topical clustering?\\nHow does tone affect the customer support conversation? Does saying sorry help?\\nCan we help companies identify new problems, or ones most affecting their customers?\\nContent\\nThe dataset is a CSV, where each row is a tweet. The different columns are described below. Every conversation included has at least one request from a consumer and at least one response from a company. Which user IDs are company user IDs can be calculated using the inbound field.\\ntweet_id\\nA unique, anonymized ID for the Tweet. Referenced by response_tweet_id and in_response_to_tweet_id.\\nauthor_id\\nA unique, anonymized user ID. @s in the dataset have been replaced with their associated anonymized user ID.\\ninbound\\nWhether the tweet is \"inbound\" to a company doing customer support on Twitter. This feature is useful when re-organizing data for training conversational models.\\ncreated_at\\nDate and time when the tweet was sent.\\ntext\\nTweet content. Sensitive information like phone numbers and email addresses are replaced with mask values like __email__.\\nresponse_tweet_id\\nIDs of tweets that are responses to this tweet, comma-separated.\\nin_response_to_tweet_id\\nID of the tweet this tweet is in response to, if any.\\nContributing\\nKnow of other brands the dataset should include? Found something that needs to be fixed? Start a discussion, or email me directly at $FIRSTNAME@$LASTNAME.com!\\nAcknowledgements\\nA huge thank you to my friends who helped bootstrap the list of companies that do customer support on Twitter! There are many rocks that would have been left un-turned were it not for your suggestions!\\nRelevant Resources\\nNLTK - casual_tokenize for social media text tokenizing, vader sentiment analysis for social media text\\nSciKit Learn - BoW Count Vectorizer, Multinomial Naive Bayes Classifier\\nTopic Modeling via Phrase detection with gensim\\nfacebook research - fastText text classifier',\n",
       " 'Context\\nRecent growing interest in cryptocurrencies, specifically as a speculative investment vehicle, has sparked global conversation over the past 12 months. Although this data is available across various sites, there is a lack of understanding as to what is driving the exponential rise of many individual currencies. This data set is intended to be a starting point for a detailed analysis into what is driving price action, and what can be done to predict future movement.\\nContent\\nConsolidated financial information for the top 200 cryptocurrencies by marketcap. Pulled from CoinMarketCap.com. Attributes include:\\nCurrency name (e.g. bitcoin)\\nDate\\nOpen\\nHigh\\nLow\\nClose\\nVolume\\nMarketcap\\nInspiration\\nFor the past few months I have been searching for a reliable source for historical price information related to cryptocurrencies. I wasn\\'t able to find anything that I could use to my liking, so I built my own data set.\\nI\\'ve written a small script that scrapes historical price information for the top 200 coins by market cap as listed on CoinMarketCap.com.\\nI plan to run some basic analysis on it to answer questions that I have a \"gut\" feeling about, but no quantitative evidence (yet!).\\nQuestions such as:\\nWhat is the correlation between bitcoin and alt coin prices?\\nWhat is the average age of the top 10 coins by market cap?\\nWhat day of the week is best to buy/sell?\\nWhich coins in the top two hundred are less than 6 months old?\\nWhich currencies are the most volatile?\\nWhat the hell happens when we go to bed and Asia starts trading?\\nFeel free to use this for your own purposes! I just ask that you share your results with the group when complete. Happy hunting!',\n",
       " 'On February 11th 2016 LIGO-Virgo collaboration gave the announce of the discovery of Gravitational Waves, just 100 years after the Einstein’s paper on their prediction. The LIGO Scientific Collaboration (LSC) and the Virgo Collaboration prepared a web page to inform the broader community about a confirmed astrophysical event observed by the gravitational-wave detectors, and to make the data around that time available for others to analyze: https://losc.ligo.org/events/GW150914/\\nYou can find much more information on the LOSC web site, and a good starting tutorial at the following link:\\nhttps://losc.ligo.org/tutorial00/\\nThese data sets contain 32 secs of data sampled at 4096Hz an 16384Hz around the GW event detected on 14/09/2015.\\nLonger sets of data can be downloaded here\\nhttps://losc.ligo.org/s/events/GW150914/H-H1_LOSC_4_V1-1126257414-4096.hdf5\\nhttps://losc.ligo.org/s/events/GW150914/L-L1_LOSC_4_V1-1126257414-4096.hdf5\\nhttps://losc.ligo.org/s/events/GW150914/H-H1_LOSC_16_V1-1126257414-4096.hdf5\\nhttps://losc.ligo.org/s/events/GW150914/L-L1_LOSC_16_V1-1126257414-4096.hdf5\\nHow to acknowledge use of this data: If your research used data from one of the data releases, please cite as:\\nLIGO Scientific Collaboration, \"LIGO Open Science Center release of S5\", 2014, DOI 10.7935/K5WD3XHR\\nLIGO Scientific Collaboration, \"LIGO Open Science Center release of S6\", 2015, DOI 10.7935/K5RN35SD\\nLIGO Scientific Collaboration, \"LIGO Open Science Center release of GW150914\", 2016, DOI10.7935/K5MW2F23\\nand please include the statement \"This research has made use of data, software and/or web tools obtained from the LIGO Open Science Center (https://losc.ligo.org), a service of LIGO Laboratory and the LIGO Scientific Collaboration. LIGO is funded by the U.S. National Science Foundation.\"\\nIf you would also like to cite a published paper, M Vallisneri et al. \"The LIGO Open Science Center\", proceedings of the 10th LISA Symposium, University of Florida, Gainesville, May 18-23, 2014; also arxiv:1410.4839\\nPublications We request that you let the LOSC team know if you publish (or intend to publish) a paper using data released from this site. If you would like, we may be able to review your work prior to publication, as we do for our colleagues in the LIGO Scientific Collaboration. Credits LOSC Development: The LOSC Team and The LIGO Scientific Collaboration\\nThe data products made available through the LOSC web service are created and maintained by LIGO Lab and the LIGO Scientific Collaboration. The development of this web page was a team effort, with all members of the LOSC team making contributions in most areas. In addition to the team members listed below, a large number of individuals in the LIGO Scientific Collaboration have contributed content and advice. The LOSC team includes:\\nAlan Weinstein: LOSC Director\\nRoy Williams: LOSC Developer, web services and data base architecture\\nJonah Kanner: LOSC Developer, tutorials, documentation, data set curation\\nMichele Vallisneri: LOSC Developer, data quality curation\\nBranson Stephens: LOSC Developer, event database and web site architecture\\nPlease send any comments, questions, or concerns to: losc@ligo.org',\n",
       " \"Drosophila Melanogaster\\nDrosophila Melanogaster, the common fruit fly, is a model organism which has been extensively used in entymological research. It is one of the most studied organisms in biological research, particularly in genetics and developmental biology.\\nWhen its not being used for scientific research, D. melanogaster is a common pest in homes, restaurants, and anywhere else that serves food. They are not to be confused with Tephritidae flys (also known as fruit flys).\\nhttps://en.wikipedia.org/wiki/Drosophila_melanogaster\\nAbout the Genome\\nThis genome was first sequenced in 2000. It contains four pairs of chromosomes (2,3,4 and X/Y). More than 60% of the genome appears to be functional non-protein-coding DNA.\\nThe genome is maintained and frequently updated at FlyBase. This dataset is sourced from the UCSC Genome Bioinformatics download page. It uses the August 2014 version of the D. melanogaster genome (dm6, BDGP Release 6 + ISO1 MT). http://hgdownload.soe.ucsc.edu/downloads.html#fruitfly\\nFiles were modified by Kaggle to be a better fit for analysis on Scripts. This primarily involved turning files into CSV format, with a header row, as well as converting the genome itself from 2bit format into a FASTA sequence file.\\nBioinformatics\\nGenomic analysis can be daunting to data scientists who haven't had much experience with bioinformatics before. We have tried to give basic explanations to each of the files in this dataset, as well as links to further reading on the biological basis for each. If you haven't had the chance to study much biology before, some light reading (ie wikipedia) on the following topics may be helpful to understand the nuances of the data provided here: Genetics, Genomics (Sequencing/Genome Assembly), Chromosomes, DNA, RNA (mRNA/miRNA), Genes, Alleles, Exons, Introns, Transcription, Translation, Peptides, Proteins, Gene Regulation, Mutation, Phylogenetics, and SNPs.\\nOf course, if you've got some idea of the basics already - don't be afraid to jump right in!\\nLearning Bioinformatics\\nThere are a lot of great resources for learning bioinformatics on the web. One cool site is Rosalind - a platform that gives you bioinformatic coding challenges to complete. You can use Kaggle Scripts on this dataset to easily complete the challenges on Rosalind (and see Myles' solutions here if you get stuck). We have set up Biopython on Kaggle's docker image which is a great library to help you with your analyses. Check out their tutorial here and we've also created a python notebook with some of the tutorial applied to this dataset as a reference.\\nFiles in this Dataset\\nDrosophila Melanogaster Genome\\ngenome.fa\\nThe assembled genome itself is presented here in FASTA format. Each chromosome is a different sequence of nucleotides. Repeats from RepeatMasker and Tandem Repeats Finder (with period of 12 or less) are show in lower case; non-repeating sequence is shown in upper case.\\nMeta Information\\nThere are 3 additional files with meta information about the genome.\\nmeta-cpg-island-ext-unmasked.csv\\nThis file contains descriptive information about CpG Islands in the genome.\\nhttps://en.wikipedia.org/wiki/CpG_site\\nmeta-cytoband.csv\\nThis file describes the positions of cytogenic bands on each chromosome.\\nhttps://en.wikipedia.org/wiki/Cytogenetics\\nmeta-simple-repeat.csv\\nThis file describes simple tandem repeats in the genome.\\nhttps://en.wikipedia.org/wiki/Repeated_sequence_(DNA) https://en.wikipedia.org/wiki/Tandem_repeat\\nDrosophila Melanogaster mRNA Sequences\\nMessenger RNA (mRNA) is an intermediate molecule created as part of the cellular process of converting genomic information into proteins. Some mRNA are never translated into proteins and have functional roles in the cell on their own. Collectively, organism mRNA information is known as a Transcriptome. mRNA files included in this dataset give insight into the activity of genes in the organism.\\nhttps://en.wikipedia.org/wiki/Messenger_RNA\\nmrna-genbank.fa\\nThis file includes all mRNA sequences from GenBank associated with Drosophila Melanogaster.\\nhttp://www.ncbi.nlm.nih.gov/genbank/\\nmrna-refseq.fa\\nThis file includes all mRNA sequences from RefSeq associated with Drosophila Melanogaster.\\nhttp://www.ncbi.nlm.nih.gov/refseq/\\nGene Predictions\\nA gene is a segment of DNA on the genome which, through mRNA, is used to create proteins in the organism. Knowing which parts of DNA are coding (genes) or non-coding is difficult, and a number of different systems for prediction exist. This dataset includes a number of different gene prediction systems applied to the drosophila melanogaster genome.\\nhttps://en.wikipedia.org/wiki/Gene_prediction\\ngenes-augustus.csv\\nAUGUSTUS is a piece of software that predicts genes ab initio using Hidden Markov Models. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC441517/\\ngenes-genscan.csv\\nGENSCAN is an older ab initio software for predicting genes. http://genes.mit.edu/GENSCANinfo.html\\ngenes-ensembl.csv\\nensembl-gtp.csv\\nensembl-pep.csv\\nensembl-source.csv\\nensembl-to-gene-name.csv\\nEnsembl provides gene annotation generated by their software Genebuild. This process combines automatic annotation alongside manual curation. http://uswest.ensembl.org/info/genome/genebuild/genome_annotation.html\\nWe have also included some supplementary files for these, including predicted protein peptide sequences for each predicted gene.\\ngenes-refseq.csv\\ngenes-xeno-refseq.csv\\nrefseq-link.csv\\nrefseq-summary.csv\\nWe have included two RefSeq gene predictions in this dataset. The first is based solely on information from the drosophila melanogaster genome. The second (genes-xeno-refseq.csv) uses genes from other organisms as a basis for predicting genes in drosophila melanogaster.\\nRefSeq RNAs were aligned against the D. melanogaster genome using blat; those with an alignment of less than 15% were discarded. When a single RNA aligned in multiple places, the alignment having the highest base identity was identified. Only alignments having a base identity level within 0.1% of the best and at least 96% base identity with the genomic sequence were kept.\\nWe have also included supplementary files for these which include information about the genes that have been identified.\\nhttp://www.ncbi.nlm.nih.gov/refseq/\\nWhat can you do with this data?\\nGenomic data is the foundation of bioinformatics, and there is an incredible array of things you can do with this data. A good place to start is to look at some of the meta supplementary files alongside the genomic sequence itself.\\nWe have a number of different gene prediction systems in the dataset, how do they compare to each other? How do they compare to the mRNA data?\\nWorking back from the refseq-summary.csv file, you can look at genes that code for particular proteins - can you find these genes in the genome?\\nHow much of the genome codes for the mRNA's found in our mRNA data? Of the mRNA's we have, how many map to the predicted genes and the predicted peptided sequence data? How much of the mRNA seems to be protein-coding vs how much looks like it is miRNA? Can you find pre-mRNA or splice variants within the mRNA data? Does meta information like cytogenic bands or CpG sites correspond with splice variants or a lack of mRNA altogether?\\nThose are just some of many ideas that could get you started.\\nLooking for Feedback\\nThis is the first genomic dataset on Kaggle and we are looking for feedback from our community about how interesting this dataset is to them, or if there are ways we could improve it to better suit analysis. Please post suggestions for supplementary data, future genomes we could host, bioinformatics packages we should include on scripts, and any other feedback on the dataset forum.\",\n",
       " \"Context\\nI wanted to see how articles clustered together if the articles were rendered into document-term matrices---would there be greater affinity among political affiliations, or medium, subject matter, etc. The data was scraped using BeautifulSoup and stored in Sqlite, but I've chopped it up into three separate CSVs here, because the entire Sqlite database came out to about 1.2 gb, beyond Kaggle's max.\\nContent\\nEach row contains:\\nan id for the Sqlite database\\nauthor name\\nfull date\\nmonth\\nyear\\ntitle\\npublication name\\narticle url (not available for all articles)\\nfull article content\\nThe publications include the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. Sampling wasn't quite scientific; I chose publications based on my familiarity of the domain and tried to get a range of political alignments, as well as a mix of print and digital publications. By count, the publications break down accordingly:\\nIt's not entirely even---this was something of a collect-it-all approach, and some sites are more prolific than others, and some have data that maintains integrity after scraping more easily than others.\\nFor each publication, I used archive.org to grab the past year-and-a-half of either home-page headlines or RSS feeds and ran those links through the scraper. That is, the articles are not the product of scraping an entire site, but rather their more prominently placed articles. For example, CNN's articles from 5/6/16 were what appeared on the homepage of CNN.com proper, not everything within the CNN.com domain. Vox's articles from 5/6/16 were everything that appeared in the Vox RSS reader. on 5/6/16, and so on. RSS readers are a breeze to scrape, and so I used them when possible, but not every publication uses them or makes them easy to find.\\nThe data primarily falls between the years of 2016 and July 2017, although there is a not-insignificant number of articles from 2015, and a possibly insignificant number from before then.\\nA note: there are some stray spaces between non-word characters at times as well as some other minor blemishes and imperfections here and there, the result of cleaning a very messy dataset.\\nAcknowledgements\\nThanks mostly go to the maesters of Stack Overflow.\",\n",
       " 'This dataset consists of 101 animals from a zoo. There are 16 variables with various traits to describe the animals. The 7 Class Types are: Mammal, Bird, Reptile, Fish, Amphibian, Bug and Invertebrate\\nThe purpose for this dataset is to be able to predict the classification of the animals, based upon the variables. It is the perfect dataset for those who are new to learning Machine Learning.\\nzoo.csv\\nAttribute Information: (name of attribute and type of value domain)\\nanimal_name: Unique for each instance\\nhair Boolean\\nfeathers Boolean\\neggs Boolean\\nmilk Boolean\\nairborne Boolean\\naquatic Boolean\\npredator Boolean\\ntoothed Boolean\\nbackbone Boolean\\nbreathes Boolean\\nvenomous Boolean\\nfins Boolean\\nlegs Numeric (set of values: {0,2,4,5,6,8})\\ntail Boolean\\ndomestic Boolean\\ncatsize Boolean\\nclass_type Numeric (integer values in range [1,7])\\nclass.csv\\nThis csv describes the dataset\\nClass_Number Numeric (integer values in range [1,7])\\nNumber_Of_Animal_Species_In_Class Numeric\\nClass_Type character -- The actual word description of the class\\nAnimal_Names character -- The list of the animals that fall in the category of the class\\nAcknowledgements\\nUCI Machine Learning: https://archive.ics.uci.edu/ml/datasets/Zoo\\nSource Information -- Creator: Richard Forsyth -- Donor: Richard S. Forsyth 8 Grosvenor Avenue Mapperley Park Nottingham NG3 5DX 0602-621676 -- Date: 5/15/1990\\nInspiration\\nWhat are the best machine learning ensembles/methods for classifying these animals based upon the variables given?',\n",
       " \"Acknowledgements\\nThe data was scraped from Booking.com. All data in the file is publicly available to everyone already. Data is originally owned by Booking.com. Please contact me through my profile if you want to use this dataset somewhere else.\\nData Context\\nThis dataset contains 515,000 customer reviews and scoring of 1493 luxury hotels across Europe. Meanwhile, the geographical location of hotels are also provided for further analysis.\\nData Content\\nThe csv file contains 17 fields. The description of each field is as below:\\nHotel_Address: Address of hotel.\\nReview_Date: Date when reviewer posted the corresponding review.\\nAverage_Score: Average Score of the hotel, calculated based on the latest comment in the last year.\\nHotel_Name: Name of Hotel\\nReviewer_Nationality: Nationality of Reviewer\\nNegative_Review: Negative Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Negative'\\nReview_Total_Negative_Word_Counts: Total number of words in the negative review.\\nPositive_Review: Positive Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Positive'\\nReview_Total_Positive_Word_Counts: Total number of words in the positive review.\\nReviewer_Score: Score the reviewer has given to the hotel, based on his/her experience\\nTotal_Number_of_Reviews_Reviewer_Has_Given: Number of Reviews the reviewers has given in the past.\\nTotal_Number_of_Reviews: Total number of valid reviews the hotel has.\\nTags: Tags reviewer gave the hotel.\\ndays_since_review: Duration between the review date and scrape date.\\nAdditional_Number_of_Scoring: There are also some guests who just made a scoring on the service rather than a review. This number indicates how many valid scores without review in there.\\nlat: Latitude of the hotel\\nlng: longtitude of the hotel\\nIn order to keep the text data clean, I removed unicode and punctuation in the text data and transform text into lower case. No other preprocessing was performed.\\nInspiration\\nThe dataset is large and informative, I believe you can have a lot of fun with it! Let me put some ideas below to futher inspire kagglers!\\nFit a regression model on reviews and score to see which words are more indicative to a higher/lower score\\nPerform a sentiment analysis on the reviews\\nFind correlation between reviewer's nationality and scores.\\nBeautiful and informative visualization on the dataset.\\nClustering hotels based on reviews\\nSimple recommendation engine to the guest who is fond of a special characteristic of hotel.\\nThe idea is unlimited! Please, have a look into data, generate some ideas and leave a master kernel here! I am ready to upvote your ideas and kernels! Cheers!\",\n",
       " 'This dataset contains historical prices as tracked by www.coinmarketcap.com for the top 100 cryptocurrencies by market capitalization as of September 22, 2017, and is current to that date.\\nEach CSV file is named by its cryptocurrency as named on www.coinmarketcap.com, with the sole exception of \"I-O Coin\" in place of I/O Coin for ease of importing.\\nAlso accompanying the zip of the top 100 CSVs is a CSV named \"Top 100.csv\", which is a list of the top 100, ordered 1 to 100 with Bitcoin at the beginning and GridCoin at the end. The second row of this CSV is the Market Cap as of September 22, 2017.\\nRow descriptions - Date, string, e.g. \"Sep 22, 2017\" - Open, float (2 decimal places), e.g. 1234.00 - High, float (2 decimal places), e.g. 1234.00 - Low, float (2 decimal places), e.g. 1234.00 - Close, float (2 decimal places), e.g. 1234.00 - Volume [traded in 24 hours], string, e.g. \"1,234,567,890\" - Market Cap [Market capitalization], string, e.g. \"1,234,567,890\"\\nThis is my first dataset and I would greatly appreciate your feedback. Thanks and enjoy!',\n",
       " \"Context\\nIHME United States Mortality Rates by County 1980-2014: National - All. (Deaths per 100,000 population)\\nTo quickly get started creating maps, like the one below, see the Quick Start R kernel.\\nHow the Dataset was Created\\nThis Dataset was created from the Excel Spreadsheet, which can be found in the download. Or, you can view the source here. If you take a look at the row for United States, for the column Mortality Rate, 1980*, you'll see the set of numbers 1.52 (1.44, 1.61). Numbers in parentheses are 95% uncertainty. The 1.52 is an age-standardized mortality rate for both sexes combined (deaths per 100,000 population).\\nIn this Dataset 1.44 will be placed in the named column Mortality Rage, 1989 (Min)* and 1.61 is in column named Mortality Rate, 1980 (Max)* . For information on how these Age-standardized mortality rates were calculated, see the December JAMA 2016 article, which you can download for free.\\nReference\\nJAMA Full Article\\nVideo Describing this Study (Short and this is worth viewing)\\nData Resources\\nHow Americans Die May Depend On Where They Live, by Anna Maria Barry-Jester (FiveThirtyEight)\\nInteractive Map from healthdata.org\\nIHME Data\\nAcknowledgements\\nThis Dataset was provided by IHME\\nInstitute for Health Metrics and Evaluation 2301 Fifth Ave., Suite 600, Seattle, WA 98121, USA Tel: +1.206.897.2800 Fax: +1.206.897.2899 © 2016 University of Washington\",\n",
       " 'Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nIndia - Annual Health Survey(AHS) 2012-13:\\nThe survey was conducted in Empowered Action Group (EAG) states Uttarakhand, Rajasthan, Uttar Pradesh, Bihar, Jharkhand, Odisha, Chhattisgarh & Madhya Pradesh and Assam. These nine states, which account for about 48 percent of the total population, 59 percent of Births, 70 percent of Infant Deaths, 75 percent of Under 5 Deaths and 62 percent of Maternal Deaths in the country, are the high focus States in view of their relatively higher fertility and mortality.\\nA representative sample of about 21 million population and 4.32 million households were covered 20k+ sample units which is spread across rural and urban area of these 9 states.\\nThe objective of the AHS is to yield a comprehensive, representative and reliable dataset on core vital indicators including composite ones like Infant Mortality Rate, Maternal Mortality Ratio and Total Fertility Rate along with their co-variates (process and outcome indicators) at the district level and map the changes therein on an annual basis. These benchmarks would help in better and holistic understanding and timely monitoring of various determinants on well-being and health of population particularly Reproductive and Child Health. Source\\nContent\\nThis dataset contains the data about the below 26 key indicators.\\nAA. Sample Particulars\\nSample Units\\nHouseholds\\nPopulation\\nEver Married Women (aged 15-49 years)\\nCurrently Married Women (aged 15-49 years)\\nChildren 12-23 months\\nBB. Household Characteristics\\nAverage Household Size\\nSC\\nST\\nAll\\nPopulation below age 15 years (%)\\nDependency Ratio\\nCurrently Married Illiterate Women aged 15-49 years (%)\\nCC. Sex Ratio\\nSex Ratio at Birth\\nSex Ratio (0- 4 years)\\nSex Ratio (All ages)\\nDD. Effective Literacy Rate\\nEE. Marriage\\nMarriages among Females below legal age (18 years) (%)\\nMarriages among Males below legal age (21 years) (%)\\nCurrently Married Women aged 20-24 years married before legal age (18 years) (%)\\nCurrently Married Men aged 25-29 years married before legal age (21 years) (%)\\nMean age at Marriage# - Male\\nMean age at Marriage# - Female\\nFF. Schooling Status\\nChildren currently attending school (Age 6-17 years) (%)\\nChildren attended before / Drop out (Age 6-17 years) (%)\\nGG. Work Status\\nChildren aged 5-14 years engaged in work (%)\\nWork Participation Rate (15 years and above)\\nHH. Disability\\nPrevalence of any type of Disability (Per 100,000 Population)\\nII. Injury\\nNumber of Injured Persons by type of Treatment received (Per 100,000 Population)\\nSevere\\nMajor\\nMinor\\nJJ. Acute Illness\\nPersons suffering from Acute Illness (Per 100,000 Population)\\nDiarrhoea/Dysentery\\nAcute Respiratory Infection (ARI)\\nFever (All Types)\\nAny type of Acute Illness\\nPersons suffering from Acute Illness and taking treatment from Any Source (%)\\nPersons suffering from Acute Illness and taking treatment from Government Source (%)\\nKK. Chronic Illness\\nHaving any kind of Symptoms of Chronic Illness (Per 100,000 Population)\\nHaving any kind of Symptoms of Chronic Illness and sought Medical Care (%)\\nHaving diagnosed for Chronic Illness (Per 100,000 Population)\\nDiabetes\\nHypertension\\nTuberculosis (TB)\\nAsthma / Chronic Respiratory Disease\\nArthritis\\nAny kind of Chronic Illness\\nHaving diagnosed for any kind of Chronic Illness and getting Regular Treatment (%)\\nHaving diagnosed for any kind of Chronic Illness and getting Regular Treatment from Government Source (%)\\nLL. Fertility\\nCrude Birth Rate (CBR)\\nNatural Growth Rate\\nTotal Fertility Rate\\nWomen aged 20-24 reporting birth of order 2 & above (%)\\nWomen reporting birth of order 3 & above (%)\\nWomen with two children wanting no more children (%)\\nWomen aged 15-19 years who were already mothers or pregnant at the time of survey (%)\\nMedian age at first live birth of Women aged 15-49 years\\nMedian age at first live birth of Women aged 25-49 years\\nLive Births taking place after an interval of 36 months (%)\\nMean number of children ever born to Women aged 15-49 years\\nMean number of children surviving to Women aged 15-49 years\\nMean number of children ever born to Women aged 45-49 years\\nMM. Abortion\\nPregnancy to Women aged 15-49 years resulting in abortion (%)\\nWomen who received any ANC before abortion (%)\\nWomen who went for Ultrasound before abortion (%)\\nAverage Month of pregnancy at the time of abortion\\nAbortion performed by skilled health personnel (%)\\nAbortion taking place in Institution (%)\\nNN. Family Planning Practices (Cmw Aged 15-49 Years)\\nCurrent Usage\\nAny Method (%)\\nAny Modern Method (%)\\nFemale Sterilization (%)\\nMale Sterilization (%)\\nCopper-T/IUD (%)\\nPills (%)\\nCondom/Nirodh (%)\\nEmergency Contraceptive Pills (%)\\nAny Traditional Method (%)\\nPeriodic Abstinence (%)\\nWithdrawal (%)\\nLAM (%)\\nOO. Unmet Need For Family Planning\\nUnmet need for Spacing (%)\\nUnmet need for Limiting (%)\\nTotal Unmet need (%)\\nPP. Ante Natal Care\\nCurrently Married Pregnant Women aged 15-49 years registered for ANC (%)\\nMothers who received any Antenatal Check-up (%)\\nMothers who had Antenatal Check-up in First Trimester (%)\\nMothers who received 3 or more Antenatal Care (%)\\nMothers who received at least one Tetanus Toxoid (TT) injection (%)\\nMothers who consumed IFA for 100 days or more (%)\\nMothers who had Full Antenatal Check-up (%)\\nMothers who received ANC from Govt. Source (%)\\nMothers whose Blood Pressure (BP) taken (%)\\nMothers whose Blood taken for Hb (%)\\nMothers who underwent Ultrasound (%)\\nQQ. Delivery Care\\nInstitutional Delivery (%)\\nDelivery at Government Institution (%)\\nDelivery at Private Institution (%)\\nDelivery at Home (%)\\nDelivery at home conducted by skilled health personnel (%)\\nSafe Delivery (%)\\nCaesarean out of total delivery taken place in Government Institutions (%)\\nCaesarean out of total delivery taken place in Private Institutions (%)\\nRR. Post Natal Care\\nLess than 24 hrs. stay in institution after delivery (%)\\nMothers who received Post-natal Check-up within 48 hrs. of delivery (%)\\nMothers who received Post-natal Check-up within 1 week of delivery (%)\\nMothers who did not receive any Post-natal Check-up (%)\\nNew borns who were checked up within 24 hrs. of birth (%)\\nSS. Janani Suraksha Yojana (JSY)\\nMothers who availed financial assistance for delivery under JSY (%)\\nMothers who availed financial assistance for institutional delivery under JSY (%)\\nMothers who availed financial assistance for Government Institutional delivery under JSY (%)\\nTT. Immunization, Vitamin A & Iron Supplement And Birth Weight\\nChildren aged 12-23 months having Immunization Card (%)\\nChildren aged 12-23 months who have received BCG (%)\\nChildren aged 12-23 months who have received 3 doses of Polio vaccine (%)\\nChildren aged 12-23 months who have received 3 doses of DPT vaccine (%)\\nChildren aged 12-23 months who have received Measles vaccine (%)\\nChildren aged 12-23 months Fully Immunized (%)\\nChildren who have received Polio dose at birth (%)\\nChildren who did not receive any vaccination (%)\\nChildren (aged 6-35 months) who received at least one Vitamin A dose during last six months (%)\\nChildren (aged 6-35 months) who received IFA tablets/syrup during last 3 months (%)\\nChildren whose birth weight was taken (%)\\nChildren with birth weight less than 2.5 Kg. (%)\\nUU. Childhood Diseases\\nChildren suffering from Diarrhoea (%)\\nChildren suffering from Diarrhoea who received HAF/ORS/ORT (%)\\nChildren suffering from Acute Respiratory Infection (%)\\nChildren suffering from Acute Respiratory Infection who sought treatment (%)\\nChildren suffering from Fever (%)\\nChildren suffering from Fever who sought treatment (%)\\nVV. Breastfeeding And Supplementation\\nChildren breastfed within one hour of birth (%)\\nChildren (aged 6-35 months) exclusively breastfed for at least six months (%)\\nChildren Who Received Foods Other Than Breast Milk During First 6 Months\\nWater (%)\\nAnimal/Formula Milk (%)\\nSemi-Solid Mashed Food (%)\\nSolid (Adult) Food (%)\\nVegetables/Fruits (%)\\nAverage Month By Which Children Received Foods Other Than Breast Milk\\nWater (%)\\nAnimal/Formula Milk (%)\\nSemi-Solid Mashed Food (%)\\nSolid (Adult) Food (%)\\nVegetables/Fruits (%)\\nWW. Birth Registration\\nBirth Registered (%)\\nChildren Whose Birth Was Registered And Received Birth Certificate (%)\\nXX. Awareness On Hiv/Aids, Rti/Sti, Haf/Ors/Ort/Zinc And Ari/Pneumonia\\nWomen Who Are Aware of:\\nHIV/AIDS\\nRTI/STI\\nHAF/ORS/ORT/ZINC\\nDanger Signs Of ARI/Pneumonia (%)\\nYY. Mortality (unit level data of mortality is available here)\\nCrude Death Rate (CDR)\\nInfant Mortality Rate (IMR)\\nNeo-Natal Mortality Rate\\nUnder Five Mortality Rate (U5MR)\\nZZ. Confidence Interval (95%) For Some Important Indicators\\nCrude Birth Rate - (Lower and Upper Limit)\\nCrude Death Rate - (Lower and Upper Limit)\\nInfant Mortality Rate - (Lower and Upper Limit)\\nUnder Five Mortality Rate (U5MR) - (Lower and Upper Limit)\\nSex Ratio At Birth - (Lower and Upper Limit)\\nAcknowledgements\\nDepartment of Health and Family Welfare, Govt. of India has published this data in Open Govt Data Platform India portal under Govt. Open Data License - India.',\n",
       " 'Context\\nThe Paradise Papers is a cache of some 13GB of data that contains 13.4 million confidential records of offshore investment by 120,000 people and companies in 19 tax jurisdictions (Tax Heavens - an awesome video to understand this); that was published by the International Consortium of Investigative Journalists (ICIJ) on November 5, 2017. Here is a brief video about the leak. The people include Queen Elizabeth II, the President of Columbia (Juan Manuel Santos), Former Prime Minister of Pakistan (Shaukat Aziz), U.S Secretary of Commerce (Wilbur Ross) and many more. According to an estimate by the Boston Consulting Group, the amount of money involved is around $10 trillion. The leak contains many famous companies, including Facebook, Apple, Uber, Nike, Walmart, Allianz, Siemens, McDonald’s and Yahoo.\\nIt also contains a lot of U. S President Donald Trump allies including Rax Tillerson, Wilbur Ross, Koch Brothers, Paul Singer, Sheldon Adelson, Stephen Schwarzman, Thomas Barrack and Steve Wynn etc. The complete list of Politicians involve is avaiable here.\\nThe Panama Papers in the cache of 38GB of data from the national corporate registry of Bahamas. It contains world’s top politicians and influential persons as head and director of offshore companies registered in Bahamas.\\nOffshore Leaks details 13,000 offshore accounts in a report.\\nI am calling all data scientists to help me stop the corruption and reveal the patterns and linkages invisible for the untrained eye.\\nContent\\nThe data is the effort of more than 100 journalists from 60+ countries\\nThe original data is available under creative common license and can be downloaded from this link.\\nI will keep updating the datasets with more leaks and data as it’s available\\nAcknowledgements\\nInternational Consortium of Investigative Journalists (ICIJ)\\nParadise Papers Update\\nParadise Papers data has been uploaded as released by ICIJ on Nov 21, 2017. You can find Paradise Papers zip file and six extracted files in CSV format, all starting with a prefix of Paradise. Happy Coding!\\nInspiration\\nSome ideas worth exploring:\\nHow many companies and individuals are there in all of the leaks data\\nHow many countries involved\\nTotal money involved\\nWhat is the biggest best tax heaven\\nCan we compare the corruption with human development index and make an argument that would correlate corruption with bad conditions in that country\\nWho are the biggest cheaters and where they live\\nWhat role Fortune 500 companies play in this game\\nI need your help to make this world corruption free in the age of NLP and Big Data',\n",
       " 'Many people say the gender gap in income levels is overstated in the United States, where some say that inequality in the labor force is a thing of the past. Is there a gender gap at all? Is it stronger in some industries than in others?\\nThis dataset, retrieved from the Bureau of Labor Statistics, shows the median weekly incomes for 535 different occupations. The data encompasses information for all working American citizens as of January 2015. The incomes are broken into male and female statistics, preceded by the total median income when including both genders. The data has been re-formatted from the original PDF-friendly arrangement to make it easier to clean and analyze.\\nAnalysis thus far has found that there is indeed a sizable gender gap between male and female incomes. Use of this dataset should cite the Bureau of Labor Statistics as per their copyright information:\\nThe Bureau of Labor Statistics (BLS) is a Federal government agency and everything that we publish, both in hard copy and electronically, is in the public domain, except for previously copyrighted photographs and illustrations. You are free to use our public domain material without specific permission, although we do ask that you cite the Bureau of Labor Statistics as the source.',\n",
       " 'OSMI Mental Health in Tech Survey 2016\\nCurrently over 1400 responses, the ongoing 2016 survey aims to measure attitudes towards mental health in the tech workplace, and examine the frequency of mental health disorders among tech workers.\\nHow Will This Data Be Used?\\nWe are interested in gauging how mental health is viewed within the tech/IT workplace, and the prevalence of certain mental health disorders within the tech industry. The Open Sourcing Mental Illness team of volunteers will use this data to drive our work in raising awareness and improving conditions for those with mental health disorders in the IT workplace.',\n",
       " 'Context\\nThe National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation.\\nThe NHANES program began in the early 1960s and has been conducted as a series of surveys focusing on different population groups or health topics. In 1999, the survey became a continuous program that has a changing focus on a variety of health and nutrition measurements to meet emerging needs. The survey examines a nationally representative sample of about 5,000 persons each year. These persons are located in counties across the country, 15 of which are visited each year.\\nThe NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel.\\nTo date, thousands of research findings have been published using the NHANES data.\\nContent\\nThe 2013-2014 NHANES datasets include the following components:\\nDemographics dataset:\\nA complete variable dictionary can be found here\\nExaminations dataset, which contains:\\nBlood pressure\\nBody measures\\nMuscle strength - grip test\\nOral health - dentition\\nTaste & smell\\nA complete variable dictionary can be found here\\nDietary data - total nutrient intake, first day:\\nA complete variable dictionary can be found here\\nLaboratory dataset, which includes:\\nAlbumin & Creatinine - Urine\\nApolipoprotein B\\nBlood Lead, Cadmium, Total Mercury, Selenium, and Manganese\\nBlood mercury: inorganic, ethyl and methyl\\nCholesterol - HDL\\nCholesterol - LDL & Triglycerides\\nCholesterol - Total\\nComplete Blood Count with 5-part Differential - Whole Blood\\nCopper, Selenium & Zinc - Serum\\nFasting Questionnaire\\nFluoride - Plasma\\nFluoride - Water\\nGlycohemoglobin\\nHepatitis A\\nHepatitis B Surface Antibody\\nHepatitis B: core antibody, surface antigen, and Hepatitis D antibody\\nHepatitis C RNA (HCV-RNA) and Hepatitis C Genotype\\nHepatitis E: IgG & IgM Antibodies\\nHerpes Simplex Virus Type-1 & Type-2\\nHIV Antibody Test\\nHuman Papillomavirus (HPV) - Oral Rinse\\nHuman Papillomavirus (HPV) DNA - Vaginal Swab: Roche Cobas & Roche Linear Array\\nHuman Papillomavirus (HPV) DNA Results from Penile Swab Samples: Roche Linear Array\\nInsulin\\nIodine - Urine\\nPerchlorate, Nitrate & Thiocyanate - Urine\\nPerfluoroalkyl and Polyfluoroalkyl Substances (formerly Polyfluoroalkyl Chemicals - PFC)\\nPersonal Care and Consumer Product Chemicals and Metabolites\\nPhthalates and Plasticizers Metabolites - Urine\\nPlasma Fasting Glucose\\nPolycyclic Aromatic Hydrocarbons (PAH) - Urine\\nStandard Biochemistry Profile\\nTissue Transglutaminase Assay (IgA-TTG) & IgA Endomyseal Antibody Assay (IgA EMA)\\nTrichomonas - Urine\\nTwo-hour Oral Glucose Tolerance Test\\nUrinary Chlamydia\\nUrinary Mercury\\nUrinary Speciated Arsenics\\nUrinary Total Arsenic\\nUrine Flow Rate\\nUrine Metals\\nUrine Pregnancy Test\\nVitamin B12\\nA complete data dictionary can be found here\\nQuestionnaire dataset, which includes information on:\\nAcculturation\\nAlcohol Use\\nBlood Pressure & Cholesterol\\nCardiovascular Health\\nConsumer Behavior\\nCurrent Health Status\\nDermatology\\nDiabetes\\nDiet Behavior & Nutrition\\nDisability\\nDrug Use\\nEarly Childhood\\nFood Security\\nHealth Insurance\\nHepatitis\\nHospital Utilization & Access to Care\\nHousing Characteristics\\nImmunization\\nIncome\\nMedical Conditions\\nMental Health - Depression Screener\\nOccupation\\nOral Health\\nOsteoporosis\\nPesticide Use\\nPhysical Activity\\nPhysical Functioning\\nPreventive Aspirin Use\\nReproductive Health\\nSexual Behavior\\nSleep Disorders\\nSmoking - Cigarette Use\\nSmoking - Household Smokers\\nSmoking - Recent Tobacco Use\\nSmoking - Secondhand Smoke Exposure\\nTaste & Smell\\nWeight History\\nWeight History - Youth\\nA complete variable dictionary can be found here\\nMedication dataset, which includes prescription medications:\\nA complete variable dictionary can be found here\\nAcknowledgements\\nOriginal data and additional documents related to the datasets or NHANES can be found here.',\n",
       " 'Twitter has played an increasingly prominent role in the 2016 US Presidential Election. Debates have raged and candidates have risen and fallen based on tweets.\\nThis dataset provides ~3000 recent tweets from Hillary Clinton and Donald Trump, the two major-party presidential nominees.',\n",
       " 'Kaggle’s March Machine Learning Mania competition challenged data scientists to predict winners and losers of the men\\'s 2016 NCAA basketball tournament. This dataset contains the 1070 selected predictions of all Kaggle participants. These predictions were collected and locked in prior to the start of the tournament.\\nHow can this data be used? You can pivot it to look at both Kaggle and NCAA teams alike. You can look at who will win games, which games will be close, which games are hardest to forecast, or which Kaggle teams are gambling vs. sticking to the data.\\nThe NCAA tournament is a single-elimination tournament that begins with 68 teams. There are four games, usually called the “play-in round,” before the traditional bracket action starts. Due to competition timing, these games are included in the prediction files but should not be used in analysis, as it’s possible that the prediction was submitted after the play-in round games were over.\\nData Description\\nEach Kaggle team could submit up to two prediction files. The prediction files in the dataset are in the \\'predictions\\' folder and named according to:\\nTeamName_TeamId_SubmissionId.csv\\nThe file format contains a probability prediction for every possible game between the 68 teams. This is necessary to cover every possible tournament outcome. Each team has a unique numerical Id (given in Teams.csv). Each game has a unique Id column created by concatenating the year and the two team Ids. The format is the following:\\nId,Pred\\n2016_1112_1114,0.6\\n2016_1112_1122,0\\n...\\nThe team with the lower numerical Id is always listed first. “Pred” represents the probability that the team with the lower Id beats the team with the higher Id. For example, \"2016_1112_1114,0.6\" indicates team 1112 has a 0.6 probability of beating team 1114.\\nFor convenience, we have included the data files from the 2016 March Mania competition dataset in the Scripts environment (you may find TourneySlots.csv and TourneySeeds.csv useful for determining matchups, see the documentation). However, the focus of this dataset is on Kagglers\\' predictions.',\n",
       " 'Context: Annotated Corpus for Named Entity Recognition using GMB(Groningen Meaning Bank) corpus for entity classification with enhanced and popular features by Natural Language Processing applied to the data set.\\nTip: Use Pandas Dataframe to load dataset if using Python for convenience.\\nContent: This is the extract from GMB corpus which is tagged, annotated and built specifically to train the classifier to predict named entities such as name, location, etc.\\nNumber of tagged entities:\\n\\'O\\': 1146068\\', geo-nam\\': 58388, \\'org-nam\\': 48034, \\'per-nam\\': 23790, \\'gpe-nam\\': 20680, \\'tim-dat\\': 12786, \\'tim-dow\\': 11404, \\'per-tit\\': 9800, \\'per-fam\\': 8152, \\'tim-yoc\\': 5290, \\'tim-moy\\': 4262, \\'per-giv\\': 2413, \\'tim-clo\\': 891, \\'art-nam\\': 866, \\'eve-nam\\': 602, \\'nat-nam\\': 300, \\'tim-nam\\': 146, \\'eve-ord\\': 107, \\'per-ini\\': 60, \\'org-leg\\': 60, \\'per-ord\\': 38, \\'tim-dom\\': 10, \\'per-mid\\': 1, \\'art-add\\': 1\\nEssential info about entities:\\ngeo = Geographical Entity\\norg = Organization\\nper = Person\\ngpe = Geopolitical Entity\\ntim = Time indicator\\nart = Artifact\\neve = Event\\nnat = Natural Phenomenon\\nTotal Words Count = 1354149 Target Data Column: \"tag\"\\nInspiration: This dataset is getting more interested because of more features added to the recent version of this dataset. Also, it helps to create a broad view of Feature Engineering with respect to this dataset.\\nWhy this dataset is helpful or playful?\\nIt might not sound so interested for earlier versions, but when you are able to pick intent and custom named entities from your own sentence with more features then, it is getting interested and helps you solve real business problems(like picking entities from Electronic Medical Records, etc)\\nPlease, feel free to ask questions, do variations and let\\'s play together!',\n",
       " 'Contents\\nThis dataset contains detailed specifications, release dates, and release prices of computer parts.\\nThe dataset contains two CSV files: gpus.csv for Graphics Processing Units (GPUs), and cpus.csv for Central Processing Units (CPUs). Each table has its own list of unique entries, but the list of features includes: clock speeds, maximum temperatures, display resolutions, power draws, number of threads, release dates, release prices, die size, virtualization support, and many other similar fields. For more specific column-level metadata refer to the Column Metadata.\\nLooking for inspiration? Try starting by reading \"Using regression to predict the GPUs of the future\".\\nInspiration\\nHow did performance over price ratio evolve over time?\\nHow about general computing power?\\nAre there any manufacturers that are known for some specific range of performance & price?\\nAcknowledgements\\nThe data given here belongs mainly to Intel, Game-Debate, and the companies involved in producing the part. I do not own the data I uploaded it solely for informative purposes, under their original license.',\n",
       " 'Context\\nEEG devices are becoming cheaper and more inconspicuous, but few applications leverage EEG data effectively, in part because there are few large repositories of EEG data. The MIDS class at the UC Berkeley School of Information is sharing a dataset collected using consumer-grade brainwave-sensing headsets, along with the software code and visual stimulus used to collect the data. The dataset includes all subjects\\' readings during the stimulus presentation, as well as readings from before the start and after the end of the stimulus.\\nContent\\nWe presented two slightly different stimuli to two different groups. Stimuli 1 is available here, and stimuli 2 is available here.\\nFor both stimuli, a group of about 15 people saw the stimuli at the same time, while EEG data was being collected. The stimuli each person saw is available in the session field of subject-metadata.csv. (Subjects who saw stimulus 2 left the room during stimulus 1, and vice versa).\\nFind the synchronized times for both stimuli in stimulus-timing.csv.\\nFor each participant, we also anonymously collected some other metadata: (1) whether or not they had previously seen the video displayed during the stimulus (a superbowl ad), (2) gender, (3) whether or not they saw hidden icons displayed during the color counting exercise, and (4) their chosen color during the color counting exercise. All of these can be found in subject-metadata.csv.\\nWe also collected the timing (in indra_time) of all stimulus events for both session 1 and session 2. These times are included in stimulus-times.csv.\\nThe server receives one data packet every second from each Mindwave Mobile device, and stores the data in one row entry.\\nAcknowledgements\\nPlease use the following citation if you publish your research results using this dataset or software code or stimulus file:\\nJohn Chuang, Nick Merrill, Thomas Maillart, and Students of the UC Berkeley Spring 2015 MIDS Immersion Class. \"Synchronized Brainwave Recordings from a Group Presented with a Common Audio-Visual Stimulus (May 9, 2015).\" May 2015.',\n",
       " \"Summary\\nThe data is a preprocessed subset of the TCIA Study named Soft Tissue Sarcoma. The data have been converted from DICOM folders of varying resolution and data types to 3D HDF5 arrays with isotropic voxel size. This should make it easier to get started and test out various approaches (NN, RF, CRF, etc) to improve segmentations.\\nTCIA Summary\\nThis collection contains FDG-PET/CT and anatomical MR (T1-weighted, T2-weighted with fat-suppression) imaging data from 51 patients with histologically proven soft-tissue sarcomas (STSs) of the extremities. All patients had pre-treatment FDG-PET/CT and MRI scans between November 2004 and November 2011. (Note: date in the TCIA images have been changed in the interest of de-identification; the same change was applied across all images, preserving the time intervals between serial scans). During the follow-up period, 19 patients developed lung metastases. Imaging data and lung metastases development status were used in the following study:\\nVallières, M. et al. (2015). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. Physics in Medicine and Biology, 60(14), 5471-5496. doi:10.1088/0031-9155/60/14/5471.\\nImaging data, tumor contours (RTstruct DICOM objects), clinical data and source code is available for this study. See the DOI below for more details and links to access the whole dataset. Please contact Martin Vallières (mart.vallieres@gmail.com) of the Medical Physics Unit of McGill University for any scientific inquiries about this dataset.\\nAcknowledgements\\nWe would like to acknowledge the individuals and institutions that have provided data for this collection: McGill University, Montreal, Canada - Special thanks to Martin Vallières of the Medical Physics Unit\\nLicense\\nThis collection is freely available to browse, download, and use for commercial, scientific and educational purposes as outlined in the Creative Commons Attribution 3.0 Unported License. See TCIA's Data Usage Policies and Restrictions for additional details. Questions may be directed to help@cancerimagingarchive.net.\\nCitation\\nData Citation\\nVallières, Martin, Freeman, Carolyn R., Skamene, Sonia R., & El Naqa, Issam. (2015). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. The Cancer Imaging Archive. http://doi.org/10.7937/K9/TCIA.2015.7GO2GSKS\\nPublication Citation\\nVallières, M., Freeman, C. R., Skamene, S. R., & Naqa, I. El. (2015, June 29). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. Physics in Medicine and Biology. IOP Publishing. http://doi.org/10.1088/0031-9155/60/14/5471\\nTCIA Citation\\nClark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F. The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045-1057. (paper)\",\n",
       " \"Context\\nZillow operates an industry-leading economics and analytics bureau led by Zillow’s Chief Economist, Dr. Stan Humphries. At Zillow, Dr. Humphries and his team of economists and data analysts produce extensive housing data and analysis covering more than 500 markets nationwide. Zillow Research produces various real estate, rental and mortgage-related metrics and publishes unique analyses on current topics and trends affecting the housing market.\\nAt Zillow’s core is our living database of more than 100 million U.S. homes, featuring both public and user-generated information including number of bedrooms and bathrooms, tax assessments, home sales and listing data of homes for sale and for rent. This data allows us to calculate, among other indicators, the Zestimate, a highly accurate, automated, estimated value of almost every home in the country as well as the Zillow Home Value Index and Zillow Rent Index, leading measures of median home values and rents.\\nContent\\nThe Zillow Rent Index is the median estimated monthly rental price for a given area, and covers multifamily, single family, condominium, and cooperative homes in Zillow’s database, regardless of whether they are currently listed for rent. It is expressed in dollars and is seasonally adjusted. The Zillow Rent Index is published at the national, state, metro, county, city, neighborhood, and zip code levels.\\nZillow produces rent estimates (Rent Zestimates) based on proprietary statistical and machine learning models. Within each county or state, the models observe recent rental listings and learn the relative contribution of various home attributes in predicting prevailing rents. These home attributes include physical facts about the home, prior sale transactions, tax assessment information and geographic location as well as the estimated market value of the home (Zestimate). Based on the patterns learned, these models estimate rental prices on all homes, including those not presently for rent. Because of the availability of Zillow rental listing data used to train the models, Rent Zestimates are only available back to November 2010; therefore, each ZRI time series starts on the same date.\\nAcknowledgements\\nThe rent index data was calculated from Zillow's proprietary Rent Zestimates and published on its website.\\nInspiration\\nWhat city has the highest and lowest rental prices in the country? Which metropolitan area is the most expensive to live in? Where have rental prices increased in the past five years and where have they remained the same? What city or state has the lowest cost per square foot?\",\n",
       " 'OpenAQ is an open-source project to surface live, real-time air quality data from around the world. Their “mission is to enable previously impossible science, impact policy and empower the public to fight air pollution.” The data includes air quality measurements from 5490 locations in 47 countries.\\nScientists, researchers, developers, and citizens can use this data to understand the quality of air near them currently. The dataset only includes the most current measurement available for the location (no historical data).\\nUpdate Frequency: Weekly\\nQuerying BigQuery tables\\nYou can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.openaq.[TABLENAME]. Fork this kernel to get started.\\nAcknowledgements\\nDataset Source: openaq.org\\nUse: This dataset is publicly available for anyone to use under the following terms provided by the Dataset Source and is provided \"AS IS\" without any warranty, express or implied.',\n",
       " \"This dataset contains headlines, URLs, and categories for 422,937 news stories collected by a web aggregator between March 10th, 2014 and August 10th, 2014.\\nNews categories included in this dataset include business; science and technology; entertainment; and health. Different news articles that refer to the same news item (e.g., several articles about recently released employment statistics) are also categorized together.\\nContent\\nThe columns included in this dataset are:\\nID : the numeric ID of the article\\nTITLE : the headline of the article\\nURL : the URL of the article\\nPUBLISHER : the publisher of the article\\nCATEGORY : the category of the news item; one of: -- b : business -- t : science and technology -- e : entertainment -- m : health\\nSTORY : alphanumeric ID of the news story that the article discusses\\nHOSTNAME : hostname where the article was posted\\nTIMESTAMP : approximate timestamp of the article's publication, given in Unix time (seconds since midnight on Jan 1, 1970)\\nAcknowledgments\\nThis dataset comes from the UCI Machine Learning Repository. Any publications that use this data should cite the repository as follows:\\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\\nThis specific dataset can be found in the UCI ML Repository at this URL\\nInspiration\\nWhat kinds of questions can we explore using this dataset? Here are a few possibilities:\\ncan we predict the category (business, entertainment, etc.) of a news article given only its headline?\\ncan we predict the specific story that a news article refers to, given only its headline?\",\n",
       " 'Context\\nA recent Guardian blog post asks: \"How many endangered languages are there in the World and what are the chances they will die out completely?\" The United Nations Education, Scientific and Cultural Organisation (UNESCO) regularly publishes a list of endangered languages, using a classification system that describes its danger (or completion) of extinction.\\nContent\\nThe full detailed dataset includes names of languages, number of speakers, the names of countries where the language is still spoken, and the degree of endangerment. The UNESCO endangerment classification is as follows:\\nVulnerable: most children speak the language, but it may be restricted to certain domains (e.g., home)\\nDefinitely endangered: children no longer learn the language as a \\'mother tongue\\' in the home\\nSeverely endangered: language is spoken by grandparents and older generations; while the parent generation may understand it, they do not speak it to children or among themselves\\nCritically endangered: the youngest speakers are grandparents and older, and they speak the language partially and infrequently\\nExtinct: there are no speakers left\\nAcknowledgements\\nData was originally organized and published by The Guardian, and can be accessed via this Datablog post.\\nInspiration\\nHow can you best visualize this data?\\nWhich rare languages are more isolated (Sicilian, for example) versus more spread out? Can you come up with a hypothesis for why that is the case?\\nCan you compare the number of rare speakers with more relatable figures? For example, are there more Romani speakers in the world than there are residents in a small city in the United States?',\n",
       " 'Curious about the growth of wind energy? The extent to which the decline of coal is an American or international trend? Interested in using energy consumption as an alternate method of comparing national economies? This dataset has you covered.\\nThe Energy Statistics Database contains comprehensive energy statistics on the production, trade, conversion and final consumption of primary and secondary; conventional and non-conventional; and new and renewable sources of energy.\\nAcknowledgements\\nThis dataset was kindly published by the United Nations Statistics Division on the UNData site. You can find the original dataset here.\\nLicense\\nPer the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference.',\n",
       " 'Introduction\\nThe Mobile phone activity dataset is composed by one week of Call Details Records (CDRs) from the city of Milan and the Province of Trentino (Italy).\\nDescription of the dataset\\nEvery time a user engages a telecommunication interaction, a Radio Base Station (RBS) is assigned by the operator and delivers the communication through the network. Then, a new CDR is created recording the time of the interaction and the RBS which handled it. The following activities are present in the dataset:\\nreceived SMS\\nsent SMS\\nincoming calls\\noutgoing calls\\nInternet activity\\nIn particular, Internet activity is generated each time a user starts an Internet connection or ends an Internet connection. Moreover, during the same connection a CDR is generated if the connection lasts for more than 15\\u2009min or the user transferred more than 5\\u2009MB.\\nThe datasets is spatially aggregated in a square cells grid. The area of Milan is composed of a grid overlay of 1,000 (squares with size of about 235×235\\u2009meters. This grid is projected with the WGS84 (EPSG:4326) standard. For more details we link the original paper http://go.nature.com/2fcOX5E\\nThe data provides CellID, CountryCode and all the aforementioned telecommunication activities aggregated every 60 minutes.\\nOriginal datasource\\nThe Mobile phone activity dataset is a part of the Telecom Italia Big Data Challenge 2014, which is a rich and open multi-source aggregation of telecommunications, weather, news, social networks and electricity data from the city of Milan and the Province of Trentino (Italy). The original dataset has been created by Telecom Italia in association with EIT ICT Labs, SpazioDati, MIT Media Lab, Northeastern University, Polytechnic University of Milan, Fondazione Bruno Kessler, University of Trento and Trento RISE. In order to make it easy-to-use, here we provide a subset of telecommunications data that allows researchers to design algorithms able to exploit an enormous number of behavioral and social indicators. The complete version of the dataset is available at the following link: http://go.nature.com/2fz4AFr\\nRelevant, external, data sources\\nThe presented datasets can be enriched by using census data provided by the Italian National Institute of Statistics (ISTAT) (http://www.istat.it/en/), a public research organization and the main provider of official statistics in Italy. The census data have been released for 1999, 2001 and 2011. The dataset (http://www.istat.it/it/archivio/104317), released in Italian, is composed of four parts: Territorial Bases (Basi Territoriali), Administrative Boundaries (Confini Amministrativi), Census Variables (Variabili Censuarie) and data about Toponymy (Dati Toponomastici).\\nMotivational video: https://www.youtube.com/watch?v=_d2_RWMsUKc\\nRelevant papers\\nBlondel, Vincent D., Adeline Decuyper, and Gautier Krings. \"A survey of results on mobile phone datasets analysis.\" EPJ Data Science 4, no. 1 (2015): 1.\\nFrancesco Calabrese, Laura Ferrari, and Vincent D. Blondel. 2014. Urban Sensing Using Mobile Phone Network Data: A Survey of Research. ACM Comput. Surv. 47, 2, Article 25 (November 2014), 20 pages.\\nEagle, Nathan, Michael Macy, and Rob Claxton. \"Network diversity and economic development.\" Science 328, no. 5981 (2010): 1029-1031.\\nLenormand, Maxime, Miguel Picornell, Oliva G. Cantú-Ros, Thomas Louail, Ricardo Herranz, Marc Barthelemy, Enrique Frías-Martínez, Maxi San Miguel, and José J. Ramasco. \"Comparing and modelling land use organization in cities.\" Royal Society open science 2, no. 12 (2015): 150449.\\nLouail, Thomas, Maxime Lenormand, Oliva G. Cantu Ros, Miguel Picornell, Ricardo Herranz, Enrique Frias-Martinez, José J. Ramasco, and Marc Barthelemy. \"From mobile phone data to the spatial structure of cities.\" Scientific reports 4 (2014).\\nDe Nadai, Marco, Jacopo Staiano, Roberto Larcher, Nicu Sebe, Daniele Quercia, and Bruno Lepri. \"The Death and Life of Great Italian Cities: A Mobile Phone Data Perspective.\" WWW, 2016.\\nCitation\\nWe kindly ask people who use this dataset to cite the following paper, where this aggregation comes from:\\nBarlacchi, Gianni, Marco De Nadai, Roberto Larcher, Antonio Casella, Cristiana Chitic, Giovanni Torrisi, Fabrizio Antonelli, Alessandro Vespignani, Alex Pentland, and Bruno Lepri. \"A multi-source dataset of urban life in the city of Milan and the Province of Trentino.\" Scientific data 2 (2015).',\n",
       " 'Magic The Gathering (MTG, or just Magic) is a trading card game first published in 1993 by Wizards of the Coast. This game has seen immense popularity and new cards are still released every few months. The strength of different cards in the game can vary wildly and as a result some cards now sell on secondary markets for as high as thousands of dollars.\\nMTG JSON has an excellent collection of every single Magic Card - stored in JSON data. Version 3.6 (collected September 21, 2016) of their database is provided here.\\nFull documentation for the data is provided here: http://mtgjson.com/documentation.html\\nAlso, if you want to include images of the cards in your writeups, you can grab them from the official Wizards of the Coast website using the following URL:\\nhttp://gatherer.wizards.com/Handlers/Image.ashx?multiverseid=180607&type=card\\nJust replace the multiverse ID with the one provided in the mtgjson file.',\n",
       " \"Context\\nI created this dataset to explore different factors affecting people's enjoyment of food and/or cooking!\\nContent\\nOver 20k recipes listed by recipe rating, nutritional information and assigned category (sparse). I may later upload a version binned by recipe creation date and also including recipe ingredients.\\nUse the 'full_format_recipes.json' file to interact with all recipe data, 'epi_r.csv' drops ingredients and directions in favour of sparse category dummies.\\nAcknowledgements\\nRecipe information lifted from: http://www.epicurious.com/recipes-menus\",\n",
       " 'Problem Statement\\nKickstarter is a community of more than 10 million people comprising of creative, tech enthusiasts who help in bringing creative project to life. Till now, more than $3 billion dollars have been contributed by the members in fueling creative projects. The projects can be literally anything – a device, a game, an app, a film etc.\\nKickstarter works on all or nothing basis i.e if a project doesn’t meet it goal, the project owner gets nothing. For example: if a projects’s goal is $500. Even if it gets funded till $499, the project won’t be a success.\\nRecently, Kickstarter released its public data repository to allow researchers and enthusiasts like us to help them solve a problem. Will a project get fully funded ?\\nIn this challenge, you have to predict if a project will get successfully funded or not.\\nData Description\\nThere are three files given to download: train.csv, test.csv and sample_submission.csv The train data consists of sample projects from the May 2009 to May 2015. The test data consists of projects from June 2015 to March 2017.',\n",
       " 'Dataset Information\\nThis dataset is a collection of state and national polls conducted from November 2015-November 2016 on the 2016 presidential election. Data on the raw and weighted poll results by state, date, pollster, and pollster ratings are included.\\nContent\\nThere are 27 variables:\\ncycle\\nbranch\\ntype\\nmatchup\\nforecastdate\\nstate:\\nstartdate\\nenddate\\npollster\\ngrade\\nsamplesize\\npopulaion\\npoll_wt\\nrawpoll_clinton\\nrawpoll_trump\\nrawpoll_johnson\\nrawpoll_mcmullin\\nadjpoll_clinton\\nadjpoll_trump\\nadjpoll_johnson\\nadjpoll_mcmullin\\nmultiversions\\nurl\\npoll_id\\nquestion_id\\ncreateddate\\ntimestamp\\nInspiration\\nSome questions for exploring this dataset are:\\nWhat are the trends of the polls over time (by day/week/month)?\\nHow do the trends vary by state?\\nWhat is the probability that Trump/Clinton will win the 2016 election?\\nAcknowledgements\\nThe original dataset is from the FiveThirtyEight 2016 Election Forecast and can be downloaded from here. Poll results were aggregated from HuffPost Pollster, RealClearPolitics, polling firms and news reports.',\n",
       " 'Context\\nThe datasets describe ratings and free-text tagging activities from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications across 27278 movies. These data were created by 138493 users between January 09, 1995 and March 31, 2015. This dataset was generated on October 17, 2016.\\nUsers were selected at random for inclusion. All selected users had rated at least 20 movies.\\nContent\\nNo demographic information is included. Each user is represented by an id, and no other information is provided.\\nThe data are contained in six files.\\ntag.csv that contains tags applied to movies by users:\\nuserId\\nmovieId\\ntag\\ntimestamp\\nrating.csv that contains ratings of movies by users:\\nuserId\\nmovieId\\nrating\\ntimestamp\\nmovie.csv that contains movie information:\\nmovieId\\ntitle\\ngenres\\nlink.csv that contains identifiers that can be used to link to other sources:\\nmovieId\\nimdbId\\ntmbdId\\ngenome_scores.csv that contains movie-tag relevance data:\\nmovieId\\ntagId\\nrelevance\\ngenome_tags.csv that contains tag descriptions:\\ntagId\\ntag\\nAcknowledgements\\nThe original datasets can be found here. To acknowledge use of the dataset in publications, please cite the following paper:\\nF. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\\nInspiration\\nSome ideas worth exploring:\\nWhich genres receive the highest ratings? How does this change over time?\\nDetermine the temporal trends in the genres/tagging activity of the movies released',\n",
       " 'Context\\nThe Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles. The answer to every question is a segment of text, or span, from the corresponding reading passage. There are 100,000+ question-answer pairs on 500+ articles.\\nContent\\nThere are two files to help you get started with the dataset and evaluate your models:\\n*train-v1.1.json*fasdf\\ndev-v1.1.json\\nAcknowledgements\\nThe original datasets can be found here.\\nInspiration\\nCan you build a prediction model that can accurately predict answers to different types of questions?\\nYou can also explore SQuAD here',\n",
       " 'Context\\nPromptCloud extracted 400 thousand reviews of unlocked mobile phones sold on Amazon.com to find out insights with respect to reviews, ratings, price and their relationships.\\nContent\\nGiven below are the fields:\\nProduct Title\\nBrand\\nPrice\\nRating\\nReview text\\nNumber of people who found the review helpful\\nData was acquired in December, 2016 by the crawlers build to deliver our data extraction services.\\nInitial Analysis\\nIt can be accessed here: http://www.kdnuggets.com/2017/01/data-mining-amazon-mobile-phone-reviews-interesting-insights.html',\n",
       " \"Background:\\nThe underlying concept behind hedging strategies is simple, create a model, and make money doing it. The hardest part is finding the features that matter. For a more in-depth look at hedging strategies, I have attached one of my graduate papers to get you started.\\nMortgage-Backed Securities\\nGeographic Business Investment\\nReal Estate Analysis\\nFor any questions, you may reach us at research_development@goldenoakresearch.com. For immediate assistance, you may reach me on at 585-626-2965. Please Note: the number is my personal number and email is preferred\\nStatistical Fields:\\nNote: All interpolated statistical data include Mean, Median, and Standard Deviation Statistics. For more information view the variable definitions document.\\nMonthly Mortgage & Owner Costs: Sum of mortgage payments, home equity loans, utilities, property taxes\\nMonthly Owner Costs: Sum of utilities, property taxes\\nGross Rent: contract rent plus the estimated average monthly cost of utilities\\nHousehold Income: sum of the householder and all other individuals +15 years who reside in the household\\nFamily Income: Sum of incomes of all members +15 years of age related to the householder.\\nLocation Fields:\\nNote: The location fields were derived from a variety of sources. The zip code, area code, and city were derived using a heuristic. All other locations used the census Geo ID to cross reference data between multiple datasets.\\nState Name, Abbreviation, Number: reported by the U.S. Census Bureau\\nCounty Name: reported by the U.S. Census Bureau\\nLocation Type: Specifies Classification of location {City, Village, Town, CPD, ..., etc.}\\nArea Code: Defined via heuristic.\\nZip Code: Defined via heuristic.\\nCity: Defined via heuristic.\\nAccess All 325,258 Location of Our Most Complete Database Ever:\\nMonetize Risk and Optimize your portfolio instantly at an unbeatable price. Don't settle. Go big and win big. Access all gross rent records and more on a scale roughly equivalent to a neighborhood, see link below:\\nFull Dataset: View Full Dataset\\nReal Estate Research: View Research\",\n",
       " \"Dataset for people who double on Music and Data Science\\nHow it began\\nOne fine day, when someone with the idea of self sufficient AI capable of writing it's own poems wanted data, he stumbled upon the idea of using songs as a source. The journey wasn't easy, since each song has it's own page with the lyrics and scraping pages one at a time (when there are over a million of them) is a slow task. I worked up some optimisations here and there. For people interested in going through the process:\\nGITHUB PROJECT.\\nContent\\nThere is a lot to play with here, though all of it vertically. There isn't a lot of variation in the types of fields until you look deep enough.\\nThe main dataset is split up into 2 files, each containing ~250,000 songs with their artists and lyrics. The files are titled Lyrics1 and Lyrics2. The fields include band name, song name and lyrics.\\nGo through the exploration scripts for how to use the lyrics data set and other interesting observations.\\nAnother file containing urls of description pages of different artists is also provided, titled ArtistUrls.\\nAcknowledgements\\nAfter seeing the response for the FIFA PLAYER DATASET it was exciting as ever to get this one off.\\nSoon it was realised, not a lot of places exist where extracting data is straightforward.\\nThen one stumbles upon the perfectly indexed page : https://www.lyrics.com/. They deserve major credit for the existence of this dataset.\\nInspiration\\nThis started off as a source for some kind of intelligent poet which writes poems on it's own. It would be great to see what the artificially intelligent world has to express once it knows enough, and beautifully if at all?\",\n",
       " 'Pokemon Sun and Moon (released November 18th, 2016) are the latest games in the widely popular Pokemon video game franchise. Pokemon games are usually released in pairs (red and blue, gold and silver, x and y, etc.) and collectively each pair that introduces new pokemon to the game is known as a Generation. Pokemon Sun and Moon are the 7th Generation, adding new pokemon to the franchise as well as adjusting the stats and movesets of some of the older pokemon. (Please note that the recently popular PokemonGo games are unrelated to the Pokemon Video Games).\\nThis dataset contains a full set of in-game statistics for all 802 pokemon in the Sun and Moon. It also includes full information on which pokemon can learn which moves (movesets.csv), what moves can do (moves.csv), and how damage is modified by pokemon type (type-chart.csv).\\nPokemon Battle Simulation\\nWith the level of detail in the data provided here it is possible to almost fully simulate pokemon battles using the information provided (status effects and some other nuances are still missing). If you are interested in simulating how Pokemon battles would pan out between different opponents make sure to read up on the math behind how Pokemon battles work.',\n",
       " 'Context\\nINSEE is the official french institute gathering data of many types around France. It can be demographic (Births, Deaths, Population Density...), Economic (Salary, Firms by activity / size...) and more.\\nIt can be a great help to observe and measure inequality in the french population.\\nContent\\nFour files are in the dataset :\\nbase_etablissement_par_tranche_effectif : give information on the number of firms in every french town, categorized by size , come from INSEE.\\nCODGEO : geographique code for the town (can be joined with *code_insee* column from \"name_geographic_information.csv\\')\\nLIBGEO : name of the town (in french)\\nREG : region number\\nDEP : depatment number\\nE14TST : total number of firms in the town\\nE14TS0ND : number of unknown or null size firms in the town\\nE14TS1 : number of firms with 1 to 5 employees in the town\\nE14TS6 : number of firms with 6 to 9 employees in the town\\nE14TS10 : number of firms with 10 to 19 employees in the town\\nE14TS20 : number of firms with 20 to 49 employees in the town\\nE14TS50 : number of firms with 50 to 99 employees in the town\\nE14TS100 : number of firms with 100 to 199 employees in the town\\nE14TS200 : number of firms with 200 to 499 employees in the town\\nE14TS500 : number of firms with more than 500 employees in the town\\nname_geographic_information : give geographic data on french town (mainly latitude and longitude, but also region / department codes and names )\\nEU_circo : name of the European Union Circonscription\\ncode_région : code of the region attached to the town\\nnom_région : name of the region attached to the town\\nchef.lieu_région : name the administrative center around the town\\nnuméro_département : code of the department attached to the town\\nnom_département : name of the department attached to the town\\npréfecture : name of the local administrative division around the town\\nnuméro_circonscription : number of the circumpscription\\nnom_commune : name of the town\\ncodes_postaux : post-codes relative to the town\\ncode_insee : unique code for the town\\nlatitude : GPS latitude\\nlongitude : GPS longitude\\néloignement : i couldn\\'t manage to figure out what was the meaning of this number\\nnet_salary_per_town_per_category : salaries around french town per job categories, age and sex\\nCODGEO : unique code of the town\\nLIBGEO : name of the town\\nSNHM14 : mean net salary\\nSNHMC14 : mean net salary per hour for executive\\nSNHMP14 : mean net salary per hour for middle manager\\nSNHME14 : mean net salary per hour for employee\\nSNHMO14 : mean net salary per hour for worker\\nSNHMF14 : mean net salary for women\\nSNHMFC14 : mean net salary per hour for feminin executive\\nSNHMFP14 : mean net salary per hour for feminin middle manager\\nSNHMFE14 : mean net salary per hour for feminin employee\\nSNHMFO14 : mean net salary per hour for feminin worker\\nSNHMH14 : mean net salary for man\\nSNHMHC14 : mean net salary per hour for masculin executive\\nSNHMHP14 : mean net salary per hour for masculin middle manager\\nSNHMHE14 : mean net salary per hour for masculin employee\\nSNHMHO14 : mean net salary per hour for masculin worker\\nSNHM1814 : mean net salary per hour for 18-25 years old\\nSNHM2614 : mean net salary per hour for 26-50 years old\\nSNHM5014 : mean net salary per hour for >50 years old\\nSNHMF1814 : mean net salary per hour for women between 18-25 years old\\nSNHMF2614 : mean net salary per hour for women between 26-50 years old\\nSNHMF5014 : mean net salary per hour for women >50 years old\\nSNHMH1814 : mean net salary per hour for men between 18-25 years old\\nSNHMH2614 : mean net salary per hour for men between 26-50 years old\\nSNHMH5014 : mean net salary per hour for men >50 years old\\npopulation : demographic information in France per town, age, sex and living mode\\nNIVGEO : geographic level (arrondissement, communes...)\\nCODGEO : unique code for the town\\nLIBGEO : name of the town (might contain some utf-8 errors, this information has better quality name_geographic_information)\\nMOCO : cohabitation mode : [list and meaning available in Data description]\\nAGE80_17 : age category (slice of 5 years) | ex : 0 -> people between 0 and 4 years old\\nSEXE : sex, 1 for men | 2 for women\\nNB : Number of people in the category\\ndepartments.geojson : contains the borders of french departments. From Gregoire David (github)\\nThese datasets can be merged by : CODGEO = code_insee\\nAcknowledgements\\nThe entire dataset has been created (and actualized) by INSEE, I just uploaded it on Kaggle after doing some jobs and checks on it. I haven\\'t seen INSEE on Kaggle yet but I think it would be great to bring the organization in as a Kaggle actor.\\nInspiration\\nFirst aim I had creating that dataset was to provide a map of french towns with the number of firm that are settled in by size.\\nNow my goal is to explore inequality between men and women, youngsters and elders, working / social classes.\\nPopulation can also be a great filter to explain some phenomenons on the maps.',\n",
       " 'At the end of 2015, the Jupyter Project conducted a UX Survey for Jupyter Notebook users. This dataset, Survey.csv, contains the raw responses.\\nSee the Google Group Thread for more context around this dataset.',\n",
       " \"Context\\nZeeshan-ul-hassan Usmani’s Genome Phenotype SNPs Raw Data\\nGenomics is a branch of molecular biology that involves structure, function, variation, evolution and mapping of genomes. There are several companies offering next generation sequencing of human genomes from complete 3 billion base-pairs to a few thousand Phenotype SNPs. I’ve used 23andMe (using Illumina HumanOmniExpress-24) for my DNA’s Phenotype SNPs. I am sharing the entire raw dataset here for the international research community for following reasons:\\nI am a firm believer in open dataset, transparency, and the right to learn, research, explores, and educate. I do not want to restrict the knowledge flow for mere privacy concerns. Hence, I am offering my entire DNA raw data for the world to use for research without worrying about privacy. I call it copyleft dataset.\\nMost of available test datasets for research come from western world and we don’t see much from under-developing countries. I thought to share my data to bridge the gap and I expect others to follow the trend.\\nI would be the happiest man on earth, if a life can be saved, knowledge can be learned, an idea can be explore, or a fact can be found using my DNA data. Please use it the way you will\\nContent\\nName: Zeeshan-ul-hassan Usmani\\nAge: 38 Years\\nCountry of Birth: Pakistan\\nCountry of Ancestors: India (Utter Pradesh - UP)\\nFile: GenomeZeeshanUsmani.csv\\nSize: 15 MB\\nSources: 23andMe Personalized Genome Report\\nThe research community is still progressively working in this domain and it is agreed upon by professionals that genomics is still in its infancy. You now have the chance to explore this novel domain via the dataset and become one of the few genomics early adopters.\\nThe data-set is a complete genome extracted from www.23andme.com and is represented as a sequence of SNPs represented by the following symbols: A (adenine), C (cytosine), G (guanine), T (thymine), D (base deletions), I (base insertions), and '_' or '-' if the SNP for particular location is not accessible. It contains Chromosomes 1-22, X, Y, and mitochondrial DNA.\\nA complete list of the exact SNPs (base pairs) available and their data-set index can be found at https://api.23andme.com/res/txt/snps.b4e00fe1db50.data\\nFor more information about how the data-set was extracted follow https://api.23andme.com/docs/reference/#genomes\\nMoreover, for a more detailed understanding of the data-set content please acquaint yourself with the description of https://api.23andme.com/docs/reference/#genotypes\\nAcknowledgements\\nUsers are allowed to use, copy, distribute and cite the dataset as follows: “Zeeshan-ul-hassan Usmani, Genome Phenotype SNPS Raw Data File by 23andMe, Kaggle Dataset Repository, Jan 25, 2017.”\\nUseful Links\\nYou may use the following human genome database sites for help:\\nGenBank - https://www.ncbi.nlm.nih.gov/genbank/\\nThe Human Genome Project - https://www.genome.gov/hgp/\\nGenomes OnLine Database (GOLD) - https://gold.jgi.doe.gov\\nComplete Genomics - http://www.completegenomics.com/public-data/\\nInspiration\\nSome ideas worth exploring:\\nIs the individual in question more susceptible to cancer?\\nDoes he tend to gain weight?\\nWhere is his place of origin?\\nWhich gene determines certain biological feature (cancer susceptibility, fat generation rate, hair color etc.\\nHow does this phenotype SNPs compare with other similar datasets from the western-world?\\nWhat would be the likely cause of death for this person?\\nWhat are the most likely diseases/illnesses this person is going to face in lifetime?\\nWhat is unique about this dataset?\\nWhat else you can extract from this dataset when it comes to personal trait, intelligence level, ancestry and body makeup?\\nSample Reports\\nPlease check out following reports to understand what can be done with this data\\nAncestry – https://www.23andme.com/published-report/eeb4f9bbd6b5474f/?share_id=f6c5562848e84586\\nWeight Report - https://you.23andme.com/published/reports/65c9af9f8223456d/?share_id=0126f129e4f3458b\",\n",
       " 'The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists of three different types of data:\\nCensus and Geographic Data;\\nEnergy Data; and\\nEconomic Data.\\nWhen creating the data set, I combined data from many different types of sources, all of which are cited below. I have also provided the fields included in the data set and what they represent below. I have not performed any research on the data yet, but am going to dive in soon. I am particularly interested in the relationships between various types of data (i.e. GDP or birth rate) in prediction algorithms. Given that I have compiled 5 years’ worth of data, this data set was primarily constructed with predictive algorithms in mind.\\nAn additional note before you delve into the fields: * There could have been many more variables added across many different fields of metrics. I have stopped here, but it could potentially be beneficial to observe the interaction of these variables with others (i.e. the GDP of certain industries, the average age in a state, the male/female gender ratio, etc.) to attempt to find additional trends.\\nCensus and Geographic Data\\nStateCodes: The state 2-letter abbreviations. Note that I added \"US\" for the United States.\\nRegion: The number corresponding to the region the state lies within, according to the 2010 census. (1 = Northeast, 2 = Midwest, 3 = South, 4 = West)\\nDivision: The number corresponding to the division the state lies within, according to the 2010 census. (1 = New England, 2 = Middle Atlantic, 3 = East North Central, 4 = West North Central, 5 = South Atlantic, 6 = East South Central, 7 = West South Central, 8 = Mountain, 9 = Pacific)\\nCoast: Whether the state shares a border with an ocean. (1 = Yes, 0 = No)\\nGreat Lakes: Whether the state shares a border with a great lake. (1 = Yes, 0 = No\\nCENSUS2010POP: 4/1/2010 resident total Census 2010 population\\nPOPESTIMATE{year}: 7/1/{year} resident total population estimate\\nRBIRTH{year}: Birth rate in period 7/1/{year - 1} to 6/30/{year}\\nRDEATH{year}: Death rate in period 7/1/{year - 1} to 6/30/{year}\\nRNATURALINC{year}: Natural increase rate in period 7/1/{year - 1} to 6/30/{year}\\nRINTERNATIONALMIG{year}: Net international migration rate in period 7/1/{year - 1} to 6/30/{year}\\nRDOMESTICMIG{year}: Net domestic migration rate in period 7/1/{year - 1} to 6/30/{year}\\nRNETMIG{year}: Net migration rate in period 7/1/{year - 1} to 6/30/{year}\\nAs noted from the census:\\nNet international migration for the United States includes the international migration of both native and foreign-born populations. Specifically, it includes: (a) the net international migration of the foreign born, (b) the net migration between the United States and Puerto Rico, (c) the net migration of natives to and from the United States, and (d) the net movement of the Armed Forces population between the United States and overseas. Net international migration for Puerto Rico includes the migration of native and foreign-born populations between the United States and Puerto Rico.\\nCodes for most of the data, information about the geographic terms and coditions, and more information about the methodology behind the population estimates can be found on the US Census website.\\nEnergy Data\\nTotalC{year}: Total energy consumption in billion BTU in given year.\\nTotalP{year}: Total energy production in billion BTU in given year.\\nTotalE{year}: Total Energy expenditures in million USD in given year.\\nTotalPrice{year}: Total energy average price in USD/million BTU in given year.\\nTotalC{first year}–{second year}: The first year’s total energy consumption divided by the second year’s total energy consumption, times 100. (The percent change between years in total energy consumption.)\\nTotalP{first year}–{second year}: The first year’s total energy production divided by the second year’s total energy production, times 100. (The percent change between years in total energy production.)\\nTotalE{first year}–{second year}: The first year’s total energy expenditure divided by the second year’s total energy expenditure, times 100. (The percent change between years in total energy expenditure.)\\nTotalPrice{first year}–{second year}: The first year’s total energy average price divided by the second year’s total energy average price, times 100. (The percent change between years in total energy average price.)\\nBiomassC{year}: Biomass total consumption in billion BTU in given year.\\nCoalC{year}: Coal total consumption in billion BTU in given year.\\nCoalP{year}: Coal total production in billion BTU in given year.\\nCoalE{year}: Coal total expenditures in million USD in given year.\\nCoalPrice{year}: Coal average price in USD per million BTU in given year.\\nElecC{year}: Electricity total consumption in billion BTU in given year.\\nElecE{year}: Electricity total expenditures in million USD in given year.\\nElecPrice{year}: Electricity average price in USD per million BTU in given year.\\nFossFuelC{year}: Fossil fuels total consumption in billion BTU in given year.\\nGeoC{year}: Geothermal energy total consumption in billion BTU in given year.\\nGeoP{year}: Geothermal energy net generation in the electric power sector in million kilowatt hours in given year.\\nHydroC{year}: Hydropower total consumption in billion BTU in given year.\\nHydroP{year}: Hydropower total net generation in million kilowatt hours in given year.\\nNatGasC{year}: Natural gas total consumption (including supplemental gaseous fuels) in billion BTU in given year.\\nNatGasE{year}: Natural gas total expenditures in million USD in given year.\\nNatGasPrice{year}: Natural gas average price in USD per million BTU in given year.\\nLPGC{year}: LPG total consumption in billion BTU in given year.\\nLPGE{year}: LPG total expenditures in million USD in given year.\\nLPGPrice{year}: LPG average price in USD per million BTU in given year.\\nNotes:\\nBTU stands for British Thermal Unit and is a unit of measurement for energy. One BTU is equal to the amount of energy used to raise the temperature of one pound of water on degree Fahrenheit.\\nMany other types of energy and their associated consumption, production, expenditure, and price totals can be found from the EIA; this is where I received the data I used in compiling this dataset.\\nEconomic Data\\nGDP{year}{quarter}: The GDP in the provided quarter of the given year (in million USD).\\nGDP{year}: The average GDP throughout the given year (in million USD).\\nNotes:\\nThe GDP is reported by the Bureau of Economic Analysis from the U.S. Department of Commerce and measures the value of the goods and services produced by the economy in a given period.\\nThe quarterly GDP data can be downloaded from the BEA.\\nThe yearly GDP data can be downloaded from the BEA.\\nImage credit: http://www.freelargeimages.com/wp-content/uploads/2014/11/Map_of_united_states-3.jpg',\n",
       " \"Background\\nWhen is my university campus gym least crowded, so I know when to work out? We measured how many people were in this gym once every 10 minutes over the last year. We want to be able to predict how crowded the gym will be in the future.\\nGoals\\nGiven a time of day (and maybe some other features, including weather), predict how crowded the gym will be.\\nFigure out which features are actually important, which are redundant, and what features could be added to make the predictions more accurate.\\nData\\nThe dataset consists of 26,000 people counts (about every 10 minutes) over the last year. In addition, I gathered extra info including weather and semester-specific information that might affect how crowded it is. The label is the number of people, which I'd like to predict given some subset of the features.\\nLabel:\\nNumber of people\\nFeatures:\\ndate (string; datetime of data)\\ntimestamp (int; number of seconds since beginning of day)\\nday_of_week (int; 0 [monday] - 6 [sunday])\\nis_weekend (int; 0 or 1) [boolean, if 1, it's either saturday or sunday, otherwise 0]\\nis_holiday (int; 0 or 1) [boolean, if 1 it's a federal holiday, 0 otherwise]\\ntemperature (float; degrees fahrenheit)\\nis_start_of_semester (int; 0 or 1) [boolean, if 1 it's the beginning of a school semester, 0 otherwise]\\nmonth (int; 1 [jan] - 12 [dec])\\nhour (int; 0 - 23)\\nAcknowledgements\\nThis data was collected with the consent of the university and the gym in question.\",\n",
       " \"Results for the men's ATP tour date back to January 2000, including Grand Slams, Masters Series, Masters Cup and International Series competitions. Historical head-to-head betting odds go back to 2001.\\nSee here for more details about the metadata : http://www.tennis-data.co.uk/notes.txt\\nSource - http://www.tennis-data.co.uk/data.php\\nThere is a lot you can do with this data set. The ultimate goal is obviously to predict the outcome of the game or to build an efficient betting strategy based on your model(s).\\nYou can also\\ncompare the betting agencies (Bet365, Bet&Win, Ladbrokes, Pinnacles...) in terms of predictions quality or measure the progress of these betting agencies over the years\\ndiscover if it's possible to predict specific events (3 sets match, retirement, walk over...) or the evolution of players.\",\n",
       " 'Context\\nThe aim of this dataset is to provide a simple way to get started with 3D computer vision problems such as 3D shape recognition.\\nAccurate 3D point clouds can (easily and cheaply) be adquired nowdays from different sources:\\nRGB-D devices: Google Tango, Microsoft Kinect, etc.\\nLidar.\\n3D reconstruction from multiple images.\\nHowever there is a lack of large 3D datasets (you can find a good one here based on triangular meshes); it\\'s especially hard to find datasets based on point clouds (wich is the raw output from every 3D sensing device).\\nThis dataset contains 3D point clouds generated from the original images of the MNIST dataset to bring a familiar introduction to 3D to people used to work with 2D datasets (images).\\nIn the 3D_from_2D notebook you can find the code used to generate the dataset.\\nYou can use the code in the notebook to generate a bigger 3D dataset from the original.\\nContent\\nfull_dataset_vectors.h5\\nThe entire dataset stored as 4096-D vectors obtained from the voxelization (x:16, y:16, z:16) of all the 3D point clouds.\\nIn adition to the original point clouds, it contains randomly rotated copies with noise.\\nThe full dataset is splitted into arrays:\\nX_train (10000, 4096)\\ny_train (10000)\\nX_test(2000, 4096)\\ny_test (2000)\\nExample python code reading the full dataset:\\n with h5py.File(\"../input/train_point_clouds.h5\", \"r\") as hf:    \\n     X_train = hf[\"X_train\"][:]\\n     y_train = hf[\"y_train\"][:]    \\n     X_test = hf[\"X_test\"][:]  \\n     y_test = hf[\"y_test\"][:]  \\ntrain_point_clouds.h5 & test_point_clouds.h5\\n5000 (train), and 1000 (test) 3D point clouds stored in HDF5 file format. The point clouds have zero mean and a maximum dimension range of 1.\\nEach file is divided into HDF5 groups\\nEach group is named as its corresponding array index in the original mnist dataset and it contains:\\n\"points\" dataset: x, y, z coordinates of each 3D point in the point cloud.\\n\"normals\" dataset: nx, ny, nz components of the unit normal associate to each point.\\n\"img\" dataset: the original mnist image.\\n\"label\" attribute: the original mnist label.\\nExample python code reading 2 digits and storing some of the group content in tuples:\\nwith h5py.File(\"../input/train_point_clouds.h5\", \"r\") as hf:    \\n    a = hf[\"0\"]\\n    b = hf[\"1\"]    \\n    digit_a = (a[\"img\"][:], a[\"points\"][:], a.attrs[\"label\"]) \\n    digit_b = (b[\"img\"][:], b[\"points\"][:], b.attrs[\"label\"]) \\nvoxelgrid.py\\nSimple Python class that generates a grid of voxels from the 3D point cloud. Check kernel for use.\\nplot3D.py\\nModule with functions to plot point clouds and voxelgrid inside jupyter notebook. You have to run this locally due to Kaggle\\'s notebook lack of support to rendering Iframes. See github issue here\\nFunctions included:\\narray_to_color Converts 1D array to rgb values use as kwarg color in plot_points()\\nplot_points(xyz, colors=None, size=0.1, axis=False)\\nplot_voxelgrid(v_grid, cmap=\"Oranges\", axis=False)\\nAcknowledgements\\nWebsite of the original MNIST dataset\\nWebsite of the 3D MNIST dataset\\nHave fun!',\n",
       " \"New Upload:\\nAdded +32,000 more locations. For information on data calculations please refer to the methodology pdf document. Information on how to calculate the data your self is also provided as well as how to buy data for $1.29 dollars.\\nWhat you get:\\nThe database contains 32,000 records on US Household Income Statistics & Geo Locations. The field description of the database is documented in the attached pdf file. To access, all 348,893 records on a scale roughly equivalent to a neighborhood (census tract) see link below and make sure to up vote. Up vote right now, please. Enjoy!\\nHousehold & Geographic Statistics:\\nMean Household Income (double)\\nMedian Household Income (double)\\nStandard Deviation of Household Income (double)\\nNumber of Households (double)\\nSquare area of land at location (double)\\nSquare area of water at location (double)\\nGeographic Location:\\nLongitude (double)\\nLatitude (double)\\nState Name (character)\\nState abbreviated (character)\\nState_Code (character)\\nCounty Name (character)\\nCity Name (character)\\nName of city, town, village or CPD (character)\\nPrimary, Defines if the location is a track and block group.\\nZip Code (character)\\nArea Code (character)\\nAbstract\\nThe dataset originally developed for real estate and business investment research. Income is a vital element when determining both quality and socioeconomic features of a given geographic location. The following data was derived from over +36,000 files and covers 348,893 location records.\\nLicense\\nOnly proper citing is required please see the documentation for details. Have Fun!!!\\nGolden Oak Research Group, LLC. “U.S. Income Database Kaggle”. Publication: 5, August 2017. Accessed, day, month year.\\nSources, don't have 2 dollars? Get the full information yourself!\\n2011-2015 ACS 5-Year Documentation was provided by the U.S. Census Reports. Retrieved August 2, 2017, from https://www2.census.gov/programs-surveys/acs/summary_file/2015/data/5_year_by_state/\\nFound Errors?\\nPlease tell us so we may provide you the most accurate data possible. You may reach us at: research_development@goldenoakresearch.com\\nfor any questions you can reach me on at 585-626-2965\\nplease note: it is my personal number and email is preferred\\nCheck our data's accuracy: Census Fact Checker\\nAccess all 348,893 location records and more:\\nDon't settle. Go big and win big. Optimize your potential. Overcome limitation and outperform expectation. Access all household income records on a scale roughly equivalent to a neighborhood, see link below:\\nWebsite: Golden Oak Research Kaggle Deals all databases $1.29 Limited time only\\nA small startup with big dreams, giving the every day, up and coming data scientist professional grade data at affordable prices It's what we do.\",\n",
       " 'Context\\nCRIME STATISTICS: INTEGRITY\\nThe South African Police Service (SAPS) has accepted a new and challeging objective of ensuring that its crime statistics are in line with international best practice. This will be achieved through a Memorandum of Understanding with Statistics South Africa (Stats SA), aimed at further enhancing the quality and integrity of the South African crime statistics.\\nThe crime statistics generated by SAPS are an important link in the value chain of the statistics system informs policy development and planning in the criminal justice system. The collaboration with StatsSA will go a long way in enhancing the integrity of the SAPS crime statistics and ensuring that policy-makers have quality data to assist them with making policy decisions.\\nContent\\nThe dataset contains South African crime statistics, broken down per province, station and crime type.\\nAcknowledgements\\nData as published from:\\nhttp://www.saps.gov.za/resource_centre/publications/statistics/crimestats/2015/crime_stats.php\\nFurther sources:\\nhttp://www.saps.gov.za/services/crimestats.php\\nAn overview presentation:\\nhttp://www.saps.gov.za/services/final-crime-stats-release-02september2016.pdf',\n",
       " \"Introduction\\nVideo games are a rich area for data extraction due to their digital nature. Notable examples such as the complex EVE Online economy, World of Warcraft corrupted blood incident and even Grand Theft Auto self-driving cars tells us that fiction is closer to reality than we really think. Data scientists can gain insight on the logic and decision-making that the players face when put in hypothetical and virtual scenarios.\\nIn this Kaggle Dataset, I provide just over 1400 competitive matchmaking matches from Valve's game Counter-strike: Global Offensive (CS:GO). The data was extracted from competitive matchmaking replays submitted to csgo-stats. I intend for this data-set to be purely exploratory, however users are free to create their own predictive models they see fit.\\nAbout Counter-Strike: Global Offensive\\nCounter-Strike: Global Offensive is a first-person shooter game pitting two teams of 5 players against each other. Within a maximum of 30 rounds, the two teams find themselves on either side as a Counter Terrorist or Terrorist. Both sides are tasked with eliminating the opposition or, as the terrorist team, planting the C4 bomb at a bomb site and allowing it to explode. Rounds are played out until either of those two objectives or if the maximum time is reached (in which the counter terrorists then win by default). At the end of the 15th round, the two teams switch sides and continue until one team reaches 16 round wins first. CS:GO is widely known for its competitive aspect of technical skill, teamwork and in-game strategies. Players are constantly rewarded with the efforts they put it in training and learning through advancing in rank.\\nClick here to read more about the competitive mechanics of CS:GO.\\nContent\\nThis dataset within the 1400 matches provides every successful entry of duels (or battle) that took place for a player. That is, each row documents an event when a player is hurt by another player (or World e.g fall damage). There are over 900,000 entries within more than 31500 rounds.\\nmm_master_demos.csv contains information on rounds fired, while mm_grenades_demos.csv contains information on grenades thrown. The fields in the two datasets are similar: highlights include shooters and victims, event coordinates, and timestamps. The datasets also includes static information on the match winner, player ranks before and after the match, and other miscellaneous match-level metadata.\\nFor further information on individual fields in the dataset, refer to the Column Metadata.\\nInterpreting Positional Data\\nThis dataset also includes a selection of the game's official radar maps, as well as a table, map_data.csv, to aid in mapping data over them. The X,Y coordinates included in the dataset are all in in-game coordinates and need to be linearly scaled to be plotted on any official radar maps. See converting to map coordinates for more information.\\nAcknowledgements\\nDefinitely the guys from csgo-stats, without them, this wouldn't have been possible! :)\\n/r/globaloffensive for many years of lulz\\nAkiver of CSGO Demo Manager for spending so much time perfecting his demo parser.\",\n",
       " 'Context\\nThis dataset expands on my earlier New York City Census Data dataset. It includes data from the entire country instead of just New York City. The expanded data will allow for much more interesting analyses and will also be much more useful at supporting other data sets.\\nContent\\nThe data here are taken from the DP03 and DP05 tables of the 2015 American Community Survey 5-year estimates. The full datasets and much more can be found at the American Factfinder website. Currently, I include two data files:\\nacs2015_census_tract_data.csv: Data for each census tract in the US, including DC and Puerto Rico.\\nacs2015_county_data.csv: Data for each county or county equivalent in the US, including DC and Puerto Rico.\\nThe two files have the same structure, with just a small difference in the name of the id column. Counties are political subdivisions, and the boundaries of some have been set for centuries. Census tracts, however, are defined by the census bureau and will have a much more consistent size. A typical census tract has around 5000 or so residents.\\nThe Census Bureau updates the estimates approximately every year. At least some of the 2016 data is already available, so I will likely update this in the near future.\\nAcknowledgements\\nThe data here were collected by the US Census Bureau. As a product of the US federal government, this is not subject to copyright within the US.\\nInspiration\\nThere are many questions that we could try to answer with the data here. Can we predict things such as the state (classification) or household income (regression)? What kinds of clusters can we find in the data? What other datasets can be improved by the addition of census data?',\n",
       " 'Content\\nThe NTSB aviation accident database contains information from 1962 and later about civil aviation accidents and selected incidents within the United States, its territories and possessions, and in international waters.\\nAcknowledgements\\nGenerally, a preliminary report is available online within a few days of an accident. Factual information is added when available, and when the investigation is completed, the preliminary report is replaced with a final description of the accident and its probable cause. Full narrative descriptions may not be available for dates before 1993, cases under revision, or where NTSB did not have primary investigative responsibility.\\nInspiration\\nHope it will teach us how to improve the quality and safety of traveling by Airplane.',\n",
       " 'Context\\nThis dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest. There are over half a million measurements total!\\nContent\\nThis dataset includes information on tree type, shadow coverage, distance to nearby landmarks (roads etcetera), soil type, and local topography.\\nAcknowledgement\\nThis dataset is part of the UCI Machine Learning Repository, and the original source can be found here. The original database owners are Jock A. Blackard, Dr. Denis J. Dean, and Dr. Charles W. Anderson of the Remote Sensing and GIS Program at Colorado State University.\\nInspiration\\nCan you build a model that predicts what types of trees grow in an area based on the surrounding characteristics? A past Kaggle competition project on this topic can be found here.\\nWhat kinds of trees are most common in the Roosevelt National Forest?\\nWhich tree types can grow in more diverse environments? Are there certain tree types that are sensitive to an environmental factor, such as elevation or soil type?',\n",
       " 'Introduction\\nThis file contains the values of the price for more than 1000 different cryptocurrencies (including scams) recorded on daily base, I decide to include all coins in order to analyze exotic coins and compare with well knows cryptocurrencies. All this dataset come from coinmarketcap historical pages, grabbed using just an R script. Thanks coinmarketcap to making this data available for free (and for every kind of usage).\\nThe dataset\\nAvailable columns in the dataset:\\nDate - the day of recorded values\\nOpen - the opening price (in USD)\\nHigh - the highest price (in USD)\\nLow - the lowest price (in USD)\\nClose - the closing price (in USD)\\nVolume - total exchanged volume (in USD)\\nMarket.Cap - the total market capitalization for the coin (in USD)\\ncoin - the name of the coin\\nDelta - calculated as (Close - Open) / Open',\n",
       " 'These data files contain election results for both the 2012 and 2016 US Presidential Elections, include proportions of votes cast for Romney, Obama (2012) and Trump, Clinton (2016).\\nThe election results were obtained from this Git repository: https://github.com/tonmcg/County_Level_Election_Results_12-16\\nThe county facts data was obtained from another Kaggle election data set: https://www.kaggle.com/benhamner/2016-us-election',\n",
       " 'Are people that use emoji happier?\\npaper --> https://arxiv.org/abs/1710.00888\\nAt ASONAM2017, PydataDubai vol 1.0 @ AWOK, PyDataBCN2017 @ EASDE we have presented the paper Happiness inside a job?... Many people in the various audiences asked why we avoid using emojis to predict and profile employees. The answer is that we prefer to use links of likes because they are more authentic than words or emojis. In the same way that google page rank is more effective when it looks at links between pages rather than content inside the pages. ... Still people keep asking about it. But there is one thing emoji are good at estimating: author sentiment and that is just possible thanks to the unique characteristics of the dataset at hand.\\nPrevious research has traditionally analyzed emoji sentiment from the point of view of the reader of the content not the author. Here, we analyze emoji sentiment from the author point of view and present a benchmark that was built from an employee happiness dataset where emoji happen to be annotated with daily happiness of the author of the comment. We also found out that people that use emoji are happier!?, muuch happier... But the question is, what did we miss?\\nContent\\nThe main table contains columns named after emoji hex codes, a 1 means the emoji appears one time in the comment (row). This dataset is an expanded version of this one, but has different formats, columns and one different table, that is why we decided to release it as separate dataset. as he scripts are not compatible.\\nOther stuff\\nThe R script written on MAC OS does not work in the kaggle platform (because numbers become factors and other little changes in how the code is interpreted...), the full working script (tested on R studio MAC OS) can be found at https://github.com/orioli/emoji-writer-sentiment\\nThank you to Lewis Michel',\n",
       " 'Context\\nHR data can be hard to come by, and HR professionals generally lag behind with respect to analytics and data visualization competency. Thus, Dr. Carla Patalano and I set out to create our own HR-related dataset, which is used in one of our graduate MSHRM courses called HR Metrics and Analytics, at New England College of Business. We created this data set ourselves.\\nContent\\nThere are multiple worksheets within the Excel workbook. These include\\nCore data set\\nProduction staff\\nSales analysis\\nSalaries\\nRecruiting sources\\nThe Excel workbook revolves around a fictitious company, called Dental Magic, and the core data set contains names, DOBs, age, gender, marital status, date of hire, reasons for termination, department, whether they are active or terminated, position title, pay rate, manager name, and performance score.\\nAcknowledgements\\nDr. Carla Patalano provided many suggestions for creating this synthetic data set, which has been used now by over 30 Human Resource Management students at the college. Students in the course learn data visualization techniques with Tableau Desktop and use this data set to complete a series of assignments.\\nInspiration\\nIs there any relationship between who a person works for and their performance score?\\nWhat is the overall diversity profile of the organization?\\nWhat are our best recruiting sources if we want to ensure a diverse organization?\\nThere are so many other interesting questions that could be addressed through this interesting data set. Dr. Patalano and I look forward to seeing what we can come up with.',\n",
       " \"Context:\\nNatural Language Processing(NLP), Text Similarity(lexical and semantic)\\nContent:\\nIn each row of the included datasets(train.csv and test.csv), products X(description_x) and Y(description_y) are considered to refer to the same security(same_security) if they have the same ticker(ticker_x,ticker_y), even if the descriptions don't exactly match. You can make use of these descriptions to predict whether each pair in the test set also refers to the same security.\\nDataset info:\\nTrain - description_x, description_y, ticker_x, ticker_y, same_security. Test - description_x, description_y, same_security(to be predicted)\\nPast Research:\\nThis dataset is pretty similar to the Quora Question Pairs . You can also check out my kernel for dataset exploration and n-gram analysis N-gram analysis on stock data.\\nHow to Approach:\\nThere are several good ways to approach this, check out this algorithm, and see how far you can go with it: https://en.wikipedia.org/wiki/Tf–idf http://scikit learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html. You can also try doing n-gram analysis(check out my kernel). I would suggest using log-loss as your evaluation metric since it gives you a number between 0 and 1 instead of binary classification, which is not so effective in this case.\\nAcknowledgements:\\nQuovo stock data.\",\n",
       " \"We’re releasing 30,000+ OCR’d political memes and their captions . With the election just days away, we hope to contribute to the pre and post-election analysis of the most meme-orable election in modern history.\\nv2 of the dataset includes 8 csv’s:\\nBern.csv: 146 rows\\nBernie.csv: 2100 rows\\nClinton.csv: 4362 rows\\nDonald.csv: 6499 rows\\nGary_Johnston.csv: 140 rows\\nHillary.csv: 7398 rows\\nJill Stein.csv: 96 rows\\nTrump.csv: 12139 rows\\nwith the following columns:\\ntimestamp (date published)\\nid (our unique identifier)\\nlink (post url)\\ncaption (meme caption via ocr)\\nauthor\\nnetwork\\nlikes/upvotes\\nLet us know any revisions you'd like to see. And of course, if you find errors in the data, do let us know.\",\n",
       " 'Context\\nThe Pronto Cycle Share system consists of 500 bikes and 54 stations located in Seattle. Pronto provides open data on individual trips, stations, and daily weather.\\nContent\\nThere are 3 datasets that provide data on the stations, trips, and weather from 2014-2016.\\nStation dataset\\nstation_id: station ID number\\nname: name of station\\nlat: station latitude\\nlong: station longitude\\ninstall_date: date that station was placed in service\\ninstall_dockcount: number of docks at each station on the installation date\\nmodification_date: date that station was modified, resulting in a change in location or dock count\\ncurrent_dockcount: number of docks at each station on 8/31/2016\\ndecommission_date: date that station was placed out of service\\nTrip dataset\\ntrip_id: numeric ID of bike trip taken\\nstarttime: day and time trip started, in PST\\nstoptime: day and time trip ended, in PST\\nbikeid: ID attached to each bike\\ntripduration: time of trip in seconds\\nfrom_station_name: name of station where trip originated\\nto_station_name: name of station where trip terminated\\nfrom_station_id: ID of station where trip originated\\nto_station_id: ID of station where trip terminated\\nusertype: \"Short-Term Pass Holder\" is a rider who purchased a 24-Hour or 3-Day Pass; \"Member\" is a rider who purchased a Monthly or an Annual Membership\\ngender: gender of rider\\nbirthyear: birth year of rider\\nWeather dataset contains daily weather information in the service area\\nAcknowledgements\\nThe original datasets can be downloaded here.\\nInspiration\\nSome ideas worth exploring:\\nWhat is the most popular bike route?\\nHow are bike uses or routes affected by user characteristics, station features, and weather?',\n",
       " 'Context\\nThe National Snow and Ice Data Center (NSIDC) supports research into our world’s frozen realms: the snow, ice, glaciers, frozen ground, and climate interactions that make up Earth’s cryosphere. NSIDC manages and distributes scientific data, creates tools for data access, supports data users, performs scientific research, and educates the public about the cryosphere.\\nContent\\nThe dataset provides the total extent for each day for the entire time period (1978-2015). There are 7 variables:\\nYear\\nMonth\\nDay\\nExtent: unit is 10^6 sq km\\nMissing: unit is 10^6 sq km\\nSource: Source data product web site: http://nsidc.org/data/nsidc-0051.html\\nhemisphere\\nAcknowledgements\\nThe original datasets can be found here and here.\\nInspiration\\nCan you visualize the change in sea ice over time?\\nDo changes in sea ice differ between the two hemispheres?',\n",
       " 'Context A data driven look into answering the common question while travelling overseas: \"how easy is it to get a job in your country?\"\\nContent This dataset contains youth unemployment rates (% of total labor force ages 15-24) (modeled ILO estimate) Latest data available from 2010 to 2014.\\nAcknowledgements International Labour Organization.\\nhttp://data.worldbank.org/indicator/SL.UEM.TOTL.ZS\\nReleased under Open license.',\n",
       " 'Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nThis dataset contains complete information about various aspects of crimes happened in India from 2001. There are many factors that can be analysed from this dataset. Over all, I hope this dataset helps us to understand better about India.\\nContent\\nI : Cases Reported and their Disposal by Police and Court\\nIndian Penal Code\\nSpecial & Local Laws\\nIA : SC/ST Cases Reported and their Disposal by Police and Court\\nCrime against SCs\\nCrime against STs\\nIB : Children Cases Reported and their Disposal by Police and Court\\nAbetment of Suicide (Section 305 IPC)\\nBuying of Girls for Prostitution (Section 373 IPC)\\nChild Marriage Restraint Act, 1929\\nExposure and Abandonment (Section 317 IPC)\\nFoeticide (Section 315 and 316 IPC)\\nInfanticide (Section 315 IPC)\\nKidnapping & Abduction (Section 360,361,363,363-A, 363 read with Section 384, 366, 367 & 369 IPC)\\nMurder (Section 302, 315 IPC)\\nOther Crimes against Children\\nOther Murder of Children (Section 302 IPC)\\nProcuration of Minor Girls (Section 366-A IPC)\\nRape (Section 376 IPC)\\nSelling of Girls for Prostitution (Section 372 IPC)\\nTotal Crimes against Children\\nII : Persons Arrested and their Disposal by Police and Court\\nIndian Penal Code\\nSpecial and Local Laws\\nIIA : SC/ST Persons Arrested and their Disposal by Police and Court\\nCrime against SCs\\nCrime against STs\\nIIB : Children Persons Arrested and their Disposal by Police and Court\\nAbetment of suicide (Section 305 IPC)\\nBuying of girls for prostitution (Section 373 IPC)\\nChild Marriage Restraint Act, 1929\\nExposure and Abandonment (Section 317 IPC)\\nFoeticide (Section 315 and 316 IPC)\\nKidnapping & Abduction (Section 360,361,363,363-A, 366, 367 & 369 IPC)\\nMurder - Infanticide (Section 315 IPC)\\nMurder - Other Murder of Children\\nMurder (Section 302, 315 IPC)\\nOther Crimes against Children\\nProcuration of minor girls (Section 366-A IPC)\\nRape (Section 376 IPC)\\nSelling of girls for prostitution (Section 372 IPC)\\nTotal Crimes against Children\\nIV : Persons Arrested by Sex and Age Group\\nIndian Penal Code\\nSpecial & Local Laws\\nV : Juveniles Apprehended\\nIndian Penal Code\\nSpecial & Local Laws\\nVI : Juveniles Arrested and their Disposal\\nVII : Property Stolen & Recovered (Crime Head)\\nDacoity\\nRobbery\\nBurglary\\nTheft\\nCriminal Breach of Trust\\nOther Property\\nTotal Property Stolen & Recovered\\nVIII : Property Stolen & Recovered (Nature of Property)\\nCommunation and Electricity Wire\\nCattle\\nCycle\\nMotor Vehicles\\nMotor Vehicles - Motor Cycle/Scooters\\nMotor Vehicles - Motor Car/Taxi/Jeep\\nMotor Vehicles - Other Motor Vehicles\\nFire Arms\\nExplosives/Explosive Substances\\nElectronic Components\\nCultural Property including Antiques\\nOther kinds of Property\\nTotal Property Stolen & Recovered\\nIX : Police Strength (Actual & Sanctioned)\\nA) Actual Civil Police (Incl. District Armed Police and Women Police)\\nA) Acual Armed Police (Incl. Women Police)\\nA) Actual Police Strength (Incl. Women)\\nB) Acual Women Civil Police (Incl. District Armed Force)\\nB) Actual Women Armed Police\\nB) Actual Women Police Strength\\nC) Sanctioned Civil Police (Incl. District Armed Police)\\nC) Santioned Armed Police (Incl. Women Police)\\nC) Santioned Police Strength (Incl. Women)\\nD) Sanctioned Women Civil Police (Incl. District Armed Police)\\nD) Sanctioned Women Armed Police\\nD) Sanctioned Women Police Strength\\nX : Police Personnel Killed or Injured on duty\\nConstables\\nHead Constables\\nAssistant Sub-Inspectos\\nSub-Inspectors\\nInspectors\\nGazetted Officers\\nTotal Police Killed or Injured\\nX-B : Age Profile of Police Personnel Killed on Duty\\nX-C : Natural Deaths and Suicides of Police Personnel\\nNatural Deaths of Police Personnel (while in service)\\nPolice Personnel Committed Suicide\\nXI : Casualties under Police Firing and LathiCharge\\nRiot Control\\nAnti Dacoity Operations\\nAgainst Extremists & Terrorists\\nAgainst Others\\nTotal Casualties\\nXII : Cases Reported Value of Property Stolen under Dacoity, Robbery, Burglary and Theft by Place of Occurance\\nResidential Premises\\nHighways\\nRiver and Sea\\nRailways 4.1 In Running Trains 4.2 Others\\nBanks\\nCommercial Establishments (Shops etc.)\\nOther Places\\nTotal\\nXIII : Particulars of Juveniles Arrested\\nEducation\\nEconomic Setup\\nFamily Background\\nRecidivism\\nXIV : Motive/Cause of Murder and Culpable Homicide not Amounting to Murder\\nXV : Victims of Rape(Age Group-wise)\\nIncest Rape Cases\\nOther Rape Cases (Otherthan Incest)\\nTotal Rape Cases\\nXV-A : Rape Offenders relation, nearness to Rape Victims\\nXVI : Persons Arrested under Recidivism\\nXVII : Anti Corruption - Cases\\nXVIII : Anti Corruption - Arrests\\nXIX : Complaints/Cases Against Police Personnel\\nComplaints Received/Cases Registered\\nPolice Personnel Involved/Action Taken\\nDepartmental Action/Punishments\\n*XX : Police Budget and Infrastructure\\nEquipments and Transport Support\\nDistribution of Police Stations by Crime Incidences\\nDistribution of Police Stations by Police Strength\\nOrganisational Set Up\\nSCs/STs and Muslims in Police Force (Actual)\\nXXI : 1. Nature of Complaints Received by Police\\nXXI : 2. Trial of Violent Crimes by Courts\\nMurder\\nAttempt to Murder\\nC H Not Amounting to Murder\\nRape\\nKidnapping & Abduction 5.1 Kidnapping & Abduction of Women & Girls 5.2 Kidnapping & Abduction of Others\\nDacoity\\nPreparation & Assembly for Dacoity\\nRobbery\\nRiots\\nArson\\nDowry Deaths\\nTotal Trials (Sum of 1-11 Above)\\nXXI : 3. Period of Trials by Courts\\nDistrict/Session Judge\\nAdditional Session Judge\\nChief Judicial Magistrate\\nJudicial Magistrate (I)\\nJudicial Magistrate (II)\\nSpecial Judicial Magistrate\\nOther courts\\nTotal Trials (Sum of 1-7 Above)\\nXXI : 4.1 Autho Theft (Stolen & Recovered)\\nMotor Cycles/ Scooters\\nMotor Car/Taxi/Jeep\\nBuses\\nGoods carrying vehicles (Trucks/Tempo etc)\\nOther Motor vehicles\\nTotal (Sum of 1-5 Above)\\nXXI : 4.2 Serious Fraud\\nCriminal Breach of Trust\\nCheating\\nXXI : 5.1 Victims of Murder (Age & Sex-Wise)\\nMale Victims\\nFemale Victims\\nTotal\\nXXI : 5.2 Victims of CH not Amounting to Murder (Age & Sex-wise)\\nMale Victims\\nFemale Victims\\nTotal\\nXXI : 5.3 Use of FireArms in Murder Cases\\nXXI : 6. Human Rights Violation by Police\\nDisappearance of Persons\\nIllegal Detention/Arrests\\nFake Encounter Killings\\nViolation Against Terrorists/Extremists\\nExtortion\\nTorture\\nFalse Implication\\nFailure in Taking Action\\nIndignity to Women\\nAtrocities on SC/ST\\nOthers\\nTotal (Sum of 1-11 Above)\\nXXI : 7. Police Housing\\nFor Officers (Dy.SP & Above)\\nUpper SubOrdinates (ASI to Inspectos)\\nLower SubOrdinates (Constables, Head Constables & Class-IV Subordinate Staff)\\nXXI : 8. Home Guards and Auxilliary force\\nXXI : 9. Unidentified Deadbodies Recovered & Inquest conducted\\nXXI : 10. Victims of Kidnapping & Abduction for Specific Purpose\\nFor Adoption\\nFor Begging\\nfor Camel Racing\\nFor Illicit Intercourse\\nFor Marriage\\nFor Prostitution\\nFor Ransom\\nFor Revenge\\nFor Sale\\nFor Selling Bodyparts\\nFor Slavery\\nFor Unlawful Activity\\nOther Purposes\\nTotal (Sum of 1-13 Above)\\nXXI : 11. Custodial Deaths\\nDeaths in Custody/Lockup of Persons Remanded to Police Custody by Court\\nDeaths in Custody/Lockup of Persons Not Remanded to Police Custody by Court\\nDeaths in Custody during production/process in courts/journey connected with investigation\\nDeaths during Hospitalisation/Treatment\\nDeaths due to Other Reasons\\nXXI : 12. Escapes from Police Custody\\nCases under Crime Against Women\\nRape\\nKidnapping & Abduction of Women & Girls\\nDowry Deaths\\nMolestation\\nSexual Harassment\\nCruelty by Husband and Relatives\\nImportation of Girls\\nImmoral Traffic Prevention Act, 1956\\nDowry Prohibition Act, 1961\\nIndecent Representation of Women(Prohibition) Act, 1986\\nSati Prevention Act, 1987\\nTotal Crimes Against Women\\nArrests under Crime Against Women\\nRape\\nKidnapping & Abduction of Women & Girls\\nDowry Deaths\\nMolestation\\nSexual Harassment\\nCruelty by Husband and Relatives\\nImportation of Girls\\nImmoral Traffic Prevention Act, 1956\\nDowry Prohibition, 1961\\nIndecent Representation of Women(Prohibition) Act, 1986\\nSati Prevention Act, 1987\\nTotal Crimes Against Women\\nSome of the data contains district level data. The districts are police districts and also include special police unit. Therefore these may be different from revenue districts. Most of the data is from 2001 to 2010. But there are few files which has data only from 2011 and few are having 2001-14.\\nInspiration\\nThere could be many things one can understand by analyzing this dataset. Few inspirations for you to start with.\\nWhat is the major reason people being kidnapped in each and every state?\\nOffenders relation to the rape victim\\nJuveniles family background, education and economic setup.\\nWhich state has more crime against children and women?\\nAge group wise murder victim\\nCrime by place of occurrence.\\nAnti corruption cases vs arrests.\\nWhich state has more number of complaints against police?\\nWhich state is the safest for foreigners?\\nAcknowledgements\\nNational Crime Records Bureau (NCRB), Govt of India has published this dataset on their website and also has shared on Open Govt Data Platform India portal under Govt. Open Data License - India.',\n",
       " 'Context\\nIf you like to eat cereal, do yourself a favor and avoid this dataset at all costs. After seeing these data it will never be the same for me to eat Fruity Pebbles again.\\nContent\\nFields in the dataset:\\nName: Name of cereal\\nmfr: Manufacturer of cereal\\nA = American Home Food Products;\\nG = General Mills\\nK = Kelloggs\\nN = Nabisco\\nP = Post\\nQ = Quaker Oats\\nR = Ralston Purina\\ntype:\\ncold\\nhot\\ncalories: calories per serving\\nprotein: grams of protein\\nfat: grams of fat\\nsodium: milligrams of sodium\\nfiber: grams of dietary fiber\\ncarbo: grams of complex carbohydrates\\nsugars: grams of sugars\\npotass: milligrams of potassium\\nvitamins: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of FDA recommended\\nshelf: display shelf (1, 2, or 3, counting from the floor)\\nweight: weight in ounces of one serving\\ncups: number of cups in one serving\\nrating: a rating of the cereals (Possibly from Consumer Reports?)\\nAcknowledgements\\nThese datasets have been gathered and cleaned up by Petra Isenberg, Pierre Dragicevic and Yvonne Jansen. The original source can be found here\\nThis dataset has been converted to CSV\\nInspiration\\nEat too much sugary cereal? Ruin your appetite with this dataset!',\n",
       " 'Context\\nNetflix in the past 5-10 years has captured a large populate of viewers. With more viewers, there most likely an increase of show variety. However, do people understand the distribution of ratings on Netflix shows?\\nContent\\nBecause of the vast amount of time it would take to gather 1,000 shows one by one, the gathering method took advantage of the Netflix’s suggestion engine. The suggestion engine recommends shows similar to the selected show. As part of this data set, I took 4 videos from 4 ratings (totaling 16 unique shows), then pulled 53 suggested shows per video. The ratings include: G, PG, TV-14, TV-MA. I chose not to pull from every rating (e.g. TV-G, TV-Y, etc.).\\nAcknowledgements\\nThe data set and the research article can be found at The Concept Center\\nInspiration\\nI was watching Netflix with my wife and we asked ourselves, why are there so many R and TV-MA rating shows?',\n",
       " \"Context\\nOur world population is expected to grow from 7.3 billion today to 9.7 billion in the year 2050. Finding solutions for feeding the growing world population has become a hot topic for food and agriculture organizations, entrepreneurs and philanthropists. These solutions range from changing the way we grow our food to changing the way we eat. To make things harder, the world's climate is changing and it is both affecting and affected by the way we grow our food – agriculture. This dataset provides an insight on our worldwide food production - focusing on a comparison between food produced for human consumption and feed produced for animals.\\nContent\\nThe Food and Agriculture Organization of the United Nations provides free access to food and agriculture data for over 245 countries and territories, from the year 1961 to the most recent update (depends on the dataset). One dataset from the FAO's database is the Food Balance Sheets. It presents a comprehensive picture of the pattern of a country's food supply during a specified reference period, the last time an update was loaded to the FAO database was in 2013. The food balance sheet shows for each food item the sources of supply and its utilization. This chunk of the dataset is focused on two utilizations of each food item available:\\nFood - refers to the total amount of the food item available as human food during the reference period.\\nFeed - refers to the quantity of the food item available for feeding to the livestock and poultry during the reference period.\\nDataset's attributes:\\nArea code - Country name abbreviation\\nArea - County name\\nItem - Food item\\nElement - Food or Feed\\nLatitude - geographic coordinate that specifies the north–south position of a point on the Earth's surface\\nLongitude - geographic coordinate that specifies the east-west position of a point on the Earth's surface\\nProduction per year - Amount of food item produced in 1000 tonnes\\nAcknowledgements\\nThis dataset was meticulously gathered, organized and published by the Food and Agriculture Organization of the United Nations.\\nInspiration\\nAnimal agriculture and factory farming is a a growing interest of the public and of world leaders.\\nCan you find interesting outliers in the data?\\nWhat are the fastest growing countries in terms of food production\\\\consumption?\\nCompare between food and feed consumption.\",\n",
       " 'Context:\\nBesides coffee, grunge and technology companies, one of the things that Seattle is most famous for is how often it rains. This dataset contains complete records of daily rainfall patterns from January 1st, 1948 to December 12, 2017.\\nContent\\nThis data was collected at the Seattle-Tacoma International Airport. The dataset contains five columns:\\nDATE = the date of the observation\\nPRCP = the amount of precipitation, in inches\\nTMAX = the maximum temperature for that day, in degrees Fahrenheit\\nTMIN = the minimum temperature for that day, in degrees Fahrenheit\\nRAIN = TRUE if rain was observed on that day, FALSE if it was not\\nAcknowledgements:\\nThis dataset was compiled by NOAA and is in the public domain.\\nInspiration:\\nCan you use this dataset to build a model of whether it will rain on a specific day given information on the previous days?\\nIs there a correlation between the minimum and maximum temperature? Can you predict one given the other?\\nCan you model changes in the amount of precipitation over time? Is there seasonality?',\n",
       " 'Context\\nSince 2008, guests and hosts have used Airbnb to travel in a more unique, personalized way. As part of the Airbnb Inside initiative, this dataset describes the listing activity of homestays in Boston, MA.\\nContent\\nThe following Airbnb activity is included in this Boston dataset: * Listings, including full descriptions and average review score * Reviews, including unique id for each reviewer and detailed comments * Calendar, including listing id and the price and availability for that day\\nInspiration\\nCan you describe the vibe of each Boston neighborhood using listing descriptions?\\nWhat are the busiest times of the year to visit Boston? By how much do prices spike?\\nIs there a general upward trend of both new Airbnb listings and total Airbnb visitors to Boston?\\nFor more ideas, visualizations of all Boston datasets can be found here.\\nAcknowledgement\\nThis dataset is part of Airbnb Inside, and the original source can be found here.',\n",
       " 'NIH Chest X-ray Dataset Sample\\nNational Institutes of Health Chest X-Ray Dataset\\nChest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, Openi was the largest publicly available source of chest X-ray images with 4,143 images available.\\nThis NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be >90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: \"ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases.\" (Wang et al.)\\nLink to paper\\n\\nFile contents - This is a random sample (5%) of the full dataset:\\nsample.zip: Contains 5,606 images with size 1024 x 1024\\nsample_labels.csv: Class labels and patient data for the entire dataset\\nImage Index: File name\\nFinding Labels: Disease type (Class label)\\nFollow-up #\\nPatient ID\\nPatient Age\\nPatient Gender\\nView Position: X-ray orientation\\nOriginalImageWidth\\nOriginalImageHeight\\nOriginalImagePixelSpacing_x\\nOriginalImagePixelSpacing_y\\n\\nClass descriptions\\nThere are 15 classes (14 diseases, and one for \"No findings\") in the full dataset, but since this is drastically reduced version of the full dataset, some of the classes are sparse with the labeled as \"No findings\"\\nHernia - 13 images\\nPneumonia - 62 images\\nFibrosis - 84 images\\nEdema - 118 images\\nEmphysema - 127 images\\nCardiomegaly - 141 images\\nPleural_Thickening - 176 images\\nConsolidation - 226 images\\nPneumothorax - 271 images\\nMass - 284 images\\nNodule - 313 images\\nAtelectasis - 508 images\\nEffusion - 644 images\\nInfiltration - 967 images\\nNo Finding - 3044 images\\n\\nFull Dataset Content\\nThe full dataset can be found here. There are 12 zip files in total and range from ~2 gb to 4 gb in size.\\n\\nData limitations:\\nThe image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be >90%.\\nVery limited numbers of disease region bounding boxes (See BBox_list_2017.csv)\\nChest x-ray radiology reports are not anticipated to be publicly shared. Parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later, maybe through manual annotation\\n\\nModifications to original data\\nOriginal TAR archives were converted to ZIP archives to be compatible with the Kaggle platform\\nCSV headers slightly modified to be more explicit in comma separation and also to allow fields to be self-explanatory\\n\\nCitations\\nWang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf\\nNIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community\\nOriginal source files and documents: https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345\\n\\nAcknowledgements\\nThis work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov).',\n",
       " 'Context\\nHearthstone is a very popular collectible card game published by Blizzard Entertainment in 2014. The goal of the game consists in building a 30 cards deck in order to beat your opponent. A few weeks ago, I decided to download all the decks posted by players at Hearthpwn. The code to download the data is available here.\\nContent\\nThis upload is composed of two files :\\ndata.json / data.csv\\nContains the actual Hearthstone deck records. Each record features :\\ndate (str) : the date of publication (or last update) of the deck.\\nuser (str) : the user who uploaded the deck.\\ndeck_class (str) : one of the nine character class in Hearthstone (Druid, Priest, ...).\\ndeck_archetype (str) : the theme of deck labelled by the user (Aggro Druid, Dragon Priest, ...).\\ndeck_format (str) : the game format of the deck on the day data was recorded (W for \"Wild\" or S for \"Standard\").\\ndeck_set (str) : the latest expansion published prior the deck publication (Naxxramas, TGT Launch, ...).\\ndeck_id (int) : the ID of the deck.\\ndeck_type (str) : the type of the deck labelled by the user :\\nRanked Deck : a deck played on ladder.\\nTheorycraft : a deck built with unreleased cards to get a gist of the future metagame.\\nPvE Adventure : a deck built to beat the bosses in adventure mode.\\nArena : a deck built in arena mode.\\nTavern Brawl : a deck built for the weekly tavern brawl mode.\\nTournament : a deck brought at tournament by a pro-player.\\nNone : the game type was not mentioned.\\nrating (int) : the number of upvotes received by that deck.\\ntitle (str) : the name of the deck.\\ncraft_cost (int) : the amount of dust (in-game craft material) required to craft the deck.\\ncards (list) : a list of 30 card ids. Each ID can be mapped to the card description using the reference file.\\nrefs.json\\nContains the reference to the cards played in Hearthstone. This file was originally proposed on HearthstoneJSON. Each record features a lot of informations about the cards, I\\'ll list the most important :\\ndbfId (int) : the id of the card (the one used in data.json).\\nrarity (str) : the rarity of the card (EPIC, RARE, ...).\\ncardClass (str) : the character class (WARLOCK, PRIEST, ...).\\nartist (str) : the artist behind the card\\'s art.\\ncollectible (bool) : whether or not the card can be collected.\\ncost (int) : the card play cost.\\nhealth (int) : the card health (if it\\'s a minion).\\nattack (int) : the card attack (if it\\'s a minion).\\nname (str) : the card name.\\nflavor (str) : the card\\'s flavor text.\\nset (str) : the set / expansion which featured this card.\\ntext (int) : the card\\'s text.\\ntype (str) : the card\\'s type (MINION, SPELL, ...).\\nrace (str) : the card\\'s race (if it\\'s a minion).\\nset (str) : the set / expansion which featured this card.\\n...\\nIf you need help cleaning the data take a look at my start over kernel!\\nWhat you could do :\\nTry to predict the deck archetype based on the cards features in the deck.\\nSeek relationships between the cost of the deck and it\\'s popularity.\\nDescribe the evolution of the meta-game over-time.\\nFind out unbalanced (overplayed) cards',\n",
       " 'Context\\nPakistan Suicide Bombing Attacks (1995-2016)\\nSuicide bombing is an operational method in which the very act of the attack is dependent upon the death of the perpetrator. Though only 3% of all terrorist attacks around the world can be classified as suicide bombing attacks these account for 48% of the casualties. Explosions and suicide bombings have become the modus operandi of terrorist organizations throughout the world. The world is full of unwanted explosives, brutal bombings, accidents, and violent conflicts, and there is a need to understand the impact of these explosions on one’s surroundings, the environment, and most importantly on human bodies. From 1980 to 2001 (excluding 9/11/01) the average number of deaths per incident for suicide bombing attacks was 13. This number is far above the average of less than one death per incident across all types of terrorist attacks over the same time period. Suicide bombers, unlike any other device or means of destruction, can think and therefore detonate the charge at an optimal location with perfect timing to cause maximum carnage and destruction. Suicide bombers are adaptive and can quickly change targets if forced by security risk or the availability of better targets. Suicide attacks are relatively inexpensive to fund and technologically primitive, as IEDs can be readily constructed.\\nWorld has seen more than 3,600 suicide bombing attacks in over 40 countries since 1982. Suicide Bombing has wreaked havoc in Pakistan in the last decade or so. From only a couple of attacks before 2000, it kept escalating after the US Operation Enduring Freedom in Afghanistan, promiscuously killing hundreds of people each year, towering as one of the most prominent security threats that every single Pakistani faces today. The conundrum of suicide bombing in Pakistan has obliterated 6,982 clean-handed civilians and injured another 17,624 in a total of 475 attacks since 1995. More than 94% of these attacks have taken place after year 2006. From 2007 to 2013 the country witnessed a suicide bombing attack on every 6th day that increased to every 4th day in 2013. Counting the dead and the injured, each attack victimizes 48 people in Pakistan.\\nPakistan Body Count (www.PakistanBodyCount.org) is the oldest and most accurate running tally of suicide bombings in Pakistan. The given database (PakistanSuicideAttacks.CSV) has been populated by using majority of the data from Pakistan Body Count, and building up on it by canvassing open source newspapers, media reports, think tank analyses, and personal contacts in media and law enforcement agencies. We provide a count of the people killed and injured in suicide attacks, including the ones who died later in hospitals or homes due to injuries caused or aggravated by these attacks (second and tertiary blast injuries), making it the most authentic source for suicide attacks related data in this region.\\nWe will keep releasing the updates every quarter at this page.\\nContent\\nGeography: Pakistan\\nTime period: 1995-2016\\nUnit of analysis: Attack\\nDataset: The dataset contains detailed information of 475 suicide bombing attacks in Pakistan that killed an estimated 6,982 and injured 17,624 people.\\nVariables: The dataset contains Serial No, Incident Date, Islamic Date (based on Islamic lunar calendar), approximate Time, Long-Lat, City, Province, Location, Location Sensitivity & Type, Target type and Sect, Open/Close Space (as it will change the impact of blast waves due to reflection), min and max number of people killed and injured, number of suicide bombers, amount of explosive being used and the name of hospitals where victims went for treatment.\\nSources: Unclassified media articles, hospital reports, think tank analysis and reports, and government official press releases.\\nAcknowledgements & References\\nPakistan Body Count has been leveraged extensively in scholarly publications, reports, media articles and books. The website and the dataset has been collected and curated by the founder Zeeshan-ul-hassan Usmani.\\nUsers are allowed to use, copy, distribute and cite the dataset as follows: “Zeeshan-ul-hassan Usmani, Pakistan Body Count, Pakistan Suicide Bombing Attacks Dataset, Kaggle Dataset Repository, Jan 25, 2017.”\\nPast Work\\nZeeshan-ul-hassan Usmani and Daniel Kirk, “Simulation of Suicide Bombing – Using Computers to Save Lives”, I-Universe, New York, NY, April 2011\\nZeeshan-ul-hassan Usmani and Daniel Kirk, “Modeling and Simulation of Explosion Effectiveness as a Function of Blast and Crowd Characteristics”, The Journal of Defense Modeling and Simulation: Applications, Methodology, Technology, Sage Publications with Society of Simulation, Vol. 6, No. 2, pp. 79-95, Vista, CA, USA, October 2009\\nMuhammad Irfan and Zeeshan-ul-hassan Usmani, “Suicide Terrorism and its New Target –Pakistan”, in Wars, Insurgencies and Terrorist Attacks: A Psychosocial Perspective from The Muslim World, by Unaiza Niaz, Oxford University Press, Canada, July 2010\\nSana Rasheed, Data Science for Suicide Bombings, I-Universe, New York, NY, December 2016\\nZeeshan-ul-hassan Usmani and Sana Rasheed, “Terrorism: What Data Sciences Can Do?”, 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2014, Data Framework Track) at Bloomberg, New York, NY, USA (August 24-27, 2014)\\nZeeshan-ul-hassan Usmani, “Suicide Bombing Forecaster – Novel Techniques to Predict Patterns of Suicide Bombing in Pakistan”, 2012 Conference on Homeland Security, part of 2012 Autumn Simulation Multi-Conference, San Diego, CA, USA, October 28 – 31, 2012\\nZeeshan-ul-hassan Usmani, “BlastSim – Simulation to Save Lives”, IEEE/SIC Winter Simulation Conference, PhD Colloquium, Austin, Texas, December 13-16, 2009\\nZeeshan-ul-hassan Usmani, Fawzi Alghamdi, and Daniel Kirk, “BlastSim – Multi-agent Simulation of Suicide Bombing“, IEEE Symposium: Computational Intelligence for Security and Defense Applications (CISDA), Ottawa, Canada, July 8-10, 2009\\nZeeshan-ul-hassan Usmani, Eyosias Imana and Daniel Kirk, “Virtual Iraq – Simulation of Insurgent Attacks”, IEEE Workshop on Computational Intelligence in Virtual Environments (CIVE), March 30-April 2, 2009\\nZeeshan-ul-hassan Usmani, Eyosias Imana, and Daniel Kirk, “Random Walk in Extreme Conditions – An Agent Based Simulation of Suicide Bombing”, IEEE Symposium on Intelligent Agents, March, 2009\\nZeeshan-ul-hassan Usmani, Eyosias Imana and Daniel Kirk, “Escaping Death – Geometrical Recommendations for High Value Targets”, IEEE International Joint Conferences on Computer, Information and Systems Sciences and Engineering (CIS2E 08), Bridgeport, CT, December 5–13, 2008\\nZeeshan-ul-hassan Usmani and Daniel Kirk, “Extreme Conditions for Intelligent Agents”, IEEE 2008 WI-IAT Doctoral Workshop, Sydney, Australia, December 9-12, 2008\\nZeeshan-ul-hassan Usmani, Andrew English & Richard Griffith, “The Effects of a Suicide Bombing: Crowd Formations”, Inter-service/Industry Training, Simulation, and Education Conference (I/ITSEC), Orlando, FL, Nov 26-29 2007\\nInspiration\\nSome ideas worth exploring:\\nHow many people got killed and injured per year?\\nVisualize suicide attacks on timeline\\nFind out any correlation with number of suicide bombing attacks with drone attacks\\nFind out any correlation with suicide bombing attacks with influencing events given in the dataset\\nCan we predict the next suicide bombing attack?\\nFind the correlation between blast/explosive weight and number of people killed and injured\\nFind the impact of holiday type on number of blast victims\\nFind the correlation between Islamic date and blast day/time/size/number of victims\\nFind the Top 10 locations of blasts\\nFind the names of hospitals sorted by number of victims\\nQuestions?\\nFor detailed visit www.PakistanBodyCount.org\\nOr contact Pakistan Body Count staff at info@pakistanbodycount.org',\n",
       " 'Full text of questions and answers from Stack Overflow that are tagged with the r tag, useful for natural language processing and community analysis.\\nThis is organized as three tables:\\nQuestions contains the title, body, creation date, score, and owner ID for each R question.\\nAnswers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.\\nTags contains the tags on each question besides the R tag.\\nFor space reasons only non-deleted and non-closed content are included in the dataset. The dataset contains questions up to 24 September 2017 (UTC).\\nLicense\\nAll Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.',\n",
       " 'Context\\nThe ATUS Eating & Health (EH) Module was fielded from 2006 to 2008 and again in 2014 to 2016. The EH Module data files contain additional information related to eating, meal preparation and health.\\nData for 2015 currently are being processed and have not yet been released. Data collection is planned to run through December 2016.\\nContent\\nThere are 3 datasets from 2014:\\nThe EH Respondent file, which contains information about EH respondents, including general health and body mass index. There are 37 variables.\\nThe EH Activity file, which contains information such as the activity number, whether secondary eating occurred during the activity, and the duration of secondary eating. There are 5 variables.\\nThe EH Replicate weights file, which contains miscellaneous EH weights. There are 161 variables.\\nThe data dictionary can be found here.\\nAcknowledgements\\nThe original datasets can be found here.\\nInspiration\\nSome ideas for exploring the datasets are:\\nWhat is the relationship between weight or BMI and meal preparation patterns, consumption of fresh/fast food, or snacking patterns?\\nDo grocery shopping patterns differ by income?',\n",
       " 'Context\\nEach January, the entertainment community and film fans around the world turn their attention to the Academy Awards. Interest and anticipation builds to a fevered pitch leading up to the Oscar telecast in February, when hundreds of millions of movie lovers tune in to watch the glamorous ceremony and learn who will receive the highest honors in filmmaking.\\nAchievements in up to 25 regular categories will be honored on February 26, 2017, at the 89th Academy Awards presentation at the Dolby Theatre at Hollywood & Highland Center.\\nContent\\nThe Academy Awards Database contains the official record of past Academy Award winners and nominees. The data is complete through the 2015 (88th) Academy Awards, presented on February 28, 2016.\\nAcknowledgements\\nThe awards data was scraped from the Official Academy Awards Database; nominees were listed with their name first and film following in some categories, such as Best Actor/Actress, and in the reverse for others.\\nInspiration\\nDo the Academy Awards reflect the diversity of American films or are the #OscarsSoWhite? Which actor/actress has received the most awards overall or in a single year? Which film has received the most awards in a ceremony? Can you predict who will receive the 2016 awards?',\n",
       " 'Context\\nUK police forces collect data on every vehicle collision in the uk on a form called Stats19. Data from this form ends up at the DfT and is published at https://data.gov.uk/dataset/road-accidents-safety-data\\nContent\\nThere are 3 CSVs in this set. Accidents is the primary one and has references by Accident_Index to the casualties and vehicles tables. This might be better done as a database.\\nInspiration\\nQuestions to ask of this data -\\ncombined with population data, how do different areas compare?\\nwhat trends are there for accidents involving different road users eg motorcycles, peds, cyclists\\nare road safety campaigns effective?\\nlikelihood of accidents for different groups / vehicles\\nmany more..\\nManifest\\ndft05-15.tgz - tar of Accidents0515.csv, Casualties0515.csv and Vehicles0515.csv tidydata.sh - script to get and tidy data.',\n",
       " 'Context\\nBeing a fan of board games, I wanted to see if there was any correlation with a games rating and any particular quality, the first step was to collect of this data.\\nContent\\nThe data was collected in March of 2017 from the website https://boardgamegeek.com/, this site has an API to retrieve game information (though sadly XML not JSON).\\nAcknowledgements\\nMainly I want to thank the people who run the board game geek website for maintaining such a great resource for those of us in the hobby.\\nInspiration\\nI wish I had some better questions to ask of the data, perhaps somebody else can think of some good ways to get some insight of this dataset.',\n",
       " \"Context\\nSan Francisco's Integrated Library System (ILS) is composed of bibliographic records including inventoried items, patron records, and circulation data. The data is used in the daily operation of the library, including circulation, online public catalog, cataloging, acquisitions, collection development, processing, and serials control. This dataset represents the usage of inventoried items by patrons (~420K records).\\nContent\\nThe dataset includes approximately 420,000 records, with each record representing an anonymized library patron. Individual columns include statistics on the type code and age of the patron, the year the patron registered (only since 2003), and how heavily the patron has been utilizing the library system (in terms of number of checkouts) since first registering.\\nFor more information on specific columns refer to the official data dictionary and the information in the Column Metadata on the /Data tab.\\nAcknowledgements\\nThe data is provided by SF Public Library via the San Francisco Open Data Portal, under the PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL).\\nPhoto via Flickr Kolya Miller (CC BY-NC-SA 2.0).\\nInspiration\\nWhat attributes are most associated with library activity (# of checkouts, # of renewals)?\\nCan you group the data into type of patrons? What classifiers would you use to predict patron type?\",\n",
       " \"Context\\nEclipses of the sun can only occur when the moon is near one of its two orbital nodes during the new moon phase. It is then possible for the Moon's penumbral, umbral, or antumbral shadows to sweep across Earth's surface thereby producing an eclipse. There are four types of solar eclipses: a partial eclipse, during which the moon's penumbral shadow traverses Earth and umbral and antumbral shadows completely miss Earth; an annular eclipse, during which the moon's antumbral shadow traverses Earth but does not completely cover the sun; a total eclipse, during which the moon's umbral shadow traverses Earth and completely covers the sun; and a hybrid eclipse, during which the moon's umbral and antumbral shadows traverse Earth and annular and total eclipses are visible in different locations. Earth will experience 11898 solar eclipses during the five millennium period -1999 to +3000 (2000 BCE to 3000 CE).\\nEclipses of the moon can occur when the moon is near one of its two orbital nodes during the full moon phase. It is then possible for the moon to pass through Earth's penumbral or umbral shadows thereby producing an eclipse. There are three types of lunar eclipses: a penumbral eclipse, during which the moon traverses Earth's penumbral shadow but misses its umbral shadow; a partial eclipse, during which the moon traverses Earth's penumbral and umbral shadows; and a total eclipse, during which the moon traverses Earth's penumbral and umbral shadows and passes completely into Earth's umbra. Earth will experience 12064 lunar eclipses during the five millennium period -1999 to +3000 (2000 BCE to 3000 CE).\\nAcknowledgements\\nLunar eclipse predictions were produced by Fred Espenak from NASA's Goddard Space Flight Center.\",\n",
       " 'The Washington Post is compiling a database of every fatal shooting in the United States by a police officer in the line of duty since January 1, 2015.\\nIn 2015, The Post began tracking more than a dozen details about each killing — including the race of the deceased, the circumstances of the shooting, whether the person was armed and whether the victim was experiencing a mental-health crisis — by culling local news reports, law enforcement websites and social media and by monitoring independent databases such as Killed by Police and Fatal Encounters.\\nThe Post is documenting only those shootings in which a police officer, in the line of duty, shot and killed a civilian — the circumstances that most closely parallel the 2014 killing of Michael Brown in Ferguson, Missouri, which began the protest movement culminating in Black Lives Matter and an increased focus on police accountability nationwide. The Post is not tracking deaths of people in police custody, fatal shootings by off-duty officers or non-shooting deaths.\\nThe FBI and the Centers for Disease Control and Prevention log fatal shootings by police, but officials acknowledge that their data is incomplete. In 2015, The Post documented more than two times more fatal shootings by police than had been recorded by the FBI.\\nThe Post’s database is updated regularly as fatal shootings are reported and as facts emerge about individual cases. The Post is seeking assistance in making the database as comprehensive as possible. To provide information about fatal police shootings, send us an email at policeshootingsfeedback@washpost.com.\\nCREDITS\\nResearch and Reporting: Julie Tate, Jennifer Jenkins and Steven Rich\\nProduction and Presentation: John Muyskens, Kennedy Elliott and Ted Mellnik',\n",
       " \"Content\\nWhich Olympic athletes have the most gold medals? Which countries are they from and how has it changed over time?\\nMore than 35,000 medals have been awarded at the Olympics since 1896. The first two Olympiads awarded silver medals and an olive wreath for the winner, and the IOC retrospectively awarded gold, silver, and bronze to athletes based on their rankings. This dataset includes a row for every Olympic athlete that has won a medal since the first games.\\nAcknowledgements\\nData was provided by the IOC Research and Reference Service and published by The Guardian's Datablog.\",\n",
       " 'Challenge Description\\nThis dataset and accompanying paper present a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. That is, a date written \"31 May 2014\" is spoken as \"the thirty first of may twenty fourteen.\" We present a dataset of general text where the normalizations were generated using an existing text normalization component of a text-to-speech (TTS) system. This dataset was originally released open-source here and is reproduced on Kaggle for the community.\\nThe Data\\nThe data in this directory are the English language training, development and test data used in Sproat and Jaitly (2016).\\nThe following divisions of data were used:\\nTraining: output_1 through output_21 (corresponding to output-000[0-8]?-of-00100 in the original dataset)\\nRuntime eval: output_91 (corresponding to output-0009[0-4]-of-00100 in the original dataset)\\nTest data: output_96 (corresponding to output-0009[5-9]-of-00100 in the original dataset)\\nIn practice for the results reported in the paper only the first 100,002 lines of output-00099-of-00100 were used (for English).\\nLines with \"\" in two columns are the end of sentence marker, otherwise there are three columns, the first of which is the \"semiotic class\" (Taylor, 2009), the second is the input token and the third is the output, following the paper cited above.\\nAll text is from Wikipedia. All data were extracted on 2016/04/08, and run through the Google Kestrel TTS text normalization system (Ebden and Sproat, 2015), so that the notion of \"token\", \"semiotic class\" and reference output are all Kestrel\\'s notion.\\nOur Research\\nIn this paper, we present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone.\\nThough our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And with open-source data, we provide a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions.\\nDisclaimer\\nThis is not an official Google product.\\nReferences\\nEbden, Peter and Sproat, Richard. 2015. The Kestrel TTS text normalization system. Natural Language Engineering. 21(3).\\nRichard Sproat and Navdeep Jaitly. 2016. RNN Approaches to Text Normalization: A Challenge. Released on arXiv.org: https://arxiv.org/abs/1611.00068\\nTaylor, Paul. 2009. Text-to-Speech Synthesis. Cambridge University Press, Cambridge.',\n",
       " \"Context\\nConsolidated draft data from http://www.pro-football-reference.com/ for all drafts from 1985 to 2015.\\nContent\\nPro-Football-Reference AV: Approximate Value is PFR's attempt to attach a single number to every player-season since 1960. Methodology can be found here: http://www.pro-football-reference.com/blog/indexd961.html?page_id=8061\\nPlayer_Id Pro Football Reference Player Id\\nYear Draft Year\\nRnd Draft Round\\nPick Draft Pick\\nTm Team\\nPlayer Player first and last name\\nPos Position unfiltered\\nPosition Standard Position standardized to one of the following QB, LB, WR, T, DE, RB, DB, DT, C, C, G, TE, FB, P, LS, K\\nFirst4AV AV accumulated for this player's first four seasons\\nAge Age at time of draft\\nTo Year of last season played\\nAP1 # of first team all-pro selections\\nPB # of pro-bowl selections\\nSt # of years as a primary starter in their primary position\\nCarAV Weighted Career AV - 100% of best season, 95% of second best season, 90% of third best season, and so on\\nDrAV AV accumulated for team that drafted this player\\nG Games played\\nCmp Pass completions\\nPass_Att Pass attempts\\nPass_Yds Yards gained by passing\\nPass_TD Passing touchdowns\\nPass_Int Interceptions thrown\\nRush_Att Rushing attempts\\nRush_Yds Rushing yards gained\\nRush_TDs Rushing touchdowns\\nRec Receptions\\nRec_Yds Receiving yards gained\\nRec_Tds Receiving touchdowns\\nTkl Tackles\\nDef_Int Defensive interceptions\\nSk Sacks\\nCollege/Univ College/University attended by player\\nAcknowledgements\\nhttp://www.pro-football-reference.com/\",\n",
       " \"Horse Racing - A different and profitable approach\\nThe traditional approach in attempting to make a profit from horse-racing, using machine learning techniques, is to use systems involving dozens and dozens of variables. These systems include the following types of variables:\\nHorse - Name, Sex, Age, Pedigree, Weight, Speed over various distances, race data with finishing times and positions - etc. Trainer info. Jockey info. Track info - Track, track conditions - etc.\\nAnd a whole lot more.\\nFinding, compiling, maintaining and updating this data is a massive task for the individual. Unless you have access to a database of such data - where would you even start?\\nWe have a different approach.\\nWe collect, maintain and use data from various 'Tipsters'. The tipsters use their skill to study the horses and make a prediction - that they think a particular horse will win a particular race. We take those tipsters predictions and put them through a machine learning algorithm (microsoft azure) asking it to predict a 'win' or 'lose' based upon the tipsters performance history.\\nWe have a database of approx. 39,000 bets using 31 odd tipsters. Fifteen tipsters are active and sixteen tipsters are inactive The betting history for the inactive tipsters is used in the dataset as it appears to add 'weight' to the system when considering active tips.\\nWe have been using this system live for three months now and although it has it's ups and downs - it makes money! One bookmaker has already closed our account.\\nWe are looking to further optimize the system to reach it's maximum efficiency coupled with a betting strategy to increase profitability. We ask for your help. If you can produce an 'Azure' system more efficient than ours - then further information will be shared with you.\\nQuestions\\nAre bets from inactive tipsters critical to performance?\\nIs it better to have all the tipsters 'stacked on top of each other' in one large dataset or is the system better served by separating them out?\\nPredicting Bets\\nWhen we ask the system to predict if a bet will Win or Lose for say Tipster A - we take the last ID number for that Tipster and add one to it - making it a new ID - outside the systems experience. That ID is used for all that Tipsters bets until the system is updated. The system is updated once a week.\\nGood hunting.\\nGunner38\",\n",
       " 'NFL-Statistics-Scrape\\nHere are the basic statistics, career statistics and game logs provided by the NFL on their website (http://www.nfl.com) for all players past and present.\\nSummary\\nThe data was scraped using a Python code. The code can be located at Github: https://github.com/kendallgillies/NFL-Statistics-Scrape\\nExplanation of Data\\nThe first main group of statistics is the basic statistics provided for each player. This data is stored in the CSV file titled Basic_Stats.csv along with the player’s name and URL identifier. If available the data pulled for each player is as follows:\\nNumber\\nPosition\\nCurrent Team\\nHeight\\nWeight\\nAge\\nBirthday\\nBirth Place\\nCollege Attended\\nHigh School Attended\\nHigh School Location\\nExperience\\nThe second main group of statistics gathered for each player are their career statistics. While each player has a main position they play, they will have statistics in other areas; therefore, the career statistics are divided into statistics types. The statistics are then stored in CSV files based on statistic type along with the player name, URL identifier and position (if available). The following are the career statistics types and accompanying CSV file names:\\nDefensive Statistics – Career_Stats_Defensive.csv\\nField Goal Kickers - Career_Stats_Field_Goal_Kickers.csv\\nFumbles - Career_Stats_Fumbles.csv\\nKick Return - Career_Stats_Kick_Return.csv\\nKickoff - Career_Stats_Kickoff.csv\\nOffensive Line - Career_Stats_Offensive_Line.csv\\nPassing - Career_Stats_Passing.csv\\nPunt Return - Career_Stats_Punt_Return.csv\\nPunting - Career_Stats_Punting.csv\\nReceiving - Career_Stats_Receiving.csv\\nRushing - Career_Stats_Rushing.csv\\nThe final group of statistics is the game logs for each player. The game logs are stored by position and have the player name, URL identifier and position (if available). The following are the game log types and accompanying CSV file names:\\nQuarterback – Game_Logs_Quarterback.csv\\nRunning back – Game_Logs_Runningback.csv\\nWide Receiver and Tight End – Game_Logs_Wide_Receiver_and_Tight_End.csv\\nOffensive Line – Game_Logs_Offensive_Line.csv\\nDefensive Lineman – Game_Logs_Defensive_Lineman.csv\\nKickers – Game_Logs_Kickers.csv\\nPunters – Game_Logs_Punters.csv\\nGlossary\\nWhile most of the abbreviations used by the NFL have been translated in the table headers in the data files, there are still a couple of abbreviations used.\\nFG: Field Goal\\nTD: Touchdown\\nInt: Interception',\n",
       " \"Context\\nNew York City’s trees shade us in the summer, beautify our neighborhoods, help reduce noise, and support urban wildlife. Beyond these priceless benefits, our urban forest provides us a concrete return on the financial investment we put into it. This return includes stormwater interception, energy conservation, air pollutant removal, and carbon dioxide storage. Our publicly owned trees are as much of an asset to us as our streets, sewers, bridges, and public buildings.\\nContent\\nThis dataset includes a record for every tree in New York City and includes the tree's location by borough and latitude/longitude, species by Latin name and common names, size, health, and issues with the tree's roots, trunk, and branches.\\nAcknowledgements\\nThe 2015, 2005, and 1995 tree censuses were conducted by NYC Parks and Recreation staff, TreesCount! program staff, and hundreds of volunteers.\",\n",
       " 'Can Happiness Predict Employee Turnover, or is it the Other Way Around?\\nIt is the summer of 2016. I am in Barcelona and it is hot and humid. By chance I go to a talk where Alex Rios - the ceo of myhappyforce.com explains his product. He has built an app where employees report daily happiness levels at work. This app is used by companies to track happiness of the workforce. After the talk I ask him if he would opensource the (anonymized data) so we can better understad the phenomenon of employee turnover. Here is what we did, we developed a model that predicts which employees will churn. Then we looked at the features (used by the model) that are common to employees that churn. The top feautures of employees that churn are:\\nlow ratio of likes received (likeability)\\nlow posting frequency (engagement),\\nlow relative happiness (employee happiness normalized by company mean).\\nSurprisingly, a priori expected explanatory features such as mean happiness level and the ratio of likes (positivity), were not significant. Precision@50 = 80% out of a test set with 116 churns, sample size N=2k. Another surprise was that raw happiness is a bad predictor of churn. But, the question is, What did we miss? Can you find more insights?\\nStarter script\\nR starter script https://www.kaggle.com/harriken/how-many-unlikes-it-takes-to-get-fired\\nContent\\nThe data consists of four tables: votes, comments, interactions and churn. A vote was obtained when an employee opened the app and answered the question: How happy are you at work today? To vote the employee indicates their feeling by touching one of four icons that appeared on the screen. After the employee indicates their happiness level, a second screen appears where they can input a text explanation (usually a complaint, suggestion or comment), this is the comments table. Out of 4,356 employees, 2,638 employees commented at least once. Finally, in a third screen the employee can see their peers’ comments and like or dislike them, this data is stored in the interactions table. 3,516 employees liked or disliked at least one of their peers’ comments. The churn table contains when an employee churned (quit or was fired).\\nAcknowledgements\\nPython script version with social graph features: http://bit.ly/2v2sEZg\\nMore detailed R scripts: https://github.com/orioli/e3\\nThe paper which was presented at ASONAM 2017 Sydney\\nSlides https://www.slideshare.net/harriken/ieee-happiness-an-inside-job-asoman-2017\\nInspiration\\nThe cost of employee turnover has been pointed out extensively in the literature. A high turnover rate not only increases human resource costs, which can reach up to 150% of the annual salary per replaced employee, but it also has social costs, as it is correlated with lower wages, lower productivity per employee, and not surprisingly, a less loyal workforce 1. For reference, in 2006, turnover at Walmart’s Sam’s Club was 44% with an average hourly pay of $10.11, while at Costco it was a much lower 17% with a higher $17.0 hourly wage 2. In addition, a more recent study correlated companies with low turnover with a series of socially positive characteristics dubbed high-involvement work practices 3. On the other hand, research on employee turnover (churn) is not a prolific topic in the engineering community. In IEEE publications, one can find just over 278 publications with titles containing the keyword churn, and the bulk of those focus on customer churn, and specifically churn in the telecommunications industry, while on the topic of employee churn there is just one title indexed 4. The goal is to clarify the characteristics of employees that will churn (or that are at risk of churning), to help companies understand the causes so they can reduce the turnover rate.',\n",
       " 'Context\\nAt RSNA 2017 there was a contest to correctly identify the age of a child from an X-ray of their hand. This is the dataset on Kaggle making it easier to experiment with and do educational demos. Additionally maybe there are some new ideas for building smarter models for handling X-ray images.\\nContent\\nA number of folders full of images (digital and scanned) with a CSV containing the age (what is to be predicted) and the gender (useful additional information)\\nAcknowledgements\\nThe dataset was originally published on CloudApp as an RSNA challenge.\\nOriginal Dataset Acknowledgements\\nThe Radiological Society of North America (RSNA) Radiology Informatics Committee (RIC) Pediatric Bone Age Machine Learning Challenge Organizing Committee:\\nKathy Andriole, Massachusetts General Hospital\\nBrad Erickson, Mayo Clinic\\nAdam Flanders, Thomas Jefferson University\\nSafwan Halabi, Stanford University\\nJayashree Kalpathy-Cramer, Massachusetts General Hospital\\nMarc Kohli, University of California - San Francisco\\nLuciano Prevedello, The Ohio State University\\nData sets used in the Pediatric Bone Age Challenge have been contributed by Stanford University, the University of Colorado and the University of California - Los Angeles.\\nThe MedICI platform (built CodaLab) used for the challenge is provided by Jayashree Kalpathy-Cramer, supported through NIH grants (U24CA180927) and a contract from Leidos.\\nInspiration\\nCan you predict with better than 4.2 months accuracy?\\nIs identifying the joints an important step?\\nWhat algorithms work best?\\nWhat do the algorithms focus on?\\nIs gender a necessary piece of information or can it be automatically derived from the image?',\n",
       " 'Context\\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\\nContent\\nThis dataset contains English word vectors pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). All tokens are in lowercase. This dataset contains 50-dimensional, 100-dimensional and 200-dimensional pre trained word vectors. For 300-dimensional word vectors and additional information, please see the project website.\\nAcknowledgements\\nThis data has been released under the Open Data Commons Public Domain Dedication and License. If you use this dataset in your work, please cite the following paper:\\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. URL: https://nlp.stanford.edu/pubs/glove.pdf\\nInspiration\\nGloVe embeddings have been used in more than 2100 papers, and counting! You can use these pre-trained embeddings whenever you need a way to quantify word co-occurrence (which also captures some aspects of word meaning.)',\n",
       " \"Context\\nThe study of human mobility and activities has opened up to an incredible number of studies in the past, most of which included the use of sensors distributed on the body of the subject. More recently, the use of smart devices has been particularly relevant because they are already everywhere and they come with accurate miniaturized sensors. Whether it is smartphones, smartwatches or smartglasses, each device can be used to describe complementary information such as emotions, precise movements, or environmental conditions.\\nContent\\nFirst of all, a smartphone is used to capture mainly contextual data. Two applications are used: a simple data collection application based on the SWIPE open-source sensing system (SWIPE), and a logbook application for obtaining real data on user activity (TimeLogger). SWIPE is a platform for sensing, recording and processing human dynamics using smartwatches and smartphones.\\nThen, a smartwatch is used primarily to capture the user's heart rate. Motion data is also collected, without being at the heart of the dataset due to its need to be configured with a low sampling frequency, which would drastically increase the dataset and drain the battery as well. An application based on SWIPE is used.\\nFinally, JINS MEME smartglasses are used. This model has the advantage of being compact and simple to carry. It does not have a camera or a screen; it simply has three types of sensors: an accelerometer (for detecting steps or activities), a gyroscope (for head movements) and an occulographic sensor (eye blinking, eye orientation). The official DataLogger application from JINS MEME is used.\\nFor more information on the dataset please refer to the corresponding publication, available at An Open Dataset for Human Activity Analysis using Smart Devices.\\nThe current dataset on Kaggle contains smartglasses data with 20ms interval (due to storage limitations), same data with 10ms interval is also available on demand. Contact sasan.jafarnejad [at] uni [dot] lu to receive the 10ms version.\\nAcknowledgements\\nThis work was performed within the eGLASSES project, which is partially funded by NCBiR, FWF, SNSF, ANR and FNR under the ERA-NET CHIST-ERAII framework.\",\n",
       " \"General Info\\nThis is a collection of over 50,000 ranked EUW games from the game League of Legends, as well as json files containing a way to convert between champion and summoner spell IDs and their names. For each game, there are fields for:\\nGame ID\\nCreation Time (in Epoch format)\\nGame Duration (in seconds)\\nSeason ID\\nWinner (1 = team1, 2 = team2)\\nFirst Baron, dragon, tower, blood, inhibitor and Rift Herald (1 = team1, 2 = team2, 0 = none)\\nChampions and summoner spells for each team (Stored as Riot's champion and summoner spell IDs)\\nThe number of tower, inhibitor, Baron, dragon and Rift Herald kills each team has\\nThe 5 bans of each team (Again, champion IDs are used)\\nThis dataset was collected using the Riot Games API, which makes it easy to lookup and collect information on a users ranked history and collect their games. However finding a list of usernames is the hard part, in this case I am using a list of usernames scraped from 3rd party LoL sites.\\nPossible Uses\\nThere is a vast amount of data in just a single LoL game. This dataset takes the most relevant information and makes it available easily for use in things such as attempting to predict the outcome of a LoL game, analysing which in-game events are most likely to lead to victory, understanding how big of an effect bans of a specific champion have, and more.\",\n",
       " \"Summary\\n“Meetup is a social networking website that aims to brings people together to do, explore, teach and learn the things that help them come alive.”\\nMeetup allows members to find and join groups unified by a common interest. As of 2017, there are 32 million users with 280 thousand groups available across 182 countries.\\nA member needs to be able to identify groups and activities which interest them the most to be able to use this platform to network effectively.\\nThe aim of our team was to use this dataset to build a recommender system which will identify and suggest groups and activities to a member based on their interest and additional interests of similar members. Furthermore, a social network analysis was done to identify the relationship between groups and people.\\nDatabase EER diagram\\nData Collection Method\\nData was collected using Meetup API.\\nPython script was used to ping meetup API and collect responses as JSON objects.\\nLogical chunks of data were exported and saved as csv files.\\nData Cleaning\\nData is filtered to include only 3 cities' information (New York, San Francisco, Chicago).\\nCharacter encoding is normalized to ASCII characters across tables.\\nExample Visualizations\",\n",
       " \"Are you curious about fertilizer use in developing economies? The growth of Chinese steel exports? American chocolate consumption? Which parts of the world still use typewriters? You'll find all of that and more here. This dataset covers import and export volumes for 5,000 commodities across most countries on Earth over the last 30 years.\\nAcknowledgements\\nThis dataset was kindly published by the United Nations Statistics Division on the UNData site. You can find the original dataset here.\\nInspiration\\nSome of these numbers are more trustworthy than others. I'd expect that British tea imports are fairly accurate, but doubt that Afghanistan exported exactly 51 sheep in 2016. Can you identify which nations appear to have the most trustworthy data? Which industries?\\nLicense\\nPer the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference.\",\n",
       " 'Job Posts dataset\\nThe dataset consists of 19,000 job postings that were posted through the Armenian human resource portal CareerCenter. The data was extracted from the Yahoo! mailing group https://groups.yahoo.com/neo/groups/careercenter-am. This was the only online human resource portal in the early 2000s. A job posting usually has some structure, although some fields of the posting are not necessarily filled out by the client (poster). The data was cleaned by removing posts that were not job related or had no structure. The data consists of job posts from 2004-2015\\nContent\\njobpost – The original job post\\ndate – Date it was posted in the group\\nTitle – Job title\\nCompany - employer\\nAnnouncementCode – Announcement code (some internal code, is usually missing)\\nTerm – Full-Time, Part-time, etc\\nEligibility -- Eligibility of the candidates\\nAudience --- Who can apply?\\nStartDate – Start date of work\\nDuration - Duration of the employment\\nLocation – Employment location\\nJobDescription – Job Description\\nJobRequirment - Job requirements\\nRequiredQual -Required Qualification\\nSalary - Salary\\nApplicationP – Application Procedure\\nOpeningDate – Opening date of the job announcement\\nDeadline – Deadline for the job announcement\\nNotes - Additional Notes\\nAboutC - About the company\\nAttach - Attachments\\nYear - Year of the announcement (derived from the field date)\\nMonth - Month of the announcement (derived from the field date)\\nIT – TRUE if the job is an IT job. This variable is created by a simple search of IT job titles within column “Title”\\nAcknowledgements\\nThe data collection and initial research was funded by the American University of Armenia’s research grant (2015).\\nInspiration\\nThe online job market is a good indicator of overall demand for labor in the local economy. In addition, online job postings data are easier and quicker to collect, and they can be a richer source of information than more traditional job postings, such as those found in printed newspapers. The data can be used in the following ways: -Understand the demand for certain professions, job titles, or industries -Help universities with curriculum development -Identify skills that are most frequently required by employers, and how the distribution of necessary skills changes over time -Make recommendations to job seekers and employers\\nPast research\\nWe have used association rules mining and simple text mining techniques to analyze the data. Some results can be found here (https://www.slideshare.net/HabetMadoyan/it-skills-analysis-63686238).',\n",
       " \"Introduction\\nVideo games are a rich area for data extraction due to its digital nature. Notable examples such as the complex EVE Online economy, World of Warcraft corrupted blood incident and even Grand Theft Auto self-driving cars tells us that fiction is closer to reality than we really think. Data scientists can gain insight on the logic and decision-making that the players face when put in hypothetical and virtual scenarios.\\nIn this Kaggle Dataset, I provide over 720,000 competitive matches from the popular game PlayerUnknown's Battlegrounds. The data was extracted from pubg.op.gg, a game tracker website. I intend for this data-set to be purely exploratory, however users are free to create their own predictive models they see fit.\\nPlayerUnknown's Battlegrounds\\nPUBG is a first/third-person shooter battle royale style game that matches over 90 players on a large island where teams and players fight to the death until one remains. Players are airdropped from an airplane onto the island where they are to scavenge towns and buildings for weapons, ammo, armor and first-aid. Players will then decide to either fight or hide with the ultimate goal of being the last one standing. A bluezone (see below) will appear a few minutes into the game to corral players closer and closer together by dealing damage to anyone that stands within the bluezone and sparing whoever is within the safe zone.\\nRead more about PUBG here\\nThe Dataset\\nThis dataset provides two zips: aggregate and deaths.\\nIn deaths, the files record every death that occurred within the 720k matches. That is, each row documents an event where a player has died in the match.\\nIn aggregate, each match's meta information and player statistics are summarized (as provided by pubg). It includes various aggregate statistics such as player kills, damage, distance walked, etc as well as metadata on the match itself such as queue size, fpp/tpp, date, etc.\\nThe uncompressed data is divided into 5 chunks of approximately 2gb each. For details on columns, please see the file descriptions.\\nInterpreting Positional Data\\nThe X,Y coordinates are all in in-game coordinates and need to be linearly scaled to be plotted on square erangel and miramar maps. The coordinate min,max are 0,800000 respectively.\\nPotential Bias in the Data\\nThe scraping methodology first starts with an initial seed player, I chose this to be my own account (a rather low rank individual). I then use the seed player to scrape for all players that it has encountered in its historical matches. I then take a random subset of 5000 players from this and then scrape for their historical games for the final dataset. What this could produce is an unrepresentative sample of all games played as it is more likely that I queued and matched with lower rated players and those players more than likely also got matched against lower rated players as well. Thus, the matches and deaths are more representative of lower tier gameplay but given the simplicity of the dataset, this shouldn't be an issue.\\nAcknowledgements\\nPubg.op.gg, if this is against the TOS, please let me know and I will take it down\",\n",
       " 'Context\\nWith this dataset we hope to do a nice cheeky wink to the \"cats and dogs\" image dataset. In fact, this dataset is aimed to be the audio counterpart of the famous \"cats and dogs\" image classification task, here available on Kaggle.\\nContent\\nThe dataset consists in many \"wav\" files for both the cat and dog classes :\\ncat has 164 WAV files to which corresponds 1323 sec of audio\\ndog has 113 WAV files to which corresponds 598 sec of audio\\nYou can have an visual description of the Wav here : Visualizing woofs & meows 🐱. In Accessing the Dataset 2 we propose a train / test split which can be used.\\nAll the WAV files contains 16KHz audio and have variable length.\\nAcknowledgements\\nWe have not much credit in proposing the dataset here. Much of the work have been done by the AE-Dataset creator (From which we extracted the two classes) and by the humans behind FreeSound From which was extracted the AE-Dataset.\\nInspiration\\nYou might use this dataset to test raw audio classification challenge ;)\\nA more challenging dataset is available here',\n",
       " \"These are 5 texts taken from Project Gutenberg, uploaded to Kaggle to encourage things like text-mining and sentiment analysis. These are fun skills to develop and many existing datasets on Kaggle don't lend themselves to these sorts of analyses.\\nThe 5 books are,\\nThe King James Bible\\nThe Quran\\nThe Book Of Mormon\\nThe Gospel of Buddha\\nMeditations, by Marcus Aurelius\\nProject Gutenberg is an online archive of books that are free to download and distribute. These files are taken without alteration (filenames or contents) from the Project Gutenberg website\",\n",
       " \"Context\\nThe diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. Automated methods to detect and classify blood cell subtypes have important medical applications.\\nContent\\nThis dataset contains 12,500 augmented images of blood cells (JPEG) with accompanying cell type labels (CSV). There are approximately 3,000 images for each of 4 different cell types grouped into 4 different folders (according to cell type). The cell types are Eosinophil, Lymphocyte, Monocyte, and Neutrophil. This dataset is accompanied by an additional dataset containing the original 410 images (pre-augmentation) as well as two additional subtype labels (WBC vs WBC) and also bounding boxes for each cell in each of these 410 images (JPEG + XML metadata). More specifically, the folder 'dataset-master' contains 410 images of blood cells with subtype labels and bounding boxes (JPEG + XML), while the folder 'dataset2-master' contains 2,500 augmented images as well as 4 additional subtype labels (JPEG + CSV). There are approximately 3,000 augmented images for each class of the 4 classes as compared to 88, 33, 21, and 207 images of each in folder 'dataset-master'.\\nAcknowledgements\\nhttps://github.com/Shenggan/BCCD_Dataset MIT License\\nInspiration\\nThe diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. Automated methods to detect and classify blood cell subtypes have important medical applications.\",\n",
       " 'Context\\nThis dataset was used for a study where the task was to generate a top-n list of restaurants according to the consumer preferences and finding the significant features. Two approaches were tested: a collaborative filter technique and a contextual approach: (i) The collaborative filter technique used only one file i.e., rating_final.csv that comprises the user, item and rating attributes. (ii) The contextual approach generated the recommendations using the remaining eight data files.\\nContent\\nThere are 9 data files and a README, and are grouped like this:\\nRestaurants\\n1 chefmozaccepts.csv\\n2 chefmozcuisine.csv\\n3 chefmozhours4.csv\\n4 chefmozparking.csv\\n5 geoplaces2.csv\\nConsumers\\n6 usercuisine.csv\\n7 userpayment.csv\\n8 userprofile.csv\\nUser-Item-Rating\\n9 rating_final.csv\\nMore detailed file descriptions can also be found in the README:\\n1 chefmozaccepts.csv\\nInstances: 1314\\nAttributes: 2\\nplaceID: Nominal\\nRpayment: Nominal, 12\\n2 chefmozcuisine.csv\\nInstances: 916\\nAttributes: 2\\nplaceID: Nominal\\nRcuisine: Nominal, 59\\n3 chefmozhours4.csv\\nInstances: 2339\\nAttributes: 3\\nplaceID: Nominal\\nhours: Nominal, Range:00:00-23:30\\ndays: Nominal, 7\\n4 chefmozparking.csv\\nInstances: 702\\nAttributes: 2\\nplaceID: Nominal\\nparking_lot: Nominal, 7\\n5 geoplaces2.csv\\nInstances: 130\\nAttributes: 21\\nplaceID: Nominal\\nlatitude: Numeric\\nlongitude: Numeric\\nthe_geom_meter: Nominal (Geospatial)\\nname: Nominal\\naddress: Nominal,Missing: 27\\ncity: Nominal, Missing: 18\\nstate: Nominal, Missing: 18\\ncountry: Nominal, Missing: 28\\nfax: Numeric, Missing: 130\\nzip: Nominal,Missing: 74\\nalcohol: Nominal, Values: 3\\nsmoking_area: Nominal, 5\\ndress_code: Nominal, 3\\naccessibility: Nominal, 3\\nprice: Nominal, 3\\nurl: Nominal, Missing: 116\\nRambience: Nominal, 2\\nfranchise: Nominal, 2\\narea: Nominal, 2\\nother_services: Nominal, 3\\n6 rating_final.csv\\nInstances: 1161\\nAttributes: 5\\nuserID: Nominal\\nplaceID: Nominal\\nrating: Numeric, 3\\nfood_rating: Numeric, 3\\nservice_rating: Numeric, 3\\n7 usercuisine.csv\\nInstances: 330\\nAttributes: 2\\nuserID: Nominal\\nRcuisine: Nominal, 103\\n8 userpayment.csv\\nInstances: 177\\nAttributes: 2\\nuserID: Nominal\\nUpayment: Nominal, 5\\n9 userprofile\\nInstances: 138\\nAttributes: 19\\nuserID: Nominal\\nlatitude: Numeric\\nlongitude: Numeric\\nthe_geom_meter: Nominal (Geospatial)\\nsmoker: Nominal\\ndrink_level: Nominal, 3\\ndress_preference:Nominal, 4\\nambience: Nominal, 3\\ntransport: Nominal, 3\\nmarital_status: Nominal, 3\\nhijos: Nominal, 3\\nbirth_year: Nominal\\ninterest: Nominal, 5\\npersonality: Nominal, 4\\nreligion: Nominal, 5\\nactivity: Nominal, 4\\ncolor: Nominal, 8\\nweight: Numeric\\nbudget: Nominal, 3\\nheight: Numeric\\nAcknowledgements\\nThis dataset was originally downloaded from the UCI ML Repository: UCI ML\\nCreators: Rafael Ponce Medellín and Juan Gabriel González Serna rafaponce@cenidet.edu.mx, gabriel@cenidet.edu.mx Department of Computer Science. National Center for Research and Technological Development CENIDET, México\\nDonors of database: Blanca Vargas-Govea and Juan Gabriel González Serna blanca.vargas@cenidet.edu.mx/blanca.vg@gmail.com, gabriel@cenidet.edu.mx Department of Computer Science. National Center for Research and Technological Development CENIDET, México\\nInspiration\\nUse this data to create a restaurant recommender or determine which restaurants a person is most likely to visit.',\n",
       " 'Content\\nThe United States Census Bureau’s International Dataset provides estimates of country populations since 1950 and projections through 2050. Specifically, the data set includes midyear population figures broken down by age and gender assignment at birth. Additionally, they provide time-series data for attributes including fertility rates, birth rates, death rates, and migration rates.\\nThe full documentation is available here. For basic field details, please see the data dictionary.\\nNote: The U.S. Census Bureau provides estimates and projections for countries and areas that are recognized by the U.S. Department of State that have a population of at least 5,000.\\nAcknowledgements\\nThis dataset was created by the United States Census Bureau.\\nInspiration\\nWhich countries have made the largest improvements in life expectancy? Based on current trends, how long will it take each country to catch up to today’s best performers?\\nUse this dataset with BigQuery\\nYou can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/international-census.',\n",
       " \"Context\\nThe National Hurricane Center (NHC) conducts a post-storm analysis of each tropical cyclone in the Atlantic basin (i.e., North Atlantic Ocean, Gulf of Mexico, and Caribbean Sea) and and the North Pacific Ocean to determine the official assessment of the cyclone's history. This analysis makes use of all available observations, including those that may not have been available in real time. In addition, NHC conducts ongoing reviews of any retrospective tropical cyclone analyses brought to its attention and on a regular basis updates the historical record to reflect changes introduced.\\nContent\\nThe NHC publishes the tropical cyclone historical database in a format known as HURDAT, short for HURricane DATabase. These databases (Atlantic HURDAT2 and NE/NC Pacific HURDAT2) contain six-hourly information on the location, maximum winds, central pressure, and (starting in 2004) size of all known tropical cyclones and subtropical cyclones.\",\n",
       " 'Context\\nThis data was originally posted on my personal oneDrive account.\\nIt represent fictitious/fake data on terminations. For each of 10 years it show employees that are active and those that terminated.\\nThe intent is to see if individual terminations can be predicted from the data provided.\\nThe thing to be predicted is status of active or terminated\\nContent\\nThe data contains\\nemployee id employee record date ( year of data) birth date hire date termination date age length of service city department job title store number gender termination reason termination type status year status business unit\\nThese might be typical types of data in hris\\nAcknowledgements\\nNone- its fake data\\nInspiration\\nA lot of turnover analyses occur at an aggregate level-such as turnover rates. But few analyses concentrate on trying to identify exactly which individuals might leave based on patterns that might be present in existing data.\\nMachine learning algorithms often showcase customer churn examples for telcos or product marketing. Those algorithms equally apply to employee churn.',\n",
       " \"Here's a data set of 1,000 most popular movies on IMDB in the last 10 years. The data points included are:\\nTitle, Genre, Description, Director, Actors, Year, Runtime, Rating, Votes, Revenue, Metascrore\\nFeel free to tinker with it and derive interesting insights.\",\n",
       " \"What you get:\\nUpvote! The database contains +40,000 records on US Gross Rent & Geo Locations. The field description of the database is documented in the attached pdf file. To access, all 325,272 records on a scale roughly equivalent to a neighborhood (census tract) see link below and make sure to upvote. Upvote right now, please. Enjoy!\\nGet the full free database with coupon code: FreeDatabase, See directions at the bottom of the description... And make sure to upvote :) coupon ends at 2:00 pm 8-23-2017\\nGross Rent & Geographic Statistics:\\nMean Gross Rent (double)\\nMedian Gross Rent (double)\\nStandard Deviation of Gross Rent (double)\\nNumber of Samples (double)\\nSquare area of land at location (double)\\nSquare area of water at location (double)\\nGeographic Location:\\nLongitude (double)\\nLatitude (double)\\nState Name (character)\\nState abbreviated (character)\\nState_Code (character)\\nCounty Name (character)\\nCity Name (character)\\nName of city, town, village or CPD (character)\\nPrimary, Defines if the location is a track and block group.\\nZip Code (character)\\nArea Code (character)\\nAbstract\\nThe data set originally developed for real estate and business investment research. Income is a vital element when determining both quality and socioeconomic features of a given geographic location. The following data was derived from over +36,000 files and covers 348,893 location records.\\nLicense\\nOnly proper citing is required please see the documentation for details. Have Fun!!!\\nGolden Oak Research Group, LLC. “U.S. Income Database Kaggle”. Publication: 5, August 2017. Accessed, day, month year.\\nFor any questions, you may reach us at research_development@goldenoakresearch.com. For immediate assistance, you may reach me on at 585-626-2965\\nplease note: it is my personal number and email is preferred\\nCheck our data's accuracy: Census Fact Checker\\nAccess all 325,272 location for Free Database Coupon Code:\\nDon't settle. Go big and win big. Optimize your potential**. Access all gross rent records and more on a scale roughly equivalent to a neighborhood, see link below:\\nWebsite: Golden Oak Research make sure to upvote\\nA small startup with big dreams, giving the every day, up and coming data scientist professional grade data at affordable prices It's what we do.\",\n",
       " 'Context\\nThe image at the top of the page is a frame from today\\'s (7/26/2016) Isis #TweetMovie from twitter, a \"normal\" day when two Isis operatives murdered a priest saying mass in a French church. (You can see this in the center left). A selection of data from this site is being made available here to Kaggle users.\\nUPDATE: An excellent study by Audrey Alexander titled Digital Decay? is now available which traces the \"change over time among English-language Islamic State sympathizers on Twitter.\\nIntent\\nThis data set is intended to be a counterpoise to the How Isis Uses Twitter data set. That data set contains 17k tweets alleged to originate with \"100+ pro-ISIS fanboys\". This new set contains 122k tweets collected on two separate days, 7/4/2016 and 7/11/2016, which contained any of the following terms, with no further editing or selection:\\nisis\\nisil\\ndaesh\\nislamicstate\\nraqqa\\nMosul\\n\"islamic state\"\\nThis is not a perfect counterpoise as it almost surely contains a small number of pro-Isis fanboy tweets. However, unless some entity, such as Kaggle, is willing to expend significant resources on a service something like an expert level Mechanical Turk or Zooniverse, a high quality counterpoise is out of reach.\\nA counterpoise provides a balance or backdrop against which to measure a primary object, in this case the original pro-Isis data. So if anyone wants to discriminate between pro-Isis tweets and other tweets concerning Isis you will need to model the original pro-Isis data or signal against the counterpoise which is signal + noise. Further background and some analysis can be found in this forum thread.\\nThis data comes from postmodernnews.com/token-tv.aspx which daily collects about 25MB of Isis tweets for the purposes of graphical display. PLEASE NOTE: This server is not currently active.\\nData Details\\nThere are several differences between the format of this data set and the pro-ISIS fanboy dataset. 1. All the twitter t.co tags have been expanded where possible 2. There are no \"description, location, followers, numberstatuses\" data columns.\\nI have also included my version of the original pro-ISIS fanboy set. This version has all the t.co links expanded where possible.',\n",
       " 'Contains DataScience programs offered by universities along with program details, world ranking and a lot lot more. Happy exploring !!!',\n",
       " \"Inspiration\\nI'm a big fan of TechCrunch for a while now. Kind of because I get to know about new startups that's coming up or maybe just because I find Tito Hamze videos fun. But TechCrunch got plenty of good content. And where we find good content we produce great exploratory analysis.\\nThis dataset is a great opportunity for you to boost your skills as an EDA expert! It provides several features that make you able to create different analyses such as time series, clustering, predictive, segmenting, classification and tons of others. Let's not forget about word2vec for that. It would be awesome to see that in action here!\\nI've made the scraper available on github, if you want to check it out, here is the link: techcrunch scraper repo\\nContent\\nThis dataset comes with a rich set of features. You will have:\\nauthors: authors of the post - can be one or multiple authors\\ncategory: post category\\ncontent: post content - each paragraph can be extracted by splitting on the \\\\n\\ndate: post date\\nid: post id - the same id used on techcrunch website\\nimg_src: post main image url\\nsection: post section - each section is one of the options on the main page dropdown menu\\ntags: post tags - can be zero or multiple tags\\ntitle: post title\\ntopics: post topics\\nurl: post url\\nAcknowledgements\\nAll posts were scraped from the TechCrunch website on mid oct-16. Each line contains information about one post and each post appear in no more than one line.\",\n",
       " 'Context\\nGenerating humor is a complex task in the domain of machine learning, and it requires the models to understand the deep semantic meaning of a joke in order to generate new ones. Such problems, however, are difficult to solve due to a number of reasons, one of which is the lack of a database that gives an elaborate list of jokes. Thus, a large corpus of over 0.2 million jokes has been collected by scraping several websites containing funny and short jokes.\\nVisit my Github repository for more information regarding collection of data and the scripts used.\\nContent\\nThis dataset is in the form of a csv file containing 231,657 jokes. Length of jokes ranges from 10 to 200 characters. Each line in the file contains a unique ID and joke.\\nDisclaimer\\nIt has been attempted to keep the jokes as clean as possible. Since the data has been collected by scraping websites, it is possible that there may be a few jokes that are inappropriate or offensive to some people.',\n",
       " 'Context\\nA dataset of 2017 songs with attributes from Spotify\\'s API. Each song is labeled \"1\" meaning I like it and \"0\" for songs I don\\'t like. I used this to data to see if I could build a classifier that could predict whether or not I would like a song.\\nI wrote an article about the project I used this data for. It includes code on how to grab this data from the Spotipy API wrapper and the methods behind my modeling. https://opendatascience.com/blog/a-machine-learning-deep-dive-into-my-spotify-data/\\nContent\\nEach row represents a song.\\nThere are 16 columns. 13 of which are song attributes, one column for song name, one for artist, and a column called \"target\" which is the label for the song.\\nHere are the 13 track attributes: acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, time_signature, valence.\\nInformation on what those traits mean can be found here: https://developer.spotify.com/web-api/get-audio-features/\\nAcknowledgements\\nI would like to thank Spotify for providing this readily accessible data.\\nInspiration\\nI\\'m a music lover who\\'s curious about why I love the music that I love.',\n",
       " \"Context\\nGrab your pans...\\nPlayer statistics for approximately 85,000 of the top PUBG players (as tracked by https://pubgtracker.com/). All statistics were gathered using aggregate region filters (all regions) and feature labels are subdivided by server type: solo, duo, and squad.\\n87,898 players with 150 numerical game-play features per player (+2 for player name and PUBG Tracker ID).\\nContent\\nFeatures include KD ratios, wins, losses, damage, wins, top 10's, and movement characteristics (walking/riding distance etc...)\\nAcknowledgements\\nSpecial thanks to pubgtracker.com for their support and aid with gathering this data. More information can be found here: https://pubgtracker.com/\\nPLAYERUNKNOWN'S BATTLEGROUNDS is a registered trademark, trademark or service mark of Bluehole, Inc. and its affiliates https://www.playbattlegrounds.com/main.pu\\nInspiration\\nAs a gamer addicted to PUBG, it was a blast putting this data set together. Some great project ideas include:\\na. Visualizations of player skill vs. specific strategies\\nb. Unsupervised clustering of players based on strategy (for matchmaking or team building)\\nc. Prediction of features based on player skill and/or strategies\",\n",
       " 'Context\\nDataset is based on box score and standing statistics from the NBA.\\nCalculations such as number of possessions, floor impact counter, strength of schedule, and simple rating system are performed.\\nFinally, extracts are created based on a perspective:\\nteamBoxScore.csv communicates game data from each teams perspective\\nofficialBoxScore.csv communicates game data from each officials perspective\\nplayerBoxScore.csv communicates game data from each players perspective\\nstanding.csv communicates standings data for each team every day during the season\\nContent\\nData Sources\\nBox score and standing statistics were obtained by a Java application using RESTful APIs provided by xmlstats.\\nCalculation Sources\\nAnother Java application performs advanced calculations on the box score and standing data.\\nFormulas for these calculations were primarily obtained from these sources:\\nhttps://basketball.realgm.com/info/glossary\\nhttps://www.nbastuffer.com/team-evaluation-metrics/\\nhttps://www.basketball-reference.com/about/glossary.html\\nInspiration\\nFavoritism\\nDoes a referee impact the number of fouls made against a player or the pace of a game?\\nForcasting\\nCan the aggregated points scored by and against a team along with their strength of schedule be used to determine their projected winning percentage for the season?\\nPredicting the Past\\nFor a given game, can games played earlier in the season help determine how a team will perform?\\nLots of data elements and possibilities. Let your imagination roam!',\n",
       " 'Context\\nI love movies. \\nI tend to avoid marvel-transformers-standardized products, and prefer a mix of classic hollywood-golden-age and obscure polish artsy movies. Throw in an occasional japanese-zombie-slasher-giallo as an alibi. Good movies don\\'t exist without bad movies. \\nOn average I watch 200+ movies each year, with peaks at more than 500 movies. Nine years ago I started to log my movies to avoid watching the same movie twice, and also assign scores. Over the years, it gave me a couple insights on my viewing habits but nothing more than what a tenth-grader would learn at school. \\nI\\'ve recently suscribed to Netflix and it pains me to see the global inefficiency of recommendation systems for people like me, who mostly swear by \"La politique des auteurs\". It\\'s a term coined by famous new-wave french movie critic André Bazin, meaning that the quality of a movie is essentially linked to the director and it\\'s capacity to execute his vision with his crew. We could debate it depends on movie production pipeline, but let\\'s not for now. Practically, what it means, is that I essentially watch movies from directors who made films I\\'ve liked.\\nI suspect Neflix calibrate their recommandation models taking into account the way the \"average-joe\" chooses a movie. A few months ago I had read a study based on a survey, showing that people chose a movie mostly based on genre (55%), then by leading actors (45%). Director or Release Date were far behind around 10% each. It is not surprising, since most people I know don\\'t care who the director is. Lots of US blockbusters don\\'t even mention it on the movie poster. I am aware that collaborative filtering is based on user proximity , which I believe decreases (or even eliminates) the need to characterize a movie. So here I\\'m more interested in content based filtering which is based on product proximity for several reasons :\\nUsers tastes are not easily accessible. It is, after all, Netflix treasure chest\\nMovie offer on Netflix is so bad for someone who likes author\\'s movies that it wouldn\\'t help\\nModeling a movie intrinsic qualities is a nice challenge\\nEnough.\\n\"The secret of getting ahead is getting started\" (Mark Twain)\\nContent\\nThe primary source is www.themoviedb.org. If you watch obscure artsy romanian homemade movies you may find only 95% of your movies referenced...but for anyone else it should be in the 98%+ range.\\nmovies details are from www.themoviedb.org API : movies/details\\nmovies crew & casting are from www.themoviedb.org API : movies/credits\\nboth can be joined by id\\nthey contain all 350k movies up, from end of 19th century to august 2017. If you remove short movies from imdb you get similar amounts of movies.\\nI uploaded the program to retrieve incremental movie details on github : https://github.com/stephanerappeneau/scienceofmovies/tree/master/PycharmProjects/GetAllMovies (need a dev API key from themoviedb.org though)\\nI have tried various supervised (decision tree) / unsupervised (clustering, NLP) approaches described in the discussions, source code is on github : https://github.com/stephanerappeneau/scienceofmovies\\nAs a bonus I\\'ve uploaded the bio summary from top 500 critically-acclaimed directors from wikipedia, for some interesting NLTK analysis\\nHere is overview of the available sources that I\\'ve tried :\\n• Imdb.com free csv dumps (ftp://ftp.funet.fi/pub/mirrors/ftp.imdb.com/pub/temporaryaccess/) are badly documented, incomplete, loosely structured and impossible to join/merge. There\\'s an API hosted by Amazon Web Service : 1€ every 100 000 requests. With around 1 million movies, it could become expensive also features are bare. So I\\'ve searched for other sources. \\n• www.themoviedb.org is based on crowdsourcing and has an excellent API, limited to 40 requests every 10 seconds. It is quite generous, well documented, and enough to sweep the 450 000 movies in a few days. For my purpose, data quality is not significantly worse than imdb, and as imdb key is also included there\\'s always the possibility to complete my dataset later (I actually did it)\\n• www.Boxofficemojo.com has some interesting budget/revenue figures (which are sorely lacking in both imdb & tmdb), but it actually tracks only a few thousand movies, mainly blockbusters. There are other professional sources that are used by film industry to get better predictive / marketing insights but that\\'s beyond my reach for this experiment.   • www.wikipedia.com is an interesting source with no real cap on API calls, however it requires a bit of webscraping and for movies or directors the layout and quality varies a lot, so I suspected it\\'d get a lot of work to get insights so I put this source in lower priority.\\n• www.google.com will ban you after a few minutes of web scraping because their job is to scrap data from others, than sell it, duh.   • It\\'s worth mentionning that there are a few dumps of Netflix anonymized user tastes on kaggle, because they\\'ve organised a few competitions to improve their recommendation models. https://www.kaggle.com/netflix-inc/netflix-prize-data\\n• Online databases are largely white anglo-saxon centric, meaning bollywood (India is the 2nd bigger producer of movies) offer is mostly absent from datasets. I\\'m fine with that, as it\\'s not my cup of tea plus I lack domain knowledge. The sheer amount of indian movies would probably skew my results anyway (I don\\'t want to have too many martial-arts-musicals in my recommendations ;-)). I have, however, tremendous respect for indian movie industry so I\\'d love to collaborate with an indian cinephile !\\nInspiration\\nStarting from there, I had multiple problem statements for both supervised / unsupervised machine learning\\nCan I program a tailored-recommendation system based on my own criteria ?\\nWhat are the characteristics of movies/directors I like the most ?\\nWhat is the probability that I will like my next movie ?\\nCan I find the data ?\\nOne of the objectives of sharing my work here is to find cinephile data-scientists who might be interested and, hopefully, contribute or share insights :) Other interesting leads : use tagline for NLP/Clustering/Genre guessing, leverage on budget/revenue, link with other data sources using the imdb normalized title, etc.\\nMotivation, Disclaimer and Acknowledgements\\nI\\'ve graduated from an french engineering school, majoring in artificial intelligence, but that was 17 years ago right in the middle of A.I-winter. Like a lot of white male rocket scientists, I\\'ve ended up in one of the leading european investment bank, quickly abandonning IT development to specialize in trading/risk project management and internal politics. My recent appointment in the Data Office made me aware of recent breakthroughts in datascience, and I thought that developing a side project would be an excellent occasion to learn something new. Plus it\\'d give me a well-needed credibility which too often lack decision makers when it comes to datascience.\\nI\\'ve worked on some of the features with Cédric Paternotte, a fellow friend of mine who is a professor of philosophy of sciences in La Sorbonne. Working with someone with a different background seem a good idea for motivation, creativity and rigor.\\nKudos to www.themoviedb.org or www.wikipedia.com sites, who really have a great attitude towards open data. This is typically NOT the case of modern-bigdata companies who mostly keep data to themselves to try to monetize it. Such a huge contrast with imdb or instagram API, which generously let you grab your last 3 comments at a miserable rate. Even if 15 years ago this seemed a mandatory path to get services for free, I predict one day governments will need to break this data monopoly.\\n[Disclaimer : I apologize in advance for my engrish (I\\'m french ^-^), any bad-code I\\'ve written (there are probably hundreds of way to do it better and faster), any pseudo-scientific assumption I\\'ve made, I\\'m slowly getting back in statistics and lack senior guidance, one day I regress a non-stationary time series and the day after I\\'ll discover I shouldn\\'t have, and any incorrect use of machine-learning models]',\n",
       " 'This dataset reflects incidents of crime in the City of Los Angeles dating back to 2010. This data is transcribed from original crime reports that are typed on paper and therefore there may be some inaccuracies within the data. Some location fields with missing data are noted as (0°, 0°). Address fields are only provided to the nearest hundred block in order to maintain privacy.\\nReporting District Shapefile Attributes\\nREPDIST, Number, min: 101 max: 2,199 avg: 1,162 count: 1,135\\nPREC, Number, min: 1 max: 21 avg: 11 count: 1,135\\nAPREC, Text, PACIFIC (74), DEVONSHIRE (70), WEST LOS ANGELES (69), NORTHEAST (64), HOLLENBECK (63), MISSION (62)... (15 more)\\nBUREAU, Text, VALLEY BUREAU (399), WEST BUREAU (288), CENTRAL BUREAU (267), SOUTH BUREAU (181)\\nBASICCAR, Text, 8A29 (17), 17A35 (17), 1A1 (15), 17A49 (14), 16A35 (14), 14A73 (14), 19A43 (13), 8A95 (12), 19A7 (12)... (160 more)\\nTOOLTIP, Text, Bureau: SOUTH BUREAU\\\\nDistrict: 562\\\\nDivision: HARBOR (1)... (1134 more)\\nOBJECTID\\nUnique ID\\nAcknowledgements\\nThis dataset was kindly released by the City of Los Angeles. You can find the original dataset, updated weekly, here.\\nInspiration\\nSome of the MO codes seem unlikely or unrelated to crime. Can you find out what would lead to the use of code 0107 God or 1021 Repair?',\n",
       " 'Data scraped from www.ycombinator.com/companies on September 8, 2016.',\n",
       " \"The strategic board game The Settlers of Catan is a modern classic. Introduced in 1995, it has sold over 22 million copies worldwide. Learning how to play the game well requires an inherent understanding of probability, economics, game theory, and social interactions.\\nThis is my personal dataset of 50 4-player games I played on playcatan.com in 2014. Using the ingame statistics page and a spreadsheet, I logged starting position choices, the distribution of dice rolls, and how each player spent the resources they acquired by the end of the game. Note, of course, because this dataset only consists of my games, any analysis done is most relevant for games involving me...\\nMy personal analysis of this dataset consisted of a best subsets regression, and resulted a 4-variable model that likely overfitted, but managed to ascertain the winner correctly, in 40 of 50 games.\\nQuestions to possibly consider:\\nHow much luck is involved in winning a Catan game?\\nDoes starting position matter? If so, what starting settlements lead to success from each position?\\nHow much information on the eventual winner can be gained from starting position/settlements alone?\\nBy looking at postgame stats, what leads to a win? Can these statistics be a guide for ingame strategy?\\nData details/guide:\\ngameNum - each game I played has 4 corresponding rows, 1 per player.\\nplayer - the starting position corresponding to each row\\npoints - how many points the player ended the game with (the game is won with 10 or more)\\nme - the position I played during the game\\n2, 3, ..., 12 - how many rolls of each value occurred during the game (game is played with 2 dice)\\nsettlement1, settlement2 - each starting settlement is logged as 3 pairs of [number, resource]:\\nL = lumber\\nC = clay\\nS = sheep\\nW = wheat\\nO = ore\\n3G = 3:1 general port\\n2(X) = 2:1 port for resource X\\nD = desert\\nEX: in game 1, player 1's first settlement was on a 6-lumber, 3-clay, and 11-clay.\\nproduction - total cards gained from settlements and cities during game\\ntradeGain - total cards gained from peer AND bank trades during game\\nrobberCardsGain - total cards gained from stealing with the robber, plus cards gained with non-knight development cards. A road building card is +4 resources.\\ntotalGain - sum of previous 3 columns.\\ntradeLoss - total cards lost from peer AND bank trades during game\\nrobberCardsLoss - total cards lost from robbers, knights, and other players' monopoly cards\\ntribute - total cards lost when player had to discard on a 7 roll (separate from previous column.)\\ntotalLoss - sum of previous 3 columns.\\ntotalAvailable - totalGain minus totalLoss.\\nI only ask that if you produce a good model, you share it with me! Please don't hesitate to ask any clarifying questions.\",\n",
       " 'Twitter Friends and hashtags\\nContext\\nThis datasets is an extract of a wider database aimed at collecting Twitter user\\'s friends (other accound one follows). The global goal is to study user\\'s interest thru who they follow and connection to the hashtag they\\'ve used.\\nContent\\nIt\\'s a list of Twitter user\\'s informations. In the JSON format one twitter user is stored in one object of this more that 40.000 objects list. Each object holds :\\navatar : URL to the profile picture\\nfollowerCount : the number of followers of this user\\nfriendsCount : the number of people following this user.\\nfriendName : stores the @name (without the \\'@\\') of the user (beware this name can be changed by the user)\\nid : user ID, this number can not change (you can retrieve screen name with this service : https://tweeterid.com/)\\nfriends : the list of IDs the user follows (data stored is IDs of users followed by this user)\\nlang : the language declared by the user (in this dataset there is only \"en\" (english))\\nlastSeen : the time stamp of the date when this user have post his last tweet.\\ntags : the hashtags (whith or without #) used by the user. It\\'s the \"trending topic\" the user tweeted about.\\ntweetID : Id of the last tweet posted by this user.\\nYou also have the CSV format which uses the same naming convention.\\nThese users are selected because they tweeted on Twitter trending topics, I\\'ve selected users that have at least 100 followers and following at least 100 other account (in order to filter out spam and non-informative/empty accounts).\\nAcknowledgements\\nThis data set is build by Hubert Wassner (me) using the Twitter public API. More data can be obtained on request (hubert.wassner AT gmail.com), at this time I\\'ve collected over 5 milions in different languages. Some more information can be found here (in french only) : http://wassner.blogspot.fr/2016/06/recuperer-des-profils-twitter-par.html\\nPast Research\\nNo public research have been done (until now) on this dataset. I made a private application which is described here : http://wassner.blogspot.fr/2016/09/twitter-profiling.html (in French) which uses the full dataset (Millions of full profiles).\\nInspiration\\nOn can analyse a lot of stuff with this datasets :\\nstats about followers & followings\\nmanyfold learning or unsupervised learning from friend list\\nhashtag prediction from friend list\\nContact\\nFeel free to ask any question (or help request) via Twitter : @hwassner\\nEnjoy! ;)',\n",
       " 'Overview\\nWe could take a music theory class to understand what a note is, but why don\\'t we just find out for ourselves? In this data set we have the notes on a guitar on open strings, and on the 1st-8th frets on every string. The notes were recorded on a nice but low-end guitar called the Yamaha C-40. The guitar is in standard tuning (from the top string to the bottom on we have E low, A, D, G, B, E high).\\nWhat to do with this dataset\\nI didn\\'t label any notes (except the open ones - an open A string is an A). If you want a challenge you can cluster the notes and see if your clustering lumps all the same notes together. If you want labels so you can do some inspection of notes that are the same you can look on google for a guitar fretboard diagram. I have done both of these experiments and learned a bit about music which I \\'m hoping to verify soon when I find a good music theory book.\\nThis data set might be interesting to use to be able to write sheet music from an audio sample of some finger-picked music. Identifying chords is a difficult computational task, but finger-style guitar, with clear, individual notes, might be easier. If this is the case, a simple script could be written to write sheet music from an audio sample.\\nOne draw back about the data set is that some non-plucked strings were vibrating when I played a note. I tried various techniques to muffle them, but there is still some noise in the background. I don\\'t know if this is because of my technique or something that happens to all players on all guitars. In any event, this noise didn\\'t hurt my analysis.\\nAbout the data\\nThey were recorded by me, Melvyn, using a program called audacity.\\nThere is a directory with the name of the string. Inside the directory you will find .wav files named either open, 1, 2, ....8 for the fingering of the string. There is also a directory called \"scale\" I recorded some notes that make a \"do-re-mi...\" scale. You can use these for a number of things.\\nI use the GuitarTuner app to tune the Guitar - I\\'m just learning so I don\\'t have an ear for notes yet. After some initial analysis it looks like the guitar might be a bit out of tune, so the resonant frequencies are a bit off from what they should be. Another thing that is interesting to think about it is how far a frequency must be from the proper on until it becomes distinguishable as a different note.',\n",
       " 'Chronist is a project to quantitatively monitor the emotional and physical changes of an individual over periods of time. My thesis is that if you can accurately show emotional or physical change over time, you can objectively pinpoint how an environmental change such as a career change, moving to a new city, starting or ending a relationship, or starting a new habit like going to the gym affected your physical and emotional health. This can lead to important insights on an individual level and for a population as a whole.\\nIf you are interested in hearing more about this project, contributing your data, or collaborating, contact me at chris@cjroth.com.\\nSee the GitHub repository to read more about the tools that were used to generate the dataset.',\n",
       " 'Context\\nSpace Apps Moscow was held on April 29th & 30th. Thank you to the 175 people who joined the International Space Apps Challenge at this location!\\nContent\\nThe dataset contains such columns as: \"wind direction\", \"wind speed\", \"humidity\" and temperature. The response parameter that is to be predicted is: \"Solar_radiation\". It contains measurements for the past 4 months and you have to predict the level of solar radiation. Just imagine that you\\'ve got solar energy batteries and you want to know will it be reasonable to use them in future?\\nAcknowledgements\\nThanks NASA for the dataset.\\nInspiration\\nPredict the level of solar radiation. Here are some intersecting dependences that i have figured out: 1. Humidity & Solar_radiation. 2.Temeperature & Solar_radiation.\\nThe best result of accuracy I could get using cross-validation was only 55%.',\n",
       " 'Context\\nData from the Coursera Course: Neurohacking In R taught by Dr. Elizabeth Sweeney , Rice Academy Postdoctoral Fellow, Ciprian M. Crainiceanu, Professor and John Muschelli III , Assistant Scientist\\nPlease see https://www.coursera.org/learn/neurohacking for the lecture notes and example code from the instructors.\\nContent\\nStructural MRI images for visualization and image processing\\nFrom the instructors:\\nAbout this course: Neurohacking describes how to use the R programming language (https://cran.r-project.org/) and its associated packages to perform manipulation, processing, and analysis of neuroimaging data. We focus on publicly-available structural magnetic resonance imaging (MRI). We discuss concepts such as inhomogeneity correction, image registration, and image visualization. By the end of this course, you will be able to: Read/write images of the brain in the NIfTI (Neuroimaging Informatics Technology Initiative) format Visualize and explore these images, perform inhomogeneity correction, brain extraction, and image registration (within a subject and to a template).\\nAcknowledgements\\nDataset is public domain and was originally posted for the Coursera online course NeuroHacking in R.\\nNotes\\nA. When you download the zip archive, double clicking might try to compress the file instead of extracting it. Unzipping on terminal (mac) correctly decompresses the archive.\\nB. The zip file contains a directory structure:\\n       BRAINIX\\n            -----DICOM\\n                ----FLAIR\\n                ----ROI\\n                ----T1\\n                ----T2\\n            -----NIFTI\\n      kirby21\\n            -----visit 1\\n                ----113\\n            -----visit 2\\n                ----113\\n      Template\\nHowever, when it is unzipped here on Kaggle environment, somehow the directory structure is not maintained, therefore files with the same names are being overwritten. As a workaround, I added the directory names to the files ie. BRAINIX_DICOM_T1_IM_0001_0011.dcm instead of just IM_0001_0011.dcm.\\nCheck out script https://www.kaggle.com/ilknuricke/d/ilknuricke/neurohackinginrimages/structural-mri-visualization/code for example use.',\n",
       " 'Context\\nHow good of an arbitrageur would you be?\\nFind it out in the World Tennis Database which gathers more than 139K matches with odds from 15 different bookies (49 MB).\\nIf you are looking to predict the outcome of a tennis match, to find arbitrage opportunities, inspecting variations in a particular player odds or simply searching to improve your Machine Learning or visualisation skills, then this dataset might be looking for you too.\\nContent\\nData is packed in CSV format, ready to spit out some interesting statistics. It is composed of the following 72 columns:\\nUrl, string\\nCountry, string\\nDate, (yyyy-mm-dd hh:mm), to ease date-time transformations.\\nDay, string\\nTournament name, string\\nDoubles, either 0 or 1, when it is not a single player match\\nPlayer 1(2) name, string\\nPlayer 1(2) score, int, number of sets won\\nPlayer 1(2) set 0 score, int, up until set 4 - indexing of sets starts at 0, ask why to python ;)\\nNo set info, either 0 or 1, when there is no informations about the final set scores\\nMissing bookies, either 0 or 1, when there is no informations about any bookies odds\\nRetired player, either 0 or 1, when one player retired\\nCancelled game, either 0 or 1, when the game got cancelled\\nComments, string used to insert any comments during the scraping process\\nWalkover, either 0 or 1, when one player chose to walkover\\nAwarded player, either 0 or 1, when one player got awarded\\nFifteen bookies were then taken into account, each having three type of infos: Player 1 odd, Player 2 odd, Payout. This result in adding to the preceding 27 columns, 45 others.\\nBookies were sorted alphabetically:\\n10Bet\\n18Bet\\n5Dimes\\nBet At Home\\nBet365\\nBetHard\\nBetOlimp\\nBetRally\\nBWin\\nJetBull\\nMarathonBet\\nPinnacle\\nTempoBet\\nTonyBet\\nUnibet\\nAcknowledgements\\nHuge kudos to the OddsPortal Website for their wonderful archiving job.\\nCover photo by Jeremy Galliani on Unsplash.\\nInspiration\\nVarious interesting infos and predictions can be made out of this dataset.\\nIndividual players trajectories and their respective odds movements.\\nBookies respective strategies. Who sets the pace?\\nDetecting patterns in arbitrage situations (arbitrageur perspective).\\nAnd of course, predicting the winner of a game, as draws are not allowed.\\nOf course, I got inspired by the European Soccer Database.\\nFinally, for details about the scraping process, visit https://dmpierre.github.io/.',\n",
       " 'Context\\nThe original dataset contains 1000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes. The link to the original dataset can be found below.\\nContent\\nIt is almost impossible to understand the original dataset due to its complicated system of categories and symbols. Thus, I wrote a small Python script to convert it into a readable CSV file. Several columns are simply ignored, because in my opinion either they are not important or their descriptions are obscure. The selected attributes are:\\nAge (numeric)\\nSex (text: male, female)\\nJob (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\\nHousing (text: own, rent, or free)\\nSaving accounts (text - little, moderate, quite rich, rich)\\nChecking account (numeric, in DM - Deutsch Mark)\\nCredit amount (numeric, in DM)\\nDuration (numeric, in month)\\nPurpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)\\nAcknowledgements\\nSource: UCI',\n",
       " 'Context\\nThere are all sorts of reasons why you\\'d want to know a hospital\\'s quality rating.\\nYour mom is having her second hip replacement. Her first one went terribly and you\\'re nervous about how she\\'ll do. Which hospital would you suggest she have her surgery?\\nYou\\'re selecting a health plan on your state\\'s Exchange, but your top two choices partner with different hospitals. How will you decide which plan to pick?\\nYour brother has Cystic Fibrosis and has to go to the ER frequently. He hates waiting. Which hospitals/states provide care in the timeliest manner?\\nYour in-laws moved to Florida recently to retire, and have been trying to convince you to move there too. You\\'re looking for any way possible to show them that your state is better. Does your state have better hospitals?\\nEvery hospital in the United States of America that accepts publicly insured patients (Medicaid or MediCare) is required to submit quality data, quarterly, to the Centers for Medicare & Medicaid Services (CMS). There are very few hospitals that do not accept publicly insured patients, so this is quite a comprehensive list.\\nContent\\nThis file contains general information about all hospitals that have been registered with Medicare, including their addresses, type of hospital, and ownership structure. It also contains information about the quality of each hospital, in the form of an overall rating (1-5, where 5 is the best possible rating & 1 is the worst), and whether the hospital scored above, same as, or below the national average for a variety of measures.\\nThis data was updated by CMS on July 25, 2017. CMS\\' overall rating includes 60 of the 100 measures for which data is collected & reported on Hospital Compare website (https://www.medicare.gov/hospitalcompare/search.html). Each of the measures have different collection/reporting dates, so it is impossible to specify exactly which time period this dataset covers. For more information about the timeframes for each measure, see: https://www.medicare.gov/hospitalcompare/Data/Data-Updated.html# For more information about the data itself, APIs and a variety of formats, see: https://data.medicare.gov/Hospital-Compare\\nAcknowledgements\\nAttention: Works of the U.S. Government are in the public domain and permission is not required to reuse them. An attribution to the agency as the source is appreciated. Your materials, however, should not give the false impression of government endorsement of your commercial products or services. See 42 U.S.C. 1320b-10.\\nInspiration\\nWhich hospital types & hospital ownerships are most common?\\nWhich hospital types & ownerships are associated with better than average ratings/mortality/readmission/etc?\\nWhat is the average hospital rating, by state?\\nWhich hospital types & hospital ownerships are more likely to have not submitted proper data (\"Not Available\" & \"Results are not available for this reporting period\")?\\nWhich parts of the country have the highest & lowest density of religious hospitals?',\n",
       " \"About This Data\\nThis is a list of 10,000 women's shoes and their product information provided by Datafiniti's Product Database.\\nThe dataset includes shoe name, brand, price, and more. Note that each shoe will have an entry for each price found for it and some shoes may have multiple entries.\\nWhat You Can Do with This Data\\nYou can use this data to determine brand markups, pricing strategies, and trends for luxury shoes. E.g.:\\nWhat is the average price of each distinct brand listed?\\nWhich brands have the highest prices?\\nWhich ones have the widest distribution of prices?\\nIs there a typical price distribution (e.g., normal) across brands or within specific brands?\\nFurther processing data would also let you:\\nCorrelate specific product features with changes in price.\\nYou can cross-reference this data with a sample of our Men's Shoe Prices to see if there are any differences between women's brands and men's brands.\\nData Schema\\nA full schema for the data is available in our support documentation.\\nAbout Datafiniti\\nDatafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.\\nWant More?\\nYou can get more data like this by joining Datafiniti or requesting a demo.\",\n",
       " 'Context\\nSpaceX has gained worldwide attention for a series of historic milestones. It is the only private company ever to return a spacecraft from low-Earth orbit, which it first accomplished in December 2010. The company made history again in May 2012 when its Dragon spacecraft attached to the International Space Station, exchanged cargo payloads, and returned safely to Earth — a technically challenging feat previously accomplished only by governments. Since then Dragon has delivered cargo to and from the space station multiple times, providing regular cargo resupply missions for NASA.\\nUnder a $1.6 billion contract with NASA, SpaceX is flying numerous cargo resupply missions to the International Space Station, for a total of at least 20 flights under the Commercial Resupply Services contract. In 2016, NASA awarded SpaceX a second version of that contract that will cover a minimum of 6 additional flights from 2019 onward. In the near future, SpaceX will carry crew as part of NASA’s Commercial Crew Program as well. Dragon was designed from the outset to carry astronauts and SpaceX is in the process of upgrading Dragon to make it crew-ready. SpaceX is the world’s fastest-growing provider of launch services and has over 70 future missions on its manifest, representing over $10 billion in contracts. These include commercial satellite launches as well as NASA and other US Government missions.\\nContent\\nThis dataset includes a record for each payload carried during a SpaceX mission into outer space.\\nAcknowledgements\\nThe data was scraped from the SpaceX and NASA website.\\nInspiration\\nHas the rate of SpaceX rocket launches increased in the past decade? How many missions do you predict will be launched in 2018?',\n",
       " 'Contact\\nEmail me at: itsawesome17@gmail.com\\nMy blog: http://blog.mathocr.com/\\nContent\\nDataset consists of jpg files(45x45) DISCLAIMER: dataset does not contain Hebrew alphabet at all. It includes basic Greek alphabet symbols like: alpha, beta, gamma, mu, sigma, phi and theta. English alphanumeric symbols are included. All math operators, set operators. Basic pre-defined math functions like: log, lim, cos, sin, tan. Math symbols like: \\\\int, \\\\sum, \\\\sqrt, \\\\delta and more.\\nAcknowledgements\\nOriginal source, that was parsed, extracted and modified is CROHME dataset.\\nVisit CROHME at http://www.isical.ac.in/~crohme/index.html.\\nInspiration\\nDue to the technological advances in recent years, paper scientific documents are used less and less. Thus, the trend in the scientific community to use digital documents has increased considerably. Among these documents, there are scientific documents and more specifically mathematics documents. So I give a tool, to research recognizing handwritten math language in variety of applications.\\nSource code\\nYou can find source code responsible for parsing original CROHME dataset here:\\nhttps://github.com/XaiNano/CROHME_extractor\\nThis parser allows you not only to extract math symbols into square images of desired size, but also lets you specify categories of classes to be extracted like: digits, greek_letters, lowercase_letters, operators, and more. It also contains visualization tools and histograms showing appearances of each class in the dataset.\\nCommercial use\\nRights for commercial usage cannot be granted.',\n",
       " 'Overview\\nThe dataset contains a number of different subsets of the full food-101 data. The idea is to make a more exciting simple training set for image analysis than CIFAR10 or MNIST. For this reason the data includes massively downscaled versions of the images to enable quick tests. The data has been reformatted as HDF5 and specifically Keras HDF5Matrix which allows them to be easily read in. The file names indicate the contents of the file. For example\\nfood_c101_n1000_r384x384x3.h5 means there are 101 categories represented, with n=1000 images, that have a resolution of 384x384x3 (RGB, uint8)\\nfood_test_c101_n1000_r32x32x1.h5 means the data is part of the validation set, has 101 categories represented, with n=1000 images, that have a resolution of 32x32x1 (float32 from -1 to 1)\\nChallenge\\nThe first goal is to be able to automatically classify an unknown image using the dataset, but beyond this there are a number of possibilities for looking at what regions / image components are important for making classifications, identify new types of food as combinations of existing tags, build object detectors which can find similar objects in a full scene.\\nData Acknowledgement\\nThe data was repackaged from the original source (gzip) available at https://www.vision.ee.ethz.ch/datasets_extra/food-101/\\nLicense\\nThe Food-101 data set consists of images from Foodspotting [1]. Any use beyond scientific fair use must be negotiated with the respective picture owners according to the Foodspotting terms of use [2].\\n[1] http://www.foodspotting.com/ [2] http://www.foodspotting.com/terms/',\n",
       " \"Historical Hourly Weather Data\\nWho amongst us doesn't small talk about the weather every once in a while?\\nThe goal of this dataset is to elevate this small talk to medium talk.\\nJust kidding, I actually originally decided to collect this dataset in order to demonstrate basic signal processing concepts, such as filtering, Fourier transform, auto-correlation, cross-correlation, etc..., (for a data analysis course I'm currently preparing).\\nI wanted to demonstrate these concepts on signals that we all have intimate familiarity with and hope that this way these concepts will be better understood than with just made up signals.\\nThe weather is excellent for demonstrating these kinds of concepts as it contains periodic temporal structure with two very different periods (daily and yearly).\\nContent\\nThe dataset contains ~5 years of high temporal resolution (hourly measurements) data of various weather attributes, such as temperature, humidity, air pressure, etc.\\nThis data is available for 30 US and Canadian Cities, as well as 6 Israeli cities.\\nI've organized the data according to a common time axis for easy use.\\nEach attribute has it's own file and is organized such that the rows are the time axis (it's the same time axis for all files), and the columns are the different cities (it's the same city ordering for all files as well).\\nAdditionally, for each city we also have the country, latitude and longitude information in a separate file.\\nAcknowledgements\\nThe dataset was aquired using Weather API on the OpenWeatherMap website, and is available under the ODbL License.\\nInspiration\\nWeather data is both intrinsically interesting, and also potentially useful when correlated with other types of data.\\nFor example, Wildfire spread is potentially related to weather conditions, demand for cabs is famously known to be correlated with weather conditions (here, here and here you can find NYC cab ride data), and use of city bikes is probably also correlated with weather in interesting ways (check out this Austin dataset, this SF dataset, this Montreal dataset, and this NYC dataset).\\nTraffic is also probably related to weather.\\nAnother potentially interesting source of correlation is between weather and crime. Here are a few crime datasets on kaggle of cities present in this weather dataset: Chicago, Philadelphia, Los Angeles, Vancouver, Austin, NYC\\nThere are many other potentially interesting connections between everyday life and the weather that we can explore together with the help of this dataset. Have fun!\",\n",
       " 'Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nThis data set contains monthly rainfall detail of 36 meteorological sub-divisions of India.\\nContent\\nTime Period: 1901 - 2015\\nGranularity: Monthly\\nLocation: 36 meteorological sub-divisions in India\\nRainfall unit: mm\\nAcknowledgements\\nIndia Meteorological Department(IMD) Govt. of India has shared this dataset under Govt. Open Data License - India.',\n",
       " \"Context\\nFor most football fans, May - July represents a lull period due to the lack of club football. What makes up for it, is the intense transfer speculation that surrounds all major player transfers today. Their market valuations also lead to a few raised eyebrows, lately more than ever. I was curious to see how good a proxy popularity could be for ability, and the predictive power it would have in a model estimating a player's market value.\\nContent\\nname: Name of the player\\nclub: Club of the player\\nage : Age of the player\\nposition : The usual position on the pitch\\nposition_cat :\\n1 for attackers\\n2 for midfielders\\n3 for defenders\\n4 for goalkeepers\\nmarket_value : As on transfermrkt.com on July 20th, 2017\\npage_views : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017\\nfpl_value : Value in Fantasy Premier League as on July 20th, 2017\\nfpl_sel : % of FPL players who have selected that player in their team\\nfpl_points : FPL points accumulated over the previous season\\nregion:\\n1 for England\\n2 for EU\\n3 for Americas\\n4 for Rest of World\\nnationality\\nnew_foreign : Whether a new signing from a different league, for 2017/18 (till 20th July)\\nage_cat\\nclub_id\\nbig_club: Whether one of the Top 6 clubs\\nnew_signing: Whether a new signing for 2017/18 (till 20th July)\\nInspiration\\nTo statistically analyse the beautiful game.\",\n",
       " 'Context\\nMy Uber Drives (2016)\\nHere are the details of my Uber Drives of 2016. I am sharing this dataset for data science community to learn from the behavior of an ordinary Uber customer.\\nContent\\nGeography: USA, Sri Lanka and Pakistan\\nTime period: January - December 2016\\nUnit of analysis: Drives\\nTotal Drives: 1,155\\nTotal Miles: 12,204\\nDataset: The dataset contains Start Date, End Date, Start Location, End Location, Miles Driven and Purpose of drive (Business, Personal, Meals, Errands, Meetings, Customer Support etc.)\\nAcknowledgements & References\\nUsers are allowed to use, download, copy, distribute and cite the dataset for their pet projects and training. Please cite it as follows: “Zeeshan-ul-hassan Usmani, My Uber Drives Dataset, Kaggle Dataset Repository, March 23, 2017.”\\nPast Research\\nUber TLC FOIL Response - The dataset contains over 4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015 https://github.com/fivethirtyeight/uber-tlc-foil-response\\n1.1 Billion Taxi Pickups from New York - http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/\\nWhat you can do with this data - a good example by Yao-Jen Kuo - https://yaojenkuo.github.io/uber.html\\nInspiration\\nSome ideas worth exploring:\\n• What is the average length of the trip?\\n• Average number of rides per week or per month?\\n• Total tax savings based on traveled business miles?\\n• Percentage of business miles vs personal vs. Meals\\n• How much money can be saved by a typical customer using Uber, Careem, or Lyft versus regular cab service?',\n",
       " \"SEPTA - Southeastern Pennsylvania Transportation Authority\\nThe SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.\\nSEPTA uses On-Time Performance (OTP) to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time. However, by industry standard, a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.\\nSEPTA has established an annual goal of 91% for Regional Rail On-Time Performance. How well are they doing? Is it even a meaningful measure?\\nData Description\\notp.csv\\ntrain_id\\ndirection ('N' or 'S' direction is demarcated as either Northbound or Southbound)^1\\norigin (See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')\\nnext_station (Think of this as the station stop, at timeStamp)\\ndate\\nstatus ('On Time', '5 min', '10 min'. This is a status on train lateness. 999 is a suspended train)\\ntimeStamp\\ntrainView.csv - GPS Train data (early release)\\nMost GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units.\\ntrain_id\\nstatus\\nnext_station\\nservice\\ndest\\nlon\\nlat\\nsource\\ntrack_change\\ntrack\\ndate\\ntimeStamp0 First timeStamp at coordinates.\\ntimeStamp1 Last timeStamp at coordinates.\\nYou can look at the example here on how to track a single train.\\nWhat To Look For...\\nAh, as a commuter you care more about the performance of the train(s) you plan to take. OTP maybe 91% or above; but, if the train you take runs late, you'll spend extra time on the tracks. If it consistently runs late, maybe the schedule should be changed.\\nLook for patterns. For example, during the Tuesday morning rush do some trains run consistently late? How long does it take to get to from point A to point B in the system? Performance is VERY important to SEPTA.\\nBelow is a map of the system and station stops. This dataset contains data on the Regional Rail Lines.\\nSEPTA train schedules can be found here. Note different Weekday, Saturday, and Sunday schedules.\",\n",
       " 'Context\\nThis dataset is born from a test with the twitter streaming api to filter and collect data from this flow on a specific topic, in this case the French election.The script used to make this data collection is available on this Github repository.\\nSince the 18th of March, the French election enter in the final straight line until the first poll the 23 April 2017 , the candidates for the position are:\\nM. Nicolas DUPONT-AIGNAN\\nMme Marine LE PEN\\nM. Emmanuel MACRON\\nM. Benoît HAMON\\nMme Nathalie ARTHAUD\\nM. Philippe POUTOU\\nM. Jacques CHEMINADE\\nM. Jean LASSALLE\\nM. Jean-Luc MÉLENCHON\\nM. François ASSELINEAU\\nM. François FILLON\\nThe idea was to collect the data from the Twitter API periodically. The acquisition process evolved as follows:\\nVersions 1, 2 and 3 Every hour a python script listens to the twitter api stream for 10 minutes during 3 weeks.\\nVersion 4+ The new versions will be based on a new data structure, and start after the validation by the French constitutional council on 18 March 2017 of the candidates.\\nThe data will be stored in a dbsqlite files(database_number of the week_number_block_weekday.sqlite format) and will be updated as often as I can (at least every week).\\nAfter the first round (version 18+) i had to readjust the number of files per week and the 20 files kaggle limitation push me to reduce the number of files to upload (but you can join for your local analytics the version 17 + version 18+)\\nExample : Illustration of the number of mentions of the different candidates\\nI add to these databases a sqlite database that contains the informations from the google trends about the top 5 candidates.In thid database there is :\\nA table that contains the overall interests by region\\nA table that contains the interests by region for each candidate\\nA table with the top25 associated queries for each candidate in top and rising ranking\\nContent\\nIn this dbsqlite file, you will find a data table that contains for every row:\\n===============Common===============\\nthe index of the line\\nthe language of the tweet\\nfor each candidate :mention_candidatename, if the candidate or his associated account has been called (0 or 1)\\nthe tweet\\nthe timestamp in milliseconds\\n===============Version 4+===============\\nthe day\\nthe hour (London timezone)\\nthe username of the user that made the tweet\\nthe username location (that he gives with his profile)\\nif the tweet is a retweet or a quote (0 or 1)\\nthe username that has been retweeted\\nthe original tweet (the one retweeted or quoted)\\nAcknowledgements\\nThis election is gonna be intense.\\nInspiration\\nThe first version of the dataset was just a test to collect the data and see the first pieces of work that the community can do with this dataset.The new versions are (I think and hope) adapted to do deep text analytics.',\n",
       " \"Content\\nThis database includes a record for each oil pipeline leak or spill reported to the Pipeline and Hazardous Materials Safety Administration since 2010. These records include the incident date and time, operator and pipeline, cause of incident, type of hazardous liquid and quantity lost, injuries and fatalities, and associated costs.\\nAcknowledgements\\nThe oil pipeline accident reports were collected and published by the DOT's Pipeline and Hazardous Materials Safety Administration.\",\n",
       " 'Context\\nThis dataset comes from a proof-of-concept study published in 1999 by Golub et al. It showed how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. These data were used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).\\nContent\\nGolub et al \"Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring\"\\nThere are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent.\\nAcknowledgements\\nMolecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression\\nScience 286:531-537. (1999). Published: 1999.10.14\\nT.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomfield, and E.S. Lander\\nThese datasets have been converted to a comma separated value files (CSV).\\nInspiration\\nThese datasets are great for classification problems. The original authors used the data to classify the type of cancer in each patient by their gene expressions.',\n",
       " 'Context\\nBrazilian politicians are entitled to refunds if they spend any of their money on an activity that is enabling them to \"better serve the people\".\\nThose expenses are public data, however, there is little monitoring/data analysis of it. A quick look at it shows a deputy with over 800 flights in one year. Another deputy has over 140.000R$ expenses on mailing (old fashion mail) in one year.\\nThere are a lot of very suspicious data regarding the deputies expending behavior. Can you help spot outliers and companies charging unusual amounts of money for a service?\\nContent\\nData is public. It was collected from the official government website:\\nhttp://www2.camara.leg.br/transparencia/cota-para-exercicio-da-atividade-parlamentar/dados-abertos-cota-parlamentar\\nIt was converted from xml to csv, filtered out irrelevant columns, and translated a few of the features to English.\\nThe uploaded data contains:\\nu\\'deputy_name\\', u\\'political_party\\', u\\'deputy_state\\', u\\'company_name\\' u\\'company_id\\' u\\'refund_date\\'\\nInspiration\\nBrazil is currently passing through thriving times.\\nIts political group has always been public involved in many scandals, but it is just now that a few brave men and women have started doing something about it. In 2016 we have had senators, former ministers and many others formally charged and arrested for their crimes.',\n",
       " \"Context\\nI am currently working on summarizing chat context where it helps an agent in understanding previous context quickly. It interests me to apply the deep learning models to existing datasets and how they perform on them. I believe news articles are rich in grammar and vocabulary which allows us to gain greater insights.\\nContent\\nThe dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. I gathered the summarized news from Inshorts and only scraped the news articles from Hindu, Indian times and Guardian. Time period ranges from febrauary to august 2017.\\nAcknowledgements\\nI would like to thank the authors of Inshorts for their amazing work\\nInspiration\\nGenerating short length descriptions(headlines) from text(news articles).\\nSummarizing large amount of information which can be represented in compressed space\\nPurpose\\nWhen I was working on the summarization task I didn't find any open source data-sets to work on, I believe there are people just like me who are working on these tasks and I hope it helps them.\\nContributions\\nIt will be really helpful if anyone found nice insights from this data and can share their work. Thankyou...!!!\\nFor those who are interested here is the link for the github code which includes the scripts for scraping. https://github.com/sunnysai12345/News_Summary\",\n",
       " 'Douban Movie Short Comments Dataset V2\\nIntroduction\\nDouban Movie is a Chinese website that allows Internet users to share their comments and viewpoints about movies. Users are able to post short or long comments on movies and give them marks. This dataset contains more than 2 million short comments of 28 movies in Douban Movie website. It can be used on text classification, text clustering, sentiment analysis, semantic web construction and some other fields that relate to web mining or NLP (of Chinese lol).\\nMetadata\\nID the ID of the comment (start from 0)\\nMovie_Name_EN the English name of the movie\\nMovie_Name_CN the Chinese name of the movie\\nCrawl_Date the date that the data are crawled\\nNumber the number of the comment\\nUsername the username of the account\\nDate the date that the comment posted\\nStar the star that users give to the movie (from 1 to 5, 5 grades)\\nComment the content of the comment\\nLike the count of \"like\" on the comment',\n",
       " \"Context\\nGoodreads Book Reviews dataset.\\nContent\\nThis data was collected duing oct (12-21) '17\\nSchema: root |-- bookID: string (nullable = false) |-- title: string (nullable = false) |-- author: string (nullable = false) |-- rating: string (nullable = false) |-- ratingsCount: string (nullable = false) |-- reviewsCount: string (nullable = false) |-- reviewerName: string (nullable = true) |-- reviewerRatings: string (nullable = true) |-- review: string (nullable = true)\\nAcknowledgements\\nThank you, Goodreads for the data. All the data here belongs to Goodreads.\\nInspiration\\nThis data was actually scrapped for WebMining Project.\",\n",
       " 'Context\\nIn January 2017, PAVIC submitted a survey focused on Smart City and collected data from 1076 people. This survey was fully anonymous and was aimed at improving the citizens\\' lives in the future Smart City\\nContent\\nThe idea of the survey is to obtain a precise insight concerning the citizens\\' reactions to different recommendations in two different contexts. In clear, respondents were asked to choose among a set of 18 recommendations those that they would be most interested in if it were proposed in two different contexts: on a sunny and warm (20°C) Saturday afternoon in Spring (referred to as the \"Sun\" context) and on a rainy and cold (8°C) Saturday afternoon in Winter (referred to as the \"Rain\" context). The recommendations concerned various subjects : social or cultural events, discounts in restaurants, useful city information, etc. and people were asked in each context which they would like to receive as push notifications on their phones. For each context, respondents could give several or no responses.\\nThe following are the precise text of the questions submitted to the respondents : - for the \"Sun\" dataset : A Saturday in spring around 4pm with a comfy temperature of 20°C or 68°F. You are downtown in the city for the afternoon and your mobile application can send you personalized services/activities notifications in real-time. Which of the following activities would you want to receive in your notifications ? (Several may be chosen). - for the \"Rain\" dataset : A rainy Saturday in the winter around 4pm with a cool temperature of 8°C or 46°F. You are downtown in the city for the afternoon and your mobile application can send you personalized services/activities notifications in real-time. Which of the following activities would you want to receive in your notifications ? (Several may be chosen).\\nInspiration\\nThese dataset could allow future applications both to simulate recommendation system algorithms, and to deduce clusters from the collected profiles.',\n",
       " \"This data set is Hacker News posts from the last 12 months (up to September 26 2016).\\nIt includes the following columns:\\ntitle: title of the post (self explanatory)\\nurl: the url of the item being linked to\\nnum_points: the number of upvotes the post received\\nnum_comments: the number of comments the post received\\nauthor: the name of the account that made the post\\ncreated_at: the date and time the post was made (the time zone is Eastern Time in the US)\\nOne fun project suggestion is a model to predict the number of votes a post will attract.\\nThe scraper is written, so I can keep this up-to-date and add more historical data. I can also scrape the comments. Just make the request in this dataset's forum.\\nThe is a fork of minimaxir's HN scraper (thanks minimaxir): https://github.com/minimaxir/get-all-hacker-news-submissions-comments\",\n",
       " 'Context\\nThe World Bank EdStats All Indicator Query holds over 4,000 internationally comparable indicators that describe education access, progression, completion, literacy, teachers, population, and expenditures. The indicators cover the education cycle from pre-primary to vocational and tertiary education.\\nContent\\nIn addition to the above mentioned indicators, this dataset also holds learning outcome data from international and regional learning assessments (e.g. PISA, TIMSS, PIRLS), equity data from household surveys, and projection/attainment data to 2050. For further information, please visit the EdStats website.\\nInspiration\\nIn your opinion, what are some of the more surprising indicators? Are there any you would consider adding?\\nWhich countries have the highest adult illiteracy rates? Have they changed over time, and do rates vary based on age bracket?\\nDo school enrollment rates have an impact on adult illiteracy rates? If so, can you determine approximately how long a change in enrollment takes in order to impact illiteracy? Does this change vary among countries, and if so, can you point to changes in policies or NGO efforts that might play a role?\\nAcknowledgements\\nData was acquired from the World Bank, and can be accessed in multiple formats here.',\n",
       " 'Content\\nThis report provides statistics for the number of illegal immigrants arrested or apprehended by the border patrol in each division (or sector) of the United States borders with Canada, Mexico, and Caribbean islands; this data is a partial measure of the flow of people illegally entering the United States.\\nAcknowledgements\\nData was compiled and published by the US Border Patrol on the Customs and Border Protection webpage.',\n",
       " 'Context\\nRoad Accidents\\nContent\\nDataset has been fetched from here and the files have been merged and cleaned to reach the final data attached. Primarily Captures Road Accidents in UK between 1979 and 2015 and has 70 features/columns and about 250K rows. Also attached with it is an excel file with Multiple Tabs that can help one to understand the Data.\\nAcknowledgements\\nData has been fetched from Open Data Platform UK and is being shared under Open Government Licence. For more details refer to Open Data UK',\n",
       " \"Context:\\nThis dataset contains the number of international tourists arriving in Brazil each month from 1989 to 2015.\\nContent:\\nContinent\\nCountry\\nState of arrival\\nWay in (by land, sea, river or air)\\nYear\\nMonth\\nCount\\nAcknowledgements:\\nI've downloaded this dataset from dados.gov.br, the Brazilian open data portal, and tried to tidy it up a bit.\",\n",
       " 'Context\\nFocuses on financial flows, trends in external debt, and other major financial indicators for developing and advanced economies (data from Quarterly External Debt Statistics and Quarterly Public Sector Debt databases). Includes over 200 time series indicators from 1970 to 2014, for most reporting countries, and pipeline data for scheduled debt service payments on existing commitments to 2022.\\nContent\\nThis dataset contains country names and indicator variables from 1970 until 2024. Additional materials and detailed descriptions of the datasets can be downloaded from here.\\nAcknowledgement\\nThe original datasets and data dictionaries can be found here.\\nInspiration\\nFew ideas for exploring the dataset:\\nCompare the current account balance across countries. Is there a pattern associated with developing vs. advanced economies?\\nHow have the debt-related indicators changed over time? Are these strongly associated with other financial indicators?',\n",
       " 'Database of tornado activity from 1950 to 2015\\nCreated by National Weather service and available at http://www.spc.noaa.gov/gis/svrgis/\\nEnhance understanding of where tornados happen, indicators of damage, and weather conditions associated with tornados (temp/El Nino, La Nina)\\nMetadata available at http://www.spc.noaa.gov/wcm/data/SPC_severe_database_description.pdf',\n",
       " \"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nThis data set contains yearly suicide detail of all the states/u.t of India by various parameters from 2001 to 2012.\\nContent\\nTime Period: 2001 - 2012 Granularity: Yearly Location: States and U.T's of India\\nParameters:\\na) Suicide causes b) Education status c) By means adopted d) Professional profile e) Social status\\nAcknowledgements\\nNational Crime Records Bureau (NCRB), Govt of India has shared this dataset under Govt. Open Data License - India. NCRB has also shared the historical data on their website\",\n",
       " 'Context\\nThis dataset is a snapshot of most of the new news content published online over one week (August 24, 2017 through August 30, 2017).\\nPrepared by Rohit Kulkarni\\nIt includes approximately 1.4 million articles, with 20,000 news sources and 20+ languages.\\nThis dataset has just four fields (as per the column metadata):\\npublish_time - earliest known time of the url appearing online in yyyyMMddHHmm format, IST timezone\\nfeed_code - unique identifier for the publisher or domain\\nsource_url - url of the article\\nheadline_text - Headline of the article (UTF8, 20 possible languages)\\nSee the \"Basic Data Exploration\" notebook for a quick look at the dataset contents.\\nInspiration\\nThe sources include news feeds, news websites, government agencies, tech journals, company websites, blogs and wikipedia updates. The data has been collected by polling RSS feeds and by crawling other large news aggregators.\\nThis 7 day slice was selected as there wasn\\'t any downtime or internet outage during the interval. New news content is produced at this rate by publishers everyday, throughout the year.\\nSeveral other News datasets exploring other attributes, countries and topics can be seen on my profile.\\nAcknowledgements\\nThis dataset is free to use with the following citation:\\nRohit Kulkarni (2017), One Week of Global Feeds [News CSV Dataset], doi:10.7910/DVN/ILAT5B, Retrieved from: [this url]',\n",
       " 'Context\\nTsunami is a Japanese word that translates to \"harbor wave\". It is a wave or a series of waves generated by an impulsive vertical displacement of the surface of the ocean or other body of water. Tsunamis have been responsible for over 500,000 fatalities throughout the world — almost half from the 2004 Indian Ocean earthquake and tsunami!\\nContent\\nThe NOAA/WDS tsunami database is a listing of historical tsunami source events and runup locations throughout the world from 2000 B.C. to the present. The events were gathered from scientific and scholarly sources, regional and worldwide catalogs, tide gauge data, deep ocean sensor data, individual event reports, and unpublished works. There are currently over 2,000 source events in the database with event validities greater than one and over 13,000 runup locations where tsunami effects were observed.\\nAcknowledgements\\nNOAA\\'s National Centers for Environmental Information (NCEI) and the World Data Service for Geophysics compiled and published this tsunami database for tsunami warning centers, engineers, oceanographers, seismologists, and the general public.',\n",
       " 'Context\\nThe term \"astronaut\" derives from the Greek words meaning \"space sailor\" and refers to all who have been launched as crew members aboard NASA spacecraft bound for orbit and beyond.\\nContent\\nThe National Aeronautics and Space Administration (NASA) selected the first group of astronauts in 1959. From 500 candidates with the required jet aircraft flight experience and engineering training in addition to a height below 5 feet 11 inches, seven military men became the nation\\'s first astronauts. The second and third groups chosen included civilians with extensive flying experience. By 1964, requirements had changed, and emphasis was placed on academic qualifications; in 1965, six scientist astronauts were selected from a group of 400 applicants who had a doctorate or equivalent experience in the natural sciences, medicine, or engineering. The group named in 1978 was the first of space shuttle flight crews and fourteen groups have been selected since then with a mix of pilots and mission specialists.\\nThere are currently 50 active astronauts and 35 management astronauts in the program; 196 astronauts have retired or resigned and 49 are deceased (as of April 2013).\\nAcknowledgements\\nThis dataset was published by the National Aeronautics and Space Administration as the \"Astronaut Fact Book\" (April 2013 edition). Active astronauts\\' mission names and flight statistics were updated from the NASA website.\\nInspiration\\nWhich American astronaut has spent the most time in space? What university has produced the most astronauts? What subject did the most astronauts major in at college? Have most astronauts served in the military? Which branch? What rank did they achieve?',\n",
       " 'Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 7 million products) that was created by extracting data from Amazon.com.\\nContent\\nThis dataset has following fields:\\nproduct_name\\nmanufacturer - The item manufacturer, as reported on Amazon. Some common \"manufacturers\", like Disney, actually outsource their assembly line.\\nprice\\nnumber_available_in_stock\\nnumber_of_reviews\\nnumber_of_answered_questions - Amazon includes a Question and Answer service on all or most of its products. This field is a count of how many questions that were asked actually got answered.\\naverage_review_rating\\namazon_category_and_sub_category - A tree-based, >>-delimited categorization for the item in question.\\ncustomers_who_bought_this_item_also_bought - References to other items that similar users bought. This is a recommendation engine component that played a big role in making Amazon popular initially.\\ndescription\\nproduct_information\\nproduct_description\\nitems_customers_buy_after_viewing_this_item\\ncustomer_questions_and_answers - A string entry with all of the product\\'s JSON question and answer pairs.\\ncustomer_reviews - A string entry with all of the product\\'s JSON reviews.\\nsellers - A string entry with all of the product\\'s JSON seller information (many products on Amazon are sold by third parties).\\nAcknowledgements\\nThis dataset was created by PromptCloud\\'s in-house web-crawling service.\\nInspiration\\nThis detailed dataset can be used to answer questions like:\\nWhat types of toys are most popular on Amazon?\\nHow dominant are brands in the Amazon toy market?\\nCan you break down reviews to analyze their sentiment and contents?',\n",
       " \"Context\\nThis dataset is a record of every building or building unit (apartment, etc.) sold in the New York City property market over a 12-month period.\\nContent\\nThis dataset contains the location, address, type, sale price, and sale date of building units sold. A reference on the trickier fields:\\nBOROUGH: A digit code for the borough the property is located in; in order these are Manhattan (1), Bronx (2), Brooklyn (3), Queens (4), and Staten Island (5).\\nBLOCK; LOT: The combination of borough, block, and lot forms a unique key for property in New York City. Commonly called a BBL.\\nBUILDING CLASS AT PRESENT and BUILDING CLASS AT TIME OF SALE: The type of building at various points in time. See the glossary linked to below.\\nFor further reference on individual fields see the Glossary of Terms. For the building classification codes see the Building Classifications Glossary.\\nNote that because this is a financial transaction dataset, there are some points that need to be kept in mind:\\nMany sales occur with a nonsensically small dollar amount: $0 most commonly. These sales are actually transfers of deeds between parties: for example, parents transferring ownership to their home to a child after moving out for retirement.\\nThis dataset uses the financial definition of a building/building unit, for tax purposes. In case a single entity owns the building in question, a sale covers the value of the entire building. In case a building is owned piecemeal by its residents (a condominium), a sale refers to a single apartment (or group of apartments) owned by some individual.\\nAcknowledgements\\nThis dataset is a concatenated and slightly cleaned-up version of the New York City Department of Finance's Rolling Sales dataset.\\nInspiration\\nWhat can you discover about New York City real estate by looking at a year's worth of raw transaction records? Can you spot trends in the market, or build a model that predicts sale value in the future?\",\n",
       " 'Arabic Handwritten Characters Dataset\\nAstract\\nHandwritten Arabic character recognition systems face several challenges, including the unlimited variation in human handwriting and large public databases. In this work, we model a deep learning architecture that can be effectively apply to recognizing Arabic handwritten characters. A Convolutional Neural Network (CNN) is a special type of feed-forward multilayer trained in supervised mode. The CNN trained and tested our database that contain 16800 of handwritten Arabic characters. In this paper, the optimization methods implemented to increase the performance of CNN. Common machine learning methods usually apply a combination of feature extractor and trainable classifier. The use of CNN leads to significant improvements across different machine-learning classification algorithms. Our proposed CNN is giving an average 5.1% misclassification error on testing data.\\nContext\\nThe motivation of this study is to use cross knowledge learned from multiple works to enhancement the performance of Arabic handwritten character recognition. In recent years, Arabic handwritten characters recognition with different handwriting styles as well, making it important to find and work on a new and advanced solution for handwriting recognition. A deep learning systems needs a huge number of data (images) to be able to make a good decisions.\\nContent\\nThe data-set is composed of 16,800 characters written by 60 participants, the age range is between 19 to 40 years, and 90% of participants are right-hand. Each participant wrote each character (from ’alef’ to ’yeh’) ten times on two forms as shown in Fig. 7(a) & 7(b). The forms were scanned at the resolution of 300 dpi. Each block is segmented automatically using Matlab 2016a to determining the coordinates for each block. The database is partitioned into two sets: a training set (13,440 characters to 480 images per class) and a test set (3,360 characters to 120 images per class). Writers of training set and test set are exclusive. Ordering of including writers to test set are randomized to make sure that writers of test set are not from a single institution (to ensure variability of the test set).\\nIn an experimental section we showed that the results were promising with 94.9% classification accuracy rate on testing images. In future work, we plan to work on improving the performance of handwritten Arabic character recognition.\\nAcknowledgements\\nAhmed El-Sawy, Mohamed Loey, Hazem EL-Bakry, Arabic Handwritten Characters Recognition using Convolutional Neural Network, WSEAS, 2017 Our proposed CNN is giving an average 5.1% misclassification error on testing data.\\nInspiration\\nCreating the proposed database presents more challenges because it deals with many issues such as style of writing, thickness, dots number and position. Some characters have different shapes while written in the same position. For example the teh character has different shapes in isolated position.\\nBenha University\\nhttp://bu.edu.eg/staff/mloey\\nhttps://mloey.github.io/',\n",
       " \"Context\\nThis dataset contains weather data for New Delhi, India.\\nContent\\nThis data was taken out from wunderground with the help of their easy to use api. It contains various features such as temperature, pressure, humidity, rain, precipitation,etc.\\nAcknowledgements\\nThis data is owned by wunderground and although I ended up using noaa's data for my research, i thought that i'd share this data here as I haven't worked on hourly data yet and this might be of huge importance.\\nInspiration\\nThe main target is to develop a prediction model accurate enough for predicting the weather. We can try something like predicting the weather in the next 24 hours like microsoft tried some time back.\\nhttps://blogs.microsoft.com/next/2015/08/10/hows-the-weather-using-artificial-intelligence-for-better-answers/#sm.018l60051a9neka10is1m5qpi6u5y\",\n",
       " \"Upon request from some users, I am uploading CSV Version.\\nYes, There is already a dataset from manas\\nHowever, I thought this dataset is different than that one.which includes player metadata, information about all the 11 players who participated in the match.\\nThoughts :\\n■ who are the valuable players for respective teams.\\n■ who are more effective bowlers to bowl in the slog overs , is it spinners ?\\n■ Dhoni's strike rate against left-arm spinners in last five overs\\nHave fun with this dataset. Files in the dataset include:\\n1. Ball_by_Ball : Includes ball by ball details of all the 577 matches.\\n2. Match : Match metadata\\n3. Player : Player metadata\\n4. Player_Match : to know , who is the captain and keeper of the match , Includes every player who take part in match even If player haven't get a chance to either bat or bowl.\\n5. Season : Season wise details , Orange cap , Purple cap , Man_Of_The_Series\\n6. Team : Team Name\\nDiagram\",\n",
       " 'Context\\nThis data is from Environmental Protection Administration, Executive Yuan, R.O.C. (Taiwan).\\nThere is air quality data and meteorological monitoring data for research and analysis (only include northern Taiwan 2015).\\nContent\\n25 observation stations data in the 2015_Air_quality_in_northern_Taiwan.csv\\nThe columns in csv file are:\\ntime - The first column is observation time of 2015\\nstation - The second column is station name, there is 25 observation stations\\n[Banqiao, Cailiao, Datong, Dayuan, Guanyin, Guting, Keelung, Linkou, Longtan, Pingzhen, Sanchong, Shilin, Songshan, Tamsui, Taoyuan, Tucheng, Wanhua, Wanli, Xindian, Xinzhuang, Xizhi, Yangming, Yonghe, Zhongli, Zhongshan]\\nitems - From the third column to the last one\\nitem - unit - description\\nSO2 - ppb - Sulfur dioxide\\nCO - ppm - Carbon monoxide\\nO3 - ppb - ozone\\nPM10 - μg/m3 - Particulate matter\\nPM2.5 - μg/m3 - Particulate matter\\nNOx - ppb - Nitrogen oxides\\nNO - ppb - Nitric oxide\\nNO2 - ppb - Nitrogen dioxide\\nTHC - ppm - Total Hydrocarbons\\nNMHC - ppm - Non-Methane Hydrocarbon\\nCH4 - ppm - Methane\\nUVB - UVI - Ultraviolet index\\nAMB_TEMP - Celsius - Ambient air temperature\\nRAINFALL - mm\\nRH - % - Relative humidity\\nWIND_SPEED - m/sec - The average of last ten minutes per hour\\nWIND_DIREC - degress - The average of last ten minutes per hour\\nWS_HR - m/sec - The average of hour\\nWD_HR - degress - The average of hour\\nPH_RAIN - PH - Acid rain\\nRAIN_COND - μS/cm - Conductivity of acid rain\\nData mark\\n# indicates invalid value by equipment inspection\\n* indicates invalid value by program inspection\\nx indicates invalid value by human inspection\\nNR indicates no rainfall\\nblank indicates no data\\nLicense\\nOpen Government Data License, version 1.0 http://data.gov.tw/license',\n",
       " 'Original Dataset Author : https://github.com/walkerkq\\nFrom https://github.com/walkerkq/musiclyrics :\\n50 Years of Pop Music Lyrics\\nBillboard has published a Year-End Hot 100 every December since 1958. The chart measures the performance of singles in the U.S. throughout the year. Using R, I’ve combined the lyrics from 50 years of Billboard Year-End Hot 100 (1965-2015) into one dataset for analysis. You can download that dataset here.\\nThe songs used for analysis were scraped from Wikipedia’s entry for each Billboard Year-End Hot 100 Songs (e.g., 2014). This is the year-end chart, not weekly rankings. Many artists have made the weekly chart but not the final year end chart. The final chart is calculated using an inverse point system based on the weekly Billboard charts (100 points for a week at number one, 1 point for a week at number 100, etc).\\nI used the xml and RCurl packages to scrape song and artist names from each Wikipedia entry. I then used that list to scrape lyrics from sites that had predictable URL strings (for example, metrolyrics.com uses metrolyrics.com/SONG-NAME-lyrics-ARTIST-NAME.html). If the first site scrape failed, I moved onto the second, and so on. About 78.9% of the lyrics were scraped from metrolyics.com, 15.7% from songlyrics.com, 1.8% from lyricsmode.com. About 3.6% (187/5100) were unavailable.\\nThe dataset features 5100 observations with the features rank (1-100), song, artist, year, lyrics, and source. The artist feature is fairly standardized thanks to Wikipedia, but there is still quite a bit of noise when it comes to artist collaborations (Justin Timberlake featuring Timbaland, for example). If there were any errors in the lyrics that were scraped, such as spelling errors or derivatives like \"nite\" instead of \"night,\" they haven\\'t been corrected.\\nFull analysis can be found here.\\nwalkerkq\\nAcknowledgements\\nDataset is a mirror of : https://github.com/walkerkq/musiclyrics All credits to gathering it goes to https://github.com/walkerkq\\nInspiration\\nWhat makes a song\\'s lyrics popular ?',\n",
       " 'Context\\nThere has been a lot of discussion of the ways in which the workforce for Silicon Valley tech companies differs from that of the United States as a whole. In particular, a lot of evidence suggests that tech workers (who tend to be more highly paid than workers in many other professions) are more likely to be white and male. This dataset will allow you to investigate the demographics for 23 Silicon Valley tech companies for yourself.\\nContents\\nThis database contains EEO-1 reports filed by Silicon Valley tech companies. It was compiled by Reveal from The Center for Investigative Reporting.\\nThere are six columns in this dataset:\\ncompany: Company name\\nyear: For now, 2016 only\\nrace: Possible values: \"American_Indian_Alaskan_Native\", \"Asian\", \"Black_or_African_American\", \"Latino\", \"Native_Hawaiian_or_Pacific_Islander\", \"Two_or_more_races\", \"White\", \"Overall_totals\"\\ngender: Possible values: \"male\", \"female\". Non-binary gender is not counted in EEO-1 reports.\\njob_category: Possible values: \"Administrative support\", \"Craft workers\", \"Executive/Senior officials & Mgrs\", \"First/Mid officials & Mgrs\", \"laborers and helpers\", \"operatives\", \"Professionals\", \"Sales workers\", \"Service workers\", \"Technicians\", \"Previous_totals\", \"Totals\"\\ncount: Mostly integer values, but contains \"na\" for a no-data variable.\\nAcknowledgements:\\nThe EEO-1 database is licensed under the Open Database License (ODbL) by Reveal from The Center for Investigative Reporting.\\nYou are free to copy, distribute, transmit and adapt the spreadsheet, so long as you:\\ncredit Reveal (including this link if it’s distributed online);\\ninform Reveal that you are using the data in your work by emailing Sinduja Rangarajan at srangarajan@revealnews.org; and\\noffer any new work under the same license.\\nInspiration:\\nHow does each company’s workforce compare to the United States population as a whole? You can find county level diversity information here.\\nWhich company is the most diverse? Least diverse?',\n",
       " 'Context:\\nGlobal food price fluctuations can cause famine and large population shifts. Price changes are increasingly critical to policymakers as global warming threatens to destabilize the food supply.\\nContent:\\nOver 740k rows of prices obtained in developing world markets for various goods. Data includes information on country, market, price of good in local currency, quantity of good, and month recorded.\\nAcknowledgements:\\nCompiled by the World Food Program and distributed by HDX.\\nInspiration:\\nThis data would be particularly interesting to pair with currency fluctuations, weather patterns, and/or refugee movements--do any price changes in certain staples predict population upheaval? Do certain weather conditions influence market prices?\\nLicense:\\nReleased under CC BY-IGO.',\n",
       " 'The 2014 killing of Michael Brown in Ferguson, Missouri, began the protest movement culminating in Black Lives Matter and an increased focus on police accountability nationwide.\\nSince Jan. 1, 2015, The Washington Post has been compiling a database of every fatal shooting in the US by a police officer in the line of duty. It\\'s difficult to find reliable data from before this period, as police killings haven\\'t been comprehensively documented, and the statistics on police brutality are much less available. As a result, a vast number of cases go unreported.\\nThe Washington Post is tracking more than a dozen details about each killing - including the race, age and gender of the deceased, whether the person was armed, and whether the victim was experiencing a mental-health crisis. They have gathered this information from law enforcement websites, local new reports, social media, and by monitoring independent databases such as \"Killed by police\" and \"Fatal Encounters\". The Post has also conducted additional reporting in many cases.\\nThere are four additional datasets. These are US census data on poverty rate, high school graduation rate, median household income, and racial demographics.\\nSource of census data: https://factfinder.census.gov/faces/nav/jsf/pages/community_facts.xhtml',\n",
       " \"Context\\nThe data have been organized in two different but related classification tasks.\\ncolumn_3C_weka.csv (file with three class labels)\\nThe first task consists in classifying patients as belonging to one out of three categories: Normal (100 patients), Disk Hernia (60 patients) or Spondylolisthesis (150 patients).\\ncolumn_2C_weka.csv (file with two class labels)\\nFor the second task, the categories Disk Hernia and Spondylolisthesis were merged into a single category labelled as 'abnormal'. Thus, the second task consists in classifying patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 patients).\\nContent\\nField Descriptions:\\nEach patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column):\\npelvic incidence\\npelvic tilt\\nlumbar lordosis angle\\nsacral slope\\npelvic radius\\ngrade of spondylolisthesis\\nAcknowledgements\\nThe original dataset was downloaded from UCI ML repository:\\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science\\nFiles were converted to CSV\\nInspiration\\nUse these biomechanical features to classify patients according to their labels\",\n",
       " 'Global Historical Climatology Network-Monthly (GHCN-M)\\nContext\\nThe Global Historical Climatology Network (GHCN) is an integrated database of climate summaries from land surface stations across the globe. This data set contains gridded mean temperature anomalies, or departures from a reference value or long-term average, from the Global Historical Climatology Network-Monthly (GHCN-M) version 3.2.1 temperature data set. The gridded anomalies were produced from GHCN-M bias corrected data. Each month of data consists of 2,592 gridded data points produced on a 5° by 5° basis for the entire globe (72 longitude by 36 latitude grid boxes).\\nFrequency: Monthly\\nPeriod: 1880 to 2016\\nContent\\nGridded data for every month from January 1880 to the most recent month is available. The data are temperature anomalies in degrees Celsius. Each gridded value was multiplied by 100 and written to file as an integer. Missing values are represented by the value -9999. The data are formatted by year, month, latitude and longitude. There are 72 longitude grid values per line -- each grid is labeled as a concatenation of \"lon\", \"w\" or \"e\", then the degree. The latitude is captured in the \"lat\" field where the value indicates the lower bound of a grid cell (e.g. 85 indicates 85-90N whereas -90 indicates 85-90S). Longitude values are written from 180°W to 180°E, and latitude values from 90°N to 90°S.\\nThis dataset permits the quantification of changes in the mean monthly temperature and precipitation for the earth\\'s surface. Changes in the observing system itself have been carefully removed to allow for the true climate variability at the earth\\'s surface to be represented in the data.\\nMany surface weather stations undergo minor relocations through their history of observation. Stations may also be subject to changes in instrumentation as measurement technology evolves. Further, the land use/land cover in the vicinity of an observing site may also change with time. Such modifications to an observing site have the potential to alter a thermometer\\'s microclimate exposure characteristics and/or change the bias of measurements, the impact of which can be a systematic shift in the mean level of temperature readings that is unrelated to true climate variations. The process of removing such \"non-climatic\" artifacts in a climate time series is called homogenization. In version 3 of the GHCN-Monthly temperature data, the apparent impacts of documented and undocumented inhomogeneities are detected and removed through automated pairwise comparisons of mean monthly temperature series as detailed in Menne and Williams [2009].\\nInspiration\\nThis granular dataset permits extensive historical analysis of the earth’s climate to answer questions about climate change, including how different regions of the planet have been affected by changes in temperature over time. Get started by forking the kernel Mapping Historical Temperature Anomalies with R.\\nAcknowledgements\\nThis data is a product of NOAA\\'s National Centers for Environmental Information (NCEI). It was compiled through the aggregation and analysis of many thousands of weather station records. The compete description of the processes and methods used may be found at https://www.ncdc.noaa.gov/ghcnm/v3.php.\\nThe Global Historical Climatology Network-Monthly (GHCN-M) temperature dataset was first developed in the early 1990s (Vose et al. 1992). A second version was released in 1997 following extensive efforts to increase the number of stations and length of the data record (Peterson and Vose, 1997). Methods for removing inhomogeneities from the data record associated with non-climatic influences such as changes in instrumentation, station environment, and observing practices that occur over time were also included in the version 2 release (Peterson and Easterling, 1994; Easterling and Peterson 1995). Since that time efforts have focused on continued improvements in dataset development methods including new quality control processes and advanced techniques for removing data inhomogeneities (Menne and Williams, 2009).\\nLicense\\nPublic Domain License',\n",
       " \"Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 14,92,992 restaurants) that was created by extracting data from TripAdvisor.com.\\nContent\\nThis dataset has following fields:\\nRestaurant URL\\nName\\nAddress\\nPhone\\nCity\\nState\\nCountry\\nNeighbourhood\\nEmail ID\\nMenu\\nWebsite\\nLatitude\\nLongitude\\nAbout Restaurant\\nCuisine\\nGood for(suitable)\\nPrice\\nCurrency\\nRating\\nRanking\\nDeal(Promotion)\\nTotal Review\\nLast Reviewed\\nRecommended\\nDining Option\\nAward\\nAcknowledgements\\nThis dataset was created by PromptCloud's in-house web-crawling service.\\nInspiration\\nThe country-wise analyses of cuisine, rating, ranking, etc. can be performed.\",\n",
       " 'Context\\nThe NYC Department of Finance collects data on every parking ticket issued in NYC (~10M per year!). This data is made publicly available to aid in ticket resolution and to guide policymakers.\\nContent\\nThere are four files, covering Aug 2013-June 2017. The files are roughly organized by fiscal year (July 1 - June 30) with the exception of the initial dataset. The initial dataset also lacks 8 columns that are included in the other three datasets (although be warned that these additional data columns are used sparingly). See the dataset descriptions for exact details. Columns include information about the vehicle ticketed, the ticket issued, location, and time.\\nAcknowledgements\\nData was produced by NYC Department of Finance. FY2018 data is found here with updates every third week of the month.\\nInspiration\\nWhen are tickets most likely to be issued? Any seasonality?\\nWhere are tickets most commonly issued?\\nWhat are the most common years and types of cars to be ticketed?',\n",
       " \"Context\\nBrazil has elections every two years, but alternating between two different types of elections, each type occurring every four years. There are the municipal elections, where mayors and city council members are elected (the last one occurred in 2016) and general elections where president, governors, senators and congressmen (regional and national) are elected (the last one occurred in 2014). Brazil has 26 federal units plus the federal district. Each one of these units (regions) elects its senators, congressmen and governors.\\nFor each federal unit, Brazil's TSE provides information on the donations declared by the three entities: candidates, parties and committees. The data comprises information describing every donation received. The donations can be divided in two categories with respect to the donor: they can come from legal persons (private citizens, identified by the CPF (CPF is an identification number used by the Brazilian tax revenue office. It is roughly the Brazilian analogue to a social security number. With the same purpose, companies are identified with a similar number called CNPJ number) or from legal entities (i.e. companies, identified by the CNPJ number). Also, some entities can make donations among them (the party can give part of the money from a given donation to a candidate). In this type of transaction, the information on the original donor is also specified in the declarations. From now on, these type of donations will be referred to as non-original donations. Apart from information concerning each Brazilian federal unit separately, one can also obtain the information declared by the parties and committees at national level and for the presidential campaign (which has national and not regional scope).\\nRelated paper:\\nhttps://arxiv.org/pdf/1707.08826.pdf\",\n",
       " 'Context\\nThis data set was created for use in the Sberbank Kaggle competition.\\nContent\\nThe data consists of three GIS shapefiles one for each of the 3 major Moscow ring roads; the MKAD, TTK (or third ring) and Sadovoe (or garden ring).\\nAcknowledgements\\nThe road shapefiles have been extracted from OpenStreetMap data, processed in QGiS to extract only the roads of interest.\\nOpenStreetMap License: https://www.OpenStreetMap.org/copyright\\nInspiration\\nWith these files and the distances given in the Sberbank dataset it should be possible to better understand the location of properties. With a better understanding of location it may be possible to improve the quality of the overall dataset which contains material amounts of missing or poorly coded data.',\n",
       " \"Overview\\nThe data are a time-series of fluorescence images measured of at OHSU Lab of Charles Allen (https://www.ohsu.edu/xd/research/centers-institutes/oregon-institute-occupational-health-sciences/research/allen/).\\nIntroduction\\nWe use a fluorescent protein as a reporter for the circadian clock gene Period1. We are able to follow the expression of this gene in many neurons for several days to understand how the neural network in the suprachiasmatic nucleus synchronizes the circadian clock of individual neurons to produce a precise circadian rhythm. We analyze each image to determine the fluorescence intensity of each neuron over multiple circadian cycles.\\nFAQ\\nHow where the images obtained: which animal and what staining?\\nThe images were taken from a transgenic mouse in which expression of the fluorescent protein Venus is driven by the promoter for the circadian clock gene Period 1.\\nWhat is the anatomy of the images, and how are they oriented?\\nThe bright line is the third ventricle, which resides on the midline of the brain. The two bright regions on either side of the ventricle are the two portions of the Suprachiasmatic nucleus (SCN). Below the ventricle and the SCN is a dark horizontal band that represents the optic chasm.\\nWhat is the bright vertical line in the top middle?\\nThe bright line is the third ventricle. Pericytes that line the ventricle express the Venus at very high levels. We don't know the function of the circadian clock in these cells.\\nChallenge\\nCurrently we have to analyze each experiment by hand to follow an individual through a couple hundred images. This takes several days. This problem is going to get worse because we have just purchased a new automated microscope stage that will allow us to simultaneously image from four suprachiasmatic nuclei.\\nPreview\\nIdeas for Analysis\\nWavelets (pywavelets) following https://www.ncbi.nlm.nih.gov/pubmed/18931366\\nQuestions\\nDo the cells move during the experiment?\\nHow regular is their signal?\\nIs the period 24 hours?\\nDo nearby cells oscillate together?\\nDo they form chunks or groups, over what range do they work?\\nAre there networks formed from time-precedence?\",\n",
       " \"Facebook is becoming an essential tool for more than just family and friends. Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs. And yes, theft.\\nCommunities work when they're connected and exchanging information. What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?\\nUse Any Facebook Public Group\\nYou can leverage the examples here for any public Facebook group. For an example of the source code used to collect this data, and a quick start docker image, take a look at the following project: facebook-group-scrape.\\nData Sources\\nThere are 4 csv files in the dataset, with data from the following 5 public Facebook groups:\\nUnofficial Cheltenham Township\\nElkins Park Happenings!\\nFree Speech Zone\\nCheltenham Lateral Solutions\\nCheltenham Township Residents\\npost.csv\\nThese are the main posts you will see on the page. It might help to take a quick look at the page. Commas in the msg field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}.\\ngid Group id (5 different Facebook groups)\\npid Main Post id\\nid Id of the user posting\\nname User's name\\ntimeStamp\\nshares\\nurl\\nmsg Text of the message posted.\\nlikes Number of likes\\ncomment.csv\\nThese are comments to the main post. Note, Facebook postings have comments, and comments on comments.\\ngid Group id\\npid Matches Main Post identifier in post.csv\\ncid Comment Id.\\ntimeStamp\\nid Id of user commenting\\nname Name of user commenting\\nrid Id of user responding to first comment\\nmsg Message\\nlike.csv\\nThese are likes and responses. The two keys in this file (pid,cid) will join to post and comment respectively.\\ngid Group id\\npid Matches Main Post identifier in post.csv\\ncid Matches Comments id.\\nresponse Response such as LIKE, ANGRY etc.\\nid The id of user responding\\nname Name of the user responding\\nmember.csv\\nThese are all the members in the group. Some members never, or rarely, post or comment. You may find multiple entries in this table for the same person. The name of the individual never changes, but they change their profile picture. Each profile picture change is captured in this table. Facebook gives users a new id in this table when they change their profile picture.\\ngid Group id\\nid Id of the member\\nname Name of the member\\nurl URL of the member\",\n",
       " 'Context\\nThe dataset contains historical product demand for a manufacturing company with footprints globally. The company provides thousands of products within dozens of product categories. There are four central warehouses to ship products within the region it is responsible for. Since the products are manufactured in different locations all over the world, it normally takes more than one month to ship products via ocean to different central warehouses. If forecasts for each product in different central with reasonable accuracy for the monthly demand for month after next can be achieved, it would be beneficial to the company in multiple ways. This dataset is all real-life data and products/warehouse and category information encoded.\\nContent\\nProduct_Code: The product name encoded. Warehouse: Warehouse name encoded. Product_Category: Product Category for each Product_Code encoded. Date: The date customer needs the product. Order_Demand: single order qty.\\nInspiration\\nIs it possible to make forecasts for thousands of products (some of them are highly variable in terms of monthly demand) for the the month after next?',\n",
       " 'If you get richer your teeth could get worse (if you eat more sugar foods) or better (because of better health assistance or, even, more education and health-conciousness). These variables can be analysed with these data, downloaded from Gapminder Data:\\nBad teeth per child (12 yr, WHO)\\nGDP/capita (US$, inflation-adjusted, World Bank)\\nGovernment health spending per person (US$, WHO)\\nSugar comsumption per person (g per day, FAO)\\nLiteracy rate, adult total (% of people ages 15 and above, UNESCO)',\n",
       " 'Context\\nWhy some countries are so different from the others?\\nFeel free to upvote :) Autor: Joni Hoppen - linkedin - https://www.linkedin.com/in/joniarroba/\\nContent\\nI have gathered manually most of the information at World Bank, Unicef and so on. Some data were not there so I used K-nn to guess some values and have a full dataset that can be used of our data science community.\\nInformation of each of the 65 variables were made available here http://bit.ly/2l2Hjh3\\nAcknowledgements\\nThanks www.aquare.la Advanced Analytics that came up with the idea of creating this dataset to test their VORTX tool. Also Thanks to professionals involved in creating Indexes and collecting them, this is such a great valuable work to help better see the world.\\nInspiration\\nWhat would be the best, way to equalize the world?',\n",
       " 'Context\\nAttached is a set of products in which we are trying to determine which products we should continue to sell, and which products to remove from our inventory. The file contains BOTH historical sales data AND active inventory, which can be discerned with the column titled \"File Type\".\\nWe suspect that data science applied to the set--such as a decision tree analysis or logistic regression, or some other machine learning model---can help us generate a value (i.e., probability score) for each product, that can be used as the main determinant evaluating the inventory. Each row in the file represents one product.\\nIt is important to note that we have MANY products in our inventory, and very few of them tend to sell (only about 10% sell each year) and many of the products only have a single sale in the course of a year.\\nContent\\nThe file contains historical sales data (identified with the column titled File_Type) along with current active inventory that is in need of evaluation (i.e., File Type = \"Active\"). The historical data shows sales for the past 6 months. The binary target (1 = sale, 0 = no sale in past six months) is likely the primary target that should drive the analysis.\\nThe other columns contain numeric and categorical attributes that we deem relevant to sales.\\nNote that some of the historical sales SKUs are ALSO included in the active inventory.\\nA few comments about the attributes included, as we realize we may have some attributes that are unnecessary or may need to be explained.\\nSKU_number: This is the unique identifier for each product.\\nOrder: Just a sequential counter. Can be ignored.\\nSoldFlag: 1 = sold in past 6 mos. 0 = Not sold\\nMarketingType = Two categories of how we market the product. This should probably be ignored, or better yet, each type should be considered independently.\\nNew_Release_Flag = Any product that has had a future release (i.e., Release Number > 1)\\nInspiration\\n(1) What is the best model to use that will provide us with a probability estimate of a sale for each SKU? We are mainly interested in a relative unit that we can continuously update based on these attributes (and others that we add, as we are able).\\n(2) Is it possible to provide a scored file (i.e., a probability score for each SKU in the file), and to provide an evaluation of the accuracy of the selected model?\\n(3) What are the next steps we should take?\\nThanks very much for any suggestions you may provide.',\n",
       " 'Context\\nThis is a dataset of Devanagari Script Characters. It comprises of 92000 images [32x32 px] corresponding to 46 characters, consonants \"ka\" to \"gya\", and the digits 0 to 9. The vowels are missing.\\nContent\\nThe CSV file is of the dimension 92000 * 1025. There are 1024 input features of pixel values in grayscale (0 to 255). The column \"character\" represents the Devanagari Character Name corresponding to each image.\\nAcknowledgements\\nThis dataset was originally created by Computer Vision Research Group, Nepal. [website archive] (https://web.archive.org/web/20160105230017/http://cvresearchnepal.com/wordpress/dhcd/)\\nExample Script\\nhttps://nbviewer.jupyter.org/github/rishianand9/devanagari-character-recognition/blob/master/DCRS.ipynb',\n",
       " 'Context\\nThe American Time Use Survey (ATUS) is the Nation’s first federally administered, continuous survey on time use in the United States. The goal of the survey is to measure how people divide their time among life’s activities.\\nIn ATUS, individuals are randomly selected from a subset of households that have completed their eighth and final month of interviews for the Current Population Survey (CPS). ATUS respondents are interviewed only one time about how they spent their time on the previous day, where they were, and whom they were with. The survey is sponsored by the Bureau of Labor Statistics and is conducted by the U.S. Census Bureau.\\nThe major purpose of ATUS is to develop nationally representative estimates of how people spend their time. Many ATUS users are interested in the amount of time Americans spend doing unpaid, nonmarket work, which could include unpaid childcare, eldercare, housework, and volunteering. The survey also provides information on the amount of time people spend in many other activities, such as religious activities, socializing, exercising, and relaxing. In addition to collecting data about what people did on the day before the interview, ATUS collects information about where and with whom each activity occurred, and whether the activities were done for one’s job or business. Demographic information—including sex, race, age, educational attainment, occupation, income, marital status, and the presence of children in the household—also is available for each respondent. Although some of these variables are updated during the ATUS interview, most of this information comes from earlier CPS interviews, as the ATUS sample is drawn from a subset of households that have completed month 8 of the CPS.\\nThe user guide can be found here.\\nContent\\nThere are 8 datasets containing microdata from 2003-2015:\\nRespondent file: The Respondent file contains information about ATUS respondents, including their labor force status and earnings.\\nRoster file: The Roster file contains information about household members and nonhousehold children (under 18) of ATUS respondents. It includes information such as age and sex.\\nActivity file: The Activity file contains information about how ATUS respondents spent their diary day. It includes information such as activity codes, activity start and stop times, and locations. Because Activity codes have changed somewhat between 2003 and 2015, this file uses activity codes that appear in the 2003-2015 ATUS Coding Lexicon (PDF).\\nActivity summary file: The Activity summary file contains information about the total time each ATUS respondent spent doing each activity on the diary day. Because Activity codes have changed somewhat between 2003 and 2015, this file uses activity codes that appear in the 2003-2015 ATUS Coding Lexicon (PDF).\\nWho file: The Who file includes codes that indicate who was present during each activity.\\nCPS 2003-2015 file: The ATUS-CPS file contains information about each household member of all individuals selected to participate in ATUS. The information on the ATUS-CPS file was collected 2 to 5 months before the ATUS interview.\\nEldercare Roster file: The ATUS Eldercare Roster file contains information about people for whom the respondent provided care. Eldercare data have been collected since 2011.\\nReplicate weights file: The Replicate weights file contains miscellaneous ATUS weights.\\nThe ATUS interview data dictionary can be found here.\\nThe ATUS Current Population Survey (CPS) data dictionary can be found here.\\nThe ATUS occupation and industry codes can be found here.\\nThe ATUS activity lexicon can be found here.\\nAcknowledgements\\nThe original datasets can be found here.\\nInspiration\\nHow do daily activities differ by:\\nlabor force status\\nincome\\nhousehold composition\\ngeographical region\\ndisability status',\n",
       " 'Context\\nCPJ began compiling detailed records on journalist deaths in 1992. CPJ applies strict journalistic standards when investigating a death. One important aspect of their research is determining whether a death was work-related. As a result, they classify deaths as \"motive confirmed\" or \"motive unconfirmed.\"\\nContent\\nThe dataset contains 18 variables:\\nType: CPJ classified deaths as motive confirmed or motive confirmed, as well as Media Workers\\nDate\\nName\\nSex\\nCountry_killed\\nOrganization\\nNationality\\nMedium\\nJob\\nCoverage\\nFreelance\\nLocal_Foreign\\nSource_fire\\nType_death\\nImpunity_for_murder\\nTaken_captive\\nThreatened\\nTortured\\nAcknowledgements\\nThe original dataset can be found here.\\nInspiration\\nSome ideas for exploring the dataset:\\nWhat is the trend in journalist deaths over time and how does this differ by type of death, job, coverage, and country?\\nAre there differences by sex and/or nationality?',\n",
       " \"Context:\\nCDC's Division of Population Health provides cross-cutting set of 124 indicators that were developed by consensus and that allows states and territories and large metropolitan areas to uniformly define, collect, and report chronic disease data that are important to public health practice and available for states, territories and large metropolitan areas. In addition to providing access to state-specific indicator data, the CDI web site serves as a gateway to additional information and data resources.\\nContent:\\nA variety of health-related questions were assessed at various times and places across the US over the past 15 years. Data is provided with confidence intervals and demographic stratification.\\nAcknowledgements:\\nData was compiled by the CDC.\\nInspiration:\\nAny interesting trends in certain groups?\\nAny correlation between disease indicators and locality hospital spending?\",\n",
       " 'Content\\nThe World Religion Project aims to provide detailed information about religious adherence worldwide since 1945. It contains data about the number of adherents by religion in each of the states in the international system for every half-decade period. Some of the religions are divided into religious families, and the breakdown of adherents within a given religion into religious families is provided to the extent data are available.\\nThe project was developed in three stages. The first stage consisted of the formation of a religions tree. A religion tree is a systematic classification of major religions and of religious families within those major religions. To develop the religion tree we prepared a comprehensive literature review, the aim of which was to define a religion, to find tangible indicators of a given religion of religious families within a major religion, and to identify existing efforts at classifying world religions. The second stage consisted of the identification of major data sources of religious adherence and the collection of data from these sources according to the religion tree classification. This created a dataset that included multiple records for some states for a given point in time, yet contained multiple missing data for specific states, specific time periods, and specific religions. The third stage consisted of cleaning the data, reconciling discrepancies of information from different sources, and imputing data for the missing cases.\\nAcknowledgements\\nThe dataset was created by Zeev Maoz, University of California-Davis, and Errol Henderson, Pennsylvania State University, and published by the Correlates of War Project.',\n",
       " \"Content\\nThis dataset includes a record for every branch of Chase Bank in the United States, including the branch's name and number, date established as a bank office and (if applicable) acquired by JP Morgan Chase, physical location as street address, city, state, zip, and latitude and longitude coordinates, and the amount deposited at the branch (or the institution, for the bank's main office) between July 1 and June 30, 2016, in US dollars.\\nAcknowledgements\\nThe location data was scraped from the Chase Bank website. The deposit data was compiled from the Federal Deposit Insurance Corporation's annual Summary of Deposits reports.\\nInspiration\\nWhere did Chase Bank customers deposit the most money last year? Which bank branch has seen the most growth in deposits? How did the bank network of branch locations grow over the past century? What city has the most bank branches per capita?\",\n",
       " \"General Information\\nCommon Voice is a corpus of speech data read by users on the Common Voice website (http://voice.mozilla.org/), and based upon text from a number of public domain sources like user submitted blog posts, old books, movies, and other public speech corpora. Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems.\\nStructure\\nThe corpus is split into several parts for your convenience. The subsets with “valid” in their name are audio clips that have had at least 2 people listen to them, and the majority of those listeners say the audio matches the text. The subsets with “invalid” in their name are clips that have had at least 2 listeners, and the majority say the audio does not match the clip. All other clips, ie. those with fewer than 2 votes, or those that have equal valid and invalid votes, have “other” in their name.\\nThe “valid” and “other” subsets are further divided into 3 groups:\\ndev - for development and experimentation\\ntrain - for use in speech recognition training\\ntest - for testing word error rate\\nOrganization and Conventions\\nEach subset of data has a corresponding csv file with the following naming convention:\\n“cv-{type}-{group}.csv”\\nHere “type” can be one of {valid, invalid, other}, and “group” can be one of {dev, train, test}. Note, the invalid set is not divided into groups.\\nEach row of a csv file represents a single audio clip, and contains the following information:\\nfilename - relative path of the audio file\\ntext - supposed transcription of the audio\\nup_votes - number of people who said audio matches the text\\ndown_votes - number of people who said audio does not match text\\nage - age of the speaker, if the speaker reported it\\nteens: '< 19'\\ntwenties: '19 - 29'\\nthirties: '30 - 39'\\nfourties: '40 - 49'\\nfifties: '50 - 59'\\nsixties: '60 - 69'\\nseventies: '70 - 79'\\neighties: '80 - 89'\\nnineties: '> 89'\\ngender - gender of the speaker, if the speaker reported it\\nmale\\nfemale\\nother\\naccent - accent of the speaker, if the speaker reported it\\nus: 'United States English'\\naustralia: 'Australian English'\\nengland: 'England English'\\ncanada: 'Canadian English'\\nphilippines: 'Filipino'\\nhongkong: 'Hong Kong English'\\nindian: 'India and South Asia (India, Pakistan, Sri Lanka)'\\nireland: 'Irish English'\\nmalaysia: 'Malaysian English'\\nnewzealand: 'New Zealand English'\\nscotland: 'Scottish English'\\nsingapore: 'Singaporean English'\\nsouthatlandtic: 'South Atlantic (Falkland Islands, Saint Helena)'\\nafrican: 'Southern African (South Africa, Zimbabwe, Namibia)'\\nwales: 'Welsh English'\\nbermuda: 'West Indies and Bermuda (Bahamas, Bermuda, Jamaica, Trinidad)'\\nThe audio clips for each subset are stored as mp3 files in folders with the same naming conventions as it’s corresponding csv file. So, for instance, all audio data from the valid train set will be kept in the folder “cv-valid-train” alongside the “cv-valid-train.csv” metadata file.\\nAcknowledgments\\nThis dataset was compiled by Michael Henretty, Tilman Kamp, Kelly Davis & The Common Voice Team, who included the following acknowledgments:\\nWe sincerely thank all of the people who donated their voice on the Common Voice website and app. You are the backbone of this project, and we thank you for making this possible!\\nWe also thank our community on Discourse (https://discourse.mozilla-community.org/c/voice) and Github (https://github.com/mozilla/voice-web), you have made this project better every step of the way.\\nAnd special thanks to Mycroft, SNIPS.ai, Mythic, Tatoeba.org, Bangor University, and SAP for joining us on this journey. We look forward to working more with each of you.\",\n",
       " 'Context\\nThere is a question in our mind that which language, skills, and experience should we add to our toolbox for getting a job in Google. Well, I think why not we find out the answer by analyzing the Google Jobs Site. Google published all of their jobs at https://careers.google.com/. So I scraped all of the job data from that site by going every job page using Selenium. I only take Job Title, Job Location, Job responsibilities, minimum and preferred qualifications for this dataset.\\nContent\\nThis dataset is collected using Selenium by scraping all of the jobs text for Google Career site. About the column\\nTitle: The title of the job\\nCategory: Category of the job\\nLocation: Location of the job\\nResponsibilities: Responsibilities for the job\\nMinimum Qualifications: Minimum Qualifications for the job\\nPreferred Qualifications: Preferred Qualifications for the job\\nAcknowledgements\\nThis dataset is collected using Selenium. This product uses the Google Career site but is not endorsed or certified by Google Career site.\\nInspiration\\nYou can find most popular skills for Google Jobs\\nCreate identical job posts\\nMost popular languages\\netc',\n",
       " \"This dataset contains the entire contents of each major API data set published by the US Energy Information Administration. That's everything from the hourly electricity consumption in the United States to natural gas futures contracts.\\nThis data has been lightly reprocessed from the EIA's bulk download facility by converting each file from a zip of jsons into a single json with the series name as the keys to the specific time series. Please note that there are thousands of time series in here, and many of them may still require additional cleaning to deal with odd date formats and so on. The file preview is unable to show a complete listing. You can usually find full details of a given time series in the 'description' field.\",\n",
       " 'Context:\\nSentiment analysis, the task of automatically detecting whether a piece of text is positive or negative, generally relies on a hand-curated list of words with positive sentiment (good, great, awesome) and negative sentiment (bad, gross, awful). This dataset contains both positive and negative sentiment lexicons for 81 languages.\\nContent:\\nThe sentiment lexicons in this dataset were generated via graph propagation based on a knowledge graph--a graphical representation of real-world entities and the links between them. The general intuition is that words which are closely linked on a knowledge graph probably have similar sentiment polarities. For this project, sentiments were generated based on English sentiment lexicons.\\nThis dataset contains sentiment lexicons for the following languages:\\nAfrikaans\\nAlbanian\\nArabic\\nAragonese\\nArmenian\\nAzerbaijani\\nBasque\\nBelarusian\\nBengali\\nBosnian\\nBreton\\nBulgarian\\nCatalan\\nChinese\\nCroatian\\nCzech\\nDanish\\nDutch\\nEsperanto\\nEstonian\\nFaroese\\nFinnish\\nFrench\\nGalician\\nGeorgian\\nGerman\\nGreek\\nGujarati\\nHaitian Creole\\nHebrew\\nHindi\\nHungarian\\nIcelandic\\nIdo\\nIndonesian\\nInterlingua\\nIrish\\nItalian\\nKannada\\nKhmer\\nKirghiz\\nKorean\\nKurdish\\nLatin\\nLatvian\\nLithuanian\\nLuxembourgish\\nMacedonian\\nMalay\\nMaltese\\nMarathi\\nNorwegian\\nNorwegian\\nPersian\\nPolish\\nPortuguese\\nRomanian\\nRomansh\\nRussian\\nScottish\\nSerbian\\nSlovak\\nSlovene\\nSpanish\\nSwahili\\nSwedish\\nTagalog\\nTamil\\nTelugu\\nThai\\nTurkish\\nTurkmen\\nUkrainian\\nUrdu\\nUzbek\\nVietnamese\\nVolapük\\nWalloon\\nWelsh\\nWestern Frisian\\nYiddish\\nFor more information and additional sentiment lexicons, please visit the project’s website.\\nAcknowledgements:\\nThis dataset was collected by Yanqing Chen and Steven Skiena. If you use it in your work, please cite the following paper:\\nChen, Y., & Skiena, S. (2014). Building Sentiment Lexicons for All Major Languages. In ACL (2) (pp. 383-389).\\nIt is distributed here under the GNU General Public License. Note that this is the full GPL, which allows many free uses, but does not allow its incorporation into any type of distributed proprietary software, even in part or in translation. For commercial applications please contact the dataset creators.\\nInspiration:\\nThese word lists contain many words with similar meanings. Can you automatically detect which words are cognates?\\nCan you use these sentiment lexicons to reverse-engineer the knowledge graphs that generated them?',\n",
       " \"Top Spotify Tracks of 2017\\nAt the end of each year, Spotify compiles a playlist of the songs streamed most often over the course of that year. This year's playlist (Top Tracks of 2017) included 100 songs. The question is: What do these top songs have in common? Why do people like them?\\nOriginal Data Source: The audio features for each song were extracted using the Spotify Web API and the spotipy Python library. Credit goes to Spotify for calculating the audio feature values.\\nData Description: There is one .csv file in the dataset. (featuresdf.csv) This file includes:\\nSpotify URI for the song\\nName of the song\\nArtist(s) of the song\\nAudio features for the song (such as danceability, tempo, key etc.)\\nA more detailed explanation of the audio features can be found in the Metadata tab.\\nExploring the Data: Some suggestions for what to do with the data:\\nLook for patterns in the audio features of the songs. Why do people stream these songs the most?\\nTry to predict one audio feature based on the others\\nSee which features correlate the most\",\n",
       " 'Context\\nThis data set was made from an html rip made by reddit user \"usheep\" who threatened to expose all the vendors on Agora to the police if they did not meet his demands (sending him a small monetary amount ~few hundred dollars in exchange for him not leaking their info). Most information about what happened to \"usheep\" and his threats is nonexistent. He posted the html rip and was never heard from again. Agora shut down a few months after. It is unknown if this was related to \"usheep\" or not, but the raw html data remained.\\nContent\\nThis is a data parse of marketplace data ripped from Agora (a dark/deep web) marketplace from the years 2014 to 2015. It contains drugs, weapons, books, services, and more. Duplicate listings have been removed and prices have been averaged of any duplicates. All of the data is in a csv file and has over 100,000 unique listings.\\nIt is organized by:\\nVendor: The seller\\nCategory: Where in the marketplace the item falls under\\nItem: The title of the listing\\nDescription: The description of the listing\\nPrice: Cost of the item (averaged across any duplicate listings between 2014 and 2015)\\nOrigin: Where the item is listed to have shipped from\\nDestination: Where the item is listed to be shipped to (blank means no information was provided, but mostly likely worldwide. I did not enter worldwide for any blanks however as to not make assumptions)\\nRating: The rating of the seller (a rating of [0 deals] or anything else with \"deals\" in it means there is not concrete rating as the amount of deals is too small for a rating to be displayed)\\nRemarks: Only remark options are blank, or \"Average price may be skewed outliar > .5 BTC found\" which is pretty self explanatory.\\nAcknowledgements\\nThough I got this data from a 3rd party, it seems as though it originally came from here: https://www.gwern.net/DNM-archives Gwern Branwen seems to have complied all of his dark net marketplace leaks and html rips and has a multitude of possible uses for the data at the link above. It is free for anyone to use as long as proper credit is given to the creator. I would be happy to parse more data if anyone would like to request a specific website and/or format.\\nInspiration\\nThis data could be used to track drug dealers across different platforms. Potentially find correlations between different drugs and from where/to they ship in the world to show correlations between types of drugs and where drug dealers that supply them are located. Prices can estimate drug economies in certain regions of the world. Similar listings from 2 different vendors can perhaps point to competition to corner a market, or even show that some vendors may work together to corner a market. There are quite a few opportunities to do some really great stuff to find correlations between illegal drugs, weapons, and more in order to curb the flow of dark net drug trade by identifying high risk regions or vendors. I can potentially do a new parse of other websites so you can find correlations across websites rather than just within Agora.',\n",
       " 'World Development Indicators provides a compilation of relevant, high-quality, and internationally comparable statistics about global development and the fight against poverty. It is intended to help policymakers, students, analysts, professors, program managers, and citizens find and use data related to all aspects of development, including those that help monitor progress toward the World Bank Group’s two goals of ending poverty and promoting shared prosperity.\\nContent\\nThis dataset includes indicators at both national and regional levels for: -Agriculture & Rural Development -Aid Effectiveness -Climate Change -Economy & Growth -Education -Energy & Mining -Environment -External Debt -Financial Sector -Gender -Health -Infrastructure -Labor & Social Protection -Poverty -Private Sector -Public Sector -Science & Technology -Social Development -Trade, Urban Development\\nAcknowledgements\\nThis dataset was kindly made available by the World Bank. Please check their instance at http://data.worldbank.org/data-catalog/world-development-indicators for updates and related information.',\n",
       " 'Context\\nI am a student exploring the possibility of making money in football betting. I am currently doing a literature review on modelling association football scores and trying to put together a machine learning system to use for my first betting campaign next season. What I have learned thus far is that outcomes of football events are partly deterministic and partly random. I do not know exactly how to go about implementing this in a machine learning system yet. I am also hoping to find useful features from this dataset.\\nContent\\nThe data here contains match statistics collected from whoscored.com europes top five leagues from 2012-2013 to 2016-2017 season. It contains just about all match statistics that anyone can ever hope for including but not limited to Goals, Corners, Possession, Ratings, Coaches, LineUps, and other relevent match statistics\\nThe features are simply just self explanatory and have been given long but meaningful names\\nAcknowledgement\\nI collected the data from the whoscored.com website. I scraped it using beautifulSoup in python and just extracted the features I thought could have some use.\\nInspiration\\nThis is just something I hope could become something but hey, it may be nothing. I am just interested to know the kind of insights that could be generated from this.',\n",
       " \"Context\\nThis is a list of the finishers of the Boston Marathon of 2015, 2016 and 2017.\\nIt's important to highlight that the Boston Marathon is the oldest marathon run in the US as it is the only marathon (other than olympic trails) that most of the participants have to qualify to participate.\\nFor the professional runners, it's a big accomplishment to win the marathon. For most of the other participants, it's an honor to be part of it.\\nContent\\nIt contains the name, age, gender, country, city and state (where available), times at 9 different stages of the race, expected time, finish time and pace, overall place, gender place and division place.\\nDecided to keep every year as a separate file, making it more manageable and easier to deal with it.\\nAcknowledgements\\nData was scrapped from the official marathon website - http://registration.baa.org/2017/cf/Public/iframe_ResultsSearch.cfm\\nI have found that other people have done this kind of scraping, so, some of those ideas together with things I have learned in my quest to become a data scientist created the set.\\nYou can actually find the scraping notebooks at - https://github.com/rojour/boston_results . Notebook it's not very clean yet, but I will get to it soon...\\nInspiration\\nI was a participant in the marathon 2016 and 2017 edition, as well as a data science student, so it was a natural curiosity.\\nI have done a preliminary study of some fun facts. You can see the kernel here as well as in the github page listed above.\\nAlready some people have created some fun analysis of the results (mostly of the first part - 2016) that was the first upload, but I am curious of what people may come up with... now that three years are available, it may spark the creative juices of some.\\nI believe it's a simple, fun dataset that can be used by the new to play with, and by some veterans to get creative.\",\n",
       " 'Dataset of all of the crimes in the DC metro police system ranging from Theft, Arson, Assault, Homicide, Sex Abuse, Robbery, and Burglary.\\nData can be easily geocoded and mapped, trends can be extracted, and predictions can be made.\\nWould be interesting to combine with other datasets, i.e. changes in housing prices, history of construction sites etc. j An informal hypothesis would be: If the local government invests in fixing the sidewalks in a neighborhood, how much would the investment decrease crime levels on a block by block basis.\\nRaw Data can be accessed from: http://crimemap.dc.gov/CrimeMapSearch.aspx#tabs-GeoOther\\nThe data is most easily accessed by downloading 1 ward at a time for the specific data range.',\n",
       " 'Context\\nExtraterrestrials, visitors, little green men, UFOs, swap gas. What do they want? Where do they come from? Do they like cheeseburgers? This dataset will likely not help you answer these questions. It does contain over 80,000 records of UFO sightings dating back as far as 1949. With the latitude and longitude data it is possible to assess the global distribution of UFO sightings (patterns could aid in planetary defence if invasion proves to be imminent). The dates and times, along with the duration of the UFO\\'s stay and description of the craft also lend themselves to predictions. Can we find patterns in their arrival times and durations? Do aliens work on weekends? Help defend the planet and learn about your fellow earthlings (and when they are most likely to see ET).\\nContent\\nDate_time - standardized date and time of sighting\\ncity - location of UFO sighting\\nstate/province - the US state or Canadian province, appears blank for other locations\\ncountry - Country of UFO sighting\\nUFO_shape - a one word description of the \"spacecraft\"\\nlength_of_encounter_seconds - standardized to seconds, length of the observation of the UFO\\ndescribed_duration _of_encounter - raw description of the length of the encounter (shows uncertainty to previous column)\\ndescription - text description of the UFO encounter. Warning column is messy, with some curation it could lend itself to some natural language processing and sentiment analysis.\\ndate_documented - when was the UFO sighting reported\\nlatitude - latitude\\nlongitude - longitude\\nNote there are missing data in the columns. I\\'ve left it as is because depending on what the user is interested in the missing values in any one column may or may not matter.\\nAcknowledgements\\nI found these data here: https://github.com/planetsig/ufo-reports Full credit to them for the curation, I added some column headers and just described what I\\'ve seen\\nInspiration\\nSome great ways to use these data would be:\\nA global plot of the locations of recorded UFO sightings.\\nCan the duration of the UFO visit be predicted from the other data?\\nIs there a pattern to the appearances? At certain times of day, on certain days of the week or days of the year? (i.e. are people on their way home from the pub more likely to see little green men?)\\nAre certain shapes of UFO more likely to be seen in different geographical regions.',\n",
       " \"Context\\nA dataset of ATP matches including individual statistics.\\nContent\\nIn these datasets there are individual csv files for ATP tournament from 2000 to 2017.\\nThe numbers in the last columns are absolute values, using them you can calculate percentages.\\nDataset legend\\nAll the match statistics are in absolute number format, you can convert to percentages using the total point number\\nace = absolute number of aces\\ndf = number of double faults\\nsvpt = total serve points\\n1stin = 1st serve in\\n1st won = points won on 1st serve\\n2ndwon = points won on 2nd serve\\nSvGms = serve games\\nbpSaved = break point saved\\nbpFaced = break point faced\\nAcknowledgement\\nThanks to Jeff Sackmann for the excellent work. Be sure to visit his github profile\\nhttps://github.com/JeffSackmann/tennis_atp\\nInspiration\\nThis dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. I'm planning to add historical odds and injuries data as soon as I have the time to get them.\",\n",
       " 'Context\\nThe United States Census Bureau conducts annual surveys to assess the finances of elementary and high schools. The attached CSV file contains a summary of revenue and expenditure for the years 1992-2015, organized by state.\\nContent\\n[elsec_main.csv] A comma-separated spreadsheet containing revenues and expenditures for all U.S. school districts, 1993-2015.\\nSTATE,ENROLL,NAME,YRDATA,TOTALREV,TFEDREV,TSTREV,TLOCREV,TOTALEXP,TCURINST,TCURSSVC,TCURONON,TCAPOUT\\nAlabama,7568,AUTAUGA CO SCH DIST,1995,31827,2821,21389,7617,27457,15228,7123,2575,2176\\nAlabama,19961,BALDWIN CO SCH DIST,1995,93379,6655,55108,31616,87973,48750,22961,6927,6795\\n[elsec_summary.csv] A comma-separated spreadsheet containing state summaries of revenues and expenditures, organized by year.\\nSTATE,YEAR,ENROLL,TOTAL_REVENUE,FEDERAL_REVENUE,STATE_REVENUE,LOCAL_REVENUE,TOTAL_EXPENDITURE,INSTRUCTION_EXPENDITURE,SUPPORT_SERVICES_EXPENDITURE,OTHER_EXPENDITURE,CAPITAL_OUTLAY_EXPENDITURE\\nAlabama,1992,,2678885,304177,1659028,715680,2653798,1481703,735036,,174053\\nAlaska,1992,,1049591,106780,720711,222100,972488,498362,350902,,37451\\nBe warned, some data will be NaN\\'s (most notably, the 1992 records contain no data for enrollment).\\nData was created from the spreadsheets in [elsec.zip] (taken from the U.S. Census Bureau site) using [chew_data.py] and [state_summary.py]. Column names are documented in [school15doc.pdf].\\nSources\\nhttps://www.census.gov/programs-surveys/school-finances/data/tables.html\\nChangelog\\n[v 0.2] Added data from 1993-2001. Data is now harvested from the main spreadsheets instead of the summary spreadsheets. Data by school district is now available.\\n[v 0.3] Added 1992 data. Added enrollment data for all years except 1992 (unavailable).\\n[v 0.4] Straightening a few things out as I play with the data in my own kernel. Changed \"program_other_expenditure\" to \"other_expenditure\" and fixed chew_data.py to properly pull that information. Removed \"non-elsec\" funding and \"program_current_expenditure\" columns.',\n",
       " \"Context\\nThis dataset is an aggregate of the screen-fixations from screen movements of StarCraft 2 replay files.\\nContent\\nThis dataset contains 21 variables:\\nGameID: Unique ID for each game\\nLeagueIndex: 1-8 for Bronze, Silver, Gold, Diamond, Master, GrandMaster, Professional leagues\\nAge: Age of each player\\nHoursPerWeek: Hours spent playing per week\\nTotalHours: Total hours spent playing\\nAPM: Action per minute\\nSelectByHotkeys: Number of unit selections made using hotkeys per timestamp\\nAssignToHotkeys: Number of units assigned to hotkeys per timestamp\\nUniqueHotkeys: Number of unique hotkeys used per timestamp\\nMinimapAttacks: Number of attack actions on minimal per timestamp\\nMinimapRightClicks: Number of right-clicks on minimal per timestamp\\nNumberOfPACs: Number of PACs per timestamp\\nGapBetweenPACs: Mean duration between PACs (milliseconds)\\nActionLatency: Mean latency from the onset of PACs to their first action (milliseconds)\\nActionsInPAC: Mean number of actions within each PAC\\nTotalMapExplored: Number of 24x24 game coordinate grids viewed by player per timestamp\\nWorkersMade: Number of SCVs, drones, probes trained per timestamp\\nUniqueUnitsMade: Unique units made per timestamp\\nComplexUnitsMade: Number of ghosts, investors, and high templars trained per timestamp\\nComplexAbilityUsed: Abilities requiring specific targeting instructions used per timestamp\\nMaxTimeStamp: Time stamp of game's last recorded event\\nInspiration\\nQuestions worth exploring:\\nHow do the replay attributes differ by level of player expertise?\\nWhat are significant predictors of a player's league?\\nAcknowledgements\\nThis dataset is from Simon Fraser University - Summit and can be found here. You must give attribution to the work; You may not use this work for commercial purposes; You may not alter, transform, or build upon this work. Any further uses require the permission of the rights holder.\",\n",
       " 'Context\\nIs crime in America rising or falling? The answer is not as simple as politicians make it out to be because of how the FBI collects crime data from the country’s more than 18,000 police agencies. National estimates can be inconsistent and out of date, as the FBI takes months or years to piece together reports from those agencies that choose to participate in the voluntary program.\\nTo try to fill this gap, The Marshall Project collected and analyzed more than 40 years of data on the four major crimes the FBI classifies as violent — homicide, rape, robbery and assault — in 68 police jurisdictions with populations of 250,000 or greater. We obtained 2015 reports, which have yet to be released by the FBI, directly from 61 of them. We calculated the rate of crime in each category and for all violent crime, per 100,000 residents in the jurisdiction, based on the FBI’s estimated population for that year. We used the 2014 estimated population to calculate 2015 crime rates per capita.\\nAcknowledgements\\nThe crime data was acquired from the FBI Uniform Crime Reporting program\\'s \"Offenses Known and Clearances by Arrest\" database for the year in question, held at the National Archives of Criminal Justice Data. The data was compiled and analyzed by Gabriel Dance, Tom Meagher, and Emily Hopkins of The Marshall Project; the analysis was published as Crime in Context on 18 August 2016.',\n",
       " 'Context:\\nThe mass movement of uprooted people is a highly charged geopolitical issue. This data, gathered by the UN High Commissioner for Refugees (UNHCR), covers movement of displaced persons (asylum seekers, refugees, internally displaced persons (IDP), stateless). Also included are destination country responses to asylum petitions.\\nContent:\\nThis dataset includes 6 csv files covering:\\nAsylum monthly applications opened (asylum_seekers_monthly.csv)\\nYearly progress through the refugee system (asylum_seekers.csv)\\nRefugee demographics (demographics.csv)\\nYearly time series data on UNHCR’s populations of concern (time_series.csv)\\nYearly population statistics on refugees by residence and destination (persons_of_concern.csv)\\nYearly data on resettlement arrivals, with or without UNHCR assistance (resettlement.csv)\\nAcknowledgements:\\nThis dataset was gathered from UNHCR. Photo by Ali Tareq.\\nInspiration:\\nWhat are the most frequent destination countries for refugees? How has refugee flow changed? Any trends that could predict future refugee patterns?',\n",
       " \"Context\\nI wanted an easy way to share all the lending club data with others. Unfortunately, the data on their site is fragmented into many smaller files. There is another lending club dataset on Kaggle, but it hasn't been updated in years. It also doesn't include the rejected loans, which I put in here.\\nI created a git repo for the code to create this data: https://github.com/nateGeorge/preprocess_lending_club_data\\nContent\\nThe definitions for the fields are here, at the bottom of the page.\\nUnfortunately, there is a limit of 500MB for dataset files (so lame!), so I had to compress the files with gzip in the Python pandas package.\\nI cleaned the data a tiny bit: I removed %s from int_rate and revol_util, and deleted the url column.\\nTo load the data in Python:\\nimport pandas as pd\\n\\naccept_df = pd.read_csv('../input/accepted_2007_to_2016.csv.gz', compression='gzip')\\nreject_df = pd.read_csv('../input/rejected_2007_to_2016.csv.gz', compression='gzip')\\n\\n# too many columns to print the info summary out, so we need to force it\\nprint(accept_df.info(verbose=True, null_counts=True))\\nIn R:\\nlibrary(data.table)\\n\\naccepted_def &lt;- read.csv(gzfile('rejected_2007_to_2016.csv.gz'), na.strings='')\\nacc_dt &lt;- as.data.table(accepted_def)\\nrejected_def &lt;- read.csv(gzfile('accepted_2007_to_2016.csv.gz'), na.strings='')\\nrej_dt &lt;- as.data.table(accepted_def)\\nThere are also separate csvs in the main input folder, but the only advantage over the lending club site is that the 2016 year is joined into one file instead of 4.\\nInspiration\\nI wanted to make this dataset easily available for others to use.\",\n",
       " 'Many new websites and online tools have come into existence to support scholarly communication in all phases of the research workflow. To what extent researchers are using these and more traditional tools has been largely unknown. This 2015-2016 survey aimed to fill that gap.\\nThe survey captured information on tool usage for 17 research activities, stance towards open access and open science, and expectations of the most important development in scholarly communication. Respondents’ demographics included research roles, country of affiliation, research discipline and year of first publication. The online survey employed an open, non-probability sample. A largely self-selected group of 20,663 researchers, librarians, editors, publishers and other groups involved in research took the survey, which was available in seven languages. The survey was open from May 10, 2015 to February 10, 2016.\\nThis data set contains:\\nFull raw (anonymized) and cleaned data files (csv, each file containing 20,663 records and 178 variables)\\nVariable lists for raw and cleaned data files (csv)\\nReadme file (txt)\\nThe dataset is also deposited in Zenodo: http://dx.doi.org/10.5281/zenodo.49583\\nThe full description of survey methodology is in a data publication in F1000 Research: http://dx.doi.org/10.12688/f1000research.8414.1\\nMore information on the project this survey is part of can be found here: http://101innovations.wordpress.com\\n[edited to add] For quick visual exploration of the data, check out the interactive dashboard on Silk: http://dashboard101innovations.silk.co/\\nContact:\\nJeroen Bosman: http://orcid.org/0000-0001-5796-2727 / j.bosman@uu.nl\\nBianca Kramer: http://orcid.org/0000-0002-5965-6560 / b.m.r.kramer@uu.nl',\n",
       " \"Context\\nThis dataset includes information about gun-death in the US in the years 2012-2014.\\nContent\\nThe data includes data regarding the victim's age, sex, race, education, intent, time (month and year) and place of death, and whether or not police was at the place of death.\\nAcknowledgements\\nI came across this thanks to FiveThirtyEight's Gun Deaths in America project. The data originated from the CDC, and can be found here.\",\n",
       " \"Context\\nBible (or Biblia in Greek) is a collection of sacred texts or scriptures that Jews and Christians consider to be a product of divine inspiration and a record of the relationship between God and humans (Wiki). And for data mining purpose, we could do many things using Bible scriptures as for NLP, Classification, Sentiment Analysis and other particular topics between Data Science and Theology perspective.\\nContent\\nHere you will find the following bible versions in sql, sqlite, xml, csv, and json format:\\nAmerican Standard-ASV1901 (ASV)\\nBible in Basic English (BBE)\\nDarby English Bible (DARBY)\\nKing James Version (KJV)\\nWebster's Bible (WBT)\\nWorld English Bible (WEB)\\nYoung's Literal Translation (YLT)\\nEach verse is accessed by a unique key, the combination of the BOOK+CHAPTER+VERSE id.\\nExample:\\nGenesis 1:1 (Genesis chapter 1, verse 1) = 01001001 (01 001 001)\\nExodus 2:3 (Exodus chapter 2, verse 3) = 02002003 (02 002 003)\\nThe verse-id system is used for faster, simplified queries.\\nFor instance: 01001001 - 02001005 would capture all verses between Genesis 1:1 through Exodus 1:5.\\nWritten simply:\\nSELECT * FROM bible.t_asv WHERE id BETWEEN 01001001 AND 02001005\\nCoordinating Tables\\nThere is also a number-to-book key (key_english table), a cross-reference list (cross_reference table), and a bible key containing meta information about the included translations (bible_version_key table). See below SQL table layout. These tables work together providing you a great basis for a bible-reading and cross-referencing app. In addition, each book is marked with a particular genre, mapping in the number-to-genre key (key_genre_english table) and common abbreviations for each book can be looked up in the abbreviations list (key_abbreviations_english table). While its expected that your programs would use the verse-id system, book #, chapter #, and verse # columns have been included in the bible versions tables.\\nA Valuable Cross-Reference Table\\nA very special and valuable addition to these databases is the extensive cross-reference table. It was created from the project at http://www.openbible.info/labs/cross-references/. See .txt version included from http://www.openbible.info website. Its extremely useful in bible study for discovering related scriptures. For any given verse, you simply query vid (verse id), and a list of rows will be returned. Each of those rows has a rank (r) for relevance, start-verse (sv), and end verse (ev) if there is one.\\nBasic Web Interaction\\nThe web folder contains two php files. Edit the first few lines of index.php to match your server's settings. Place these in a folder on your webserver. The references search box can be multiple comma separated values. (i.e. John 3:16, Rom 3:23, 1 Jn 1:9, Romans 10:9-10) You can also directly link to a verse by altering the URI: [http://localhost/index.php?b=John 3:16, Rom 3:23, 1 Jn 1:9, Romans 10:9-10](http://localhost/index.php?b=John 3:16, Rom 3:23, 1 Jn 1:9, Romans 10:9-10)\\nbible-mysql.sql (MySQL) is the main database and most feature-oriented due to contributions from developers. It is suggested you use that for most things, or at least convert the information from it.\\ncross_references-mysql.sql (MySQL) is the cross-reference table. It has been separated to become an optional feature. This is converted from the project at http://www.openbible.info/labs/cross-references/.\\nbible-sqlite.db (SQLite) is a basic simplified database for simpler applications (includes cross-references too).\\ncross_references.txt is the source cross-reference file obtained from http://www.openbible.info/labs/cross-references/\\nIn CSV folder, you will find (same list order with the other formats):\\nbible_version_key.csv\\nkey_abbreviations_english.csv\\nkey_english.csv\\nkey_genre_english.csv\\nt_asv.csv, t_bbe.csv, t_dby.csv, t_wbt.csv, t_web.csv, t_ylt.csv\\nAcknowledgements\\nIn behalf of the original contributors (Github)\\nInspirations\\nWordNet as an additional semantic resource for NLP\",\n",
       " \"This data set used in the CoIL 2000 Challenge contains information on customers of an insurance company. The data consists of 86 variables and includes product usage data and socio-demographic data derived from zip area codes. The data was collected to answer the following question: Can you predict who would be interested in buying a caravan insurance policy and give an explanation why?\\nAcknowledgements\\nDISCLAIMER\\nThis dataset is owned and supplied by the Dutch datamining company Sentient Machine Research, and is based on real world business data. You are allowed to use this dataset and accompanying information for non commercial research and education purposes only. It is explicitly not allowed to use this dataset for commercial education or demonstration purposes. For any other use, please contact Peter van der Putten, info@smr.nl.\\nThis dataset has been used in the CoIL Challenge 2000 datamining competition. For papers describing results on this dataset, see the TIC 2000 homepage: http://www.wi.leidenuniv.nl/~putten/library/cc2000/\\nPlease cite/acknowledge:\\nP. van der Putten and M. van Someren (eds) . CoIL Challenge 2000: The Insurance Company Case. Published by Sentient Machine Research, Amsterdam. Also a Leiden Institute of Advanced Computer Science Technical Report 2000-09. June 22, 2000.\\nThe Data\\nOriginally, this dataset was broken into two parts: the training set and the evaluation set. As this was a competition, the responses to the evaluation set were not given as part of the original release; they were, however, released after the end of the competition in a separate file. This dataset contains all three of these files, combined into one.\\nThe field ORIGIN in the caravan-insurance-challenge.csv file has the values train and test, corresponding to the training and evaluation sets, respectively. To simulate the original challenge, you can ignore the test rows, and test your model's prediction on those observations once you've trained only on the training set.\\nEach observation corresponds to a postal code. Variables beginning with M refer to demographic statistics of the postal code, while variables beginning with P and A (as well as CARAVAN, the target variable) refer to product ownership and insurance statistics in the postal code.\\nThe data file contains the following fields:\\nORIGIN: train or test, as described above\\nMOSTYPE: Customer Subtype; see L0\\nMAANTHUI: Number of houses 1 - 10\\nMGEMOMV: Avg size household 1 - 6\\nMGEMLEEF: Avg age; see L1\\nMOSHOOFD: Customer main type; see L2\\n** Percentages in each group, per postal code (see L3)**:\\nMGODRK: Roman catholic\\nMGODPR: Protestant ...\\nMGODOV: Other religion\\nMGODGE: No religion\\nMRELGE: Married\\nMRELSA: Living together\\nMRELOV: Other relation\\nMFALLEEN: Singles\\nMFGEKIND: Household without children\\nMFWEKIND: Household with children\\nMOPLHOOG: High level education\\nMOPLMIDD: Medium level education\\nMOPLLAAG: Lower level education\\nMBERHOOG: High status\\nMBERZELF: Entrepreneur\\nMBERBOER: Farmer\\nMBERMIDD: Middle management\\nMBERARBG: Skilled labourers\\nMBERARBO: Unskilled labourers\\nMSKA: Social class A\\nMSKB1: Social class B1\\nMSKB2: Social class B2\\nMSKC: Social class C\\nMSKD: Social class D\\nMHHUUR: Rented house\\nMHKOOP: Home owners\\nMAUT1: 1 car\\nMAUT2: 2 cars\\nMAUT0: No car\\nMZFONDS: National Health Service\\nMZPART: Private health insurance\\nMINKM30: Income < 30.000\\nMINK3045: Income 30-45.000\\nMINK4575: Income 45-75.000\\nMINK7512: Income 75-122.000\\nMINK123M: Income >123.000\\nMINKGEM: Average income\\nMKOOPKLA: Purchasing power class\\n** Total number of variable in postal code (see L4)**:\\nPWAPART: Contribution private third party insurance\\nPWABEDR: Contribution third party insurance (firms) ...\\nPWALAND: Contribution third party insurane (agriculture)\\nPPERSAUT: Contribution car policies\\nPBESAUT: Contribution delivery van policies\\nPMOTSCO: Contribution motorcycle/scooter policies\\nPVRAAUT: Contribution lorry policies\\nPAANHANG: Contribution trailer policies\\nPTRACTOR: Contribution tractor policies\\nPWERKT: Contribution agricultural machines policies\\nPBROM: Contribution moped policies\\nPLEVEN: Contribution life insurances\\nPPERSONG: Contribution private accident insurance policies\\nPGEZONG: Contribution family accidents insurance policies\\nPWAOREG: Contribution disability insurance policies\\nPBRAND: Contribution fire policies\\nPZEILPL: Contribution surfboard policies\\nPPLEZIER: Contribution boat policies\\nPFIETS: Contribution bicycle policies\\nPINBOED: Contribution property insurance policies\\nPBYSTAND: Contribution social security insurance policies\\nAWAPART: Number of private third party insurance 1 - 12\\nAWABEDR: Number of third party insurance (firms) ...\\nAWALAND: Number of third party insurance (agriculture)\\nAPERSAUT: Number of car policies\\nABESAUT: Number of delivery van policies\\nAMOTSCO: Number of motorcycle/scooter policies\\nAVRAAUT: Number of lorry policies\\nAAANHANG: Number of trailer policies\\nATRACTOR: Number of tractor policies\\nAWERKT: Number of agricultural machines policies\\nABROM: Number of moped policies\\nALEVEN: Number of life insurances\\nAPERSONG: Number of private accident insurance policies\\nAGEZONG: Number of family accidents insurance policies\\nAWAOREG: Number of disability insurance policies\\nABRAND: Number of fire policies\\nAZEILPL: Number of surfboard policies\\nAPLEZIER: Number of boat policies\\nAFIETS: Number of bicycle policies\\nAINBOED: Number of property insurance policies\\nABYSTAND: Number of social security insurance policies\\nCARAVAN: Number of mobile home policies 0 - 1\\nKeys (L1 - L4)\\nL0: Customer subtype\\n1: High Income, expensive child\\n2: Very Important Provincials\\n3: High status seniors\\n4: Affluent senior apartments\\n5: Mixed seniors\\n6: Career and childcare\\n7: Dinki's (double income no kids)\\n8: Middle class families\\n9: Modern, complete families\\n10: Stable family\\n11: Family starters\\n12: Affluent young families\\n13: Young all american family\\n14: Junior cosmopolitan\\n15: Senior cosmopolitans\\n16: Students in apartments\\n17: Fresh masters in the city\\n18: Single youth\\n19: Suburban youth\\n20: Etnically diverse\\n21: Young urban have-nots\\n22: Mixed apartment dwellers\\n23: Young and rising\\n24: Young, low educated\\n25: Young seniors in the city\\n26: Own home elderly\\n27: Seniors in apartments\\n28: Residential elderly\\n29: Porchless seniors: no front yard\\n30: Religious elderly singles\\n31: Low income catholics\\n32: Mixed seniors\\n33: Lower class large families\\n34: Large family, employed child\\n35: Village families\\n36: Couples with teens 'Married with children'\\n37: Mixed small town dwellers\\n38: Traditional families\\n39: Large religous families\\n40: Large family farms\\n41: Mixed rurals\\nL1: average age keys:\\n1: 20-30 years 2: 30-40 years 3: 40-50 years 4: 50-60 years 5: 60-70 years 6: 70-80 years\\nL2: customer main type keys:\\n1: Successful hedonists\\n2: Driven Growers\\n3: Average Family\\n4: Career Loners\\n5: Living well\\n6: Cruising Seniors\\n7: Retired and Religeous\\n8: Family with grown ups\\n9: Conservative families\\n10: Farmers\\nL3: percentage keys:\\n0: 0%\\n1: 1 - 10%\\n2: 11 - 23%\\n3: 24 - 36%\\n4: 37 - 49%\\n5: 50 - 62%\\n6: 63 - 75%\\n7: 76 - 88%\\n8: 89 - 99%\\n9: 100%\\nL4: total number keys:\\n0: 0\\n1: 1 - 49\\n2: 50 - 99\\n3: 100 - 199\\n4: 200 - 499\\n5: 500 - 999\\n6: 1000 - 4999\\n7: 5000 - 9999\\n8: 10,000 - 19,999\\n9: >= 20,000\",\n",
       " 'Dataset with the text of 10% of questions and answers from the Stack Overflow programming Q&A website.\\nThis is organized as three tables:\\nQuestions contains the title, body, creation date, closed date (if applicable), score, and owner ID for all non-deleted Stack Overflow questions whose Id is a multiple of 10.\\nAnswers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.\\nTags contains the tags on each of these questions\\nDatasets of all R questions and all Python questions are also available on Kaggle, but this dataset is especially useful for analyses that span many languages.\\nExample projects include:\\nIdentifying tags from question text\\nPredicting whether questions will be upvoted, downvoted, or closed based on their text\\nPredicting how long questions will take to answer\\nLicense\\nAll Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.',\n",
       " \"About This Data\\nThis is a list of 1,000 hotels and their reviews provided by Datafiniti's Business Database. The dataset includes hotel location, name, rating, review data, title, username, and more.\\nWhat You Can Do With This Data\\nYou can use this data to compare hotel reviews on a state-by-state basis; experiment with sentiment scoring and other natural language processing techniques. The review data lets you correlate keywords in the review text with ratings. E.g.:\\nWhat are the bottom and top states for hotel reviews by average rating?\\nWhat is the correlation between a state’s population and their number of hotel reviews?\\nWhat is the correlation between a state’s tourism budget and their number of hotel reviews?\\nData Schema\\nA full schema for the data is available in our support documentation.\\nAbout Datafiniti\\nDatafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.\\nWant More?\\nYou can get more data like this by joining Datafiniti or requesting a demo.\",\n",
       " 'Content\\nThe Human Development Index (HDI) is a summary measure of achievements in key dimensions of human development: a long and healthy life, access to knowledge, and a decent standard of living. The HDI is the geometric mean of normalized indices for each of the three dimensions. The health dimension is assessed by life expectancy at birth, the education dimension is measured by mean of years of education for adults aged 25 years and more and expected years of education for children, and the standard of living dimension is measured by gross national income per capita. The Inequality-Adjusted Human Development Index (IHDI) adjusts the HDI for inequality in the distribution of each dimension across the population.\\nThe Gender Development Index (GDI) measures gender inequalities in achievement in three basic dimensions of human development: health, measured by female and male life expectancy at birth; education, measured by female and male expected years of education for children and female and male mean years of education for adults ages 25 and older; and command over economic resources, measured by female and male estimated earned income. The Gender Inequality Index (GII) reflects gender-based disadvantage in three dimensions—reproductive health, empowerment, and the labour market—for as many countries as data of reasonable quality allow. It shows the loss in potential human development due to inequality between female and male achievements in these dimensions.\\nThe Multidimensional Poverty Index (MPI) identifies multiple deprivations at the household level in education, health, and standard of living as indicators of poverty. It uses micro data from household surveys, and — unlike the IHDI — all the indicators needed to construct the measure must come from the same survey.',\n",
       " 'Context\\nThe ecological footprint measures the ecological assets that a given population requires to produce the natural resources it consumes (including plant-based food and fiber products, livestock and fish products, timber and other forest products, space for urban infrastructure) and to absorb its waste, especially carbon emissions. The footprint tracks the use of six categories of productive surface areas: cropland, grazing land, fishing grounds, built-up (or urban) land, forest area, and carbon demand on land.\\nA nation’s biocapacity represents the productivity of its ecological assets, including cropland, grazing land, forest land, fishing grounds, and built-up land. These areas, especially if left unharvested, can also absorb much of the waste we generate, especially our carbon emissions.\\nBoth the ecological footprint and biocapacity are expressed in global hectares — globally comparable, standardized hectares with world average productivity.\\nIf a population’s ecological footprint exceeds the region’s biocapacity, that region runs an ecological deficit. Its demand for the goods and services that its land and seas can provide — fruits and vegetables, meat, fish, wood, cotton for clothing, and carbon dioxide absorption — exceeds what the region’s ecosystems can renew. A region in ecological deficit meets demand by importing, liquidating its own ecological assets (such as overfishing), and/or emitting carbon dioxide into the atmosphere. If a region’s biocapacity exceeds its ecological footprint, it has an ecological reserve.\\nAcknowledgements\\nThe ecological footprint measure was conceived by Mathis Wackernagel and William Rees at the University of British Columbia. Ecological footprint data was provided by the Global Footprint Network.\\nInspiration\\nIs your country running an ecological deficit, consuming more resources than it can produce per year? Which countries have the greatest ecological deficits or reserves? Do they consume less or produce more than the average country? When will Earth Overshoot Day, the day on the calendar when humanity has used one year of natural resources, occur in 2017?',\n",
       " 'Context\\nComing Soon\\nContent\\nComing Soon\\nAcknowledgements\\nThis data is taken from coinmarketcap and it is free to use the data. https://coinmarketcap.com/\\nWarning\\n実際の取引にこの情報を使うときは十分ご注意ください。弊社およびコミュニティメンバーは損失の責任を取ることができません。',\n",
       " \"ZIP Code data show selected income and tax items classified by State, ZIP Code, and size of adjusted gross income. Data are based on individual income tax returns filed with the IRS. The data include items, such as:\\nNumber of returns, which approximates the number of households\\nNumber of personal exemptions, which approximates the population\\nAdjusted gross income\\nWages and salaries\\nDividends before exclusion\\nInterest received\\nContent\\nFor details of the exact fields available, please see the field_definitions.csv. Please note that the exact fields available can change from year to year, this definitions file was generated by retaining only the most recent year's entry from the years which had pdf manuals. The associated IRS form numbers are the most likely to change over time.\\nAcknowledgements\\nThis data was generated by the Internal Revenue Service.\",\n",
       " 'Description\\nOver 8 million GitHub issue titles and descriptions from 2017. Prepared from instructions at How To Create Data Products That Are Magical Using Sequence-to-Sequence Models.\\nOriginal Source\\nThe data was adapted from GitHub data accessible from GitHub Archive. The constructocat image is from https://octodex.github.com/constructocat-v2.\\nLicense\\nMIT License\\nCopyright (c) 2018 David Shinn\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.',\n",
       " 'Context\\nFavicons are the (usually tiny) image files that browsers may use to represent websites in tabs, in the URL bar, or for bookmarks. Kaggle, for example, uses an image of a blue lowercase \"k\" as its favicon. This dataset contains about 360,000 favicons from popular websites.\\nContent and Acknowledgements\\nThese favicons were scraped in July 2016. I wrote a crawler that went through Alexa\\'s top 1 million sites, and made a request for \\'favicon.ico\\' at the site root. If I got a 200 response code, I saved the result as ${site_url}.ico. For domains that were identical but for the TLD (e.g. google.com, google.ca, google.jp...), I scraped only one favicon. My scraping/cleaning code is on GitHub here.\\nOf 1m sites crawled, 540k responded with a 200 code. The dataset has 360k images, which were the remains after filtering out:\\nempty files (-140k)\\nnon-image files, according to the file command (-40k). These mostly had type HTML, ASCII, or UTF-*.\\ncorrupt/malformed image files - i.e. those that were sufficiently messed up that ImageMagick failed to parse them. (-1k)\\nThe remaining files are exactly as I received them from the site. They are mostly ICO files, with the most common sizes being 16x16, 32x32, and 48x48. But there\\'s a long tail of more exotic formats and sizes (there is at least one person living among us who thought that 88x31 was a fine size for a favicon).\\nThe favicon files are divided among 6 zip files, full-0.zip, full-1.zip... full-5.zip. (If you wish to download the full dataset as a single tarball, you can do so from the Internet Archive)\\nfavicon_metadata.csv is a csv file with one row per favicon in the dataset. The split_index says which of the zip files the image landed in. For an example of loading and interacting with particular favicons in a kernel context, check out the Favicon helper functions kernel.\\nAs mentioned above, the full dataset is a dog\\'s breakfast of different file formats and dimensions. I\\'ve created \\'standardized\\' subsets of the data that may be easier to work with (particularly for machine learning applications, where it\\'s necessary to have fixed dimensions).\\n16_16.tar.gz is a tarball containing all 16x16 favicons in the dataset, converted to PNG. It has 290k images. ICO is a container format, and many of the ico files in the raw dataset contain several versions of the same favicon at different resolutions. 16x16 favicons that were stuffed together in an ICO file with images of other sizes are included in this set. But I did no resizing - if a favicon has no \\'native\\' 16x16 version, it isn\\'t in this set.\\n16_16_distinct.tar.gz is identical to the above, but with 70k duplicate or near-duplicate images removed. There are a small number of commonly repeated favicons like the Blogger \"B\" that occur thousands of times, which could be an annoyance depending on the use case - e.g. a generative model might get stuck in a local maximum of spitting out Blogger Bs.\\nAlexa\\'s top 1-million list includes \\'adult\\' sites, so some URLs and favicons may be NSFW or offensive. (It\\'s pretty hard to make a credible depiction of nudity in 256 pixels, but there are some occasional attempts.)\\nInspiration\\nI hope this dataset might be especially useful for small-scale deep learning experiments. Scaling photographs down to 16x16 would render many of them unintelligible, but these favicons were born tiny. The 16_16 fold has more instances than MNIST, and the images are even smaller! (Though, unlike MNIST, most of the images in this dataset are not grayscale.)\\nIf you liked this, you should also check out the recently released Large Logo Dataset. They\\'ve currently made available 550k favicons resized to 32x32. Their data was collected more recently, and their scraping process was more robust, so their dataset should probably be preferred (though you might still want to use this one if you need the raw favicon files, or if you prefer to use 16x16 non-resized images).',\n",
       " 'Context\\nSome camera enthusiast went and described 1,000 cameras based on 13 properties!\\nContent\\nRow one describes the datatype for each column and can probably be removed.\\nThe 13 properties of each camera:\\nModel\\nRelease date\\nMax resolution\\nLow resolution\\nEffective pixels\\nZoom wide (W)\\nZoom tele (T)\\nNormal focus range\\nMacro focus range\\nStorage included\\nWeight (inc. batteries)\\nDimensions\\nPrice\\nAcknowledgements\\nThese datasets have been gathered and cleaned up by Petra Isenberg, Pierre Dragicevic and Yvonne Jansen. The original source can be found here.\\nThis dataset has been converted to CSV.',\n",
       " \"Context:\\nAs the price of installing solar has gotten less expensive, more homeowners are turning to it as a possible option for decreasing their energy bill. We want to make installing solar panels easy and understandable for anyone. Project Sunroof puts Google's expansive data in mapping and computing resources to use, helping calculate the best solar plan for you.\\nContent:\\nSee metadata for indepth description. Data is at census-tract level. Project Sunroof computes how much sunlight hits your roof in a year. It takes into account: Google's database of imagery and maps 3D modeling of your roof Shadows cast by nearby structures and trees All possible sun positions over the course of a year Historical cloud and temperature patterns that might affect solar energy production\\nAcknowledgements:\\nData was compiled by Google Project Sunroof. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too.\\nInspiration:\\nWhich tracts have the highest potential possible coverage? Carbon offsets?\\nWhich tracts have the highest estimated solar panel utilization? As a percent of carbon offsets?\\nIf you want more energy data, check out 30 Years of European Wind Generation and 30 Years of European Solar Generation.\",\n",
       " 'Context\\nOn the morning of 10 January 2017, Opplysningsrådet for Veitrafikken (OFV), Norwegian road association, held a business breakfast for its member organizations, where they presented the annual presentation under the title \"Car Year 2016. Status and trends\" (Bilåret 2016 – status og trender). Among the highlights for the year, OFV reported all-time-high sales of electric cars, with fully electric and plug-in hybrid cars accounting for 40,2% of all new car sales (compare to 7.4% for Sweden and 3.6% for Denmark). No other country in the world has this level of popularity of battery-equipped vehicles! In November 2016, 12 out of 15 most popular cars sold in Norway were either hybrids of fully electric vehicles with BWM-i3 snapping the title as the most popular car in Norway, ahead of undisputed leader of the last decade VW Golf (including eGolf), according to bilnorge.no. Among 10 most popular cars for the year, OFV reported, there was only one(!) fossil fuel vehicle.\\nOFV makes annual forecast of new passenger car sales. Short summary of their methodology:\\nBased on OFV statistics over several years\\nTaking into account the actual monthly figures for the last four years\\nActual same-month sales for the previous year is combined with the average for the eight previous months, weighed by the month\\'s proportion in a year, adjusted by year\\'s actual sales compared with those of the last year.\\nOFV forecast for 2016 was 157 500 new passenger cars. Actual sales were 154 603 cars. Applying the same model for 2017, OFV forecasts 152 400 new passenger cars to be sold in Norway.\\nContent\\nDataset includes two tables:\\n1) Monthly sales of new passenger cars by make (manufacturer brand) - norway_new_car_sales_by_make.csv\\nYear - year of sales\\nMonth - month of sales\\nMake - car make (e.g. Volkswagen, Toyota, Tesla)\\nQuantity - number of units sold\\nPct - percent share in monthly total\\n2) Monthly summary of top-20 most popular models (by make and model) - norway_new_car_sales_by_model.csv\\nYear - year of sales\\nMonth - month of sales\\nMake - car make (e.g. Volkswagen, Toyota, Tesla)\\nModel - car model (e.g. BMW-i3, Volkswagen Golf, Tesla S75)\\nQuantity - number of units sold\\nPct - percent share in monthly total\\n3) Summary stats for car sales in Norway by month - norway_new_car_sales_by_month.csv\\nYear - year of sales\\nMonth - month of sales\\nQuantity - total number of units sold\\nQuantity_YoY - change YoY in units\\nImport - total number of units imported (used cars)\\nImport_YoY - change YoY in units\\nUsed - total number of units owner changes inside the country (data available from 2012)\\nUsed_YoY - change YoY in units\\nAvg_CO2 - average CO2 emission of all cars sold in a given month (in g/km)\\nBensin_CO2 - average CO2 emission of bensin-fueled cars sold in a given month (in g/km)\\nDiesel_CO2 - average CO2 emission of diesel-fueled cars sold in a given month (in g/km)\\nQuantity_Diesel - number of diesel-fueled cars sold in the country in a given month\\nDiesel_Share - share of diesel cars in total sales (Quantity_Diesel / Quantity)\\nDiesel_Share_LY - share of diesel cars in total sales a year ago\\nQuantity_Hybrid - number of new hybrid cars sold in the country (both PHEV and BV)\\nQuantity_Electric - number of new electric cars sold in the country (zero emission vehicles)\\nImport_Electric - number of used electric cars imported to the country (zero emission vehicles)\\nNote: The numbers on sales of hybrid and electric cars is unavailable prior to 2011.\\nData is complied from monthly tables published on OFV website (example here). Additional datapoints added from summary tables published on dinside.no\\nAcknowledgements\\nOpplysningsrådet for Veitrafikken (OFV) is a politically independent membership organization that works to get politicians and authorities to build safer and more efficient roads in Norway. The organization has about 60 members, representing different types of road users. Members are leading players in road safety, car owner associations, public transportation companies, shippers, car dealers, oil companies, banking, finance and insurance, road builders and general contractors.\\nSite: http://www.ofvas.no and http://www.ofv.no\\nMonthly summary statistics and market news: http://www.dinside.no/emne/bilsalget and http://statistikk.ofv.no/ofv_bilsalg_small.asp\\nDetailed sales per model: http://www.ofvas.no/co2-utslippet/category406.html (using http://www.newocr.com/)\\nInspiration\\n1) How did Norway get here? When did they start on the journey towards electric-powered vehicles and what might have contributed? 2) Did you now that until recently (September 2016), Norway has been second most important market for Tesla Motors (after US)? 3) Can you beat the forecast accuracy of OFV for 2016 and produce a better estimate for 2017?',\n",
       " \"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nTo better understand the imports and exports by India and how it changed in 3 years.\\nContent\\nImport and export data available by principle commodity and country wise for 3 years from Apr'2014 to Mar'2017.\\nColumn Descriptions\\npc_code: Integer, Principal Commodity Code\\npc: String, Principal Commodity Name\\nunit: String, measurement of quantity\\ncountry_code: Integer, country code\\ncountry_name: String, country name\\nquantity: Integer, quantify of export or import\\nvalue: Integer, monetary valeu of the quantity (in million USD)\\nAcknowledgements\\nMinistry of Commerce and Industry, Govt of India has published these datasets in Open Govt Data Platform India portal under Govt. Open Data License - India.\\nInspiration\\nSome of questions I would like to be answered are\\nTop countries by growth percentage.\\nTop commodity by quantity or value.\\nYoY growth of export and import.\",\n",
       " 'Context\\nAccording to The Oregonian hundreds of National Guard armories across the U.S. may have been contaminated with lead from indoor firing ranges. It was reported that areas populated by children under 7 years of age should have less than 40 micrograms of lead per square foot.\\nContent\\nThe Oregonian collected over 23,000 pages of public records following a Freedom Of Information Act request. The dataset covers armory inspections conducted since 2012 and may facilitate investigation of lead contamination in the U.S.\\nAcknowledgements\\nThe data assembly process is described by Melissa Lewis here.\\nInspiration\\nThis dataset can be used to conduct research in the realm of public health. It will be especially useful if 1) you know about health effects of exposure to lead in relatively short terms periods; 2) you are able to find relevant health data to conduct a study on lead poisoning.',\n",
       " 'Context\\nCartolaFC is the most popular fantasy football in Brazil. Before each round of the Brazilian Football League, players choose which athletes they want for their teams, and they score points based on their real-life performances.\\nContent\\nData is divided in 7 kinds of files:\\nAthletes (atletas)\\n\"atleta_id\": id,\\n\"nome\": athlete\\'s full name,\\n\"apelido\": athlete\\'s nickname\\nClubs (clubes)\\n\"id\": id,\\n\"nome\": club\\'s name,\\n\"abreviacao\": name abbreviation,\\n\"slug\": used for some API calls\\nMatches (partidas)\\n\"rodada_id\": current round,\\n\"clube_casa_id\": home team id,\\n\"clube_visitante_id\": away team id,\\n\"clube_casa_posicao\": home team\\'s position on the league,\\n\"clube_visitante_posicao\": away team\\'s position on the league,\\n\"aproveitamento_mandante\": home team\\'s outcome on the last five matches (d: loss, e: draw, v: victory),\\n\"aproveitamento_visitante\": away team\\'s outcome on the last five matches (d: loss, e: draw, v: victory),\\n\"placar_oficial_mandante\": home team\\'s score,\\n\"placar_oficial_visitante\": away team\\'s score,\\n\"partida_data\": match date,\\n\"local\": stadium,\\n\"valida\": match valid for scoring\\nScouts\\n\"atleta_id\": reference to athlete,\\n\"rodada_id\": current round,\\n\"clube_id\": reference to club,\\n\"posicao_id\": reference to position,\\n\"status_id\": reference to status,\\n\"pontos_num\": points scored on current round,\\n\"preco_num\": current price,\\n\"variacao_num\": price variation from previous round,\\n\"media_num\": average points per played round,\\n\"jogos_num\": number of matches played,\\n\"FS\": suffered fouls,\\n\"PE\": missed passes,\\n\"A\": assistances,\\n\"FT\": shots on the post,\\n\"FD\": defended shots,\\n\"FF\": shots off target,\\n\"G\": goals,\\n\"I\": offsides,\\n\"PP\": missed penalties,\\n\"RB\": successful tackes,\\n\"FC\": fouls commited,\\n\"GC\": own goals,\\n\"CA\": yellow cards,\\n\"CV\": red cards,\\n\"SG\": clean sheets (only defenders),\\n\"DD\": difficult defenses (only goalies),\\n\"DP\": defended penalties (only goalies),\\n\"GS\": suffered goals (only goalies)\\nPositions (posicoes)\\n\"id\": id,\\n\"nome\": name,\\n\"abreviacao\": abbreviation\\nStatus\\n\"id\": id,\\n\"nome\": name\\nPoints (pontuacao)\\n\"abreviacao\": abbreviation,\\n\"nome\": name,\\n\"pontuacao\": points earned for respective scout\\nAcknowledgements\\nThe datasets from 2014 to 2016 were taken from here: https://github.com/thevtm/CartolaFCDados.\\nData from 2017 until round 11 was taken from this repo: https://github.com/henriquepgomide/caRtola.\\nFrom 2017 round 12 and on, I\\'ve been extracting the data from CartolaFC\\'s API (which is not officially public).\\nInspiration\\nIt would be interesting to see analyses on which factors make an athlete or team more likely to score points, and also predictive models for future scores.',\n",
       " \"Falls among the elderly is an important health issue. Fall detection and movement tracking are therefore instrumental in addressing this issue. This paper responds to the challenge of classifying different movements as a part of a system designed to fulfill the need for a wearable device to collect data for fall and near-fall analysis. Four different fall trajectories (forward, backward, left and right), three normal activities (standing, walking and lying down) and near-fall situations are identified and detected.\\nFalls are a serious public health problem and possibly life threatening for people in fall risk groups. We develop an automated fall detection system with wearable motion sensor units fitted to the subjects’ body at six different positions. Each unit comprises three tri-axial devices (accelerometer, gyroscope, and magnetometer/compass). Fourteen volunteers perform a standardized set of movements including 20 voluntary falls and 16 activities of daily living (ADLs), resulting in a large dataset with 2520 trials. To reduce the computational complexity of training and testing the classifiers, we focus on the raw data for each sensor in a 4 s time window around the point of peak total acceleration of the waist sensor, and then perform feature extraction and reduction.\\nWe successfully distinguish falls from ADLs using six machine learning techniques (classifiers): the k-nearest neighbor (k-NN) classifier, least squares method (LSM), support vector machines (SVM), Bayesian decision making (BDM), dynamic time warping (DTW), and artificial neural networks (ANNs). We compare the performance and the computational complexity of the classifiers and achieve the best results with the k-NN classifier and LSM, with sensitivity, specificity, and accuracy all above 95%. These classifiers also have acceptable computational requirements for training and testing. Our approach would be applicable in real-world scenarios where data records of indeterminate length, containing multiple activities in sequence, are recorded.\\nIf you are using this dataset don't forget to cite\\nÖzdemir, Ahmet Turan, and Billur Barshan. “Detecting Falls with Wearable Sensors Using Machine Learning Techniques.” Sensors (Basel, Switzerland) 14.6 (2014): 10691–10708. PMC. Web. 23 Apr. 2017.\",\n",
       " 'Context:\\nThe Open Access Series of Imaging Studies (OASIS) is a project aimed at making MRI data sets of the brain freely available to the scientific community. By compiling and freely distributing MRI data sets, we hope to facilitate future discoveries in basic and clinical neuroscience. OASIS is made available by the Washington University Alzheimer’s Disease Research Center, Dr. Randy Buckner at the Howard Hughes Medical Institute (HHMI)( at Harvard University, the Neuroinformatics Research Group (NRG) at Washington University School of Medicine, and the Biomedical Informatics Research Network (BIRN).\\nContent:\\nCross-sectional MRI Data in Young, Middle Aged, Nondemented and Demented Older Adults: This set consists of a cross-sectional collection of 416 subjects aged 18 to 96. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 100 of the included subjects over the age of 60 have been clinically diagnosed with very mild to moderate Alzheimer’s disease (AD). Additionally, a reliability data set is included containing 20 nondemented subjects imaged on a subsequent visit within 90 days of their initial session.\\nLongitudinal MRI Data in Nondemented and Demented Older Adults: This set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate Alzheimer’s disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.\\nAcknowledgements:\\nWhen publishing findings that benefit from OASIS data, please include the following grant numbers in the acknowledgements section and in the associated Pubmed Central submission: P50 AG05681, P01 AG03991, R01 AG021910, P20 MH071616, U24 RR0213\\nInspiration:\\nCan you predict dementia? Alzheimer’s?',\n",
       " 'Project Description:\\n1) Data Background\\nIn the Data Mining class, we had the opportunity to analyze data by performing data mining algorithms to a dataset. Our dataset is from Office of Foreign Labor Certification (OFLC). OFLC is a division of the U.S. Department of Labor. The main duty of OFLC is to assist the Secretary of Labor to enforce part of the Immigration and Nationality Act (INA), which requires certain labor conditions exist before employers can hire foreign workers. H-1B is a visa category in the United States of America under the INA, section 101(a)(15)(H) which allows U.S. employers to employ foreign workers. The first step employer must take to hire a foreign worker is to file the Labor Condition Application. In this project, we will analyze the data from the Labor Condition Application.\\n1.1) Introduction to H1B Dataset\\nThe H-1B Dataset selected for this project contains data from employer’s Labor Condition Application and the case certification determinations processed by the Office of Foreign Labor Certification (OFLC) where the date of the determination was issues on or after October 1, 2016 and on or before June 30, 2017.\\nThe Labor Condition Application (LCA) is a document that a perspective H-1B employer files with U.S. Department of Labor Employment and Training Administration (DOLETA) when it seeks to employ non-immigrant workers at a specific job occupation in an area of intended employment for not more than three years.\\n1.2) Goal of the Project\\nOur goal for this project is to predict the case status of an application submitted by the employer to hire non-immigrant workers under the H-1B visa program. Employer can hire non-immigrant workers only after their LCA petition is approved. The approved LCA petition is then submitted as part of the Petition for a Non-immigrant Worker application for work authorizations for H-1B visa status.\\nWe want to uncover insights that can help employers understand the process of getting their LCA approved. We will use WEKA software to run data mining algorithms to understand the relationship between attributes and the target variable.\\n2)Dataset Information:\\na) Source: Office of Foreign Labor Certification, U.S. Department of Labor Employment and Training Administration\\nb) List Link: https://www.foreignlaborcert.doleta.gov/performancedata.cfm\\nc) Dataset Type: Record – Transaction Data\\nd) Number of Attributes: 40\\ne) Number of Instances: 528,147\\nf) Date Created: July 2017\\n3) Attribute List:\\nThe detailed description of each attribute below is given in the Record Layout file available in the zip folder H1B Disclosure Dataset Files.\\nThe H-1B dataset from OFLC contained 40 attributes and 528,147 instances. The attributes are in the table below. The attributes highlighted bold were removed during the data cleaning process.\\n1) CASE_NUMBER\\n2)CASE_SUBMITTED\\n3)DECISION_DATE\\n4)VISA_CLASS\\n5)EMPLOYMENT_START_DATE\\n6)EMPLOYMENT_END_DATE\\n7)EMPLOYER_NAME\\n8)EMPLOYER_ADDRESS\\n9)EMPLOYER_CITY\\n10)EMPLOYER_STATE\\n11)EMPLOYER_POSTAL_CODE\\n12)EMPLOYER_COUNTRY\\n13)EMPLOYER_PROVINCE\\n14)EMPLOYER_PHONE\\n15)EMPLOYER_PHONE_EXT\\n16)AGENT_ATTORNEY_NAME\\n17)AGENT_ATTORNEY_CITY\\n18)AGENT_ATTORNEY_STATE\\n19)JOB_TITLE\\n20)SOC_CODE\\n21)SOC_NAME\\n22)NAICS_CODE\\n23)TOTAL_WORKERS\\n24)FULL_TIME_POSITION\\n25)PREVAILING_WAGE\\n26)PW_UNIT_OF_PAY\\n27)PW_SOURCE\\n28)PW_SOURCE_YEAR\\n29)PW_SOURCE_OTHER\\n30)WAGE_RATE_OF_PAY_FROM\\n31)WAGE_RATE_OF_PAY_TO\\n32)WAGE_UNIT_OF_PAY\\n33)H-1B_DEPENDENT\\n34) WILLFUL_VIOLATOR\\n35) WORKSITE_CITY\\n36)WORKSITE_COUNTY\\n37)WORKSITE_STATE\\n38)WORKSITE_POSTAL_CODE\\n39)ORIGINAL_CERT_DATE\\n40)CASE_STATUS* - __Class Attribute - To be predicted\\n3.1) Class Attribute\\nFor the H-1B Dataset our class attribute is ‘CASE_STATUS’. There are 4 categories of Case Status. The values of Case_Status attributes are:\\n1) Certified\\n2) Certified_Withdrawn\\n3) Withdrawn\\n4) Denied\\nCertified means the LCA of an employer was approved. Certified Withdrawn means the case was withdrawn after it was certified by OFLC. Withdrawn means the case was withdrawn by the employer. Denied means the case was denied OFLC.',\n",
       " 'Context\\nInvasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide.\\nContent\\nThe original dataset consisted of 162 whole mount slide images of Breast Cancer (BCa) specimens scanned at 40x. From that, 277,524 patches of size 50 x 50 were extracted (198,738 IDC negative and 78,786 IDC positive). Each patch’s file name is of the format: u_xX_yY_classC.png — > example 10253_idx5_x1351_y1101_class0.png . Where u is the patient ID (10253_idx5), X is the x-coordinate of where this patch was cropped from, Y is the y-coordinate of where this patch was cropped from, and C indicates the class where 0 is non-IDC and 1 is IDC.\\nAcknowledgements\\nThe original files are located here: http://gleason.case.edu/webdata/jpi-dl-tutorial/IDC_regular_ps50_idx5.zip Citation: https://www.ncbi.nlm.nih.gov/pubmed/27563488 and http://spie.org/Publications/Proceedings/Paper/10.1117/12.2043872\\nInspiration\\nBreast cancer is the most common form of cancer in women, and invasive ductal carcinoma (IDC) is the most common form of breast cancer. Accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error.',\n",
       " 'Context\\nPatients with Liver disease have been continuously increasing because of excessive consumption of alcohol, inhale of harmful gases, intake of contaminated food, pickles and drugs. This dataset was used to evaluate prediction algorithms in an effort to reduce burden on doctors.\\nContent\\nThis data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The \"Dataset\" column is a class label used to divide groups into liver patient (liver disease) or not (no disease). This data set contains 441 male patient records and 142 female patient records.\\nAny patient whose age exceeded 89 is listed as being of age \"90\".\\nColumns:\\nAge of the patient\\nGender of the patient\\nTotal Bilirubin\\nDirect Bilirubin\\nAlkaline Phosphotase\\nAlamine Aminotransferase\\nAspartate Aminotransferase\\nTotal Protiens\\nAlbumin\\nAlbumin and Globulin Ratio\\nDataset: field used to split the data into two sets (patient with liver disease, or no disease)\\nAcknowledgements\\nThis dataset was downloaded from the UCI ML Repository:\\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\\nInspiration\\nUse these patient records to determine which patients have liver disease and which ones do not.',\n",
       " 'Context\\nThe Affordable Care Act (ACA) is the name for the comprehensive health care reform law and its amendments which addresses health insurance coverage, health care costs, and preventive care. The law was enacted in two parts: The Patient Protection and Affordable Care Act was signed into law on March 23, 2010 by President Barack Obama and was amended by the Health Care and Education Reconciliation Act on March 30, 2010.\\nContent\\nThis dataset provides health insurance coverage data for each state and the nation as a whole, including variables such as the uninsured rates before and after Obamacare, estimates of individuals covered by employer and marketplace healthcare plans, and enrollment in Medicare and Medicaid programs.\\nAcknowledgements\\nThe health insurance coverage data was compiled from the US Department of Health and Human Services and US Census Bureau.\\nInspiration\\nHow has the Affordable Care Act changed the rate of citizens with health insurance coverage? Which states observed the greatest decline in their uninsured rate? Did those states expand Medicaid program coverage and/or implement a health insurance marketplace? What do you predict will happen to the nationwide uninsured rate in the next five years?',\n",
       " 'Content\\nThe United States census count (also known as the Decennial Census of Population and Housing) is a count of every resident of the US. The census occurs every 10 years and is conducted by the United States Census Bureau. Census data is publicly available through the census website, but much of the data is available in summarized data and graphs. The raw data is often difficult to obtain, is typically divided by region, and it must be processed and combined to provide information about the nation as a whole. The United States census dataset includes nationwide population counts from the 2000 and 2010 censuses. Data is broken out by gender, age and location using zip code tabular areas (ZCTAs) and GEOIDs. ZCTAs are generalized representations of zip codes, and often, though not always, are the same as the zip code for an area. GEOIDs are numeric codes that uniquely identify all administrative, legal, and statistical geographic areas for which the Census Bureau tabulates data. GEOIDs are useful for correlating census data with other censuses and surveys.\\nDataset Description\\n| geo_id | STRING | Geo code | |-------------|---------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | minimum_age | INTEGER | The minimum age in the age range. If null, this indicates the row as a total for male, female, or overall population. | | maximum_age | INTEGER | The maximum age in the age range. If null, this indicates the row as having no maximum (such as 85 and over) or the row is a total of the male, female, or overall population. | | gender | STRING | male or female. If empty, the row is a total population summary. | | population | INTEGER | The total count of the population for this segment. |\\nAcknowledgements\\nThis dataset was created by the United States Census Bureau.\\nUse this dataset with BigQuery\\nYou can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/international-census.',\n",
       " \"Context\\nAn asteroid's orbit is computed by finding the elliptical path about the sun that best fits the available observations of the object. That is, the object's computed path about the sun is adjusted until the predictions of where the asteroid should have appeared in the sky at several observed times match the positions where the object was actually observed to be at those same times. As more and more observations are used to further improve an object's orbit, we become more and more confident in our knowledge of where the object will be in the future.\\nWhen the discovery of a new near Earth asteroid is announced by the Minor Planet Center, Sentry automatically prioritizes the object for an impact risk analysis. If the prioritization analysis indicates that the asteroid cannot pass near the Earth or that its orbit is very well determined, the computationally intensive nonlinear search for potential impacts is not pursued. If, on the other hand, a search is deemed necessary then the object is added to a queue of objects awaiting analysis. Its position in the queue is determined by the estimated likelihood that potential impacts may be found.\\nContent\\nSentry is a highly automated collision monitoring system that continually scans the most current asteroid catalog for possibilities of future impact with Earth over the next 100 years. This dataset includes the Sentry system's list of possible asteroid impacts with Earth and their probability, in addition to a list of all known near Earth asteroids and their characteristics.\\nAcknowledgements\\nThe asteroid orbit and impact risk data was collected by NASA's Near Earth Object Program at the Jet Propulsion Laboratory (California Institute of Technology).\\nInspiration\\nDuring which year is Earth at the highest risk of an asteroid impact? How do asteroid impact predictions change over time? Which possible asteroid impact would be the most devastating, given the asteroid's size and speed?\",\n",
       " 'Context:\\nThe Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQS). Data collection agencies report their data to the EPA via this system and it calculates several types of aggregate (summary) data for EPA internal use.\\nContent:\\nField descriptions:\\nState Code: The FIPS code of the state in which the monitor resides.\\nCounty Code: The FIPS code of the county in which the monitor resides.\\nSite Num:A unique number within the county identifying the site.\\nParameter Code: The AQS code corresponding to the parameter measured by the monitor.\\nPOC: This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site.\\nLatitude: The monitoring site’s angular distance north of the equator measured in decimal degrees.\\nLongitude: The monitoring site’s angular distance east of the prime meridian measured in decimal degrees.\\nDatum: The Datum associated with the Latitude and Longitude measures.\\nParameter Name: The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants.\\nSample Duration: The length of time that air passes through the monitoring device before it is analyzed (measured). So, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).\\nPollutant Standard:A description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.)\\nMetric Used: The base metric used in the calculation of the aggregate statistics presented in the remainder of the row. For example, if this is Daily Maximum, then the value in the Mean column is the mean of the daily maximums.\\nMethod Name: A short description of the processes, equipment, and protocols used in gathering and measuring the sample.\\nYear: The year the annual summary data represents.\\nUnits of Measure: The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations.\\nEvent Type: Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question, the data will have multiple records for each monitor.\\nObservation Count: The number of observations (samples) taken during the year.\\nObservation Percent: The percent representing the number of observations taken with respect to the number scheduled to be taken during the year. This is only calculated for monitors where measurements are required (e.g., only certain parameters).\\nCompleteness Indicator: An indication of whether the regulatory data completeness criteria for valid summary data have been met by the monitor for the year. Y means yes, N means no or that there are no regulatory completeness criteria for the parameter.\\nValid Day Count: The number of days during the year where the daily monitoring criteria were met, if the calculation of the summaries is based on valid days.\\nRequired Day Count: The number of days during the year which the monitor was scheduled to take samples if measurements are required.\\nExceptional Data Count: The number of data points in the annual data set affected by exceptional air quality events (things outside the norm that affect air quality).\\nNull Data Count: The count of scheduled samples when no data was collected and the reason for no data was reported.\\nPrimary Exceedance Count: The number of samples during the year that exceeded the primary air quality standard.\\nSecondary Exceedance Count: The number of samples during the year that exceeded the secondary air quality standard.\\nCertification Indicator: An indication whether the completeness and accuracy of the information on the annual summary record has been certified by the submitter. Certified means the submitter has certified the data (due May 01 the year after collection). Certification not required means that the parameter does not require certification or the deadline has not yet passed. Uncertified (past due) means that certification is required but is overdue. Requested but not yet concurred means the submitter has completed the process, but EPA has not yet acted to certify the data. Requested but denied means the submitter has completed the process, but EPA has denied the request for cause. Was certified but data changed means the data was certified but data was replaced and the process has not been repeated.\\nNum Obs Below MDL: The number of samples reported during the year that were below the method detection limit (MDL) for the monitoring instrument. Sometimes these values are replaced by 1/2 the MDL in summary calculations.\\nArithmetic Mean: The average (arithmetic mean) value for the year.\\nArithmetic Standard Dev: The standard deviation about the mean of the values for the year.\\n1st Max Value: The highest value for the year.\\n1st Max DateTime: The date and time (on a 24-hour clock) when the highest value for the year (the previous field) was taken.\\n2nd Max Value: The second highest value for the year.\\n2nd Max DateTime: The date and time (on a 24-hour clock) when the second highest value for the year (the previous field) was taken.\\n3rd Max Value: The third highest value for the year.\\n3rd Max DateTime: The date and time (on a 24-hour clock) when the third highest value for the year (the previous field) was taken.\\n4th Max Value: The fourth highest value for the year.\\n4th Max DateTime: The date and time (on a 24-hour clock) when the fourth highest value for the year (the previous field) was taken.\\n1st Max Non Overlapping Value: For 8-hour CO averages, the highest value of the year.\\n1st NO Max DateTime: The date and time (on a 24-hour clock) when the first maximum non overlapping value for the year (the previous field) was taken.\\n2nd Max Non Overlapping Value: For 8-hour CO averages, the second highest value of the year that does not share any hours with the 8-hour period of the first max non overlapping value.\\n2nd NO Max DateTime: The date and time (on a 24-hour clock) when the second maximum non overlapping value for the year (the previous field) was taken.\\n99th Percentile: The value from this monitor for which 99 per cent of the rest of the measured values for the year are equal to or less than.\\n98th Percentile: The value from this monitor for which 98 per cent of the rest of the measured values for the year are equal to or less than.\\n95th Percentile: The value from this monitor for which 95 per cent of the rest of the measured values for the year are equal to or less than.\\n90th Percentile: The value from this monitor for which 90 per cent of the rest of the measured values for the year are equal to or less than.\\n75th Percentile: The value from this monitor for which 75 per cent of the rest of the measured values for the year are equal to or less than.\\n50th Percentile: The value from this monitor for which 50 per cent of the rest of the measured values for the year are equal to or less than (i.e., the median).\\n10th Percentile: The value from this monitor for which 10 per cent of the rest of the measured values for the year are equal to or less than.\\nLocal Site Name: The name of the site (if any) given by the State, local, or tribal air pollution control agency that operates it.\\nAddress: The approximate street address of the monitoring site.\\nState Name: The name of the state where the monitoring site is located.\\nCounty Name: The name of the county where the monitoring site is located.\\nCity Name: The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas.\\nCBSA Name: The name of the core bases statistical area (metropolitan area) where the monitoring site is located.\\nDate of Last Change: The date the last time any numeric values in this record were updated in the AQS data system.\\nAcknowledgements:\\nThese data come from the EPA. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on Google BigQuery, too: https://cloud.google.com/bigquery/public-data/epa\\nInspiration:\\nWithin these data are tons of ways for you to learn about air pollution and how it can affect our health and environment. You can also compare key air emissions to gross domestic product, vehicle miles traveled, population, and energy consumption back to 1970. Best of all, you can check out air trends where you live!',\n",
       " 'Content\\nThis report lists each failure of a commercial bank, savings association, and savings bank since the establishment of the FDIC in 1933. Each record includes the institution name and FIN number, institution and charter types, location of headquarters (city and state), effective date, insurance fund and certificate number, failure transaction type, total deposits and total assets last reported prior to failure (in thousands of dollars), and the estimated cost of resolution. Data on estimated losses are not available for FDIC insured failures prior to 1986 or for FSLIC insured failures from 1934-88.\\nAcknowledgements\\nThe bank failure report was downloaded from the FDIC website.\\nInspiration\\nWhat type of banking institution is the most likely to fail? How have bank failure rates changed over time? What commercial bank failure cost the federal government the most to resolve?',\n",
       " \"Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.\\nAdversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.\\nTo accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Examples and Defenses within the NIPS 2017 competition track. This dataset contains the development images for this competition.\\nThe competition on Adversarial Examples and Defenses consist of three sub-competitions:\\nNon-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.\\nTargeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.\\nDefense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.\\nIn each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.\",\n",
       " 'The Price Paid Data includes information on all registered property sales in England and Wales that are sold for full market value. Address details have been truncated to the town/city level.\\nYou might also find the HM Land Registry transaction records to be a useful supplement to this dataset: https://www.kaggle.com/hm-land-registry/uk-land-registry-transactions\\nThe available fields are as follows:\\nTransaction unique identifier A reference number which is generated automatically recording each published sale. The number is unique and will change each time a sale is recorded.\\nPrice Sale price stated on the transfer deed.\\nDate of Transfer Date when the sale was completed, as stated on the transfer deed.\\nProperty Type D = Detached, S = Semi-Detached, T = Terraced, F = Flats/Maisonettes, O = Other Note that: - we only record the above categories to describe property type, we do not separately identify bungalows. - end-of-terrace properties are included in the Terraced category above. - ‘Other’ is only valid where the transaction relates to a property type that is not covered by existing values.\\nOld/New Indicates the age of the property and applies to all price paid transactions, residential and non-residential. Y = a newly built property, N = an established residential building\\nDuration Relates to the tenure: F = Freehold, L= Leasehold etc. Note that HM Land Registry does not record leases of 7 years or less in the Price Paid Dataset.\\nTown/City\\nDistrict\\nCounty\\nPPD Category Type Indicates the type of Price Paid transaction. A = Standard Price Paid entry, includes single residential property sold for full market value. B = Additional Price Paid entry including transfers under a power of sale/repossessions, buy-to-lets (where they can be identified by a Mortgage) and transfers to non-private individuals. Note that category B does not separately identify the transaction types stated. HM Land Registry has been collecting information on Category A transactions from January 1995. Category B transactions were identified from October 2013.\\nRecord Status - monthly file only Indicates additions, changes and deletions to the records.(see guide below). A = Addition C = Change D = Delete.\\nNote that where a transaction changes category type due to misallocation (as above) it will be deleted from the original category type and added to the correct category with a new transaction unique identifier.\\nThis data was kindly released by HM Land Registry under the Open Government License 3.0. You can find their current release here.\\nData produced by HM Land Registry © Crown copyright 2017.',\n",
       " 'Context\\nHumans (and many other animals) have the ability to reduce or suppress their brains\\' responses to sensory consequences that are a result of their own actions. The nervous system accomplishes this with a corollary discharge forward model system in which an \"efference copy\" of an impending motor plan is transmitted from motor to sensory cortex where it generates a \"corollary discharge\" representation of the expected sensory consequences of the imminent motor act. For example, when you move your eyes from left to right, your brain knows the environment is not shifting. When you speak, your auditory cortex has a reduced response to the expected sound of your voice.\\nSchizophrenia is a chronic mental illness that affects about 1% of people across the globe. One possible explanation for some of the symptoms of schizophrenia is that one or more problems with the corollary discharge process in the nervous system makes it difficult for patients to differentiate between internally and externally generated stimuli. Therefore, studying this process and its relationship to symptoms in the illness might allow us to better understand abnormal brain processes in patients with this diagnosis.\\nIn a previously published EEG experiment (full report), we used a simple button pressing task in which subjects either (1) pressed a button to immediately generated a tone, (2) passively listened to the same tone, or (3) pressed a button without generating a tone to study the corollary discharge in people with schizophrenia and comparison controls. We found that comparison controls suppressed the N100, a negative deflection in EEG brain wave 100 milliseconds after the onset of a sound, when they pressed a button to generate a tone compared to passive playback, but patients with schizophrenia did not. This data set is a larger sample replication of that previous study. Specifically, EEG data from 22 controls and 36 patients with schizophrenia have been combined with 10 controls and 13 patients from the previous report.\\nMethods\\nDue to the size of the raw EEG data, some pre-processing was done prior to upload. EEG data acquisition parameters and the experimental task was identical to that described in our paper. However, pre-processing differed. All individual subject data had at least the following data processing steps applied, in this order:\\nRe-reference to averaged ear lobes\\n0.1 Hz high-pass filter\\nInterpolation of outlier channels in the continuous EEG data (outliers defined as in this paper)\\nChop continous data into single trial epochs 1.5 seconds before and after task events (3s total)\\nBaseline correction -100 to 0ms\\nCanonical correlation analysis to remove muscle and high-frequency white noise artifacts\\nRejection of outlier single trials (outliers defined as in this paper)\\nRemoval of outlier components from a spatial independent components analysis (outliers defined as in this paper)\\nInterpolation of outlier channels within single trials (outliers defined as in this paper)\\nDerived data includes event-related potential (ERP) averages for 9 electrode sites analyzed in our previous report, including Fz, FCz, Cz, FC3, FC4, C3, C4, CP3, CP4 (pictured below):\\nThe ERPs are calculated by averaging across trials for every sample in the time series, separately for each subject, electrode, and condition.\\nContent\\nThe single trial data from all 64 channels are too large to be uploaded for all 81 subjects, but those interested in that type of data will find one subject (subject 21 in 21.csv) among the data files. This includes all his data after the pre-processing step 9 listed above.\\nFor those interested in comparing patients with schizophrenia to control subjects, the ERPdata.csv file contains the averaged, ERP time series for all subjects, conditions, and the 9 electrodes mentioned above. These data, along with the subject information in demographic.csv could be used to replicate the analyses in our prior report.\\nFor those interested in single trial categorization/prediction like the grasp-and-lift challenge or the face decoding challenge, the mergedTrialData.csv contains summary measurements from nearly 24,000 individual trials (all subjects and conditions are included).\\nAcknowledgements\\nFunding for the study procedures, initial analyses and publications came from the National Institute of Mental Health. Please see grant info for additional details, and cite this NIMH project number (R01MH058262) in any work related to these data. All study participants gave written, informed consent to participate in this study, which received Institutional Review Board approval.',\n",
       " 'Mexican cuisine is often the best food option is southern California. And the burrito is the hallmark of delicious taco shop food: tasty, cheap, and filling. Appropriately, an effort was launched to critique burritos across the county and make this data open to the lay burrito consumer. At this time, the data set contains ratings from over 200 burritos from around 50 restaurants.\\nThere are 10 core dimensions of the San Diego burrito. * Volume * Tortilla quality *Temperature * Meat quality * Non-meat filling quality * Meat-to-filling ratio * Uniformity * Salsa quality * Flavor synergy * Wrap integrity\\nAll of these measures (except for Volume) are rated on a scale from 0 to 5, 0 being terrible, and 5 being optimal. Other information available for each burrito includes an overall rating, cost, Yelp rating of the restaurant, and more.\\nMore information about the data set, as well as a link to the continuously updated dataset, can be found here.',\n",
       " 'Context\\nMetal-Archives.com (MA for short) is an encyclopedia website which includes information of nearly all heavy bands and albums on the earth. This information is collected and submitted by metalheads from all around the world. This dataset includes all \"death metal\" bands and albums on MA (by Nov. 2016). It\\'s the search result by genre key word \"death metal\" which includes all bands which contain phrase \"death metal\" in their genre. (e.g. \"technical death metal\", \"brutal death metal\", \"melodic death metal\" ... )\\nThe banner of dataset is the cover art of New Jersey-based death metal band Disma\\'s debut full-length album \"Towards the Megalith\" (2011, Profound Lore Records). It\\'s beautiful but not quite typical for this dataset\\'s theme. But 1900+ resolution picture about death metal is rare, so I\\'ve chosen this one.\\nContent\\nThere are three csv files included in the dataset:\\nbands.csv contains 37,723 bands. Each record is consisted of 8 fields:\\nid: sequential integer id.\\nname: the band\\'s name which can contains non-english character, punctuations, numbers and other weird characters.\\ncountry: country the band is from. \"International\" means the members of the band are from multiple countries.\\nstatus: band\\'s current activity-status: \\'Unknown\\', \\'Split-up\\', \\'Active\\', \\'Changed name\\', \\'On hold\\' and \\'Disputed\\'.\\nfrom_in: the year in which the band formed.\\ngenre: the description of the band\\'s genre. It\\'s irregular, so you\\'d better not deem it as category but short text.\\ntheme: the description of the band\\'s lyric theme.\\nactive: the time-span in which the band is active.\\nalbums.csv contains 28,069 albums. Each record is consisted of 4 fields:\\nid: sequential integer id.\\nband: foreign key to band\\'s id in bands.csv.\\ntitle: album title.\\nyear: the album\\'s release year.\\nreviews.csv contains 21,510 reviews. Each record is consisted of 5 fields:\\nid: sequential integer id.\\nalbum: foreign key to album\\'s id in albums.csv.\\ntitle: the review\\'s title.\\nscore: the score for that album. Float number from 0.0 to 1.0 (from negative to positive).\\ncontent: the review\\'s text.\\nNotice\\nThis dataset only contains full-length studio albums (excluding EPs, singles, live albums, split albums and others).\\nAll commas in dataset are replaced by \"|\" to make comma available for fields separator.\\nNA value is \"N/A\".\\nAll text is in utf-8 encoding.\\nAcknowledgements\\nMetal-Archives! \\\\m/\\nInspiration\\nStatistical analysis\\nEmotional analysis (reviews)\\nGenre classification (by NLP of title and/or theme)\\nScore prediction (by NLP of review\\'s content)',\n",
       " \"Context\\nThese datasets were created for a college course work. It was an opportunity to test Deep Learning capabilities for computer vision in a very restricted problem.\\nThe idea is to explore a classification problem for a single coin and a regression problem for a group of coins, trying to count how much money they sum. You can see my initial approach here.\\nContent\\nThere are two datasets. One for classification and another for regression. The first contains 3000 images with just a single coin in it. The second contains the first one and another 3000 images with two or more coins present in each example.\\nIn the classification problem there are five classes: 5, 10, 25, 50 and 100 cents. For regression, there are examples from 5 to 175 cents.\\nEvery file contains its value in money as its filename. For example: 5_1477146780.jpg contains a single 5 cents coin. If there's only one coin, it's the coin value itself. In the 80_1477851090.jpg you're going to find enough coins to sum 80 cents. An example (90_1477854720.jpg):\\nDifferent coins from each type were used to make it more interesting. My fingers appear in some images!\\nI tried to keep the distance, illumination and background constant for all of them, but some differences will be noticed, specially in the illumination. Changing the coin position has an great impact in how the light reflects over it. The structure used to take the pictures (in fact, a second light source was added):\\nInspiration\\nA model that can sum coins and tell how much money we have in a group of coins could be used for people with vision disabilities. Can Deep Learning count, classify and sum in a single model? Should we split the problem into segmentation, classification and then sum it? What can be done with this amount of data? Can we achieve a good generalization and predict sums beyond the dataset greatest value?\\nCitation\\nIf you want to use this dataset for any purpose contemplated by its license, add the reference:\\nMONEDA, L. (2016) Brazilian Coins Dataset. Retrieved from: http://lgmoneda.github.io/\\nAcknowledgment\\nI'd like to thanks Luciana Harada and Rafael de Souza, my group in the college course that generated these datasets.\",\n",
       " 'Current Population Survey - August 2016\\nContext\\nThe Current Population Survey (CPS) is one of the oldest, largest, and most well-recognized surveys in the United States. It is immensely important, providing information on many of the things that define us as individuals and as a society – our work, our earnings, and our education.\\nFrequency: Monthly\\nPeriod: August 2016\\nContent\\nIn addition to being the primary source of monthly labor force statistics, the CPS is used to collect data for a variety of other studies that keep the nation informed of the economic and social well-being of its people. This is done by adding a set of supplemental questions to the monthly basic CPS questions. Supplemental inquiries vary month to month and cover a wide variety of topics such as child support, volunteerism, health insurance coverage, and school enrollment. Supplements are usually conducted annually or biannually, but the frequency and recurrence of a supplement depend completely on what best meets the needs of the supplement’s sponsor.\\nData Dictionary: http://thedataweb.rm.census.gov/pub/cps/basic/201501-/January_2015_Record_Layout.txt\\nAcknowledgements\\nThe Current Population Survey (CPS) is administered, processed, researched and disseminated by the U.S. Census Bureau on behalf of the Bureau of Labor Statistics (BLS).',\n",
       " \"Severe Weather Data Inventory\\nContext\\nThe Severe Weather Data Inventory (SWDI) is an integrated database of severe weather records for the United States. Severe weather is a phenomenon that risks the physical well-being of people and property. In fact, the frozen precipitation resulting from fast updrafts during strong thunderstorms can lead to serious damage and harm. Each year, the U.S. sees approximately $1 billion in property and crop damage due to severe weather incidents.\\nFrequency: Event-level\\nPeriod: 2015\\nContent\\nThe records in SWDI come from a variety of sources in the National Climatic Data Center archive and cover a number of weather phenomena. This extract from 2015 covers hail detections including the probability of a weather event as well as the size and severity of hail -- all of which help understand potential damage to property and injury to people. Records are event-level records. Individual storm cells with a high probability of yielding hail are included in this dataset -- a total of n = 10,824,080.\\nInspiration\\nThink about the geospatial and spatial statistical techniques that can be applied to this data to uncover patterns in storms.\\nHow often does serious severe weather happen?\\nWhere do these severe weather events normally occur?\\nWhat correlations exist between severe weather and other environmental phenomena?\\nAcknowledgements\\nThis data is a product of NOAA's National Centers for Environmental Information (NCEI). The dataset is generated by a variety of products that have been submitted to NOAA's weather and climate archives at NCEI. The datasets and methods are described at http://www.ncdc.noaa.gov/swdi/.\\nSWDI provides a uniform way to access data from a variety of sources, but it does not provide any additional quality control beyond the processing which took place when the data were archived. The data sources in SWDI will not provide complete severe weather coverage of a geographic region or time period, due to a number of factors (eg, reports for a location or time period not provided to NOAA). The absence of SWDI data for a particular location and time should not be interpreted as an indication that no severe weather occurred at that time and location. Furthermore, much of the data in SWDI is automatically derived from radar data and represents probable conditions for an event, rather than a confirmed occurrence.\\nLicense\\nPublic Domain License\",\n",
       " 'A dataset of Stack Overflow programming questions. For each question, it includes:\\nQuestion ID\\nCreation date\\nClosed date, if applicable\\nScore\\nOwner user ID\\nNumber of answers\\nTags\\nThis dataset is ideal for answering questions such as:\\nThe increase or decrease in questions in each tag over time\\nCorrelations among tags on questions\\nWhich tags tend to get higher or lower scores\\nWhich tags tend to be asked on weekends vs weekdays\\nThis dataset was extracted from the Stack Overflow database at 2016-10-13 18:09:48 UTC and contains questions up to 2016-10-12. This includes 12583347 non-deleted questions, and 3654954 deleted ones.\\nThis is all public data within the Stack Exchange Data Dump, which is much more comprehensive (including question and answer text), but also requires much more computational overhead to download and process. This dataset is designed to be easy to read in and start analyzing. Similarly, this data can be examined within the Stack Exchange Data Explorer, but this offers analysts the chance to work with it locally using their tool of choice.\\nNote that for space reasons only non-deleted questions are included in the sqllite dataset, but the csv.gz files include deleted questions as well (with an additional DeletionDate file).\\nSee the GitHub repo for more.',\n",
       " \"Context\\nEach day, Backblaze takes a snapshot of each operational hard drive that includes basic hard drive information (e.g., capacity, failure) and S.M.A.R.T. statistics reported by each drive. This dataset contains data from the first two quarters in 2016.\\nContent\\nThis dataset contains basic hard drive information and 90 columns or raw and normalized values of 45 different S.M.A.R.T. statistics. Each row represents a daily snapshot of one hard drive.\\ndate: Date in yyyy-mm-dd format\\nserial_number: Manufacturer-assigned serial number of the drive\\nmodel: Manufacturer-assigned model number of the drive\\ncapacity_bytes: Drive capacity in bytes\\nfailure: Contains a “0” if the drive is OK. Contains a “1” if this is the last day the drive was operational before failing.\\n90 variables that begin with 'smart': Raw and Normalized values for 45 different SMART stats as reported by the given drive\\nInspiration\\nSome items to keep in mind as you process the data:\\nS.M.A.R.T. statistic can vary in meaning based on the manufacturer and model. It may be more informative to compare drives that are similar in model and manufacturer\\nSome S.M.A.R.T. columns can have out-of-bound values\\nWhen a drive fails, the 'failure' column is set to 1 on the day of failure, and starting the day after, the drive will be removed from the dataset. Each day, new drives are also added. This means that total number of drives each day may vary.\\nS.M.A.R.T. 9 is the number of hours a drive has been in service. To calculate a drive's age in days, divide this number by 24.\\nGiven the hints above, below are a couple of questions to help you explore the dataset:\\nWhat is the median survival time of a hard drive? How does this differ by model/manufacturer?\\nCan you calculate the probability that a hard drive will fail given the hard drive information and statistics in the dataset?\\nAcknowledgement\\nThe original collection of data can be found here. When using this data, Backblaze asks that you cite Backblaze as the source; you accept that you are solely responsible for how you use the data; and you do not sell this data to anyone.\",\n",
       " 'Context\\nSince 2008, guests and hosts have used Airbnb to travel in a more unique, personalized way. As part of the Airbnb Inside initiative, this dataset describes the listing activity of homestays in Seattle, WA.\\nContent\\nThe following Airbnb activity is included in this Seattle dataset: * Listings, including full descriptions and average review score * Reviews, including unique id for each reviewer and detailed comments * Calendar, including listing id and the price and availability for that day\\nInspiration\\nCan you describe the vibe of each Seattle neighborhood using listing descriptions?\\nWhat are the busiest times of the year to visit Seattle? By how much do prices spike?\\nIs there a general upward trend of both new Airbnb listings and total Airbnb visitors to Seattle?\\nFor more ideas, visualizations of all Seattle datasets can be found here.\\nAcknowledgement\\nThis dataset is part of Airbnb Inside, and the original source can be found here.',\n",
       " 'Content\\nThe motor vehicle collision database includes the date and time, location (as borough, street names, zip code and latitude and longitude coordinates), injuries and fatalities, vehicle number and types, and related factors for all 65,500 collisions in New York City during 2015 and 2016.\\nAcknowledgements\\nThe vehicle collision data was collected by the NYPD and published by NYC OpenData.',\n",
       " 'The dataset contains the following variables: water consumption per user (cubic meters) from 2009 to 2016, land use, type of user (e.g. industrial, housing, public infrastructure, etc.), zip code, and others. The challenge is to treat NAs in a way that do not distort the overall dataset. You should also check whether there are any missing values. If so, can you ﬁll them in, and do you understand why they are missing? This dataset is property of a local water provider called AguaH and its part of a research developed between 2014 and 2016.',\n",
       " 'The Marvel Universe\\nMarvel Comics, originally called Timely Comics Inc., has been publishing comic books for several decades. \"The Golden Age of Comics\" name that was given due to the popularity of the books during the first years, was later followed by a period of decline of interest in superhero stories due to World War ref. In 1961, Marvel relaunched its superhero comic books publishing line. This new era started what has been known as the Marvel Age of Comics. Characters created during this period such as Spider-Man, the Hulk, the Fantastic Four, and the X-Men, together with those created during the Golden Age such as Captain America, are known worldwide and have become cultural icons during the last decades. Later, Marvel\\'s characters popularity has been revitalized even more due to the release of several recent movies which recreate the comic books using spectacular modern special effects. Nowadays, it is possible to access the content of the comic books via a digital platform created by Marvel, where it is possible to subscribe monthly or yearly to get access to the comics. More information about the Marvel Universe can be found here.\\nContent\\nThe dataset contains heroes and comics, and the relationship between them. The dataset is divided into three files:\\nnodes.csv: Contains two columns (node, type), indicating the name and the type (comic, hero) of the nodes.\\nedges.csv: Contains two columns (hero, comic), indicating in which comics the heroes appear.\\nhero-edge.csv: Contains the network of heroes which appear together in the comics. This file was originally taken from http://syntagmatic.github.io/exposedata/marvel/\\nPast Research (Acknowledgements)\\nThe Marvel Comics character collaboration graph was originally constructed by Cesc Rosselló, Ricardo Alberich, and Joe Miro from the University of the Balearic Islands. They compare the characteristics of this universe to real-world collaboration networks, such as the Hollywood network, or the one created by scientists who work together in producing research papers. Their original sources can be found here. With this dataset, the authors published the paper titled: \"Marvel Universe looks almost like a real social network\".',\n",
       " 'The original MNIST image dataset of handwritten digits is a popular benchmark for image-based machine learning methods but researchers have renewed efforts to update it and develop drop-in replacements that are more challenging for computer vision and original for real-world applications. As noted in one recent replacement called the Fashion-MNIST dataset, the Zalando researchers quoted the startling claim that \"Most pairs of MNIST digits (784 total pixels per sample) can be distinguished pretty well by just one pixel\". To stimulate the community to develop more drop-in replacements, the Sign Language MNIST is presented here and follows the same CSV format with labels and pixel values in single rows. The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (excluding J and Z which require motion).\\nThe dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2....pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. The original hand gesture image data represented multiple users repeating the gesture against different backgrounds. The Sign Language MNIST data came from greatly extending the small number (1704) of the color images included as not cropped around the hand region of interest. To create new data, an image pipeline was used based on ImageMagick and included cropping to hands-only, gray-scaling, resizing, and then creating at least 50+ variations to enlarge the quantity. The modification and expansion strategy was filters (\\'Mitchell\\', \\'Robidoux\\', \\'Catrom\\', \\'Spline\\', \\'Hermite\\'), along with 5% random pixelation, +/- 15% brightness/contrast, and finally 3 degrees rotation. Because of the tiny size of the images, these modifications effectively alter the resolution and class separation in interesting, controllable ways.\\nThis dataset was inspired by the Fashion-MNIST 2 and the machine learning pipeline for gestures by Sreehari 4.\\nA robust visual recognition algorithm could provide not only new benchmarks that challenge modern machine learning methods such as Convolutional Neural Nets but also could pragmatically help the deaf and hard-of-hearing better communicate using computer vision applications. The National Institute on Deafness and other Communications Disorders (NIDCD) indicates that the 200-year-old American Sign Language is a complete, complex language (of which letter gestures are only part) but is the primary language for many deaf North Americans. ASL is the leading minority language in the U.S. after the \"big four\": Spanish, Italian, German, and French. One could implement computer vision in an inexpensive board computer like Raspberry Pi with OpenCV, and some Text-to-Speech to enabling improved and automated translation applications.',\n",
       " \"Context:\\nThere has been increased interest among the public about the Environment and living conditions in India. Especially, after since many manufacturing units are being planned, people are worried about how it will affect the underground water quality and the environment. Government of India, under the Ministry of Drinking Water and Sanitation has released the Water Quality Affected Data for 2009, 2010, 2011 and 2012. The objective here is to analyze this data alongside with Forest, Industries, Habitation, and development projects data in the same area (panchayat) to figure out whether there is any connection between the development effort and the quality of water getting affected. This effort will identify such associations and create awareness such that people and the Govt. can act in time to avoid further deterioration of the water resources.\\nContent:\\nCurrently, there is this data set of areas with affected water quality for the years 2009, 2010, 2011 and 2012. Further datasets are expected for subsequent years. These datasets identify the state, district and specific localities in which water quality degradation has been reported in that particular year. Focus should be on the area (Panchayat/Village) rather than the district or the state as a whole, and observations should be made if there are any associations between the other sets of data available for the same area (from industrial, habitation, manufacturing and other sources, which I intend to add here also).\\nAcknowledgements:\\nMy deep gratitude to the Government of India for making this data available through the Open Data initiative.\\nInspiration:\\nLet's explore if there are any repetitive patterns of water quality degradation in the same area for multiple years.\\nAs a whole, which areas in India has a lot of water quality degradation issues over the years (heat maps)\\nWhich chemical is predominantly present in most of the water quality issues (heat maps). And why (from the associations with other developmental data like industry, manufacturing, development initiatives, housing, habitation, etc.)\\nAs a whole, for the country, is the water quality degrading or upgrading (number of instances reported of water quality getting affected)?\\nLet's explore if there are any associations between the water quality data and the other developmental data. If there is, then what is the extent (visualisation) and how can we address it (prescriptive).\\nLet's predict if there are GOING TO BE water quality issues in areas that are not affected right now based on the developmental and water quality data that is available right now. Prevention is better than cure!\\nIt would be great if we could have water quality and industrial/development experts in this analysis, so that they can contribute their valuable inputs!\",\n",
       " \"Context\\nThis is real real-time bidding data that is used to predict if an advertiser should bid for a marketing slot e.g. a banner on a webpage. Explanatory variables are things like browser, operation system or time of the day the user is online, marketplace his identifiers were traded on earlier, etc. The column 'convert' is 1, when the person clicked on the ad, and 0 if this is not the case.\\nContent\\nUnfortunately, the data had to be anonymized, so you basically can't do a lot of feature engineering. I just applied PCA and kept 0.99 of the linear explanatory power. However, I think it's still really interesting data to just test your general algorithms on imbalanced data. ;)\\nInspiration\\nSince it's heavily imbalanced data, it doesn't make sense to train for accuracy, but rather try to get obtain a good AUC, F1Score, MCC or recall rate, by cross-validating your data. It's interesting to compare different models (logistic regression, decision trees, svms, ...) over these metrics and see the impact that your split in train:test data has on the data.\\nIt might be good strategy to follow these Tactics to combat imbalanced classes.\",\n",
       " 'This dataset contains information on player reconnaissance in over 500 professional-level Starcraft games. From the perspective of one player (the Terran), it contains information on how many enemy (Protoss) units the player has observed, can observe, has seen destroyed, etc., along with an overall measure of how much enemy territory the player can see.\\nAcknowledgements\\nThis dataset was downloaded from this webpage. It was the basis for the following paper:\\nHostetler, J., Dereszynski, E., Dietterich, T., and Fern, A. (2012). Inferring strategies from limited reconnaissance in real-time strategy games. Proc. 28th Conference on Uncertainty in Artificial Intelligence (UAI 2012) (to appear).\\nThe Data\\nGames are divided into 30 second chunks, with the first 7 minutes of each game being represented in this dataset. Values of variables at any given time cycle represent their values over the entire chunk that ends at that time.\\nThis dataset contains the following fields:\\ngame: a unique identifier for the game being played\\ncycle: the cycle (in game frames, which are typically 24 fps)\\nunit: the enemy unit that this row gives info for\\nlosses: how many of this enemy unit were lost during this time chunk?\\nobservable-units: how many of this enemy unit could the player see during this time chunk?\\nobserved-losses: how many of this enemy did the player observe being lost during this time chunk?\\nproduction: how many of this enemy unit became observable (i.e., was produced, or -- in the case of buildings -- was under construction) during this time chunk?\\nscouting: how many of this enemy unit did the player scout during this time chunk?\\nvision: what proportion of the total enemy territory could the player observe during this time chunk? -- NOTE that vision appears once per unit; however, the vision variable is not linked to any one unit. Its value spans the time chunk, and is identical in every row that represents a given time chunk',\n",
       " 'Context\\nI have the idea to build a virtual companion, capable of holding long and interesting conversations. But the lack of a good technique, and good datasets apparently is holding the advances of AI in that sense. Chatbots don\\'t have personality, nor context awareness, and datasets used to train them are just pair of question/answers, or IT conversations. This dataset is being built using rDany bot for Telegram, Kik and Messenger.\\nIf you want to see this dataset grow, please use and share it.\\nTelegram: https://t.me/rDanyBot\\nKik: https://kik.me/rDanyBot\\nMessenger: https://m.me/rDanyBot\\nYou can also support the development on Patreon: https://www.patreon.com/rDanyBot\\nThis bot have a personality:\\nCandid\\nTrue\\nFun\\nOptimistic\\nEmpathic\\nGender neutral\\nLikes art\\nAnd knows a very limited word, its room, Wikipedia, and a schematic view of the world. And speaks Spanish (native), English (with some errors), and other languages (using automatic translation). You can learn more about it on rDany\\'s Telegram channel: https://t.me/rDany\\nContent\\nThe dataset consists on 157 anonymized (modified personal information) conversations between a human and other human acting as a companion bot. Each conversation and messages are labeled with hashed IDs.\\n{\\n    \"2059a7bf16436f39b3e713f7b5fe756776cd5e5a601186a1ba17c017027781d9\": [\\n        {\\n            \"date\": 0,\\n            \"hashed_message_id\": \"0fd2e5cf87ae0f148db113f06ab746e0c76a55de04819e1a45c9454a34ba8a97\",\\n            \"source\": \"human\",\\n            \"text\": \"[START]\"\\n        },\\n        {\\n            \"date\": 108,\\n            \"hashed_message_id\": \"a8c5a80334c8177f07913a192f048d05cd5ad5cc77752eb0abb8d0705eccedfb\",\\n            \"source\": \"human\",\\n            \"text\": \"hello\"\\n        },\\n        {\\n            \"date\": 15097,\\n            \"hashed_message_id\": \"73e9765c0d0eab4dfd6f9be2d665e32cc97c5fc3e0fd9c2d12ef920d18ecf349\",\\n            \"source\": \"robot\",\\n            \"text\": \"Hi! How are you?!\"\\n        }\\n    ]\\n}\\ndate: Seconds since first message\\nhashed_message_id: Message ID\\nsource: human if the message is from the user, and robot if is from rDany\\ntext: text of the message, or \"[START]\" for the start command, or \"[photo]\", \"[document]\", \"[audio]\", \"[voice]\", \"[unknown]\" for other types of messages.\\nAcknowledgements\\nThanks to all the amazing people that spent time speaking to a crazy human pretending to be a robot :)\\nInspiration\\nCan you train your own virtual companion that says hello using this dataset?',\n",
       " 'Context\\nThis dataset was collected by me from car sale advertisements for study/practice purposes in 2016. Though there is couple well known car features datasets they seems quite simple and outdated. Car topic is really interesting. But I wanted to practice with real raw data which has all inconvenient moments (as NA’s for example).\\nThis dataset contains data for more than 9.5K cars sale in Ukraine. Most of them are used cars so it opens the possibility to analyze features related to car operation. At the end of the day I look at this data as a subset from all Ukrainian car fleet.\\nContent\\nDataset contains 9576 rows and 10 variables with essential meanings:\\ncar: manufacturer brand\\nprice: seller’s price in advertisement (in USD)\\nbody: car body type\\nmileage: as mentioned in advertisement (‘000 Km)\\nengV: rounded engine volume (‘000 cubic cm)\\nengType: type of fuel (“Other” in this case should be treated as NA)\\nregistration: whether car registered in Ukraine or not\\nyear: year of production\\nmodel: specific model name\\ndrive: drive type\\nData has gaps, so be careful and check for NA’s. I tried to check and drop repeated offers, but theoretically duplications are possible.\\nInspiration\\nData will be handy to study and practice different models and approaches. As a further step you can compare patters in Ukrainian market to your own domestic car market characteristics.',\n",
       " 'Content\\nThe dataset contains a record of each reported wildlife strike of a military, commercial, or civil aircraft between 1990 and 2015. Each row contains the incident date, aircraft operator, aircraft make and model, engine make and model, airport name and location, species name and quantity, and aircraft damage.\\nAcknowledgements\\nThe wildlife strike database was compiled from reports received from airports, airlines, and pilots and published by the Federal Aviation Association.',\n",
       " \"Context\\nThe Museum of Modern Art (MoMA) acquired its first artworks in 1929, the year it was established. Today, the Museum’s evolving collection contains almost 200,000 works from around the world spanning the last 150 years. The collection includes an ever-expanding range of visual expression, including painting, sculpture, printmaking, drawing, photography, architecture, design, film, and media and performance art.\\nContent\\nMoMA is committed to helping everyone understand, enjoy, and use our collection. The Museum’s website features 72,706 artworks from 20,956 artists. The artworks dataset contains 130,262 records, representing all of the works that have been accessioned into MoMA’s collection and cataloged in our database. It includes basic metadata for each work, including title, artist, date, medium, dimensions, and date acquired by the Museum. Some of these records have incomplete information and are noted as “not curator approved.” The artists dataset contains 15,091 records, representing all the artists who have work in MoMA's collection and have been cataloged in our database. It includes basic metadata for each artist, including name, nationality, gender, birth year, and death year.\\nInspiration\\nWhich artist has the most works in the museum collection or on display? What is the largest work of art in the collection? How many pieces in the collection were made during your birth year? What gift or donation is responsible for the most artwork in the collection?\",\n",
       " \"This dataset contains hourly estimates of an area's energy potential for 1986-2015 as a percentage of a power plant's maximum output.\\nThe overall scope of EMHIRES is to allow users to assess the impact of meteorological and climate variability on the generation of solar power in Europe and not to mime the actual evolution of solar power production in the latest decades. For this reason, the hourly solar power generation time series are released for meteorological conditions of the years 1986-2015 (30 years) without considering any changes in the solar installed capacity. Thus, the installed capacity considered is fixed as the one installed at the end of 2015. For this reason, data from EMHIRES should not be compared with actual power generation data other than referring to the reference year 2015.\\nContent\\nThe data is available at both the national level and the NUTS 2 level. The NUTS 2 system divides the EU into 276 statistical units.\\nPlease see the manual for the technical details of how these estimates were generated.\\nThis product is intended for policy analysis over a wide area and is not the best for estimating the output from a single system. Please don't use it commercially.\\nAcknowledgements\\nThis dataset was kindly made available by the European Commission's STETIS program. You can find the original dataset here.\\nInspiration\\nHow clean is the dataset? Older solar estimates used to contain impossible values around sunset (ie more energy than the sun releases) or negative sunlight.\\nWhat does a typical year look like? One common approach is to stitch together 12 months of raw data, using the 12 most typical months per this ISO standard.\\nIf you like\\nIf you like this dataset, you might also enjoy: - 30 years of European wind - Google's Project Sunroof\",\n",
       " 'Context:\\nSome words, like “the” or “and” in English, are used a lot in speech and writing. For most Natural Language Processing applications, you will want to remove these very frequent words. This is usually done using a list of “stopwords” which has been complied by hand.\\nContent:\\nThis dataset contains a list of stopwords for the following languages (Languages which are not from the Indo-European language family have been starred):\\nEnglish\\nFrench\\nGerman\\nItalian\\nSpanish\\nPortuguese\\nFinnish*\\nSwedish\\nArabic*\\nRussian\\nHungarian\\nBulgarian\\nRomanian\\nCzech\\nPolish\\nPersian/Farsi\\nHindi\\nMarathi\\nBengali\\nAcknowledgements:\\nThis dataset is Copyright (c) 2005, Jacques Savoy and distributed under the BSD License. More information can be found here.\\nInspiration:\\nThis dataset is mainly helpful for use during NLP analysis, however there may some interesting insights to be found in the data.\\nWhat qualities do stopwords share across languages? Given a novel language, could you predict what its stopwords should be?\\nWhat stopwords are shared across languages?\\nOften, related languages will have words with the same meaning and similar spellings. Can you automatically identify any of these pairs of words?\\nYou may also like:\\nStopword Lists for 9 African Languages',\n",
       " 'Context\\nThis data set deals with the financial distress prediction for a sample of companies.\\nContent\\nFirst column: Company represents sample companies.\\nSecond column: Time shows different time periods that data belongs to. Time series length varies between 1 to 14 for each company.\\nThird column: The target variable is denoted by \"Financial Distress\" if it is greater than -0.50 the company should be considered as healthy (0). Otherwise, it would be regarded as financially distressed (1).\\nFourth column to the last column: The features denoted by x1 to x83, are some financial and non-financial characteristics of the sampled companies. These features belong to the previous time period, which should be used to predict whether the company will be financially distressed or not (classification). Feature x80 is categorical variable.\\nFor example, company 1 is financially distressed at time 4 but company 2 is still healthy at time 14.\\nThis data set is imbalanced (there are 136 financially distressed companies against 286 healthy ones i.e., 136 firm-year observations are financially distressed while 3546 firm-year observations are healthy) and skewed, so f-score should be employed as the performance evaluation criterion.\\nIt should be noted that 30% of this data set should be randomly assigned as hold-out test set so the remaining 70% is used for feature selection and model selection i.e., train set.\\nNote: 1- This data could be viewed as a classification problem. 2- This data could also be considered as a regression problem and then the result will be converted into a classification. 3- This data could be regarded as a multivariate time series classification.\\nInspiration\\nWhich features are most indicative of financial distress?\\nWhat types of machine learning models perform best on this dataset?',\n",
       " 'Context\\nThis dataset includes taxi trips for 2016, reported to the City of Chicago in its role as a regulatory agency. To protect privacy but allow for aggregate analyses, the Taxi ID is consistent for any given taxi medallion number but does not show the number, Census Tracts are suppressed in some cases, and times are rounded to the nearest 15 minutes. Due to the data reporting process, not all trips are reported but the City believes that most are. See http://digital.cityofchicago.org/index.php/chicago-taxi-data-released for more information about this dataset and how it was created.\\nContent\\nPlease see the data dictionary for details of specific fields. We also shrunk the original files by roughly two thirds by dropping redundant columns and remapping several others to use shorter IDs. For example, the taxi_id column used to be a 128 character string. We’ve replaced it with an integer containing at most four digits.\\nThe redundant columns were unique_key, pickup_location, and dropoff_location. The remapped columns were taxi_id, company, pickup_census_tract, dropoff_census_tract, pickup_latitude, pickup_longitude, dropoff_latitude, and dropoff_longitude. The original versions of those columns can be unpacked using the column_remapping.json.\\nAcknowledgements\\nThis dataset was kindly made publically available by the City of Chicago at: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew\\nPlease note that this site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one’s own risk.\\nInspiration\\nHow centralized is Chicago? In other words, what portion of trips are to or from downtown?\\nChicago has an extensive metro system. Are taxis competing with the trains by covering similar routes or supplementing public transit by getting people to and from train stations?\\nUse this dataset with BigQuery\\nYou can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/chicago-taxi. BigQuery hosts the full version of this dataset, which extends from 2013 through the present.',\n",
       " \"Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 615,000 hotels) that was created by extracting data from MakeMyTrip.com, a travel portal in India. The complete dataset is available on DataStock, a web data repository with historical records from several industries.\\nContent\\nThis dataset has following fields:\\narea\\ncity\\ncountry\\ncrawl_date\\nhighlight_value\\nhotel_overview\\nhotel_star_rating\\nimage_urls\\nin_your_room\\nis_value_plus\\nlatitude\\nlongitude\\nmmt_holidayiq_review_count\\nmmt_location_rating\\nmmt_review_count\\nmmt_review_rating\\nmmt_review_score\\nmmt_traveller_type_review_count\\nmmt_tripadvisor_count\\npageurl\\nproperty_address\\nproperty_id\\nproperty_name\\nproperty_type\\nqts\\nquery_time_stamp\\nroom_types\\nsite_review_count\\nsite_review_rating\\nsitename\\nstate\\ntraveller_rating\\nAcknowledgements\\nThis dataset was created by PromptCloud's in-house web-crawling service.\\nInspiration\\nAnalyses of the reviews, ratings and property description can be performed.\",\n",
       " \"Context\\nThe British National Health Service releases data covering every public sector prescription made in the country. This covers a single year of that data.\\nContent\\nCovering all general practices in England, the data includes figures on the number of prescription items that are dispensed each month and information relating to costs.\\nFor each GP practice, the total number of items that were prescribed and then dispensed is shown. The total Net Ingredient Cost and the total Actual Cost of these items is shown.\\nChemical level\\nAll prescribed and dispensed medicines (by chemical name), dressings and appliances (at section level) are listed for each GP practice.\\nPresentation level\\nAll prescribed and dispensed medicines, dressings and appliances are listed at presentation level, for each GP practice. (Presentation level gives the individual drug name, the form, and strength or size accordingly). The total quantity of drugs dispensed (in terms of number of tablets or millilitres, for example) is shown. This data does not list each individual prescription and does not contain any patient identifiable data.\\nThe data have been edited from their original version. During the data preparation process I:\\nDropped obsolete and redundant columns.\\nNormalized the BNF (British National Formulary) codes, BNF names, and practice codes. These steps reduced the total file size by roughly 75%, at the cost of requiring one table join to access some of the data.\\nFor further details, please see:\\nFAQ\\nGlossary of Terms\\nAcknowledgements\\nThis dataset was kindly released by the United Kingdom's National Health Service under their government open data license v3. You can find this and other datasets at their open data site.\\nInspiration\\nWhat trends can you see in the data? For example, can you identify the onset of winter based on the types of drugs being prescribed?\\nThe BNF Name entries contain dosage data that I haven't yet cleaned and extracted. Can you unpack that field into item dispensed, units, and dosage? If so, let me know in the forums and I'll add it to the dataset!\\nPer this blog from Oxford, the raw BNF codes contain quite a bit of information about a drug's function. Can you find a source of open data for translating these codes? It's probable that one exists somewhere at https://www.nhsbsa.nhs.uk/nhs-prescription-services.\",\n",
       " 'Context:\\nEvery year since 1947, representatives of UN member states gather at the annual sessions of the United Nations General Assembly. The centrepiece of each session is the General Debate. This is a forum at which leaders and other senior officials deliver statements that present their government’s perspective on the major issues in world politics. These statements are akin to the annual legislative state-of-the-union addresses in domestic politics. This dataset, the UN General Debate Corpus (UNGDC), includes the corpus of texts of General Debate statements from 1970 (Session 25) to 2016 (Session 71).\\nContent:\\nThis dataset includes the text of each country’s statement from the general debate, separated by country, session and year and tagged for each. The text was scanned from PDFs of transcripts of the UN general sessions. As a result, the original scans included page numbers in the text from OCR (Optical character recognition) scans, which have been removed. This dataset only includes English.\\nAcknowledgements:\\nThis dataset was prepared by Alexander Baturo, Niheer Dasandi, and Slava Mikhaylov, and is presented in the paper \"Understanding State Preferences With Text As Data: Introducing the UN General Debate Corpus\" Research & Politics, 2017.\\nInspiration:\\nThis dataset includes over forty years of data from different countries, which allows for the exploration of differences between countries and over time. This allows you to ask both country-specific and longitudinal questions. Some questions that might be interesting:\\nHow has the sentiment of each country’s general debate changed over time?\\nWhat topics have been more or less popular over time and by region?\\nCan you build a classifier which identifies which country a given text is from?\\nAre there lexical or syntactic changes over time or differences between region?\\nHow does the latitude of a country affect lexical complexity?',\n",
       " \"Utility Data\\nThe data is extracted from geonames, a very exhaustive list of worldwide toponyms. It can be joined with datasets containing geographic fields to facilitate geospatial analysis including mapping.\\nThis datapackage only lists cities above 15,000 inhabitants. Each city is associated with its country and subcountry to reduce the number of ambiguities. Subcountry can be the name of a state (e.g., in United Kingdom or the United States of America) or the major administrative section (e.g., ''region'' in France''). See admin1 field on geonames website for further info about subcountry.\\nNotice that:\\nSome cities like Vatican City or Singapore are a whole state so they don't belong to any subcountry. Therefore subcountry is N/A.\\nThere is no guaranty that a city has a unique name in a country and subcountry (At the time of writing, there are about 60 ambiguities). But for each city, the source data primary key geonameid is provided.\\nPreparation\\nYou can run the script yourself to update the data and publish them to GitHub/Kaggle: see scripts README\\nAcknowledgments and License\\nAll data is licensed under the Creative Common Attribution License as is the original data from geonames. This means you have to credit geonames when using the data. And while no credit is formally required a link back or credit to Lexman and the Open Knowledge Foundation is much appreciated. This dataset description is reproduced here from its original source with slight modifications.\",\n",
       " 'Routes database\\nAs of January 2012, the OpenFlights/Airline Route Mapper Route Database contains 59036 routes between 3209 airports on 531 airlines spanning the globe.\\nContent\\nThe data is ISO 8859-1 (Latin-1) encoded.\\nEach entry contains the following information:\\nAirline 2-letter (IATA) or 3-letter (ICAO) code of the airline.\\nAirline ID Unique OpenFlights identifier for airline (see Airline).\\nSource airport 3-letter (IATA) or 4-letter (ICAO) code of the source airport.\\nSource airport ID Unique OpenFlights identifier for source airport (see Airport)\\nDestination airport 3-letter (IATA) or 4-letter (ICAO) code of the destination airport.\\nDestination airport ID Unique OpenFlights identifier for destination airport (see Airport)\\nCodeshare \"Y\" if this flight is a codeshare (that is, not operated by Airline, but another carrier), empty otherwise.\\nStops Number of stops on this flight (\"0\" for direct)\\nEquipment 3-letter codes for plane type(s) generally used on this flight, separated by spaces\\nThe special value \\\\N is used for \"NULL\" to indicate that no value is available.\\nNotes:\\nRoutes are directional: if an airline operates services from A to B and from B to A, both A-B and B-A are listed separately.\\nRoutes where one carrier operates both its own and codeshare flights are listed only once.\\nAcknowledgements\\nThis dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website, so check them out!',\n",
       " 'Context\\nThis data set can be paired with the shot logs data set from the same season.\\nContent\\nFull players stats from the 2014-2015 season + personal details such as height. weight, etc.\\nThe data was scraped and copied from: http://www.basketball-reference.com/teams/ and http://stats.nba.com/leaders#!?Season=2014-15&SeasonType=Regular%20Season&StatCategory=MIN&CF=MIN*G*2&PerMode=Totals',\n",
       " 'EMNIST\\nThe EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset. Further information on the dataset contents and conversion process can be found in the paper available at https://arxiv.org/abs/1702.05373v1.\\nFormat\\nThere are six different splits provided in this dataset and each are provided in two formats:\\nBinary (see emnist_source_files.zip)\\nCSV (combined labels and images)\\nEach row is a separate image\\n785 columns\\nFirst column = class_label (see mappings.txt for class label definitions)\\nEach column after represents one pixel value (784 total for a 28 x 28 image)\\nByClass and ByMerge datsets\\nThe full complement of the NIST Special Database 19 is available in the ByClass and ByMerge splits. These two datasets have the same image information but differ in the number of images in each class. Both datasets have an uneven number of images per class and there are more digits than letters. The number of letters roughly equate to the frequency of use in the English language.\\ntrain: 697,932\\ntest: 116,323\\ntotal: 814,255\\nclasses: ByClass 62 (unbalanced) / ByMerge 47 (unbalanced)\\nBalanced dataset\\nThe EMNIST Balanced dataset is meant to address the balance issues in the ByClass and ByMerge datasets. It is derived from the ByMerge dataset to reduce mis-classification errors due to capital and lower case letters and also has an equal number of samples per class. This dataset is meant to be the most applicable.\\ntrain: 112,800\\ntest: 18,800\\ntotal: 131,600\\nclasses: 47 (balanced)\\nLetters datasets\\nThe EMNIST Letters dataset merges a balanced set of the uppercase and lowercase letters into a single 26-class task.\\ntrain: 88,800\\ntest: 14,800\\ntotal: 103,600\\nclasses: 37 (balanced)\\nDigits and MNIST datsets\\nThe EMNIST Digits and EMNIST MNIST dataset provide balanced handwritten digit datasets directly compatible with the original MNIST dataset.\\ntrain: Digits 240,000 / MNIST 60,000\\ntest: Digits 40,000 / MNIST 10,000\\ntotal: Digits 280,000 / MNIST 70,000\\nclasses: 47 (balanced)\\nVisual breakdown of EMNIST datasets\\nPlease refer to the EMNIST paper for details on the structure of the dataset https://arxiv.org/abs/1702.05373v1.\\nAcknowldgements\\nCohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters.\\nDataset retrieved from https://www.nist.gov/itl/iad/image-group/emnist-dataset\\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik\\nThe MARCS Institute for Brain, Behaviour and Development\\nWestern Sydney University\\nPenrith, Australia 2751',\n",
       " 'Context\\nThis is probably the dumbest dataset on Kaggle. The whole point is, however, to provide a common dataset for linear regression. Although such a dataset can easily be generated in Excel with random numbers, results would not be comparable.\\nContent\\nThe training dataset is a CSV file with 700 data pairs (x,y). The x-values are numbers between 0 and 100. The corresponding y-values have been generated using the Excel function NORMINV(RAND(), x, 3). Consequently, the best estimate for y should be x. The test dataset is a CSV file with 300 data pairs.\\nAcknowledgements\\nThank you, Dan Bricklin and Bob Frankston for inventing the first spreadsheet.\\nInspiration\\nI hope this dataset will encourage all newbies to enter the world of machine learning, possibly starting with a simple linear regression.\\nData license\\nObviously, data is free.',\n",
       " 'Context:\\nORCID provides a persistent digital identifier that distinguishes you from every other researcher and, through integration in key research workflows such as manuscript and grant submission, supports automated linkages between you and your professional activities ensuring that your work is recognized. Find out more.\\nContent:\\nThis data is a subset of the entire ORCID collection. The subset here was produced by John Bohannon. You can see his excellent Ipython notebook and the entire (300GB!) ORCID archives here.\\nThe data covers ~742k unique researchers and includes:\\norcid_id\\nphd_year\\ncountry_2016\\nearliest_year\\nearliest_country\\nhas_phd\\nphd_country\\nhas_migrated\\nAcknowledgements:\\nBohannon J, Doran K (2017) Introducing ORCID. Science 356(6339) 691-692. http://dx.doi.org/10.1126/science.356.6339.691\\nAdditionally, please cite the Dryad data package:\\nBohannon J, Doran K (2017) Data from: Introducing ORCID. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.48s16\\nInspiration:\\nWhere do most researchers move to?\\nWhat countries experience the largest ‘brain drain’? As a % of population?\\nCan you predict researcher migration?',\n",
       " 'Context\\nAs a former transportation student I know how the weather can influence traffic. Both the increase of traffic, as well as the decrease of road conditions increases the travel time.\\nContent\\nWeather data collected from the National Weather Service. It contains the first six months of 2016, for a weather station in central park. It contains for each day the minimum temperature, maximum temperature, average temperature, precipitation, new snow fall, and current snow depth. The temperature is measured in Fahrenheit and the depth is measured in inches. T means that there is a trace of precipitation.\\nAcknowledgements\\nThe data was retrieved on 20th of July, 2017 on the website http://w2.weather.gov/climate/xmacis.php?wfo=okx.',\n",
       " 'Context\\nThis dataset contains a randomized sample of roughly one quarter of all stories and comments from Hacker News from its launch in 2006. Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham\\'s investment fund and startup incubator, Y Combinator. In general, content that can be submitted is defined as \"anything that gratifies one\\'s intellectual curiosity\".\\nContent\\nEach story contains a story ID, the author that made the post, when it was written, and the number of points the story received.\\nPlease note that the text field includes profanity. All texts are the author’s own, do not necessarily reflect the positions of Kaggle or Hacker News, and are presented without endorsement.\\nAcknowledgements\\nThis dataset was kindly made publicly available by Hacker News under the MIT license.\\nInspiration\\nRecent studies have found that many forums tend to be dominated by a very small fraction of users. Is this true of Hacker News?\\nHacker News has received complaints that the site is biased towards Y Combinator startups. Do the data support this?\\nIs the amount of coverage by Hacker News predictive of a startup’s success?\\nUse this dataset with BigQuery\\nYou can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data in BigQuery, too: https://cloud.google.com/bigquery/public-data/hacker-news\\nThe BigQuery version of this dataset has roughly four times as many articles.',\n",
       " 'Context\\nPantheon is a project celebrating the cultural information that endows our species with these fantastic capacities. To celebrate our global cultural heritage we are compiling, analyzing and visualizing datasets that can help us understand the process of global cultural development. Dive in, visualize, and enjoy.\\nContent\\nThe Pantheon 1.0 data measures the global popularity of historical characters using two measures. The simpler of the two measures, which we denote as L, is the number of different Wikipedia language editions that have an article about a historical character. The more sophisticated measure, which we name the Historical Popularity Index (HPI) corrects L by adding information on the age of the historical character, the concentration of page views among different languages, the coefficient of variation in page views, and the number of page views in languages other than English.\\nFor annotations of specific values visit the column metadata in the /Data tab. A more comprehensive breakdown is available on the Parthenon website.\\nAcknowledgements\\nPantheon is a project developed by the Macro Connections group at the Massachusetts Institute of Technology Media Lab. For more on the dataset and to see visualizations using it, visit its landing page on the MIT website.\\nInspiration\\nWhich historical figures have a biography in the most languages? Who received the most Wikipedia page views? Which occupations or industries are the most popular? What country has the most individuals with a historical popularity index over twenty?',\n",
       " 'Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into 1000 classes, like \"Zebra\", \"Dalmatian\", and \"Dishwasher\".\\nHere\\'s code on GitHub to train Inception-v3',\n",
       " 'Context\\nThis corpus contains a metadata-rich collection of fictional conversations extracted from raw movie scripts:\\n220,579 conversational exchanges between 10,292 pairs of movie characters\\ninvolves 9,035 characters from 617 movies\\nin total 304,713 utterances\\nmovie metadata included:\\ngenres\\nrelease year\\nIMDB rating\\nnumber of IMDB votes\\nIMDB rating\\ncharacter metadata included:\\ngender (for 3,774 characters)\\nposition on movie credits (3,321 characters)\\nContent\\nIn all files the original field separator was \" +++$+++ \" and have been converted to tabs (\\\\t). Additionally, the original file encoding was ISO-8859-2. It\\'s possible that the field separator conversion and decoding may have left some artifacts.\\nmovie_titles_metadata.txt\\ncontains information about each movie title\\nfields:\\nmovieID,\\nmovie title,\\nmovie year,\\nIMDB rating,\\nno. IMDB votes,\\ngenres in the format [\\'genre1\\',\\'genre2\\',É,\\'genreN\\']\\nmovie_characters_metadata.txt\\ncontains information about each movie character\\nfields:\\ncharacterID\\ncharacter name\\nmovieID\\nmovie title\\ngender (\"?\" for unlabeled cases)\\nposition in credits (\"?\" for unlabeled cases)\\nmovie_lines.txt\\ncontains the actual text of each utterance\\nfields:\\nlineID\\ncharacterID (who uttered this phrase)\\nmovieID\\ncharacter name\\ntext of the utterance\\nmovie_conversations.txt\\nthe structure of the conversations\\nfields\\ncharacterID of the first character involved in the conversation\\ncharacterID of the second character involved in the conversation\\nmovieID of the movie in which the conversation occurred\\nlist of the utterances that make the conversation, in chronological order: [\\'lineID1\\',\\'lineID2\\',É,\\'lineIDN\\'] has to be matched with movie_lines.txt to reconstruct the actual content\\nraw_script_urls.txt\\nthe urls from which the raw sources were retrieved\\nAcknowledgements\\nThis corpus comes from the paper, \"Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs\" by Cristian Danescu-Niculescu-Mizil and Lillian Lee.\\nThe paper and up-to-date data can be found here: http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\\nPlease see the README for more information on the authors\\' collection procedures.\\nThe file formats were converted to TSV and may contain a few errors\\nInspiration\\nWhat are all of these imaginary people talking about? Are they representative of how real people communicate?\\nCan you identify themes in movies from certain writers or directors?\\nHow does the dialog change between characters?',\n",
       " 'Airline database\\nAs of January 2012, the OpenFlights Airlines Database contains 5888 airlines. Some of the information is public data and some is contributed by users.\\nContent\\nThe data is ISO 8859-1 (Latin-1) encoded.\\nEach entry contains the following information: - Airline ID Unique OpenFlights identifier for this airline. - Name Name of the airline. - Alias Alias of the airline. For example, All Nippon Airways is commonly known as \"ANA\". - IATA 2-letter IATA code, if available. - ICAO 3-letter ICAO code, if available. - Callsign Airline callsign. - Country Country or territory where airline is incorporated. - Active \"Y\" if the airline is or has until recently been operational, \"N\" if it is defunct. This field is not reliable: in particular, major airlines that stopped flying long ago, but have not had their IATA code reassigned (eg. Ansett/AN), will incorrectly show as \"Y\".\\nThe special value \\\\N is used for \"NULL\" to indicate that no value is available. This is from a MySQL database where \\\\N is used for NULL.\\nNotes: Airlines with null codes/callsigns/countries generally represent user-added airlines. Since the data is intended primarily for current flights, defunct IATA codes are generally not included. For example, \"Sabena\" is not listed with a SN IATA code, since \"SN\" is presently used by its successor Brussels Airlines.\\nAcknowledgements\\nThis dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website, so check them out!',\n",
       " \"The dataset contains a broad set of macroeconomic and financial data for the UK stretching back in some cases to the C13th and with one or two benchmark estimates available for 1086, the year of the Domesday Book. The dataset was originally called the 'Three centuries of macroeconomic data' spreadsheet but has now been renamed given its broader coverage. Version 3 of the dataset has now been updated to 2016.\\nContent\\nThe Excel file contains the original data. It contains hundreds of time series, while the csv is an extract of several dozen headline time series.\\nIf you would like to see more of the data made available in CSV format; please let me know what you would like extracted and I'll be happy to add it. Please see excel_sheet_names.csv for details of what other data has yet to be unpacked.\\nAcknowledgements\\nThis dataset was kindly made available by the Bank of England. You can find the original dataset here.\\nInspiration\\nWhich metrics give similar answers about when the industrial revolution began? How clear is the cutoff point?\\nIf you like\\nIf you enjoyed this dataset, you might also like the Allen-Unger Global Commodity Prices dataset, which provides historic commodity prices from locations around the world.\",\n",
       " 'What I talk about when I talk about Catalonia\\nThis is my grain of sand to help in the Catalonia Independence crisis. The Iberian media has been a key driver to incubate disaffection between Catalonia and Spain. For example, a leading newspaper tweeted boycott and Catalonia 9 times in the last month. In this dataset we analyze:\\ntweets\\ntopics of tweets and\\nsentiment differentials in news\\nContext\\nThe dramatic Catalonia independence crisis offers a unique opportunity to analyze bias in news reporting as opinions on the issue are quite polarized (See #catalanReferendum on twitter). In this dataset, we compare how different newspapers (NYT, Washington-Post, Bloomberg...) have reported one singular specific event in the saga: The reply of the Spanish Government to M.H. Puigdemont speech of October 11th of 2017. For each of the 30 newspapers considered, the most popular news article that reported this news is represented as a row in the dataset. Why this news? The Spanish government, published a pdf called (\"requerimiento\") which was faxed to Puigdemont. The document requires that Puigdemont reply in five days a clarification of the meaning of his speech. This \"clean\" news offers a rare example where the news is about a written document rather than a speech or an action (usually subjected to more interpretations and biases)\\nContent\\nnews_...csv each row contains the news article and its translation to English.\\nall3.csv contains 100k tweets.\\nAcknowledgements\\nAll the journalists who made this dataset possible. Thanks to @DataCanary for helping make the visualizations better!\\nInspiration\\nI always thought that sentiment analysis was a useless topic, but here there is a chance to use an objective measure to show how polarized reporting has become, (even if sentiment does not account for fakenews, nuances or sarcasm). The linear regressions shows that news written in Spanish language are less positive about the event than the global mean. In other words, sentiment seems strongly biased by language. Bias by location of the newspapers is also analyzed.\\nDisclaimer\\nNote that the \\'bing\\' scale is used. Other scales such AFINN might yield different results.',\n",
       " 'In 2015, 30,092 people were killed in the US in traffic accidents. This presents a 7.2% rise in fatalities over 2014, a startling change to a 50 year trend of declining fatality rates.\\nBecause of this, the US government has publicly released this data and posted a call to action to try and investigate the data, to try and gain insights into why this is happening.\\nI am posting the data here in raw format. There are very many files, so I will try to create a more cohesive dataset and add it here in the future. For now I am just uploading the *_AUX.csv files, which contain the most data. I look forward to seeing what visualisations you guys can make from this!\\nThe data is compiled by the National Highway Traffic Safety Administration, and was released in this blog post.',\n",
       " \"This dataset includes county-level data from the 2016 US Presidential Election.\\nData are from Michael W. Kearney's GitHub page, by way of Max Galka's County-Level Results Map on metrocosm.com.\",\n",
       " \"Disclaimer\\nThis is a data set of mine that I though might be enjoyable to the community. It's concerning Next generation sequencing and Transcriptomics. I used several raw datasets, that are public, but the processing to get to this dataset is extensive. This is my first contribution to kaggle, so be nice, and let me know how I can improve the experience. NGS machines are combined the biggest data producer worldwide. So why not add some (more? ) to kaggle.\\nA look into Yeast transcriptomics\\nBackground\\nYeasts ( in this case saccharomyces cerevisiae) are used in the production of beer, wine, bread and a whole lot of Biotech applications such as creating complex pharmaceuticals. They are living eukaryotic organisms (meaning quite complex). All living organisms store information in their DNA, but action within a cell is carried out by specific Proteins. The path from DNA to Protein (from data to action) is simple. a specific region on the DNA gets transcribed to mRNA, that gets translated to proteins. Common assumption says that the translation step is linear, more mRNA means more protein. Cells actively regulate the amount of protein by the amount of mRNA it creates. The expression of each gene depends on the condition the cell is in (starving, stressed etc..) Modern methods in Biology show us all mRNA that is currently inside a cell. Assuming the linearity of the process, we can get more protein the more specific mRNA is available to a cell. Making mRNA an excellent marker for what is actually happening inside a cell. It is important to consider that mRNA is fragile. It is actively replenished only when it is needed. Both mRNA and proteins are expensive for a cell to produce .\\nYeasts are good model organisms for this, since they only have about 6000 genes. They are also single cells which is more homogeneous, and contain few advanced features (splice junctions etc.)\\n( all of this is heavily simplified, let me know if I should go into more details )\\nThe data\\nfiles\\nThe following files are provided SC_expression.csv expression values for each gene over the available conditions **labels_CC.csv ** labels for the individual genes , their status and where known intracellular localization ( see below)\\nMaybe this would be nice as a little competition, I'll see how this one is going before I'll upload the other label files. Please provide some feedback on the presentation, and whatever else you would want me to share.\\nbackground\\nI used 92 samples from various openly available raw datasets, and ran them through a modern RNAseq pipeline. Spanning a range of different conditions (I hid the raw names). The conditions covered stress conditions, temperature and heavy metals, as well as growth media changes and the deletion of specific genes. Originally I had 150 sets, 92 are of good enough quality. Evaluation was done on gene level. Each gene got it's own row, Samples are columns (some are in replicates over several columns) . Expression levels were normalized with by TPM (transcripts per million), a default normalization procedure. Raw counts would have been integers, normalized they are floats.\\nAnalysis and labels\\nGenes\\nThe function of individual genes is a matter of dispute. Clearly living cells are complex. The inner machinations of cells are not visible. Gene functionality is commonly inferred indirectly by removing a gene, and test the cells behavior. This is time consuming and not very precise. As you can see in the dataset, there is still much to be done to fully understand even single cell yeasts.\\nThe provided dataset is allows for a different approach to functional classification of genes. The label files contained in the set correspond a gene to a specific label. The classification is based on the official Gene Onthology associations classification. I simplified the nomenclature. Gene functionality is usually given in a hierarchical structure. [inside cell --> cytoplasma --> associated to complex A ... ] I'm only keeping high level associations, and using readable terms instead of GO terms. I'll extend if people are interested.\\nLabels\\nCC labels concern Cellular Component.\\nWhere the gene is within a cell. goes into details of found associations. the term 'cellular_component' should be seen as E.g the label 'cellular_component' is synonymous with 'unknown location' . CC is the easiest label to attach to a gene. It is the one that can be studied the easiest. Still there are many genes missing.\\nMF labels concern Molecular Function. What is the gene doing. [upcoming] BP labels concern Biological Processes. What is the genes involvement. [upcoming]\\nThe core interest here is whether it is possible to improve the genes classification by modeling the data. A common assumption says that genes that are expressed in the same conditions have functional relations. There are a bunch of possible applications out there, many of which are limited by our current state of knowledge on the complex systems we observe, or fail to do so. Bringing biology into the realm of data science is an ongoing effort. Having a better insight into the data might very well help.\\nNote\\nThe dataset is real, and therefore noisy the labels are incomplete even though I'm using the current state of the art. That is how much is known. Using expression levels for classification was already attempted by softwares like SPELL (Serial Pattern of Expression Levels Locator).\\nAcknowledgements\\nI guess I own the dataset. It is a by product of another project of mine. If someone is interested in publishing this, contact me.\\nInspiration\\nUnraveling genetic mechanisms is a complex but rewarding task. Humans and yeast are quite similar in many ways. So apart from the fact that we use it for food and medicine, we might actually use knowledge gained from yeast eventually for studying diseases.\\nAgain, any feedback is welcome, Enjoy, CE\",\n",
       " 'Context\\nMatch details of over 6000 T20 matches, including innings scores and results.\\nContent\\nThe first 3 columns show the original data that was scraped. The remaining columns are individual data points extracted from these columns.\\n• match_details: summary of match including stage of tournament (if applicable), home/away teams, venue and date of match.\\n• result: summary of final result. Includes ties (and any winners as a result of bowl-outs/super overs etc.), no results and abandoned matches.\\n• scores: summary of scores of both innings. Includes scores even if match ends in a no result. Blank indicates that match was abandoned without a ball bowled.\\n• date: date of match in standard date format, dd/mm/yyyy. If match goes to reserve day, this date is used.\\n• venue: city of match. Can be assumed to be main stadium within city. If more than one stadium in a city, it is usually labelled.\\n• round: stage within tournament, e.g. final, semi-final, group stage etc. Also, includes 1st, 2nd T20I etc. for bilateral series.\\n• home: home or designated home team.\\n• away: away or designated away team.\\n• winner: winner of match including any winners by any method to determine a winner after a tie.\\n• win_by_runs: number of runs team batting first wins by.\\n• win_by_wickets: number of wickets team batting second wins by.\\n• balls_remaining: number of balls remaining for team batting second after win.\\n• innings1: team batting first\\n• innings1_runs: first innings score\\n• innings1_wickets: first innings wickets\\n• innings1_overs_batted: actual length of first innings\\n• innings1_overs: maximum length of first innings\\n• innings2: team batting second\\n• innings2_runs: second innings score\\n• innings2_wickets: second innings wickets\\n• innings2_overs_batted: actual length of second innings\\n• innings2_overs: maximum length of second innings\\n• D/L method: 1 means that the D/L method (or VJB method) was used to determine winner.\\n• target: rain-adjusted target. If blank, target is first innings score plus 1, as normal.\\nNEW: all T20 series added.\\nPlease let me know if you spot any mistakes!',\n",
       " \"Context\\nThis data represents the top arrest charge of those processed at Baltimore's Central Booking & Intake Facility. This data does not contain those who have been processed through Juvenile Booking.\\nContent\\nThe data set was created on October 18, 2011. The data set was last updated on November 18, 2016. It is updated on a monthly basis.\\nMetadata\\nArrest-ID\\nAge\\nSex\\nRace\\nArrestDate\\nArrestTime\\nArrestLocation\\nIncidentOffense\\nIncidentLocation\\nCharge\\nChargeDescription\\nDistrict\\nPost\\nNeighborhood\\nLocation1(Location Coordinates)\\nPast Research\\nI have done my own analysis on the data which can be found on the following GitHub repository. Feel free to give any suggestions regarding the data.\\nGithub Link\\nInspiration\\nHow arrests vary across different gender, race, age ?\\nWhich area in Baltimore has most number of arrests made ?\\nWhat are the top offences and/or charges made while making arrests ?\\nAcknowledgements\\nThe data is hosted on:\\nData set Source\\nBaltimore Police Depratment's website:\\nBaltimore Police Department\",\n",
       " \"Context\\nThis dataset was assembled to investigate the possibility of predicting congressional election results by campaign finance reports from the period leading up to the election.\\nContent\\nEach row represents a candidate, with information on their campaign including the state, district, office, total contributions, total expenditures, etc. The content is specific to the year leading up to the 2016 election: (1/1/2015 through 10/19/2016).\\nAcknowledgements\\nCampaign finance information came directly from FEC.gov. Election results and vote totals for house races were taken from CNN's election results page.\\nInspiration\\nHow much of an impact does campaign spending and fundraising have on an election? Is the impact greater in certain areas? Given this dataset, to what degree of accuracy could we have predicted the election results?\",\n",
       " 'Context\\nThe Federal Housing Finance Agency House Price Index (HPI) is a broad measure of the movement of single-family house prices. The HPI is a weighted, repeat-sales index, meaning that it measures average price changes in repeat sales or refinancings on the same properties. The technical methodology for devising the index, collection, and publishing the data is at: http://www.fhfa.gov/PolicyProgramsResearch/Research/PaperDocuments/1996-03_HPI_TechDescription_N508.pdf\\nContent\\nContains monthly and quarterly time series from January 1991 to August 2016 for the U.S., state, and MSA categories. Analysis variables are the aggregate non-seasonally adjusted value and seasonally adjusted index values. The index value is 100 beginning January 1991.\\nAcknowledgements\\nThis data is found on Data.gov\\nInspiration\\nCan this data be combined with the corresponding census growth projections either at the state or MSA level to forecast 24 months out the highest and lowest home index values?',\n",
       " 'FAOSTAT provides access to over 3 million time-series and cross sectional data relating to food and agriculture. The full FAO data can be found in the large zipfile, while a (somewhat out of date) summary of FAOSTAT is in the top level csv files. FAOSTAT contains data for 200 countries and more than 200 primary products and inputs in its core data set. The national version of FAOSTAT, CountrySTAT, is being implemented in about 20 countries and three regions. It offers a two-way bridge amongst sub-national, national, regional and international statistics on food and agriculture.\\nAcknowledgements\\nThis dataset was kindly published by the United Nation on the UNData site. You can find the original dataset here.\\nLicense\\nPer the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference.',\n",
       " \"Introduction\\nFlaredown is an app that helps patients of chronic autoimmune and invisible illnesses improve their symptoms by avoiding triggers and evaluating their treatments. Each day, patients track their symptom severity, treatments and doses, and any potential environmental triggers (foods, stress, allergens, etc) they encounter.\\nAbout the data\\nInstead of coupling symptoms to a particular illness, Flaredown asks users to create their unique set of conditions, symptoms and treatments (“trackables”). They can then “check-in” each day and record the severity of symptoms and conditions, the doses of treatments, and “tag” the day with any unexpected environmental factors.\\nUser: includes an ID, age, sex, and country.\\nCondition: an illness or diagnosis, for example Rheumatoid Arthritis, rated on a scale of 0 (not active) to 4 (extremely active).\\nSymptom: self-explanatory, also rated on a 0–4 scale.\\nTreatment: anything a patient uses to improve their symptoms, along with an optional dose, which is a string that describes how much they took during the day. For instance “3 x 5mg”.\\nTag: a string representing an environmental factor that does not occur every day, for example “ate dairy” or “rainy day”.\\nFood: food items were seeded from the publicly-available USDA food database. Users have also added many food items manually.\\nWeather: weather is pulled automatically for the user's postal code from the Dark Sky API. Weather parameters include a description, precipitation intensity, humidity, pressure, and min/max temperatures for the day.\\nIf users do not see a symptom, treatment, tag, or food in our database (for instance “Abdominal Pain” as a symptom) they may add it by simply naming it. This means that the data requires some cleaning, but it is patient-centered and indicates their primary concerns.\\nSuggested Questions\\nDoes X treatment affect Y symptom positively/negatively/not at all? What are the most strongly-correlated symptoms and treatments?\\nAre there subsets within our current diagnoses that could more accurately represent symptoms and predict effective treatments?\\nCan we reliably predict what triggers a flare for a given user or all users with a certain condition?\\nCould we recommend treatments more effectively based on similarity of users, rather than specific symptoms and conditions? (Netflix recommendations for treatments)\\nCan we quantify a patient’s level of disease activity based on their symptoms? How different is it from our existing measures?\\nCan we predict which symptom should be treated to have the greatest effect on a given illness?\\nHow accurately can we guess a condition based on a user’s symptoms?\\nCan we detect new interactions between treatments?\\nPlease email logan@flaredown.com if you have questions about the project\",\n",
       " 'Context: BDRW is a real-world image dataset for developing machine learning and vision algorithms with minimal requirement on data pre-processing and formatting to identify digits of the decimal number system appearing in Bengali script. It can be seen as similar in flavor to SVHN (e.g., the images are of small cropped digits), but incorporates higher visual heterogeneity and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). BDRW is obtained from numbers appearing in photographs, printed materials, sign boards, wall writings, calendar or book pages, etc.\\nFile: BDRW_train.zip (contains BDRW_train_1.zip, BDRW_train_2.zip)\\nThe data in the two zip files are to be used together and together contain a set of .jpg images of different sized which are cropped from different photographs, magazine prints, wall writing images, etc. Each image represents a digit from the decimal number system written in Bengali (https://en.wikipedia.org/wiki/Bengali_numerals). The file labels.xls contains the number represented in each image which can be used as the ground truth labels for training a learning based system to recognize the Bengali numbers.\\nInspiration: This dataset is released for a machine vision challenge being hosted at IEEE TechSym 2016. The challenge will also include a testing set which includes samples not present in the training set released here and would be released after the challenge is closed.',\n",
       " \"Context\\nThe Philippine Statistics Authority (PSA) spearheads the conduct of the Family Income and Expenditure Survey (FIES) nationwide. The survey, which is undertaken every three (3) years, is aimed at providing data on family income and expenditure, including, among others, levels of consumption by item of expenditure, sources of income in cash, and related information affecting income and expenditure levels and patterns in the Philippines.\\nContent\\nInside this data set is some selected variables from the latest Family Income and Expenditure Survey (FIES) in the Philippines. It contains more than 40k observations and 60 variables which is primarily comprised of the household income and expenditures of that specific household\\nAcknowledgements\\nThe Philippine Statistics Authority for providing the publisher with their raw data\\nInspiration\\nSocio-economic classification models in the Philippines has been very problematic. In fact, not one SEC model has been widely accepted. Government bodies uses their own SEC models and private research entities uses their own. We all know that household income is the greatest indicator of one's socio-economic classification that's why the publisher would like to find out the following:\\n1) Best model in predicting household income 2) Key drivers of household income, we want to make the model as sparse as possible 3) Some exploratory analysis in the data would also be useful\",\n",
       " 'Context\\nWhile exploring the Aerial Bombing Operations of World War Two dataset (https://www.kaggle.com/usaf/world-war-ii), and recalling that the D-Day landings were nearly postponed due to poor weather, I sought out weather reports from the period to compare with missions in the bombing operations dataset.\\nContent\\nThe dataset contains information on weather conditions recorded on each day at various weather stations around the world. Information includes precipitation, snowfall, temperatures, wind speed and whether the day included thunder storms or other poor weather conditions.\\nAcknowledgements\\nThe data are taken from the United States National Oceanic and Atmospheric Administration (https://www.kaggle.com/noaa) National Centres for Environmental Information website: https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/world-war-ii-era-data\\nInspiration\\nThis dataset is mostly to assist with the analysis of the Aerial Bombing Operations dataset, also hosted on Kaggle.',\n",
       " 'Context:\\nBeing able to automatically answer questions accurately remains a difficult problem in natural language processing. This dataset has everything you need to try your own hand at this task. Can you correctly generate the answer to questions given the Wikipedia article text the question was originally generated from?\\nContent:\\nThere are three question files, one for each year of students: S08, S09, and S10, as well as 690,000 words worth of cleaned text from Wikipedia that was used to generate the questions.\\nThe \"question_answer_pairs.txt\" files contain both the questions and answers. The columns in this file are as follows:\\nArticleTitle is the name of the Wikipedia article from which questions and answers initially came.\\nQuestion is the question.\\nAnswer is the answer.\\nDifficultyFromQuestioner is the prescribed difficulty rating for the question as given to the question-writer.\\nDifficultyFromAnswerer is a difficulty rating assigned by the individual who evaluated and answered the question, which may differ from the difficulty in field 4.\\nArticleFile is the name of the file with the relevant article\\nQuestions that were judged to be poor were discarded from this data set.\\nThere are frequently multiple lines with the same question, which appear if those questions were answered by multiple individuals.\\nAcknowledgements:\\nThese data were collected by Noah Smith, Michael Heilman, Rebecca Hwa, Shay Cohen, Kevin Gimpel, and many students at Carnegie Mellon University and the University of Pittsburgh between 2008 and 2010. It is released here under CC BY_SA 3.0. Please cite this paper if you write any papers involving the use of the data above:\\nSmith, N. A., Heilman, M., & Hwa, R. (2008, September). Question generation as a competitive undergraduate course project. In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge.\\nYou may also like:\\nQuestion-Answer Jokes: Jokes of the question-answer form from Reddit\\'s r/jokes\\nStanford Question Answering Dataset: New Reading Comprehension Dataset on 100,000+ Question-Answer Pairs\\nQuestion Pairs Dataset: Can you identify duplicate questions?',\n",
       " \"In 2010, Kaggle launched its first competition, which was won by Jure Zbontar, who used a simple linear model. Since then a lot has changed. We've seen the rebirth of neural networks, the rise of Python, the creation of powerful libraries like XGBoost, Keras and Tensorflow.\\nThis is data set is a dump of all winners' posts from the Kaggle blog starting with Jure Zbontar. It allows us to track trends in the techniques, tools and libraries that win competitions.\\nThis is a simple dump. If there's demand, I can upload more detail (including comments and tags).\",\n",
       " \"Context\\nOn the data team at Stack Overflow, we spend a lot of time and energy thinking about tech ecosystems and how technologies are related to each other. One way to get at this idea of relationships between technologies is tag correlations, how often technology tags at Stack Overflow appear together relative to how often they appear separately. One place we see developers using tags at Stack Overflow is on their Developer Stories, or professional profiles/CVs/resumes. If we are interested in how technologies are connected and how they are used together, developers' own descriptions of their work and careers is a great place to get that.\\nContent\\nA network of technology tags from Developer Stories on the Stack Overflow online developer community website.\\nThis is organized as two tables:\\nstack_network_links contains links of the network, the source and target tech tags plus the value of the the link between each pair stack_network_nodes contains nodes of the network, the name of each node, which group that node belongs to (calculated via a cluster walktrap), and a node size based on how often that technology tag is used\\nAcknowledgements\\nAll Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.\",\n",
       " \"NASA tracks about 15,000 near-Earth objects -- small Solar System bodies whose orbits bring them less than 1.3 AU from the Sun (i.e., within 130% of the the average distance between the Earth and the Sun). Of these 15,000, 160 are comets. This dataset provides orbital data for these comets.\\nThe Data\\nNotes on Time and Space\\nTiming information for each of these comets is given in Barycentric Dynamical Time, or TDB. This is, very roughly, the number of days since January 1st, 4713 BC (see the Wikipedia article on Julian Day for more info). Check out those Wikipedia articles for details.\\nFor information on inclination, argument, and longitude of the ascending node, look at this article.\\nThe non-gravitational forces are effects that accelerate or decelerate the comet, such as jets of gas.\\nThis dataset contains the following fields:\\nObject: the name of the comet\\nEpoch: the epoch for the comet, in TDB\\nTP: time of perihelion passage, in TDB; this is the time when the comet was closest to the Sun\\ne: the orbital eccentricity of the comet\\ni: Inclination of the orbit with respect to the ecliptic plane and the equinox of J2000 (J2000-Ecliptic), in degrees\\nw: Argument of perihelion (J2000-Ecliptic), in degrees\\nNode: Longitude of the ascending node (J2000-Ecliptic), in degrees\\nq: comet's distance at perihelion, in AU\\nQ: comet's distance at aphelion, in AU\\nP: orbital period, in Julian years\\nA1: Non-gravitational force parameter A1\\nA2: Non-gravitational force parameter A2\\nA3: Non-gravitational force parameter A3\\nMOID (AU): Minimum orbit intersection distance (the minimum distance between the osculating orbits of the NEO and the Earth)\\nref: Orbital solution reference\\nWhat Should We Try?\\nWhat can we do with this dataset? - plot the comets' orbits - combine with Earth's orbital data to predict close approaches\\nAcknowledgements\\nThis dataset was downloaded from the NASA data portal.\",\n",
       " 'Context\\nOpen Payments is a national disclosure program created by the Affordable Care Act (ACA) and managed by Centers for Medicare & Medicaid Services (CMS). The purpose of the program is to promote transparency into the financial relationships between pharmaceutical and medical device industries, and physicians and teaching hospitals. The financial relationships may include consulting fees, research grants, travel reimbursements, and payments from industry to medical practitioners.\\nContent\\nThere are 3 datasets that represent 3 different payment types:\\nGeneral Payments: Payments not made in connection with a research agreement. This dataset contains 65 variables.\\nResearch Payments: Payments made in connection with a research agreement. This dataset contains 166 variables.\\nPhysician Ownership or Investment Interest: Information about physicians who hold ownership or investment interest in the manufacturer/GPO or who have an immediate family member holding such interest. This dataset contains 29 variables.\\nDeleted/Removed Records: Contains any deleted/removed records.\\nA comprehensive methodology overview and data dictionary for each dataset can be found here.\\nAcknowledgements\\nThe original datasets can be found here.\\nInspiration\\nUsing the General Payments dataset, can you determine any trends in the total amount of payment to hospitals and physicians across the medical specialties or by the form/nature of the payments?\\nAccording to the Research Payments dataset, which area(s) of research or the type of drug/medical device receive the most amount of payment?',\n",
       " 'International Financial Statistics (IFS) is a standard source of international statistics on all aspects of international and domestic finance. It reports, for most countries of the world, current data needed in the analysis of problems of international payments and of inflation and deflation, i.e., data on exchange rates, international liquidity, international banking, money and banking, interest rates, prices, production, international transactions, government accounts, and national accounts. Last update in UNdata: 14 May 2010 If you need more current data, the IMF has made their current database available for bulk download for personal use.\\nAcknowledgements\\nThis dataset was kindly published by the United Nations on the UNData site. You can find the original dataset here.\\nLicense\\nPer the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference.',\n",
       " \"Context\\nThis dataset includes a log of all physical item checkouts from Seattle Public Library. The dataset begins with checkouts occurring in April 2005, and is regularly updated. Renewals are not included.\\nContent\\nThe dataset contains several types of files:\\nCheckout records hold the raw data. I've dropped several columns from these files in order to shrink the total dataset size down from a couple of dozen gigabytes; they can be rebuilt by merging with the library collection inventory.\\nThe data dictionary allows you to decode the 'ItemType' column from the checkout records.\\nThe library collection inventory is a dataset in its own right and stores important metadata about each title, such as the author and subjects.\\nInspiration\\nCan you predict which books will be checked out in the coming month? SPL posts fresh data every month, so you can check your forecast by downloading the new data from them.\\nWith a bit of imagination, this can be a great dataset for logistics modeling. Make some assumptions about each location's storage capacity and where a book was checked out from and you've got a warehouse resource allocation problem.\\nAcknowledgements\\nThis dataset was kindly made available by the Seattle Public Library. You can find the original copies of the three component datasets at the following links: - Collection data - Data dictionary - Checkout records\\nDisclaimer\\nThe data made available here has been modified for use from its original source, which is the City of Seattle. Neither the City of Seattle nor the Office of the Chief Technology Officer (OCTO) makes any claims as to the completeness, timeliness, accuracy or content of any data contained in this application; makes any representation of any kind, including, but not limited to, warranty of the accuracy or fitness for a particular use; nor are any such warranties to be implied or inferred with respect to the information or data furnished herein. The data is subject to change as modifications and updates are complete. It is understood that the information contained in the web feed is being used at one's own risk.\\nFor the complete terms of use, please see the City of Seattle Data Policy.\",\n",
       " \"Context\\nCollection of chat messages in night urban city between boys and girls.\\nContent\\nData set of messages (more than 1 million of rows) in Russian language from teenager population taken in period from 2012 to 2016 inclusive\\nAcknowledgements\\nAll personal info in the message' body were taken from public web source, and, though, are free of use.\\nInspiration\\nThis dataset can be used to classify chat messages as male / female.\\nKey objectives\\nExtract phone numbers from messages. All phone numbers are located in Ukraine and belongs to one from next operators\\n+380 50\\n+380 95\\n+380 66\\n+380 99\\n+380 63\\n+380 73\\n+380 93\\n+380 68\\n+380 67\\n+380 96\\n+380 97\\n+380 98\\nClassify chat messages by gender (male/female)\",\n",
       " \"Context\\nSince industrialization, there has been an increasing concern about environmental pollution. As mentioned in the WHO report 7 million premature deaths annually linked to air pollution , air pollution is the world's largest single environmental risk. Moreover as reported in the NY Times article, India’s Air Pollution Rivals China’s as World’s Deadliest it has been found that India's air pollution is deadlier than even China's.\\nUsing this dataset, one can explore India's air pollution levels at a more granular scale.\\nContent\\nThis data is combined(across the years and states) and largely clean version of the Historical Daily Ambient Air Quality Data released by the Ministry of Environment and Forests and Central Pollution Control Board of India under the National Data Sharing and Accessibility Policy (NDSAP).\\nVisualization of the Mean RSPM values over the years\\nInspiration\\nCan we detect local trends? Can we relate the air quality changes to changes in Environmental policy in India?\\nAcknowledgements\\nVishal Subbiah (Data downloading)\",\n",
       " 'Context\\nThis dataset complements https://github.com/vmalyi/run-or-walk project which aims to detect whether the person is running or walking based on deep neural network and sensor data collected from iOS device.\\nThis dataset has been accumulated with help of \"Data Collection\" iOS app specially developed for this purpose: https://github.com/vmalyi/run-or-walk/tree/master/ios_app_data_collection.\\nPlease note that this app is not available in the AppStore yet.\\nContent\\nCurrently, the dataset contains a single file which represents 88588 sensor data samples collected from accelerometer and gyroscope from iPhone 5c in 10 seconds interval and ~5.4/second frequency. This data is represented by following columns (each column contains sensor data for one of the sensor\\'s axes):\\nacceleration_x\\nacceleration_y\\nacceleration_z\\ngyro_x\\ngyro_y\\ngyro_z\\nThere is an activity type represented by \"activity\" column which acts as label and reflects following activities:\\n\"0\": walking\\n\"1\": running\\nApart of that, the dataset contains \"wrist\" column which represents the wrist where the device was placed to collect a sample on:\\n\"0\": left wrist\\n\"1\": right wrist\\nAdditionally, the dataset contains \"date\", \"time\" and \"username\" columns which provide information about the exact date, time and user which collected these measurements.',\n",
       " \"Context\\nThe data was scraped from several websites in Czech Republic and Germany over a period of more than a year. Originally I wanted to build a model for estimating whether a car is a good buy or a bad buy based on the posting. But I was unable to create a model I could be satisfied with and now have no use for this data. I'm a great believer in open data, so here goes.\\nContent\\nThe scrapers were tuned slowly over the course of the year and some of the sources were completely unstructured, so as a result the data is dirty, there are missing values and some values are very obviously wrong (e.g. phone numbers scraped as mileage etc.)\\nThere are roughly 3,5 Million rows and the following columns:\\nmaker - normalized all lowercase\\nmodel - normalized all lowercase\\nmileage - in KM\\nmanufacture_year\\nengine_displacement - in ccm\\nengine_power - in kW\\nbody_type - almost never present, but I scraped only personal cars, no motorcycles or utility vehicles\\ncolor_slug - also almost never present\\nstk_year - year of the last emission control\\ntransmission - automatic or manual\\ndoor_count\\nseat_count\\nfuel_type - gasoline, diesel, cng, lpg, electric\\ndate_created - when the ad was scraped\\ndate_last_seen - when the ad was last seen. Our policy was to remove all ads older than 60 days\\nprice_eur - list price converted to EUR\\nInspiration\\nWhich factors determine the price of a car?\\nWith what accuracy can the price be predicted?\\nCan a model trained on all cars be used to accurately predict prices of models with only a few samples?\\nIn my analysis, there is too much variance even within individual models to reliably predict the price, can you prove me wrong? I would love to understand what I did wrong if you can.\",\n",
       " \"Context\\nStarting something in FinTech is the most difficult thing. You have no open data. These days I'm trying to do some algo-trading. Maybe not in true sense, because it's not high frequency scalping. But anyway that's that.\\nWhat?\\nThe data gives almost-Realtime data for half of the Nifty 50 stocks for last week of May and first 2 Weeks of July.\\nNow here is the obvious question. The dataset does not have timestamp. That's because it is collected via Web-Socket streaming as it happens. Sometimes once in a couple of seconds, sometimes 10-15 times in the same span. So there is no point to timestamp IMHO. Anyway it'll be client-side timestamp, so not a true timestamp.\\nDescription\\ntick_data.csv contains only the price-volume data.\\nvolume: total volumes traded for the day\\nlast_price: denotes the quote price for latest trade\\nList item instrument_list.csv contains description of the underlying instrument.\\nP.S:\\n*All the data points are not tick-by-tick update. Rather it is mostly an update after 600 ms, provided a trade happened *\",\n",
       " 'What is the world saying about Donald Trump? Find out in this dataset of over 37,000 Reddit comments about the new US president.\\nPhoto credit: Gage Skidmore, CC BY-SA 2.0',\n",
       " \"Context\\nList of all UFC fights since 2013 with summed up entries of each fighter's round by round record preceding that fight. Created in the attempt to create a UFC fight winner predictor. Dataset may not be great, I'm still new to this thing so appreciate any tips on cleaning up the set.\\nContent\\nEach row represents a single fight - with each fighter's previous records summed up prior to the fight. blank stats mean its the fighter's first fight since 2013 which is where granular data for UFC fights beings\\nAcknowledgements\\nhttps://github.com/valish/ufc-api for the UFC api Beautifulsoup and it's creators and Hitkul my partner in crime\\nInspiration\\nCan we draw decent predictions from this dataset?\",\n",
       " \"Context\\nThis is all of Shakespeare's plays.\\nContent\\nThis is a dataset comprised of all of Shakespeare's plays. It includes the following:\\nThe first column is the Data-Line, it just keeps track of all the rows there are.\\nThe second column is the play that the lines are from.\\nThe third column is the actual line being spoken at any given time.\\nThe fourth column is the Act-Scene-Line from which any given line is from.\\nThe fifth column is the player who is saying any given line.\\nThe sixth column is the line being spoken.\\nInspiration\\nI've been doing Shakespeare for a while and I wanted to make a Shakespearean chatbot.\",\n",
       " 'US Adult Census data relating income to social factors such as Age, Education, race etc.\\nThe Us Adult income dataset was extracted by Barry Becker from the 1994 US Census Database. The data set consists of anonymous information such as occupation, age, native country, race, capital gain, capital loss, education, work class and more. Each row is labelled as either having a salary greater than \">50K\" or \"<=50K\".\\nThis Data set is split into two CSV files, named adult-training.txt and adult-test.txt.\\nThe goal here is to train a binary classifier on the training dataset to predict the column income_bracket which has two possible values \">50K\" and \"<=50K\" and evaluate the accuracy of the classifier with the test dataset.\\nNote that the dataset is made up of categorical and continuous features. It also contains missing values The categorical columns are: workclass, education, marital_status, occupation, relationship, race, gender, native_country\\nThe continuous columns are: age, education_num, capital_gain, capital_loss, hours_per_week\\nThis Dataset was obtained from the UCI repository, it can be found on\\nhttps://archive.ics.uci.edu/ml/datasets/census+income, http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/\\nUSAGE This dataset is well suited to developing and testing wide linear classifiers, deep neutral network classifiers and a combination of both. For more info on Combined Deep and Wide Model classifiers, refer to the Research Paper by Google https://arxiv.org/abs/1606.07792\\nRefer to this kernel for sample usage : https://www.kaggle.com/johnolafenwa/wage-prediction\\nComplete Tutorial is available from http://johnolafenwa.blogspot.com.ng/2017/07/machine-learning-tutorial-1-wage.html?m=1',\n",
       " 'Context\\nThis dataset consist of data From 1985 Ward\\'s Automotive Yearbook. Here are the sources\\nSources:\\n1) 1985 Model Import Car and Truck Specifications, 1985 Ward\\'s Automotive Yearbook. 2) Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038 3) Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037\\nContent\\nThis data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process \"symboling\". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\\nThe third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.\\nNote: Several of the attributes in the database could be used as a \"class\" attribute.\\nInspiration\\nPlease bring it on whatever inferences you can get it.',\n",
       " 'Context\\nThe previous New York City policies eliminated all housing resources for homeless families and single adults. I wanted to see the consequences.\\nContent\\n\"These raw data sets contain Point-in-Time (PIT) estimates and national PIT estimates of homelessness as well as national estimates of homelessness by state and estimates of chronic homelessness from 2007 - 2016. Estimates of homeless veterans are also included beginning in 2011. The accompanying Housing Inventory Count (HIC) data is available as well from 2007 - 2016.\" (Department of Housing and Urban Development\\nAcknowledgements\\nI would like to thank Matthew Schnars for providing this dataset from: https://www.hudexchange.info/resource/3031/pit-and-hic-data-since-2007/\\nInspiration\\nMany of our fellow human beings go through hardships that we would never know about. But it\\'s our obligation as a society to take care of one another. That\\'s why I was hoping this dataset shine light on some of the challenges our cities and states are still facing in this topic.',\n",
       " \"Context\\nThis dataset is a compilation of 2.7 million news headlines published by Times of India from 2001 to 2017, 17 years.\\nA majority of the data is focusing on Indian local news including national, city level and entertainment.\\nAgency Website: https://timesofindia.indiatimes.com\\nPrepared by Rohit Kulkarni\\nContent\\nCSV Rows: 2,735,347\\npublish_date: Date of the article being published online in yyyyMMdd format\\nheadline_category: Category of the headline, ascii, dot delimited, lowercase values\\nheadline_text: Text of the Headline in English, very rare non-ascii characters\\nStart Date: 2001-01-01 End Date: 2017-12-31\\nSee This Kernal for Overview of Trends and Categories\\nInspiration\\nThis News Dataset is a persistent historical archive of noteable events in the Indian subcontinent from start-2001 to end-2017, recorded in real time by the journalists of India.\\nTimes Group as a news agency, reaches out a very wide audience across Asia and drawfs every other agency in the quantity of English Articles published per day. Due to the heavy daily volume (avg. 650 articles) over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues and talking points and how they have unfolded over time.\\nIt is possible to chop this dataset into a smaller piece for a more focused analysis, based on one or more facets.\\nTime Range: Records during 2014 election, 2006 Mumbai Bombings\\nOne or more Categories: like Mumbai, Movie Releases, ICC updates, Magazine, Middle East\\nOne or more Keywords: like crime or ecology related words; names of political parties, celebrities, corporations.\\nAcknowledgements\\nThe headlines are extracted from several GB of raw HTML files using Jsoup, Java and Bash. The entire process takes 3.5 minutes.\\nThis logic also : chooses the best worded headline for each article (longest one is usually picked) ; clusters about 17k categories to 200 large groups ; removes records where the date is ambiguous (9k cases) ; finally cleans the selected headline via a string 'domestication' function (which I use for any wild text from the internet).\\nThe final categories are as per the latest sitemap. Around 1.5k rare categories remain and these records (~20k) can be filtered out easily during analysis. The category is unknown for ~200k records.\\nSimilar news datasets exploring other attributes, countries and topics can be seen on my profile.\\nCitation for usage:\\nRohit Kulkarni (2017), News Headlines of India 2001-2017 [CSV data file], doi:10.7910/DVN/J7BYRX, Retrieved from: [this url]\",\n",
       " 'Context\\nThe data is technical spec of cars. The dataset is downloaded from UCI Machine Learning Repository\\nContent\\nTitle: Auto-Mpg Data\\nSources: (a) Origin: This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The dataset was used in the 1983 American Statistical Association Exposition. (c) Date: July 7, 1993\\nPast Usage:\\nSee 2b (above)\\nQuinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\nRelevant Information:\\nThis dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute \"mpg\", 8 of the original instances were removed because they had unknown values for the \"mpg\" attribute. The original dataset is available in the file \"auto-mpg.data-original\".\\n\"The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (Quinlan, 1993)\\nNumber of Instances: 398\\nNumber of Attributes: 9 including the class attribute\\nAttribute Information:\\nmpg: continuous\\ncylinders: multi-valued discrete\\ndisplacement: continuous\\nhorsepower: continuous\\nweight: continuous\\nacceleration: continuous\\nmodel year: multi-valued discrete\\norigin: multi-valued discrete\\ncar name: string (unique for each instance)\\nMissing Attribute Values: horsepower has 6 missing values\\nAcknowledgements\\nDataset: UCI Machine Learning Repository Data link : https://archive.ics.uci.edu/ml/datasets/auto+mpg\\nInspiration\\nI have used this dataset for practicing my exploratory analysis skills.',\n",
       " 'Context\\nThis dataset contains the prevalence and trends of tobacco use for 1995-2010. Percentages are weighted to population characteristics. Data are not available if it did not meet Behavioral Risk Factor Surveillance System (BRFSS) stability requirements. For more information on these requirements, as well as risk factors and calculated variables, see the Technical Documents and Survey Data for a specific year - http://www.cdc.gov/brfss/annual_data/annual_data.htm.\\nContent\\nThis dataset has 7 variables:\\nYear\\nState\\nSmoke everyday\\nSmoke some days\\nFormer smoker\\nNever smoked\\nLocation 1\\nAcknowledgements\\nThe original dataset can be found here.\\nRecommended citation: Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System. Atlanta, Georgia: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, [appropriate year].\\nInspiration\\nHow does tobacco use change over time?\\nDoes tobacco use differ by state?',\n",
       " 'Context\\nLatest poverty and inequality indicators compiled from officially recognized international sources. Poverty indicators include the poverty headcount ratio, poverty gap, and number of poor at both international and national poverty lines. Inequality indicators include the Gini index and income or consumption distributions. The database includes national, regional and global estimates.\\nContent\\nThis dataset contains country names and indicator variables from 1974 until 2015. Additional materials and detailed descriptions of the datasets can be downloaded from here.\\nAcknowledgement\\nThe original datasets and data dictionaries can be found here.\\nInspiration\\nSome ideas for exploring the dataset:\\nHow does the poverty headcount ratio differ across countries? Can you visualize the temporal trend?\\nWhich countries have the highest or lowest GINI index as estimated by the World Bank? Is this indicator correlated with other indicators such as urban/rural poverty or income status?',\n",
       " 'Context\\nThe Gun Violence Archive is an online archive of gun violence incidents collected from over 2,000 media, law enforcement, government and commercial sources daily in an effort to provide near-real time data about the results of gun violence. GVA in an independent data collection and research group with no affiliation with any advocacy organization.\\nContent\\nThis dataset includes files that separate gun violence incidents by category, including deaths and injuries of children and teens, and a collection of mass shootings.\\nInspiration\\nWhat has been the trend of gun violence in the past few years?\\nWhat states have the highest incidents per capita per year? How has this metric changed over time?\\nAre officer involved shootings on the rise? Where are they most concentrated? Do they correlate with the rates of accidental deaths and mass shootings?\\nAcknowledgements\\nThis dataset is owned by the Gun Violence Archive, and can be accessed in its original form here.',\n",
       " 'This dataset consists of the official statistics on the 11,538 athletes and 306 events at the 2016 Olympic Games in Rio de Janeiro. The athletes file includes the name, nationality (as a three letter IOC country code), gender, age (as date of birth), height in meters, weight in kilograms, sport, and quantity of gold, silver, and/or bronze medals won for every Olympic athlete at Rio. The events file lists the name, sport, discipline (if available), gender of competitors, and venue(s) for every Olympic event at Rio 2016.\\nCREDITS\\nSource Data: Rio 2016 website\\nData Files: GitHub user flother',\n",
       " \"Context\\nThese datasets provides an opportunity to perform analyses on the fashion trend of innerwear and swimwear products.\\nContent\\nThey were created by extracting data from from the popular retail sites via PromptCloud's data extraction solutions.\\nSites covered:\\nAmazon\\nVictoria's Secret\\nBtemptd\\nCalvin Klein\\nHanky Panky\\nAmerican Eagle\\nMacy's\\nNordstrom\\nTopshop USA\\nTime period: June, 2017 to July, 2017\\nInspiration\\nSome of the most common questions that can be answered are:\\nHow does the pricing differ depending on the brand?\\nTopic modelling on the product description\\nWhat are the most common color used by different brands?\\nAnalyses on the product ratings (wherever applicable)\\nCommon style attributes (wherever applicable)\",\n",
       " 'Context\\nBankSim is an agent-based simulator of bank payments based on a sample of aggregated transactional data provided by a bank in Spain. The main purpose of BankSim is the generation of synthetic data that can be used for fraud detection research. Statistical and a Social Network Analysis (SNA) of relations between merchants and customers were used to develop and calibrate the model. Our ultimate goal is for BankSim to be usable to model relevant scenarios that combine normal payments and injected known fraud signatures. The data sets generated by BankSim contain no personal information or disclosure of legal and private customer transactions. Therefore, it can be shared by academia, and others, to develop and reason about fraud detection methods. Synthetic data has the added benefit of being easier to acquire, faster and at less cost, for experimentation even for those that have access to their own data. We argue that BankSim generates data that usefully approximates the relevant aspects of the real data.\\nContent\\nWe ran BankSim for 180 steps (approx. six months), several times and calibrated the parameters in order to obtain a distribution that get close enough to be reliable for testing. We collected several log files and selected the most accurate. We injected thieves that aim to steal an average of three cards per step and perform about two fraudulent transactions per day. We produced 594643 records in total. Where 587443 are normal payments and 7200 fraudulent transactions. Since this is a randomised simulation the values are of course not identical to original data.\\nAcknowledgements\\nThis research was conducted during my PhD studies in Sweden at Blekinge Institute of Technology (BTH ww.bth.se). More about it: http://edgarlopez.net\\nOriginal paper\\nPlease refer to this dataset using the following citations:\\nLopez-Rojas, Edgar Alonso ; Axelsson, Stefan Banksim: A bank payments simulator for fraud detection research Inproceedings 26th European Modeling and Simulation Symposium, EMSS 2014, Bordeaux, France, pp. 144–152, Dime University of Genoa, 2014, ISBN: 9788897999324. https://www.researchgate.net/publication/265736405_BankSim_A_Bank_Payment_Simulation_for_Fraud_Detection_Research',\n",
       " \"Context\\nReligious texts play a key role in ISIS ideology, propaganda, and recruitment. This dataset is a compilation of all of the religious and ideological texts (Muslim, Christian, Jewish, and other) used in ISIS English-based magazines.\\nContent\\nWe scraped 15 issues of Dabiq (6/2014 to 7/2016) and 9 issues of Rumiyah (9/2016 to 5/2017) producing a total of 2,685 texts. We classified the data as follows:\\nMagazine: Dabiq or Rumiyah\\nIssue #\\nDate (Month and Year)\\nType of Text (Qur'an, Hadeeth, Religious Figure, etc)\\nSource: What the source of the text was\\nThe quote itself\\nPurpose: Support ISIS, Refute Another Group\\nThe article from which the quote is derived\\nAcknowledgements\\nWe would like to thank Asma Shah for helping to compile this dataset. Asma is a junior at the University of Maryland, College Park, where she is studying criminal justice and computer science. She is currently an intern with the Department of Justice where she does data science work. She has previously done counter-terrorism research with Fifth Tribe and the National Consortium for the Studies of Terrorism and Responses to Terrorism.\\nWe would also like to express our gratitude to Abdul Aziz Suraqah (Canada), Saif ul Hadi (India), and Abdulbasit Kassam (Nigeria) for helping with the classification of some of the more obscure texts.\\nInspiration\\nWe would like this data to be analyzed by religious clerics to develop rebuttals of ISIS propaganda, data scientists to generate insights from the texts, and policymakers to understand how faith can shape countering violent extremism efforts. We also need help classifying some of the data that could not be identified and has been marked 'unknown.'\",\n",
       " \"Context\\nThe script used to acquire all of the following data can be found in this GitHub repository. This repository also contains the modeling codes and will be updated continually, so welcome starring or watching!\\nStock market data can be interesting to analyze and as a further incentive, strong predictive models can have large financial payoff. The amount of financial data on the web is seemingly endless. A large and well structured dataset on a wide array of companies can be hard to come by. Here provided a dataset with historical stock prices (last 12 years) for 29 of 30 DJIA companies (excluding 'V' because it does not have the whole 12 years data).\\n      ['MMM', 'AXP', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DIS', 'XOM', 'GE',\\n\\n      'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE', 'PFE',\\n\\n      'PG', 'TRV', 'UTX', 'UNH', 'VZ', 'WMT', 'GOOGL', 'AMZN', 'AABA']\\nIn the future if you wish for a more up to date dataset, this can be used to acquire new versions of the .csv files.\\nContent\\nThe data is presented in a couple of formats to suit different individual's needs or computational limitations. I have included files containing 13 years of stock data (in the all_stocks_2006-01-01_to_2018-01-01.csv and corresponding folder) and a smaller version of the dataset (all_stocks_2017-01-01_to_2018-01-01.csv) with only the past year's stock data for those wishing to use something more manageable in size.\\nThe folder individual_stocks_2006-01-01_to_2018-01-01 contains files of data for individual stocks, labelled by their stock ticker name. The all_stocks_2006-01-01_to_2018-01-01.csv and all_stocks_2017-01-01_to_2018-01-01.csv contain this same data, presented in merged .csv files. Depending on the intended use (graphing, modelling etc.) the user may prefer one of these given formats.\\nAll the files have the following columns: Date - in format: yy-mm-dd\\nOpen - price of the stock at market open (this is NYSE data so all in USD)\\nHigh - Highest price reached in the day\\nLow Close - Lowest price reached in the day\\nVolume - Number of shares traded\\nName - the stock's ticker name\\nInspiration\\nThis dataset lends itself to a some very interesting visualizations. One can look at simple things like how prices change over time, graph an compare multiple stocks at once, or generate and graph new metrics from the data provided. From these data informative stock stats such as volatility and moving averages can be easily calculated. The million dollar question is: can you develop a model that can beat the market and allow you to make statistically informed trades!\\nAcknowledgement\\nThis Data description is adapted from the dataset named 'S&P 500 Stock data'. This data is scrapped from Google finance using the python library 'pandas_datareader'. Special thanks to Kaggle, Github and the Market.\",\n",
       " 'This dataset does not have a description yet.',\n",
       " 'Context\\nThis dataset contains 4242 images of flowers. The data collection is based on the data flicr, google images, yandex images. You can use this datastet to recognize plants from the photo.\\nContent\\nThe pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion. For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!',\n",
       " 'Description:\\nHappyDB is a corpus of more than 100,000 happy moments crowd-sourced via Amazon’s Mechanical Turk.\\nEach worker is given the following task: What made you happy today? Reflect on the past 24 hours, and recall three actual events that happened to you that made you happy. Write down your happy moment in a complete sentence. (Write three such moments.)\\nThe goal of the corpus is to advance the understanding of the causes of happiness through text-based reflection.\\nMore information is available on the HappyDB website (https://rit-public.github.io/HappyDB/).\\nContent:\\ncleaned_hm.csv: the cleaned-up corpus of 100,000 crowd-sourced happy moments. For cleaning up, we have done spell checking over the whole corpus, and remove empty or one-word statements. The raw happy moments are retained for reference, and the author of each happy moment is represented by the his/her worker ID.\\ndemographic.csv: the demographic information of the worker who provided the moment. The information includes worker id, age, country, gender, marital status, and status of parenthood.\\nHave fun with the data! Feel free to contact us with any questions.\\nSample questions:\\nTo provide some inspiration, here are a few sample interesting exploration questions.\\nWhat are the popular sports/movies/books/purchased products/tourist destinations/... that make people happy?\\nCan we predict gender/marriage status/parenthood/age groups based on happy moment texts?\\nHow many indoor and outdoor activities are in the corpus respectively?\\nCan we find interesting ways of clustering happy moments?\\nCitation:\\nPlease cite the following publication if you are using the dataset for your work:\\nHappyDB: A Corpus of 100,000 Crowdsourced Happy Moments, LREC 2018 (to appear)\\nAkari Asai, Sara Evensen, Behzad Golshan, Alon Halevy, Vivian Li, Andrei Lopatenko, Daniela Stepanov, Yoshihiko Suhara, Wang-Chiew Tan and Yinzhan Xu',\n",
       " \"Context\\nDeskdrop is an internal communications platform developed by CI&T, focused in companies using Google G Suite. Among other features, this platform allows companies employees to share relevant articles with their peers, and collaborate around them.\\nContent\\nThis rich and rare dataset contains a real sample of 12 months logs (Mar. 2016 - Feb. 2017) from CI&T's Internal Communication platform (DeskDrop).\\nI contains about 73k logged users interactions on more than 3k public articles shared in the platform.\\nThis dataset features some distinctive characteristics:\\nItem attributes: Articles' original URL, title, and content plain text are available in two languages (English and Portuguese).\\nContextual information: Context of the users visits, like date/time, client (mobile native app / browser) and geolocation.\\nLogged users: All users are required to login in the platform, providing a long-term tracking of users preferences (not depending on cookies in devices).\\nRich implicit feedback: Different interaction types were logged, making it possible to infer the user's level of interest in the articles (eg. comments > likes > views).\\nMulti-platform: Users interactions were tracked in different platforms (web browsers and mobile native apps)\\nIf you like it, please upvote!\\nTake a look in these featured Python kernels:\\n- Deskdrop datasets EDA: Exploratory analysis of the articles and interactions in the dataset\\n- DeskDrop Articles Topic Modeling: A statistical analysis of the main articles topics using LDA\\n- Recommender Systems in Python 101: A practical introduction of the main Recommender Systems approaches: Popularity model, Collaborative Filtering, Content-Based Filtering and Hybrid Filtering.\\nAcknowledgements\\nWe thank CI&T for the support and permission to share a sample of real usage data from its internal communication platform: Deskdrop.\\nInspiration\\nThe two main approaches for Recommender Systems are Collaborative Filtering and Content-Based Filtering.\\nIn the RecSys community, there are some popular datasets available with users ratings on items (explicit feedback), like MovieLens and Netflix Prize, which are useful for Collaborative Filtering techniques.\\nTherefore, it is very difficult to find open datasets with additional item attributes, which would allow the application of Content-Based filtering techniques or Hybrid approaches, specially in the domain of ephemeral textual items (eg. articles and news).\\nNews datasets are also reported in academic literature as very sparse, in the sense that, as users are usually not required to log in in news portals, IDs are based on device cookies, making it hard to track the users page visits in different portals, browsing sessions and devices.\\nThis difficult scenario for research and experiments on Content Recommender Systems was the main motivation for the sharing of this dataset.\",\n",
       " 'Brazilian? You can read a Portuguese version of this article here.\\nContext\\nLast year, while I was attending a data science course in Germany, my country was impeaching its president. My colleagues asked me to explain what was happening in Brazil and the possible political outcomes in South America. Although I was able to give a general context and tell multiple arguments in favor and against the impeachment, deep inside, my answer was \"I really don\\'t know\".\\nUnderstanding what happens in Politics is something that takes a lot of effort and research. When I decided I had to use my tech skills to make myself a better citizen, I dived into government data and started Operation Serenata de Amor.\\nAfter reporting hundreds of politicians for small acts of corruption and learning how to encourage the population to engage in the democratic processes, my studies drove me to understand the legislative activity.\\nBrazilians elect 594 citizens to be their representatives in the National Congress. How can we be sure that they are not defending their own interests or those who paid for their campaigns? My way, as a data scientist, is to ask the data.\\nContent\\nThe National Congress of Brazil is composed of a Lower (Chamber of Deputies) and an Upper House (Federal Senate). In the first version of this dataset, you are going to find data only from the Chamber of Deputies. With 513 representatives, 86% of the congresspeople, I hope you have enough data to explore for some time.\\nWould be impossible for me, a citizen without government ties, to collect this data without the help of public servants. I processed 9,717 fixed-width files and 73 XML\\'s made officially available by the Chamber of Deputies and created 5 CSV\\'s containing the same information. Multiple fields of the same file telling the same thing (e.g. body_id, body_name and body_abbreviation) were removed.\\nData on session attendance, votes, and propositions since past century were collected and scripted in a reproducible manner. The data collection and pre-processing scripts are available in a GitHub repository, under an open source license.\\nEverything was collected from the Chamber of Deputies website at December 27, 2017, containing the whole legislative activity of the year. Presence and votes date from 1999, propositions go as far as 1946.\\nWhen in question about the legislative process and how the sessions work in real world, the Internal Regulation of the Chamber of Deputies is the best Portuguese documentation for research. It\\'s free!\\nAcknowledgements\\nSince the data was collected from a government website and the Brazilian law states that access to this information is free to any citizen, I am placing my own work published here in Public Domain.\\nI\\'d like to thank the hundreds of people financially supporting the work of Operation Serenata de Amor and those responsible for passing the Information Access bill in 2011.\\nInspiration\\nThe legislative activity should tell the history while it\\'s happening. How much has the Congress changed over the past decades? Do the congresspeople maintain the same political views or they vary on a weekly basis? Do people vote together with their state or party peers? How often? Can you model an algorithm to tell us the real parties inside Brazilian Congress?',\n",
       " 'Introduction\\nHere you find a very simple, beginner-friendly data set. No sparse matrices, no fancy tools needed to understand what\\'s going on. Just a couple of rows and columns. Super simple stuff. As explained below, this data set is used for a competition. As it turns out, this competition tends to reveal a common truth in data science: KISS - Keep It Simple Stupid\\nWhat is so special about this data set is, given it\\'s simplicity, it pays off to use \"simple\" classifiers as well. This year\\'s competition was won by a C5.0 . Can you do better?\\nDescription\\nWe are looking at cold call results. Turns out, same salespeople called existing insurance customers up and tried to sell car insurance. What you have are details about the called customers. Their age, job, marital status, whether the have home insurance, a car loan, etc. As I said, super simple.\\nWhat I would love to see is some of you applying some crazy XGBoost classifiers, which we can square off against some logistic regressions. It would be curious to see what comes out on top. Thank you for your time, I hope you enjoy using the data set.\\nAcknowledgements\\nThanks goes to the Decision Science and Systems Chair of Technical University of Munich (TUM) for getting the data set from a real world company and making it available to be shared publicly. Also Vladimir Fux, who oversees the challenge associated with this data set.\\nInspiration\\nThis is a data set used for teaching entry level data mining skills at the TUM. Every year there is a competition as part of the curriculum of a particular course. This Data Mining Cup teaches some of the very fundamentals that are always worthy to be revisited, especially by pros abundant at Kaggle. For some of my thoughts see the verbose comments in the Kernel.',\n",
       " 'Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nMinistry of Human Resource Development (MHRD), Govt of India has initiated an All India Survey on Higher Education (AISHE) in the year 2010-11 to build a robust database and to assess the correct picture of higher Education in the country.\\nThe main objectives of the survey was to\\nidentify & capture all the institutions of higher learning in the country\\nCollect the data from all the higher education institutions on various aspects of higher education.\\nData was collected on following broad items\\nInstitution’s Basic Details\\nTeacher’s Details\\nDetails of Non-Teaching Staff\\nProgramme conducted under various Faculties/Schools & Departments/Centres\\nStudents enrolled in these Programme\\nExamination result of terminal year of each Programme\\nFinancial Information such as Receipt and Expenditure under various heads\\nAvailability of Infrastructure\\nScholarships, Loans & Accreditation\\nsource: AISHE(pdf)\\nContent\\nThis dataset contains unit level data from AISHE from 2011-12 to 2015-16.\\ncsv file list:\\naccreditation.csv\\ncollege.csv\\ncollege_institution.csv\\ncollege_institution_accreditation.csv\\ncollege_institution_department.csv\\ncollege_institution_faculty.csv\\ncollege_institution_non_teaching_staff_count.csv\\ncollege_institution_student_hostel.csv\\ncollege_institution_teaching_staff.csv\\ncollege_institution_teaching_staff_sanctioned_strength.csv\\ncourse.csv\\ncourse_enrolled_foreign_student_count.csv\\ncourse_enrolled_student_count.csv\\ncourse_examination_result.csv\\ndepartment.csv\\neducational_institution_course.csv\\nenrolled_distance_student_university.csv\\nenrolled_distance_student_university_count.csv\\nenrolled_foreign_student_count.csv\\nenrolled_student_count.csv\\nexamination_result.csv\\nfaculty.csv\\nfaculty_department.csv\\ninfrastructure.csv\\nloan.csv\\nMetaData.csv\\nnon_teaching_staff_count.csv\\nother_minority_college_regular .csv\\nother_minority_standalone_distance.csv\\nother_minority_standalone_regular .csv\\nother_minority_university_distance.csv\\nother_minority_university_regular .csv\\npersons_count_by_category.csv\\nprivate_students_result.csv\\nref_broad_discipline_group.csv\\nref_broad_discipline_group_category.csv\\nref_college_institution_statutory_body.csv\\nref_count_by_category_remarks.csv\\nref_country.csv\\nref_course_level.csv\\nref_course_mode.csv\\nref_course_type.csv\\nref_diploma_course.csv\\nref_district.csv\\nref_examination_system.csv\\nref_institute_type.csv\\nref_institution_management.csv\\nref_non_teaching_staff_group.csv\\nref_non_teaching_staff_type.csv\\nref_programme.csv\\nref_programme_broad_discipline_group_and_category.csv\\nref_programme_statutory_body.csv\\nref_speciality.csv\\nref_standalone_institution.csv\\nref_state.csv\\nref_state_body.csv\\nref_student_hostel_type.csv\\nref_teaching_staff_designation.csv\\nref_teaching_staff_selection_mode.csv\\nref_university.csv\\nref_university_college_type.csv\\nref_university_type.csv\\nregional_center.csv\\nscholarship.csv\\nstaff_quarter.csv\\nstandalone_institution.csv\\nstandalone_institution_accreditation.csv\\nstandalone_institution_department.csv\\nstandalone_institution_faculty.csv\\nstandalone_institution_non_teaching_staff_count.csv\\nstandalone_institution_student_hostel.csv\\nstandalone_institution_teaching_staff.csv\\nstandalone_institution_teaching_staff_sanctioned_strength.csv\\nstudent_hostel.csv\\nteaching_staff.csv\\nteaching_staff_count.csv\\nteaching_staff_sanctioned_strength.csv\\nuniversity.csv\\nuniversity_accreditation.csv\\nuniversity_department.csv\\nuniversity_enrolled_distance_student.csv\\nuniversity_faculty.csv\\nuniversity_non_teaching_staff_count.csv\\nuniversity_private_students_result.csv\\nuniversity_student_hostel.csv\\nuniversity_teaching_staff.csv\\nuniversity_teaching_staff_sanctioned_strength.csv\\nAcknowledgements\\nMinistry of Human Resource Development (MHRD), Govt of India has published this dataset on Open Govt Data India Platform under Govt. open data license - India.\\nMHRD has also published some reports from this survey.\\nInspiration\\nThis is an interesting dataset to get the holistic picture of higher education system in India. One of the main objective of dept. of higher education is to increase the gross enrolment ratio (GRT) to 15% by 2011-12 and to 21% by 12th five year plan (2012-17). One can look at things like the objective like this has been achieved or can be achieved based on the progress of past data. There are several other things that can be analysed from this dataset.\\nPupil-Teacher Ratio (PTR)\\nOut-Turn\\nGender Parity Index (GPI) etc.,',\n",
       " 'Context\\nContains weekly purchased quantities of 800 over products over 52 weeks. These data were used in the paper \"Time series clustering: A superior alternative for market basket analysis\" by Tan, Swee Chuan and San Lau, Jess Pei.\\nContent\\nEach row represents a different product\\nEach column represents a week of the year (52 total weeks). The last half of the columns are normalized for you.\\nValues represent quantity of the products sold during the week\\n52 weeks: W0, W1, ..., W51\\nNormalised vlaues of weekly data: Normalised 0, Normalised 1, ..., Normalised 51\\nAcknowledgements\\nTan, Swee Chuan and San Lau, Jess Pei, Time series clustering: A superior alternative for market basket analysis.\\nThis dataset was downloaded from the UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly',\n",
       " 'Context\\nI was thinking about a dataset that I could provide and when I was reading through the LiveFromNewYork subreddit I got the idea: what about a Saturday Night Live dataset? Wouldn\\'t it be fun to analyze the data about a TV show that airs since the 70s?\\nContent\\nI aim to improve the dataset over time and update the files with more data. But I think that I have enough already so that people can work with it.\\nThere are files for the following objects:\\nseason\\nepisode\\ntitle\\nactor\\nactor_title (mapping of actors and titles)\\nrating (episode rating from IMDb.com)\\nAcknowledgements\\nA lot of the data comes from http://www.snlarchives.net where Joel Navaroli (@snlmedia) created a great snl archive. You can find everything about SNL there. Want to know about the 5th sketch in the 3rd episode in season 13? Go there to find out!\\nI got the ratings from IMDb.com.\\nI used Scrapy to get the data from the websites.\\nInspiration\\nSince SNL is such a long running TV show I thought it would be interesting to see how it developed over time. There are also some prejudices around, like \"there was a big slump from season X to Y\". Do the user ratings reflect that? I provided an example analysis, so that everyone can get started easily with the data.\\nSource\\nYou can find everything about the dataset in the GitHub repository: http://www.github.com/hhllcks/snldb',\n",
       " 'Planespotters.net has a full database on airlines around the world and the airplanes that each owns and operates. This dataset collects the top 100+ airlines in the world (by the size of their fleet). It is combined with information found on Wikipedia on the respective airline\\'s fleet and the average value/cost of the manufactured airplane.\\nUpdated January 2017.\\nDataset includes:\\nParent Airline: i.e. International Airlines Group (IAG)\\nAirline: i.e. Iberia, Aer Lingus, British Airways...etc. which are owned by IAG\\nAircraft Type: Manufacturer & Model\\nCurrent: Quantity of airplanes in Operation\\nFuture: Quantity of airplanes on order, from planespotter.net\\nOrder: Quantity airplanes on order, from Wikipedia\\nUnit Cost: Average unit cost ($M) of Aircraft Type, as found by Wikipedia and various google searches\\nTotal Cost: Current quantity * Unit Cost ($M)\\nAverage Age: Average age of \"Current\" airplanes by \"Aircraft Type\"\\nSources: Planespotters.net Wikipedia.org',\n",
       " \"Context\\nI've always wanted to have a proper sample Forex currency rates dataset for testing purposes, so I've created one.\\nContent\\nThe data contains Forex EURUSD currency rates in 15-minute slices (OHLC - Open High Low Close, and Volume). BID price only. Spread is not provided, so be careful.\\n(Quick reminder: Bid price + Spread = Ask price)\\nThe dates are in the yyyy-mm-dd hh:mm format, GMT. Volume is in Units.\\nAcknowledgements\\nDukascopy Bank SA https://www.dukascopy.com/swiss/english/marketwatch/historical/\\nInspiration\\nJust would like to see if there is still an way to beat the current Forex market conditions, with the prop traders' advanced automatic algorithms running in the wild.\",\n",
       " \"Context:\\nGlottolog (http://glottolog.org) provides a comprehensive catalogue of the world's languages, language families and dialects. It assigns a unique and stable identifier (the Glottocode) to (in principle) all languoids, i.e. all families, languages, and dialects.\\nContent:\\nThis dataset contains information on 1) the geographic location of languages and dialects, 2) their familial relationships and 3) a list of scholarly sources where information on languages was found.\\nAcknowledgements:\\nThis dataset was the current version of Glottolog as of July 20, 2017. If you publish work using this dataset, please use the following citation:\\nHammarström, Harald & Haspelmath, Martin & Forkel, Robert. 2017. Glottolog 3.0. Jena: Max Planck Institute for the Science of Human History. (Available online at http://glottolog.org, Accessed on 2017-03-23.)\\nInspiration:\\nCan you plot the geographic location of each language family or langoid?\\nWhere are most extinct languages found?\\nCan you find which language was documented in what decade? Which areas of the focus of more or less documentation?\\nYou may also be interested in:\\nAtlas of Pidgin and Creole Language Structures\\nThe Sign Language Analyses (SLAY) Database\\nWorld Atlas of Language Structures: Information on the linguistic structures in 2,679 languages\",\n",
       " 'Context:\\nCrime in major metropolitan areas, such as London, occurs in distinct patterns. This data covers the number of criminal reports by month, LSOA borough, and major/minor category from Jan 2008-Dec 2016.\\nContent:\\n13M rows containing counts of criminal reports by month, LSOA borough, and major/minor category.\\nAcknowledgements:\\nTxt file was pulled from Google Cloud Platform and converted to csv. Photo by James Sutton.\\nInspiration:\\nAre there seasonal or time-of-week/day changes in crime occurrences? Any boroughs where particular crimes are increasing or decreasing? Policy makers use this data to plan upcoming budgets and deployment--can you use previous year crime reports to reliably predict later trends? If you normalize by borough population, can you find any areas where crime is more or less likely?',\n",
       " \"Context\\nPM2.5 readings are often included in air quality reports from environmental authorities and companies. PM2.5 refers to atmospheric particulate matter (PM) that have a diameter less than 2.5 micrometers. In other words, it's used as a measure of pollution.\\nContent\\nThe time period for this data is between Jan 1st, 2010 to Dec 31st, 2015. Missing data are denoted as NA.\\nNo: row number\\nyear: year of data in this row\\nmonth: month of data in this row\\nday: day of data in this row\\nhour: hour of data in this row\\nseason: season of data in this row\\nPM: PM2.5 concentration (ug/m^3)\\nDEWP: Dew Point (Celsius Degree)\\nTEMP: Temperature (Celsius Degree)\\nHUMI: Humidity (%)\\nPRES: Pressure (hPa)\\ncbwd: Combined wind direction\\nIws: Cumulated wind speed (m/s)\\nprecipitation: hourly precipitation (mm)\\nIprec: Cumulated precipitation (mm)\\nAcknowledgements\\nLiang, X., S. Li, S. Zhang, H. Huang, and S. X. Chen (2016), PM2.5 data reliability, consistency, and air quality assessment in five Chinese cities, J. Geophys. Res. Atmos., 121, 10220â€“10236.\\nThe files were downloaded from the UCI Machine Learning Repository and have not been modified. https://archive.ics.uci.edu/ml/datasets/PM2.5+Data+of+Five+Chinese+Cities#\",\n",
       " 'Context:\\nHazardous air pollutants, also known as toxic air pollutants or air toxics, are those pollutants that are known or suspected to cause cancer or other serious health effects, such as reproductive effects or birth defects, or adverse environmental effects. The Environmental Protection Agency (EPA) tracks 187 air pollutants. See https://www.epa.gov/haps/ for more information.\\nContent:\\nThe daily summary file contains data for every monitor (sampled parameter) in the Environmental Protection Agency (EPA) database for each day. This file will contain a daily summary record that is: 1. The aggregate of all sub-daily measurements taken at the monitor. 2. The single sample value if the monitor takes a single, daily sample (e.g., there is only one sample with a 24-hour duration). In this case, the mean and max daily sample will have the same value.\\nFields Descriptions: 1. State Code: The Federal Information Processing Standards (FIPS) code of the state in which the monitor resides.\\nCounty Code: The FIPS code of the county in which the monitor resides.\\nSite Num: A unique number within the county identifying the site.\\nParameter Code: The AQS code corresponding to the parameter measured by the monitor.\\nPOC: This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site.\\nLatitude: The monitoring site’s angular distance north of the equator measured in decimal degrees.\\nLongitude: The monitoring site’s angular distance east of the prime meridian measured in decimal degrees.\\nDatum: The Datum associated with the Latitude and Longitude measures.\\nParameter Name: The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants.\\nSample Duration: The length of time that air passes through the monitoring device before it is analyzed (measured). So, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).\\nPollutant Standard: A description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.)\\nDate Local: The calendar date for the summary. All daily summaries are for the local standard day (midnight to midnight) at the monitor.\\nUnits of Measure: The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations.\\nEvent Type: Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question, the data will have multiple records for each monitor.\\nObservation Count: The number of observations (samples) taken during the day.\\nObservation Percent: The percent representing the number of observations taken with respect to the number scheduled to be taken during the day. This is only calculated for monitors where measurements are required (e.g., only certain parameters).\\nArithmetic Mean: The average (arithmetic mean) value for the day.\\n1st Max Value: The highest value for the day.\\n1st Max Hour: The hour (on a 24-hour clock) when the highest value for the day (the previous field) was taken.\\nAQI: The Air Quality Index for the day for the pollutant, if applicable.\\nMethod Code: An internal system code indicating the method (processes, equipment, and protocols) used in gathering and measuring the sample. The method name is in the next column.\\nMethod Name: A short description of the processes, equipment, and protocols used in gathering and measuring the sample.\\nLocal Site Name: The name of the site (if any) given by the State, local, or tribal air pollution control agency that operates it.\\nAddress: The approximate street address of the monitoring site.\\nState Name: The name of the state where the monitoring site is located.\\nCounty Name: The name of the county where the monitoring site is located.\\nCity Name: The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas.\\nCBSA Name: The name of the core bases statistical area (metropolitan area) where the monitoring site is located.\\nDate of Last Change: The date the last time any numeric values in this record were updated in the AQS data system.\\nAcknowledgements:\\nThese data came from the EPA and are current up to May 01, 2017. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/epa.\\nInspiration:\\nPeople exposed to toxic air pollutants at sufficient concentrations and durations may have an increased chance of getting cancer or experiencing other serious health effects. These health effects can include damage to the immune system, as well as neurological, reproductive (e.g., reduced fertility), developmental, respiratory and other health problems. In addition to exposure from breathing air toxics, some toxic air pollutants such as mercury can deposit onto soils or surface waters, where they are taken up by plants and ingested by animals and are eventually magnified up through the food chain. Like humans, animals may experience health problems if exposed to sufficient quantities of air toxics over time. Use this dataset to find out where the highest concentrations of hazardous air pollutants are for each state. You could also use the GPS locations to find out where the EPA has the most monitoring stations and identify places that could use more.',\n",
       " 'The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing NLP models of any kind.\\nAcknowledgements\\nThis dataset was kindly made available bye the Stanford Natural Language Processing Group. Please cite it as:\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)\\nInspiration\\nThis dataset has been used to evaluate academic work on sentence encoding-based models for 3 way classification, with previous scores tabulated at https://nlp.stanford.edu/projects/snli/. Most of the entries use deep learning. How close to those scores (peak of 88.8% test accuracy) can you get with less computationally intensive methods?',\n",
       " 'Context\\nNext time you take a bite, consider this: roughly one in six (or 48 million) people in the United States get sick from eating contaminated food per year. More than 250 pathogens and toxins have been known to cause foodborne illness and almost all of them can cause an outbreak.\\nA foodborne disease outbreak occurs when two or more people get the same illness from the same contaminated food or drink. While most foodborne illnesses are not part of a recognized outbreak, outbreaks provide important information on how germs spread, which foods cause illness, and how to prevent infection.\\nPublic health agencies in all 50 states, the District of Columbia, U.S. territories, and Freely Associated States have primary responsibility for identifying and investigating outbreaks and use a standard form to report outbreaks voluntarily to CDC. During 1998–2008, reporting was made through the electronic Foodborne Outbreak Reporting System (eFORS).\\nContent\\nThis dataset provides data on foodborne disease outbreaks reported to CDC from 1998 through 2015. Data fields include year, state (outbreaks occurring in more than one state are listed as \"multistate\"), location where the food was prepared, reported food vehicle and contaminated ingredient, etiology (the pathogen, toxin, or chemical that caused the illnesses), status (whether the etiology was confirmed or suspected), total illnesses, hospitalizations, and fatalities. In many outbreak investigations, a specific food vehicle is not identified; for these outbreaks, the food vehicle variable is blank.\\nInspiration\\nAre foodborne disease outbreaks increasing or decreasing? What contaminant has been responsible for the most illnesses, hospitalizations, and deaths? What location for food preparation poses the greatest risk of foodborne illness?',\n",
       " \"Fatality Analysis Reporting System (FARS) was created in the United States by the National Highway Traffic Safety Administration (NHTSA) to provide an overall measure of highway safety, to help suggest solutions, and to help provide an objective basis to evaluate the effectiveness of motor vehicle safety standards and highway safety programs.\\nFARS contains data on a census of fatal traffic crashes within the 50 States, the District of Columbia, and Puerto Rico. To be included in FARS, a crash must involve a motor vehicle traveling on a trafficway customarily open to the public and result in the death of a person (occupant of a vehicle or a non-occupant) within 30 days of the crash. FARS has been operational since 1975 and has collected information on over 989,451 motor vehicle fatalities and collects information on over 100 different coded data elements that characterizes the crash, the vehicle, and the people involved.\\nFARS is vital to the mission of NHTSA to reduce the number of motor vehicle crashes and deaths on our nation's highways, and subsequently, reduce the associated economic loss to society resulting from those motor vehicle crashes and fatalities. FARS data is critical to understanding the characteristics of the environment, trafficway, vehicles, and persons involved in the crash.\\nNHTSA has a cooperative agreement with an agency in each state government to provide information in a standard format on fatal crashes in the state. Data is collected, coded and submitted into a micro-computer data system and transmitted to Washington, D.C. Quarterly files are produced for analytical purposes to study trends and evaluate the effectiveness highway safety programs.\\nContent\\nThere are 40 separate data tables. You can find the manual, which is too large to reprint in this space, here.\\nQuerying BigQuery tables\\nYou can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.nhtsa_traffic_fatalities.[TABLENAME]. Fork this kernel to get started.\\nAcknowledgements\\nThis dataset was provided by the National Highway Traffic Safety Administration.\",\n",
       " 'Context\\nThis dataset is a snapshot of all of the country profiles provided in the World Factbook as of early 2017. The World Factbook is a reference almanac published by the United States Central Intelligence Agency on a continual basis. It is often used as a reference text in other academic works.\\nContent\\nThis dataset includes high-level textual information on the economy, politics, demography, culture, military, and society of every country in the world.\\nAcknowledgements\\nThis data was scraped here, then concatenated into a single entity before upload to Kaggle.\\nInspiration\\nThis dataset is an ideal basis of comparison for various world countries.\\nAnalyzing international data? This dataset is a rich mix-in dataset for contextualizing such analyses.',\n",
       " 'This database consist of over 400000 infos for android Apps scraped with Scrapy from Google Play. Those fields are included:\\nname\\ndatePublished\\nnumDownloadsMin\\nfileSize\\npackageName\\nprice\\naggregateRating\\nsoftwareVersion\\nratingCount\\ndateCrawled\\nurl',\n",
       " 'Context\\nCompetitions like LUNA (http://luna16.grand-challenge.org) and the Kaggle Data Science Bowl 2017 (https://www.kaggle.com/c/data-science-bowl-2017) involve processing and trying to find lesions in CT images of the lungs. In order to find disease in these images well, it is important to first find the lungs well. This dataset is a collection of 2D and 3D images with manually segmented lungs.\\nChallenge\\nCome up with an algorithm for accurately segmenting lungs and measuring important clinical parameters (lung volume, PD, etc)\\nPercentile Density (PD)\\nThe PD is the density (in Hounsfield units) the given percentile of pixels fall below in the image. The table includes 5 and 95% for reference. For smokers this value is often high indicating the build up of other things in the lungs.',\n",
       " 'The Numenta Anomaly Benchmark (NAB) is a novel benchmark for evaluating algorithms for anomaly detection in streaming, online applications. It is comprised of over 50 labeled real-world and artificial timeseries data files plus a novel scoring mechanism designed for real-time applications. All of the data and code is fully open-source, with extensive documentation, and a scoreboard of anomaly detection algorithms: github.com/numenta/NAB. The full dataset is included here, but please go to the repo for details on how to evaluate anomaly detection algorithms on NAB.\\nNAB Data Corpus\\nThe NAB corpus of 58 timeseries data files is designed to provide data for research in streaming anomaly detection. It is comprised of both real-world and artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics. All data files contain anomalies, unless otherwise noted.\\nThe majority of the data is real-world from a variety of sources such as AWS server metrics, Twitter volume, advertisement clicking metrics, traffic data, and more. All data is included in the repository, with more details in the data readme. We are in the process of adding more data, and actively searching for more data. Please contact us at nab@numenta.org if you have similar data (ideally with known anomalies) that you would like to see incorporated into NAB.\\nThe NAB version will be updated whenever new data (and corresponding labels) is added to the corpus; NAB is currently in v1.0.\\nReal data\\nrealAWSCloudwatch/\\nAWS server metrics as collected by the AmazonCloudwatch service. Example metrics include CPU Utilization, Network Bytes In, and Disk Read Bytes.\\nrealAdExchange/\\nOnline advertisement clicking rates, where the metrics are cost-per-click (CPC) and cost per thousand impressions (CPM). One of the files is normal, without anomalies.\\nrealKnownCause/\\nThis is data for which we know the anomaly causes; no hand labeling.\\nambient_temperature_system_failure.csv: The ambient temperature in an office setting.\\ncpu_utilization_asg_misconfiguration.csv: From Amazon Web Services (AWS) monitoring CPU usage – i.e. average CPU usage across a given cluster. When usage is high, AWS spins up a new machine, and uses fewer machines when usage is low.\\nec2_request_latency_system_failure.csv: CPU usage data from a server in Amazon\\'s East Coast datacenter. The dataset ends with complete system failure resulting from a documented failure of AWS API servers. There\\'s an interesting story behind this data in the Numenta blog.\\nmachine_temperature_system_failure.csv: Temperature sensor data of an internal component of a large, industrial mahcine. The first anomaly is a planned shutdown of the machine. The second anomaly is difficult to detect and directly led to the third anomaly, a catastrophic failure of the machine.\\nnyc_taxi.csv: Number of NYC taxi passengers, where the five anomalies occur during the NYC marathon, Thanksgiving, Christmas, New Years day, and a snow storm. The raw data is from the NYC Taxi and Limousine Commission. The data file included here consists of aggregating the total number of taxi passengers into 30 minute buckets.\\nrogue_agent_key_hold.csv: Timing the key holds for several users of a computer, where the anomalies represent a change in the user.\\nrogue_agent_key_updown.csv: Timing the key strokes for several users of a computer, where the anomalies represent a change in the user.\\nrealTraffic/\\nReal time traffic data from the Twin Cities Metro area in Minnesota, collected by the Minnesota Department of Transportation. Included metrics include occupancy, speed, and travel time from specific sensors.\\nrealTweets/\\nA collection of Twitter mentions of large publicly-traded companies such as Google and IBM. The metric value represents the number of mentions for a given ticker symbol every 5 minutes.\\nArtificial data\\nartificialNoAnomaly/\\nArtifically-generated data without any anomalies.\\nartificialWithAnomaly/\\nArtifically-generated data with varying types of anomalies.\\nAcknowledgments\\nWe encourage you to publish your results on running NAB, and share them with us at nab@numenta.org. Please cite the following publication when referring to NAB:\\nLavin, Alexander and Ahmad, Subutai. \"Evaluating Real-time Anomaly Detection Algorithms – the Numenta Anomaly Benchmark\", Fourteenth International Conference on Machine Learning and Applications, December 2015. [PDF]',\n",
       " 'Withdrawal of a particular form of currency (such a gold coins, currency notes) from circulation is known as demonetization .\\nContext:\\nOn November 8th, India’s Prime Minister announced that 86% of the country’s currency would be rendered null and void in 50 days and it will withdraw all 500 and 1,000 rupee notes — the country’s most popular currency denominations from circulation, while a new 2,000 rupee note added in. It was posited as a move to crackdown on corruption and the country’s booming under-regulated and virtually untaxed grassroots economy.\\nContent:\\nThe field names are following:\\nID\\nQUERY\\nTWEET_ID\\nINSERTED DATE\\nTRUNCATED\\nLANGUAGE\\npossibly_sensitive coordinates\\nretweeted_status\\ncreated_at_text\\ncreated_at\\nCONTENT\\nfrom_user_screen_name\\nfrom_user_id from_user_followers_count\\nfrom_user_friends_count\\nfrom_user_listed_count\\nfrom_user_statuses_count\\nfrom_user_description\\nfrom_user_location\\nfrom_user_created_at\\nretweet_count\\nentities_urls\\nentities_urls_counts\\nentities_hashtags\\nentities_hashtags_counts\\nentities_mentions\\nentities_mentions_counts\\nin_reply_to_screen_name\\nin_reply_to_status_id\\nsource\\nentities_expanded_urls\\njson_output\\nentities_media_count\\nmedia_expanded_url\\nmedia_url\\nmedia_type\\nvideo_link\\nphoto_link\\ntwitpic\\nAcknowledgements:\\nDataset is created by pulling tweets by hashtag from twitter.\\nInspiration:\\nDataset can be used to understand trending tweets. Dataset can be used for sentiment analysis and topic mining. Dataset can be used for time series analysis of tweets.\\nWhat questions would you like answered by the community ?\\nWhat is the general sentiment of tweets ?\\nConclusion regarding tweet sentiments varying over time.\\nWhat feedback would be helpful on the data itself ?\\nAn in depth analysis of data.',\n",
       " \"These datasets contain information from NBA draft classes and subsequent advanced stats years. The idea is to evaluate which draft classes are better than others, and in which ways they are better (did they produce more stars in the top 10, or more solid role players at the middle or end. I posted an analysis of this data here, but there's a lot more to be done. For example, my initial analysis just looks at drafts as a whole, not breaking it down by top 10, top 30, or top 60 picks. I also just analyzed drafts since 2000, but the dataset I uploaded has info all the way back to 1978. This data was scraped from baskeball-reference.com\\nThere are 2 datasets: season78, which has the fields:\\nSeason: The NBA season data is drawn from. The later year is the Season value (e.g. 2015-2016 season is 2016)\\nPlayer: Name of the player\\nWS: Win Shares produced that season.\\ndraft78 has the fields:\\nPick: The draft pick the player was.\\nPlayer: Name of the player\\nYrs: Number of years the player played in the NBA\\nDraft: Year of the draft.\",\n",
       " 'Context\\nThis dataset contains ground state energies of 16,242 molecules calculated by quantum mechanical simulations.\\nContent\\nThe data contains 1277 columns. The first 1275 columns are entries in the Coulomb matrix that act as molecular features. The 1276th column is the Pubchem Id where the molecular structures are obtained. The 1277th column is the atomization energy calculated by simulations using the Quantum Espresso package.\\nIn the csv file, the first column (X1) is the data index and unused.\\nPast Research\\nThe data is used for a publication in Journal of Chemical Physics. A blog post was also published explaining the data and the research behind it in less technical terms.\\nA Github repository is available that contains the source code used for generating the data, as well as some of the R scripts used for analysis.\\nInspiration\\nSimulations of molecular properties are computationally expensive. The purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database of simulations. If this can be done with high accuracy, properties of new molecules can be calculated using the trained model. This could open up many possibilities in computational design and discovery of molecules, compounds and new drugs.\\nThe purpose is to use the 1275 molecular features to predict the atomization energy. This is a regression problem so mean squared error is minimized during training.\\nI am looking for Kagglers to find the best model and reduce mean squared error as much as possible!',\n",
       " 'These three extremists attacks happened in the last 24 hours (as of 18th of July, 2016):\\nExtremists send rockets into a residential neighborhood, taking out a child and two women. Aleppo, Syria.\\nSuicide bombers attack Yemeni army checkpoints, killing 10. Yemen, Mukalla.\\n5 killed, 9 injured in Almaty terrorist attack on police station [GRAPHIC]. Kazakhstan,Almaty.\\nCan we come together to predict where the next terrorist attacks will likely occur?\\nThe best weapon to fight extremists might be information. In fact, machine learning is already being used to predict and prevent terrorist attacks. We can do the same with data gathered from The Religion of Peace website.\\nThis website gathers events that resulted in killings and which were committed out of religious duty since 2002. This dataset contains a table summary of their data, including:\\nDate\\nCountry\\nCity\\n(# of people) Killed\\n(# of people) Injured\\nDescription (including type of attack descriptors such as \"bomb\",\"car\",\"shooting\",\"rocket\")\\nI webscraped the data using R rvest.\\n# Webscraping in terror data by ping_freud\\n# You can use the following gadget to find out which nodes to select in a page \\n# https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html\\nlibrary(dplyr) #For %>% pipe flow\\nlibrary(ggplot2)\\nlibrary(rvest) #Webscraping\\n\\n# Building strings and declaring variables.\\n# Link with data.\\ndata.link <-\"https://www.thereligionofpeace.com/attacks/attacks.aspx?\"\\nYears <- as.character(2002:2016)\\nlinks <- paste(data.link,\"Yr=\",Years,sep = \"\")\\nterror.nodes <- terror.data <- vector(\"list\",length(links))\\n\\n# For loop to extract data\\nfor (i in 1:length(links)){ \\n  terror.nodes[[i]] <- read_html(links[i]) %>% html_nodes(xpath=\"//table\")\\n  #11 is the node where the table with data is \\n  terror.data[[i]] <- as.data.frame(html_table(terror.nodes[[i]][11])) \\n  terror.data[[i]] <- terror.data[[i]][nrow(terror.data[[i]]):1,]\\n}\\n\\n# Combines data frames\\nterror.alldata <- do.call(\"rbind\",terror.data)\\n# Convert strings with dates to date format\\nterror.alldata$Date <- as.Date(terror.alldata$Date,\"%Y.%m.%d\")\\nrow.names(terror.alldata) <- as.character(1:nrow(terror.alldata))\\nwrite.csv(terror.alldata,\"attacks_data.csv\")\\nI have not worked on the analysis yet, but we have geospacial distribution, type (hidden in the Description strings) and magnitude of the attacks. There\\'s also the possibility of using socioeconomical data available for the places listed.',\n",
       " 'Context\\nFantasy Premier League is the online global competition for Football Enthusiasts to try their luck at picking up their \"Dream Team\" and collect points. It involves a deep understanding of the Sport, Clubs, Players and the Fixtures apart from many other things. All this makes for a compelling Data Science (read Machine Learning Problem).\\nEnglish Premier League (EPL) - one of the famous leagues in the sport of football. Most viewed and followed across the globe. FPL provides an opportunity for enthusiasts to try their hand at decision making. To predict the best set of players who perform every game. Points are given based on various parameters.\\nGoal - To get the maximum Team Score every week\\nContent\\nTime Period - Year 2016-17 Season\\nDataset consists of\\nFPL users data (basic information)\\nFixtures (Week-wise)\\nPoints earned by every user (Gameweek-wise)\\nData Extraction\\nDetailed Information about the Code Github repo - https://github.com/ChaiBapchya/fantasypremierleague-datascience\\nAcknowledgements\\nThanks to Fantasy Premier League, without which this data would not have been available.\\nInspiration\\nDiego Costa scores a goal every 15 minutes of the match he plays.\\nHarry Kane is the youngest player to have scored 100 Premier League goals.\\nSuch statistics (if true) are a compelling read.\\nBut, to know -\\nNathaniel Chalobah has 70% probability of scoring against Stoke City this weekend.\\nAlexis Sanchez will score around 2 goals this month.\\nCesc Fabregas is going to assist this weekend.\\nThere\\'s 70% David De Gea is going to have a clean-sheet.\\nSuch statistics and much more lend so much credibility to decision making. It would enable Fantasy Premier League team owners to decide :-\\na. When to choose which player?\\nb. When is the right time to use the WildCard?\\nc. If it is time to be patient with a signing?\\nd. Who to watch out for?\\nIn order to do this, one needs data to back your predictions. Hence, I was keen on retrieving all this data.\\nFuture Scope -\\nPredict the best team for the upcoming week.\\nPredict the best player (Highest value for money) (Goalkeeper, Defender, MId-fielder, Attacker)\\nSuggest possible changes in formation if need be.',\n",
       " \"Context\\nIn the last decade, new ways of shopping online have increased the possibility of buying products and services more easily and faster than ever. In this new context, personality is a key determinant in the decision making of the consumer when shopping. A person's buying choices are influenced by psychological factors like impulsiveness; indeed some consumers may be more susceptible to making impulse purchases than others. Since affective metadata are more closely related to the user's experience than generic parameters, accurate predictions reveal important aspects of user's attitudes, social life, including attitude of others and social identity. This work proposes a highly innovative research that uses a personality perspective to determine the unique associations among the consumer's buying tendency and advert recommendations. In fact, the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of recent algorithms. We present the ADS Dataset, a publicly available benchmark consisting of 300 real advertisements (i.e., Rich Media Ads, Image Ads, Text Ads) rated by 120 unacquainted individuals, enriched with Big-Five users' personality factors and 1,200 personal users' pictures.\\nContent\\nThe content of the zip files are folders. The directory tree of this disk is as follows:\\n20 Ads folder: Ads belong to 20 product/service categories. all the ads are here. 120 Users Folders: Each folder contains data for one of the involved subjects. 300 real advertisements have been scored, Ratings according to the users’ interests (1 star to 5 stars), ~1,200 personal pictures (labelled as positive/negative), Big-Five personality scores (O-C-E-A-N).\\nData can be easily analysed in Matlab, or Python\\nAcknowledgements\\nIf you use our dataset please cite:\\n[1] Roffo, G., & Vinciarelli, A. (2016, August). Personality in computational advertising: A benchmark. In 4 th Workshop on Emotions and Personality in Personalized Systems (EMPIRE) 2016 (p. 18).\\nInspiration\\nWe collected and introduced a representative benchmark for computational advertising enriched with affective-like metadata such as personality factors. The benchmark allows to (i) explore the relationship between consumer characteristics, attitude toward online shopping and advert recommendation, (ii) identify the underlying dimensions of consumer shopping motivations and attitudes toward online in-store conversions, and (iii) have a reference benchmark for comparison of state-of-the-art advertisement recommender systems (ARSs). To the best of our knowledge, the ADS dataset is the first attempt at providing a set of advertisements scored by the users according to their interest into the content. We hope that this work motivates researchers to take into account the use of personality factors as an integral part of their future work, since there is a high potential that incorporating these kind of users' characteristics into ARS could enhance recommendation quality and user experience.\",\n",
       " \"Recent Updates (11-27-2017)\\nI've posted a clustered full dataset. This might give you a boost on the work you're doing with the data!\\nI've posted a vectorized subset w/ 100,000 data points sampled after manual reduction of the dataset after EDA.\\nContext\\nCleaned data used in researching public comments for FCC Proceeding 17-108 (Net Neutrality Repeal).\\nData collected from the beginning of submissions (April 2017) until Oct 27th, 2017. The long-running comment scraping script suffered from a couple of disconnections and I estimate that I lost ~50,000 comments because of it. Even though the Net Neutrality Public Comment Period ended on August 30, 2017, the FCC ECFS system continued to take comments afterward, which were included in the analysis.\\nI did a write-up on the results here: https://hackernoon.com/more-than-a-million-pro-repeal-net-neutrality-comments-were-likely-faked-e9f0e3ed36a6\\nContent\\nDocument id, Text of the post, and number of duplicates found for that text was all that was necessary to generate the results. Text is raw & unchanged from the original. I'm working hard to get online a fuller set of data w/ other important metadata fields.\\nCleaned-up notebooks used are available on github. I am posting the notebook for Exploratory Data Analysis first, and will include others as they are cleaned up. Please share with the rest of us what interesting insights you glean from the data! Tweet at me @jeffykao.\",\n",
       " 'Content\\nEmojiNet is the largest machine-readable emoji sense inventory that links Unicode emoji representations to their English meanings extracted from the Web. EmojiNet is a dataset consisting of:\\n12,904 sense labels over 2,389 emoji, which were extracted from the web and linked to machine-readable sense definitions seen in BabelNet\\ncontext words associated with each emoji sense, which are inferred through word embedding models trained over Google News corpus and a Twitter message corpus for each emoji sense definition\\nspecification of the most likely platform-based emoji sense for a selected set of emoji (since emoji presentation is different on different platforms)\\nAcknowledgements:\\nEmojiNet was developed by Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth and Derek Doran. EmojiNet is licensed under aCreative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0) license.. Please cite the following paper when using EmojiNet dataset(s):\\nSanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth, Derek Doran. EmojiNet: An Open Service and API for Emoji Sense Discovery. In 11th International AAAI Conference on Web and Social Media (ICWSM 2017). Montreal, Canada; 2017.\\nYou can also find more information about the dataset on the project website.\\nThe banner photo is by Frank Behrens and is licensed under a CC BY-SA 2.0 license.\\nInspiration:\\nCan you use these senses to create a sentiment lexicon for emoji?\\nCan you cluster emoji based on their sense?\\nWhich emoji are the most different across platforms?',\n",
       " \"Context:\\nEveryone who speaks a language, speaks it with an accent. A particular accent essentially reflects a person's linguistic background. When people listen to someone speak with a different accent from their own, they notice the difference, and they may even make certain biased social judgments about the speaker.\\nThe speech accent archive is established to uniformly exhibit a large set of speech accents from a variety of language backgrounds. Native and non-native speakers of English all read the same English paragraph and are carefully recorded. The archive is constructed as a teaching tool and as a research tool. It is meant to be used by linguists as well as other people who simply wish to listen to and compare the accents of different English speakers.\\nThis dataset allows you to compare the demographic and linguistic backgrounds of the speakers in order to determine which variables are key predictors of each accent. The speech accent archive demonstrates that accents are systematic rather than merely mistaken speech.\\nAll of the linguistic analyses of the accents are available for public scrutiny. We welcome comments on the accuracy of our transcriptions and analyses.\\nContent:\\nThis dataset contains 2140 speech samples, each from a different talker reading the same reading passage. Talkers come from 177 countries and have 214 different native languages. Each talker is speaking in English.\\nThis dataset contains the following files:\\nreading-passage.txt: the text all speakers read\\nspeakers_all.csv: demographic information on every speaker\\nrecording: a zipped folder containing .mp3 files with speech\\nAcknowledgements:\\nThis dataset was collected by many individuals (full list here) under the supervision of Steven H. Weinberger. The most up-to-date version of the archive is hosted by George Mason University. If you use this dataset in your work, please include the following citation:\\nWeinberger, S. (2013). Speech accent archive. George Mason University.\\nThis datasets is distributed under a CC BY-NC-SA 2.0 license.\\nInspiration:\\nThe following types of people may find this dataset interesting:\\nESL teachers who instruct non-native speakers of English\\nActors who need to learn an accent\\nEngineers who train speech recognition machines\\nLinguists who do research on foreign accent\\nPhoneticians who teach phonetic transcription\\nSpeech pathologists\\nAnyone who finds foreign accent to be interesting\",\n",
       " 'Context\\nThe Kepler Space Observatory is a NASA-build satellite that was launched in 2009. The telescope is dedicated to searching for exoplanets in star systems besides our own, with the ultimate goal of possibly finding other habitable planets besides our own. The original mission ended in 2013 due to mechanical failures, but the telescope has nevertheless been functional since 2014 on a \"K2\" extended mission.\\nKepler had verified 1284 new exoplanets as of May 2016. As of October 2017 there are over 3000 confirmed exoplanets total (using all detection methods, including ground-based ones). The telescope is still active and continues to collect new data on its extended mission.\\nContent\\nThis dataset is a cumulative record of all observed Kepler \"objects of interest\" — basically, all of the approximately 10,000 exoplanet candidates Kepler has taken observations on.\\nThis dataset has an extensive data dictionary, which can be accessed here. Highlightable columns of note are:\\nkepoi_name: A KOI is a target identified by the Kepler Project that displays at least one transit-like sequence within Kepler time-series photometry that appears to be of astrophysical origin and initially consistent with a planetary transit hypothesis\\nkepler_name: [These names] are intended to clearly indicate a class of objects that have been confirmed or validated as planets—a step up from the planet candidate designation.\\nkoi_disposition: The disposition in the literature towards this exoplanet candidate. One of CANDIDATE, FALSE POSITIVE, NOT DISPOSITIONED or CONFIRMED.\\nkoi_pdisposition: The disposition Kepler data analysis has towards this exoplanet candidate. One of FALSE POSITIVE, NOT DISPOSITIONED, and CANDIDATE.\\nkoi_score: A value between 0 and 1 that indicates the confidence in the KOI disposition. For CANDIDATEs, a higher value indicates more confidence in its disposition, while for FALSE POSITIVEs, a higher value indicates less confidence in that disposition.\\nAcknowledgements\\nThis dataset was published as-is by NASA. You can access the original table here. More data from the Kepler mission is available from the same source here.\\nInspiration\\nHow often are exoplanets confirmed in the existing literature disconfirmed by measurements from Kepler? How about the other way round?\\nWhat general characteristics about exoplanets (that we can find) can you derive from this dataset?\\nWhat exoplanets get assigned names in the literature? What is the distribution of confidence scores?\\nSee also: the Kepler Labeled Time Series and Open Exoplanets Catalogue datasets.',\n",
       " \"This dataset contains stats on players, coaches, and teams in men's professional basketball leagues from 1937 to 2012.\\nAcknowledgments\\nThis dataset was downloaded from the Open Source Sports website. It did not come with an explicit license, but based on other datasets from Open Source Sports, we treat it as follows:\\nThis database is copyright 1996-2015 by Sean Lahman.\\nThis work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. For details see: http://creativecommons.org/licenses/by-sa/3.0/\\nThe Data\\nThis dataset contains 11 files, each corresponding to a data table. There are five main tables:\\nmaster: biographical information for all the players and coaches\\nteams: stats on each team, per year\\nplayers: stats for each player, per year\\ncoaches: stats for each coach, per year\\nseries_post: information on post-season winners, per year\\nAnd there are six supplementary tables:\\nabbrev: a key to the abbreviations used in other tables\\nawards_coaches: coaching awards, per year\\nawards_players: player awards, per year\\ndraft: draft information, per year\\nhof: Hall of Fame information, per year\\nplayer_allstar: individual player stats for the All-Star Game, per year\",\n",
       " 'The Counted is a project by the Guardian – and you – working to count the number of people killed by police and other law enforcement agencies in the United States throughout 2015 and 2016, to monitor their demographics and to tell the stories of how they died.\\nThe database will combine Guardian reporting with verified crowdsourced information to build a more comprehensive record of such fatalities. The Counted is the most thorough public accounting for deadly use of force in the US, but it will operate as an imperfect work in progress – and will be updated by Guardian reporters and interactive journalists frequently.\\nAny deaths arising directly from encounters with law enforcement will be included in the database. This will inevitably include, but will likely not be limited to, people who were shot, tasered and struck by police vehicles as well those who died in police custody. Self-inflicted deaths during encounters with law enforcement or in police custody or detention facilities will not be included.\\nThe US government has no comprehensive record of the number of people killed by law enforcement. This lack of basic data has been glaring amid the protests, riots and worldwide debate set in motion by the fatal police shooting of Michael Brown in August 2014. The Guardian agrees with those analysts, campaign groups, activists and authorities who argue that such accounting is a prerequisite for an informed public discussion about the use of force by police.\\nContributions of any information that may improve the quality of our data will be greatly welcomed as we work toward better accountability. Please contact us at thecounted@theguardian.com.\\nCREDITS\\nResearch and Reporting: Jon Swaine, Oliver Laughland, Jamiles Lartey\\nDesign and Production: Kenan Davis, Rich Harris, Nadja Popovich, Kenton Powell',\n",
       " \"Can You Predict The Result?\\nHorse racing is one of the sport which involved many gambling activities. Million of people in the world tried to find their 'winning formula' in order to gain profit from betting. Since there are many factors which could affect the race result, data analysis on horse racing became much interesting.\\nHong Kong horse racing is especially interesting due to the follow reasons:\\n- The handicap system made the race more competitive\\n- Horse pool is small compared to other countries so that horses will meet their rivalries very often in the races\\n- Limited number of jockey/trainer\\n- Data are well managed by the official :)\\nThe Dataset\\nThe dataset contains the race result of 1561 local races throughout Hong Kong racing seasons 2014-16 and more information will be added into the dataset. The dataset is divided into two tables (which can be joined by race_id). Most of the column description can be found below with one extra piece of information:\\nfinishing_position - For special incident, please refer to here\\nSo, can you find any pattern for the winner under some condition? Did you spot out a winning strategy? (FYI, betting on all horse equally will bring a loss of ~17.5% on average) Which jockey/trainer is worth to follow?\\nDon't wait and start the data analysis! You may find some of the kernels I created useful. Enjoy! And please remember to share your finding with the community!\\nAcknowledgement\\nThe data are extracted from the website of The Hong Kong Jockey Club\\nHow to get started?\\nIn case you are not familiar with Hong Kong horse racing, please see this notebook as a get started tutorial.\",\n",
       " 'Context\\nA refugee is a person outside his or her country of nationality who is unable or unwilling to return to his or her country of nationality because of persecution or a well-founded fear of persecution on account of race, religion, nationality, membership in a particular social group, or political opinion. An asylee is a person who meets the definition of refugee and is already present in the United States or is seeking admission at a port of entry. Refugees are required to apply for lawful permanent resident (“green card”) status one year after being admitted, and asylees may apply for green card status one year after being granted asylum.\\nContent\\nThe Office of Immigration Statistics (OIS) Annual Flow Reports on refugees and asylees contain information obtained from the Worldwide Refugee Admissions Processing System (WRAPS) of the Bureau of Population, Refugees, and Migration of the US Department of State on the numbers and demographic profiles of persons admitted to the United States as refugees and those granted asylum status during a given fiscal year.',\n",
       " 'Context\\nInvariance to translation and rotation is an important attribute we would like image classifiers to have in many applications. For many problems, even if there doesn\\'t seem to be a lot of translation in the data, augmenting it with these transformations is often beneficial. There are not many datasets where these transformations are clearly relevant, though. The \"Snake Eyes\" dataset seeks to provide a problem where rotation and translation are clearly a fundamental aspect of the problem, and not just something intuitively believed to be involved.\\nImage classifiers are frequently utilized in a pipeline where a bounding box is first extracted from the complete image, and this process might provide centered data to the classifier. Some translation might still be present in the data the classifier sees, though, making the phenomenon relevant to classification nevertheless. A Snake Eyes classifier can clearly benefit from such a pre-processing. But the point here is trying to learn how much a classifier can learn to do by itself. In special we would like to demonstrate the \"built-in\" invariance to translations from CNNs.\\nContent\\nSnake Eyes contains artificial images simulating the a roll of one or two dice. The face patterns were modified to contain at most 3 black spots, making it impossible to solve the problem by merely counting them. The data was synthesized using a Python program, each image produced from a set of floating-point parameters modeling the position and angle of each dice.\\nThe data format is binary, with records of 401 bytes. The first byte contains the class (1 to 12, notice it does not start at 0), and the other 400 bytes are the image rows. We offer 1 million images, split in 10 files with 100k records each, and an extra test set with 10,000 images.\\nInspiration\\nWe were inspired by the popular \"tiny image\" datasets often studied in ML research: MNIST, CIFAR-10 and Fashion-MNIST. Our dataset has smaller images, though, only 20x20, and 12 classes. The reduced proportions should help approximate the actual 3D and 6D manifolds of each class with the available number of data points (1 million images).\\nThe data is artificial, with limited and very well-defined patterns, noise-free and properly anti-aliased. This is not about improving from 95% to 97% accuracy and wondering if 99% is possible with a deeper network. We don\\'t expect less than 100% precision to be achieved with any method eventually. What we are interested to see is how do different methods compare in efficiency, how hard is it to train different models, and how the translation and rotation invariance is enforced or achieved.\\nWe are also interested in studying the concept of manifold learning. The data has some intra-class variability due to different possible face combinations with two dice. But most of the variation comes from translation and rotation. We hope to have sampled enough data to really allow for the extraction of these manifolds in 400 dimensions, and to investigate topics such as the role of pre-training, and the relation between modeling the manifold of the whole data and of the separate classes.\\nTranslations alone already create quite non-convex manifolds, but our classes also have the property that some linear combinations are actually a different class (e.g. two images from the \"2\" face make an image from the \"4\" class). We are curious to see how this property can make the problem more challenging to different techniques.\\nWe are also secretly hoping to have created the image-detection version of the infamous \"spiral\" problem for neural networks. We are offering the prize of one ham sandwich, collected at my local café, to the first person who manages to train a neural network to solve this problem, convolutional or not, and using just traditional techniques such as logistic or ReLU activation functions and SGD training. 99% accuracy is enough. The resulting network may be susceptible to adversarial instances, this is fine, but we\\'ll be constantly complaining about it in your ear while you eat the sandwich.',\n",
       " \"Context\\nWhere's Waldo is a popular children's book series where the reader is presented with a sequence of scenes. Each scene contains potentially hundreds of individuals doing different things. Exactly one of these figures is Waldo: a tall man in a striped red shirt, red beanie, and glasses, and the objective of the game is to find Waldo is the least time possible. This dataset is raw data from the books for these challenges.\\nContent\\nThis dataset contains a number of cuts of Where's Waldo scenes, including scenes. See the complimentary kernel to learn more about the dataset contents!\\nAcknowledgements\\nThis dataset was collected and published as-is by Valentino Constantinou (vc1492a) on GitHub (here).\\nInspiration\\nCan you come up with a strategy better than randomly scanning the page for this task? Can you identify Waldos and not-Waldos?\",\n",
       " \"Context\\nAlong with their core mission of counting the US population, the United States Census Bureau gathers a wide range of economic data. This dataset covers 16 of their economic reports and surveys:\\nAdvance Monthly Sales for Retail and Food Services\\nConstruction Spending\\nHousing Vacancies and Homeownership\\nManufactured Housing Survey (1980-2013)\\nManufactured Housing Survey (Current)\\nManufacturers' Shipments, Inventories, and Orders\\nManufacturing and Trade Inventories and Sales\\nMonthly Retail Trade and Food Services\\nMonthly Wholesale Trade: Sales and Inventories\\nNew Home Sales\\nNew Residential Construction\\nQuarterly Financial Report\\nQuarterly Services Survey\\nQuarterly Summary of State & Local Taxes\\nQuarterly Survey of Public Pensions\\nU.S. International Trade in Goods and Services\\nContent\\nThe data csv is arranged in a long format, with the time_series_code column tying it back to the metadata csv. If you're trying to figure out what data is available, you'll want to start with the metadata.\\nJust over a third of the time series store error codes, usually confidence intervals, rather than actual values. The metadata for these time series will have values in the columns et_code, et_desc, and et_unit.\\nAll of the dates are stored as complete beginning of the period dates, but all of the time series are at either monthly, quarterly, or annual resolution. Exact days and months are provided for convenience when aligning time series and so that you don't have to unpack period codes like 'Q22009'.\\nThere may be many time series bundled under a given data category or description. For example, the largest category (taxes) contains dozens of types of tax categories, and each of those contains a separate time series for each state in the country.\\nTwo of the error code time series have non-numeric values. To convert the values column into reasonable units you'll need to drop all entries equal to the string Less than .05 percent.\\nThe data have been substantially reformatted from how they are provided by the Census Bureau. You can find the script I used to prepare the data here.\\nAcknowledgements\\nThis data was kindly made available by the United States Census. You can find the original data here. If you enjoyed this dataset you might also like one of the other US Census datasets available on Kaggle.\\nInspiration\\nThe National Bureau of Economic Research's macroeconomic history of the United States covers many similar time series, but before the census data was reported. Can you integrate it with this census data? This should allow you to generate many time series stretching from the present back to the 19th century.\",\n",
       " 'Content\\nThe data is images and labels / annotations for mammography scans. More about the database can be found at MIAS. The \\'Preview\\' kernel shows how the Info.txt and PGM files can be parsed correctly.\\nLabels\\n1st column: MIAS database reference number.\\n2nd column: Character of background tissue: F Fatty G Fatty-glandular D Dense-glandular\\n3rd column: Class of abnormality present: CALC Calcification CIRC Well-defined/circumscribed masses SPIC Spiculated masses MISC Other, ill-defined masses ARCH Architectural distortion ASYM Asymmetry NORM Normal\\n4th column: Severity of abnormality; B Benign M Malignant\\n5th, 6th columns: x,y image-coordinates of centre of abnormality.\\n7th column: Approximate radius (in pixels) of a circle enclosing the abnormality. There are also several things you should note:\\nThe list is arranged in pairs of films, where each pair represents the left (even filename numbers) and right mammograms (odd filename numbers) of a single patient. The size of all the images is 1024 pixels x 1024 pixels. The images have been centered in the matrix. When calcifications are present, centre locations and radii apply to clusters rather than individual calcifications. Coordinate system origin is the bottom-left corner. In some cases calcifications are widely distributed throughout the image rather than concentrated at a single site. In these cases centre locations and radii are inappropriate and have been omitted.\\nAcknowledgements/LICENCE\\nMAMMOGRAPHIC IMAGE ANALYSIS SOCIETY MiniMammographic Database\\n                   LICENCE AGREEMENT\\nThis is a legal agreement between you, the end user and the Mammographic Image Analysis Society (\"MIAS\"). Upon installing the MiniMammographic database (the \"DATABASE\") on your system you are agreeing to be bound by the terms of this Agreement.\\nGRANT OF LICENCE MIAS grants you the right to use the DATABASE, for research purposes ONLY. For this purpose, you may edit, format, or otherwise modify the DATABASE provided that the unmodified portions of the DATABASE included in a modified work shall remain subject to the terms of this Agreement.\\nCOPYRIGHT The DATABASE is owned by MIAS and is protected by United Kingdom copyright laws, international treaty provisions and all other applicable national laws. Therefore you must treat the DATABASE like any other copyrighted material. If the DATABASE is used in any publications then reference must be made to the DATABASE within that publication.\\nOTHER RESTRICTIONS You may not rent, lease or sell the DATABASE.\\nLIABILITY To the maximum extent permitted by applicable law, MIAS shall not be liable for damages, other than death or personal injury, whatsoever (including without limitation, damages for negligence, loss of business, profits, business interruption, loss of business information, or other pecuniary loss) arising out of the use of or inability to use this DATABASE, even if MIAS has been advised of the possibility of such damages. In any case, MIAS\\'s entire liability under this Agreement shall be limited to the amount actually paid by you or your assignor, as the case may be, for the DATABASE.\\nInspiration\\nAutomatically finding lesions would be a very helpful tool for physicians, also predicting malignancy based on a found/marked lesion',\n",
       " 'Monthly/Annual carbon dioxide emissions from electricity generation from the Energy Information Administration. Data is broken down by fuel type.\\nhttp://www.eia.gov/electricity/data.cfm#elecenv',\n",
       " 'Introduction\\nWith multispectral images, we can capture more data per pixel, and understand objects based on their chemical composition or the variation of composition that encompasses an object. Examples of this might be, is the image you see an apple or an orange? Further, is the apple or the orange real? If it is plastic, was it made in Mexico or India? Real life impacts of using spectral data as part of object detection in images could one day save a life, if a self driving car could not only detect faces, but also the difference between skin and plastic, a lone pedestrian could avoid being if it was a choice between them or a group of three manikins.\\nThis sample data contains a series of multispectral images of handwritten numbers between 0 and 9, from six different peoples, using two different pens. And here I am asking the great kagglers to explore and build models to tell each of the numbers from one another and with what ink each was written in.\\nData\\nEach csv file contains pixels for 10 grayscale images (350 * 350) that represent 10 channels for the multispectral image, where X, Y represent the location of the pixel, and channel0 - channel9 represent channels.\\nAnd we also have a labels csv that contains labels for each pixel csv file.\\nLicence\\nYou can do whatever you want with the data.',\n",
       " 'Context: Diversity of United States Counties\\nContent: Diversity Index of Every US County using the Simpson Diversity Index: D = 1 - ∑(n/N)^2 (where n = number of people of a given race and N is the total number of people of all races, to get the probability of randomly selecting two people and getting two people of different races (ecological entropy))',\n",
       " 'From the data, you will have:\\n- Results from 12k+ participants, with the fastest one of 2hr12mins from world class athlete - Midway time at 10km, halfway and 30km\\n- Overall and gender ranking\\nThe original source\\nThe data are captured from its official site\\nhttp://www.hkmarathon.com/Results/Search_2016_Results.htm\\nOnly marathon results are included (but not 10km nor half marathon) because only this results has midway time, which can serve better analysis purposes.\\nThe fields:\\nRace No: runner ID\\nCategory: gender and age group. (e.g. MMS and MFS denote male and female while the age group are the same.)\\nOfficial Time: the \"gun time\"\\nNet Time: the time between one passes the starting line and final line. It is usually a few minutes less than Official Time.\\n10km Time, Half Way Time, 30km Time: they are the midway times as described\\nThe files\\nMarathon challenge and Marathon Run 1 uses the same running path for racing but with a different starting time. Athletes in challenge group are generally run faster.\\nImproving the dataset:\\n- Comparing the results of different marathons all over the world to find which one is the toughest or having the best participants, etc. - Please let me know if there is any centralized database collecting the results from different races.',\n",
       " \"Context\\nThe Digit Recognizer competition uses the popular MNIST dataset to challenge Kagglers to classify digits correctly. In this dataset, the images are represented as strings of pixel values in train.csv and test.csv. Often, it is beneficial for image data to be in an image format rather than a string format. Therefore, I have converted the aforementioned datasets from text in .csv files to organized .jpg files.\\nContent\\nThis dataset is composed of four files:\\ntrainingSet.tar.gz (10.2 MB) - This file contains ten sub folders labeled 0 to 9. Each of the sub folders contains .jpg images from the Digit Recognizer competition's train.csv dataset, corresponding to the folder name (ie. folder 2 contains images of 2's, etc.). In total, there are 42,000 images in the training set.\\ntestSet.tar.gz (6.8 MB) - This file contains the .jpg images from the Digit Recognizer competition's test.csv dataset. In total, there are 28,000 images in the test set.\\ntrainingSample.zip (407 KB) - This file contains ten sub folders labeled 0 to 9. Each sub folder contains 60 .jpg images from the training set, for a total of 600 images.\\ntestSample.zip (233 KB) - This file contains a 350 image sample from the test set.\\nAcknowledgements\\nAs previously mentioned, all data presented here is simply a cleaned version of the data presented in Kaggle's Digit Recognizer competition. The division of the MNIST dataset into training and test sets exactly mirrors that presented in the competition.\\nInspiration\\nI created this dataset when exploring TensorFlow's Inception model. Inception is a massive CNN built by Google to compete in the ImageNet competition. By way of Transfer Learning, the final layer of Inception can be retrained, rendering the model useful for general classification tasks. In retraining the model, .jpg images must be used, thereby necessitating to the creation of this dataset.\\nMy hope in experimenting with Inception was to achieve an accuracy of around 98.5% or higher on the MNIST dataset. Unfortunately, the maximum accuracy I reached with Inception was only 95.314%. If you are interested in my code for said attempt, it is available on my GitHub repository Kaggle MNIST Inception CNN.\\nTo learn more about retraining Inception, check out TensorFlow for Poets.\",\n",
       " 'The Statistics of Income (SOI) division bases its ZIP code data on administrative records of individual income tax returns (Forms 1040) from the Internal Revenue Service (IRS) Individual Master File (IMF) system. Included in these data are returns filed during the 12-month period, January 1, 2015 to December 31, 2015. While the bulk of returns filed during the 12-month period are primarily for Tax Year 2014, the IRS received a limited number of returns for tax years before 2014 and these have been included within the ZIP code data.\\nThere is data for more years here:\\nhttps://www.irs.gov/uac/soi-tax-stats-individual-income-tax-statistics-zip-code-data-soi\\nSee documentation file attached. Crucially:\\nZIPCODE - 5-digit Zip code  \\nAGI_STUB - Size of adjusted gross income\\n1 = $1 under $25,000 2 = $25,000 under $50,000 3 = $50,000 under $75,000 4 = $75,000 under $100,000 5 = $100,000 under $200,000 6 = $200,000 or more',\n",
       " \"This is a database of the first 151 pokemon; the ones you can find in the PokemonGO game. The stats include Pokemon Number, Name, First and Second Type, Max CP, Max HP and a url from the bulbagarden.net gallery.\\nPokemon No: Number or ID of the pokemon.\\nName: The original name of the pokemon.\\nFirst Type: What type of pokemon it is.\\nSecond Type: Some pokemon can have two types, if they don't, this cell is empty.\\nMax CP: This is the maximum amount of damage a pokemon can infringe.\\nMax HP: The maximum amount of damage a pokemon can receive.\\nURL: This is a link to the pokemon's image on bulbagarden.\\nThis database presents a great way of helping new generations of pokemon players learn about data science and pokemon at the same time. This data was scrapped from http://handbooks.bulbagarden.net/pokemongo/pokemon-index\",\n",
       " 'Tweets scraped by Chris Albon on the day of the 2016 United States elections.\\nChris Albon\\'s site only posted tweet IDs, rather than full tweets. We\\'re in the process of scraping the full information, but due to API limiting this is taking a very long time. Version 1 of this dataset contains just under 400k tweets, about 6% of the 6.5 million originally posted.\\nThis dataset will be updated as more tweets become available.\\nAcknowledgements\\nThe original data was scraped by Chris Albon, and tweet IDs were posted to his Github page.\\nThe Data\\nSince I (Ed King) used my own Twitter API key to scrape these tweets, this dataset contains a couple of fields with information on whether I have personally interacted with particular users or tweets. Since Kaggle encouraged me to not remove any data from a dataset, I\\'m leaving it in; feel free to build a classifier of the types of users I follow.\\nThe dataset consists of the following fields:\\ntext: text of the tweet\\ncreated_at: date and time of the tweet\\ngeo: a JSON object containing coordinates [latitude, longitude] and a `type\\'\\nlang: Twitter\\'s guess as to the language of the tweet\\nplace: a Place object from the Twitter API\\ncoordinates: a JSON object containing coordinates [longitude, latitude] and a `type\\'; note that coordinates are reversed from the geo field\\nuser.favourites_count: number of tweets the user has favorited\\nuser.statuses_count: number of statuses the user has posted\\nuser.description: the text of the user\\'s profile description\\nuser.location: text of the user\\'s profile location\\nuser.id: unique id for the user\\nuser.created_at: when the user created their account\\nuser.verified: bool; is user verified?\\nuser.following: bool; am I (Ed King) following this user?\\nuser.url: the URL that the user listed in their profile (not necessarily a link to their Twitter profile)\\nuser.listed_count: number of lists this user is on (?)\\nuser.followers_count: number of accounts that follow this user\\nuser.default_profile_image: bool; does the user use the default profile pic?\\nuser.utc_offset: positive or negative distance from UTC, in seconds\\nuser.friends_count: number of accounts this user follows\\nuser.default_profile: bool; does the user use the default profile?\\nuser.name: user\\'s profile name\\nuser.lang: user\\'s default language\\nuser.screen_name: user\\'s account name\\nuser.geo_enabled: bool; does user have geo enabled?\\nuser.profile_background_color: user\\'s profile background color, as hex in format \"RRGGBB\" (no \\'#\\')\\nuser.profile_image_url: a link to the user\\'s profile pic\\nuser.time_zone: full name of the user\\'s time zone\\nid: unique tweet ID\\nfavorite_count: number of times the tweet has been favorited\\nretweeted: is this a retweet?\\nsource: if a link, where is it from (e.g., \"Instagram\")\\nfavorited: have I (Ed King) favorited this tweet?\\nretweet_count: number of times this tweet has been retweeted\\nI\\'ve also included a file called bad_tweets.csv , which includes all of the tweet IDs that could not be scraped, along with the error message I received while trying to scrape them. This typically happens because the tweet has been deleted, the user has deleted their account (or been banned), or the user has made their tweets private. The fields in this file are id and exception.response.',\n",
       " \"Context\\nSpecific Language Impairment is a condition that effects roughly 7% of 5-year old children. It is characterised by a lack of language ability in comparison to your peers but with no obvious mental or physical disability. Diagnosis can tend to be laborious, thus automating this process using NLP and ML techniques might be of interest to paediatricians and speech pathologists.\\nContent\\nThis study evaluated three datasets obtained via the CHILDES project. All the datasets consist of narratives from a child attempting to complete a wordless picture task. The choice to use only narrative corpora was based on previous research which indicated it has the best ability to distinguish a language impairment in children. The first dataset consists of samples from British adolescents, the second from Canadian children aged 4 to 9, and the third from U.S. children aged 4 to 12.\\nUnfortunately finding transcript data of this kind is rare, I have tried to find more data to no avail, so 1163 samples will have to do.\\nConti-Ramsden 4:\\nThe Conti-Ramsden 4 dataset was collected for a study to assess the effectiveness of narrative tests on adolescents. It consists of 99 TD and 19 SLI samples of children between the ages of 13.10 and 15.90. Ideally all the corpora would only be from children, as SLI is most prominent in children aged five years old, and is best detected early. However, it was included to enable a direct comparison between classifiers created by Gabani and this study.\\nThe corpus contains transcripts of a story telling task based on Mayer’s wordless picture book “Frog, Where Are You”. The children first viewed the picture book in their own time before being prompted to retell the story in the past tense. If the children started telling the story in the present tense the interviewer would prompt them with the phrase “What happened next?” in order to attempt to revert them back to the past tense. If they failed to start to retell the story in the past tense after two prompts no further prompts were made.\\nENNI\\nThe ENNI dataset was collected during the course of a study aimed at identifying SLI children using an index of storytelling ability based on the story grammar model. The corpus consists of 300 TD and 77 SLI samples of children aged between 4 and 9 years old. Each child was presented with two wordless picture stories with one more complicated than the other. Unlike Conti-Ramsden 4 the examiner held the book and turned the page after the child appeared to be finished telling the story for a particular picture. The children were also given the opportunity to practice on a training story, where the examiner gave more explicit prompts to the child about what to do.\\nGillam\\nThe Gillam dataset is based on another tool for narrative assessment known as “The Test of Narrative Language (TNL). It consists of 250 language impaired children, and 520 controls aged between 5 and 12. A detailed description of each of the participants does not exist. The TNL consists of four storytelling tasks, the first is a recall of a script based story, the rest being wordless picture books. The first picture set depicts a story with a main protagonist having repeated attempts at the goal, and the rest are single picture stories. The single picture stories require more input from the child, and thus is better suited to older children. The TNL appears to be intermediary in difficulty compared to ENNI.\\nFeatures\\nAttribute | Name | Description\\nY | The label | 0 for typically developing children 1 for language impaired\\nchild_TNW | Total Number of Words | The total number of words in the transcript\\nchild_TNS | Total Number of Sentences | Children with SLI are more likely to speak in short sentences\\ngroup | Same as Y | BEWARE: Is the same as Y but easier to graph in Python and R\\nexaminer_TNW | Total Number of Words spoken by the examiner | Children with SLI are more likely to need support\\nfreq_ttr | Frequency of Word Types to Word Token Ratio | Divides word types by word tokens and provides a rough measure of lexical diversity.\\nr_2_i_verbs| Ratio of raw to inflected verbs | Children with SLI often have difficulty with the morphemes -ed, -s, be, and do. This results in the use of raw verbs instead of their inflected forms.\\nmor_words | Number of words in the %mor tier |\\nnum_pos_tags | Number of different Part-of-Speech tags |\\nn_dos | Number of Do's | The number of time the word 'do' is used\\nrepetition | Number of Repetitions | Counts the number of repetitions as tagged in the CHAT format inside square brackets e.g., milk milk milk milk = milk [x 4]\\nretracing | Number of Retracings | A retracing is defined as when a speaker abandons an utterance but then continues again.\\nfillers | Number of Fillers | Counts the number of fillers used in total. A list of fillers was created by searching through the entire corpus (all 1038 samples) for all common variants of fillers such as um, umm, uh, uhh, etc.\\ns_1g_ppl | Perplexity of 1-gram SLI | The perplexity of this sample in comparison to a language model trained on all the SLI group for this corpora except the sample\\ns_2g_ppl | Perplexity of 2-gram SLI | Same as above but with a 2-gram LM\\ns_3g_ppl | Perplexity of 3-gram SLI | Same as above but with a 3-gram LM\\nd_1g_ppl | Perplexity of 1-gram TD | The perplexity of this sample in comparison to a language model trained on all the TD group for this corpora except the sample\\nd_2g_ppl | Perplexity of 2-gram TD | Same as above but with a 2-gram LM\\nd_3g_ppl | Perplexity of 3-gram TD | Same as above but with a 3-gram LM\\nz_mlu_sli | Sample Z-score using SLI group's Mean Length of Utterance |\\nz_mlu_td | Sample Z-score using TD group's Mean Length of Utterance |\\nz_ndw_sli | Sample Z-score using SLI group's Raw:Inflected Verbs Ratio |\\nz_ndw_td | Sample Z-score using TD group's Raw:Inflected Verbs Ratio |\\nz_ipsyn_sli | Sample Z-score using SLI group's Developmental Sentence Score |\\nz_ipsyn_td | Sample Z-score using TD group's Developmental Sentence Score |\\nz_utts_sli |Sample Z-score using SLI group's Number of Verb Utterances |\\nz_utts_td |Sample Z-score using TD group's Number of Verb Utterances |\\ntotal_syl | Total number of Syllables | Using a technique from\\naverage_syl | Average number of Syllables per word |\\nmlu_words | Mean Length of Utterance of Words |\\nmlu_morphemes | Mean Length of Utterance of Morphemes |\\nmlu100_utts | Mean Length of Utterance of 1st 100 words |\\nverb_utt | Number of verb utterances |\\ndss | Developmental Sentence Score |\\nipsyn_total | Index of Productive Syntax Score |\\nThe following fields are counts of instances of Brown's Stages of Morphological Development (see https://www.speech-language-therapy.com/index.php?option=com_content&view=article&id=33:brown&catid=2:uncategorised&Itemid=117)\\npresent_progressive\\npropositions_in\\npropositions_on\\nplural_s\\nirregular_past_tense\\npossessive_s\\nuncontractible_copula\\narticles\\nregular_past_ed\\nregular_3rd_person_s\\nirregular_3rd_person\\nuncontractible_aux\\ncontractible_copula\\ncontractible_aux\\nBack to normal\\nword_errors | Number of Word Errors | As marked in the transcripts\\nf_k | Flesch-Kincaid Score | See https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\\nn_v | Number of Nouns followed immediately by a verb\\nn_aux | Number of Nouns followed immediately by an Auxillary verb\\nn_3s_v | Number of Third Singular Nouns followed immediately by a verb\\ndet_n_pl | * Number of Determinant Nouns followed by a Personal Pronoun*\\ndet_pl_n | * Number of Determinant Pronouns followed by a Noun\\npro_aux | * Pronouns followed by Auxillary Verb*\\npro_3s_v | * 3rd. singular nominative pronoun followed by Verb*\\ntotal_error | Total number of morphosyntactic errors | Sum of the columns from nouns verbs down\\nThis table will take some time to finish, will get to it within a few days\\nPast Research\\nI have spent the last few months playing around with this data, I have uploaded it here mainly to speed up the computation of the analysis I have already done. But I'm excited to see what other people can do with this. There are some nice graphs to be made; especially using the MLU attributes.\\nThus far using the combined corpora the best I have managed to get in terms of creating a predictive classifier is using Neural Networks with Feature Extraction and SMOTE to get a mean ROC of 0.8709 under 10-repeated-10-k-folds CV. You'll find that Random Forest and SVM with an RBF kernel do comparably well with SMOTE.\\nAcknowledgements\\nAll the data here was derived by me using the open source transcripts provided on the CHILDES Talkbank (http://childes.talkbank.org/). The methods I used are very close to those in:\\nK. Gabani, T. Solorio, Y. Liu, K.-n. Hassanali, and C. A. Dollaghan, “Exploring a corpus-based approach for detecting language impairment in monolingual english-speaking children,” Artificial Intelligence in Medicine, vol. 53, no. 3, pp. 161–170, 2011.\\nConti-4: D. Wetherell, N. Botting, and G. Conti-Ramsden, “Narrative skills in adolescents with a history of SLI in relation to non-verbal IQ scores,” Child Language Teaching and Therapy, vol. 23, no. 1, pp. 95–113, 2007.\\nENNI: P. Schneider, D. Hayward, and R. V. Dub, “Storytelling from pictures using the Edmonton Narrative Norms Instrument,” 2006.\\nGillam: R. Gillam and N. Pearson, Test of Narrative Language. Austin, TX: Pro-Ed Inc., 2004.\\nInspiration\\nI'm hoping somebody can beat my score. I'm keen to learn more and see if this can become anything that might be of use to some child/family someday.\",\n",
       " 'Context\\nAn insurance group consists of 10 property and casualty insurance, life insurance and insurance brokerage companies. The property and casualty companies in the group operate in a 17-state region. The group is a major regional property and casualty insurer, represented by more than 4,000 independent agents who live and work in local communities through a six-state region. Define the metrics to analyse agent performance based on several attributes like demography, products sold, new business, etc. The goal is to improve their existing knowledge used for agent segmentation in a supervised predictive framework.\\nInspiration\\nVizualization:\\na. Summary stats by agency\\nb. Product line: Commercial Line/Personal Line wise analysis\\nc. Agency wise - state wise - distribution of Retention ratio (Top 10)\\nd. Quote system wise hit ratio distribution (also by PL/CL)\\ne. Add few more based on your understanding of the data\\nGrowth rates from 2006 to 2013 have to be computed and converted to independent attributes and include in the data for modeling.\\nCompute Hit ratio based on bounds and quotes for each Quotes system\\nCompute required aggregations at Agency id and state and year\\nDecide if binning the data works for this situation\\nSome suggested approaches:\\na. Model Building - Either Regression or classification\\nb. Pattern extraction - Classification Model\\nc. Patterns from the data using Decision Trees',\n",
       " 'Context\\nUnlike This dataset, (which proved to be unusable). And This one which was filled with unnecessary columns; This Donald trump dataset has the cleanest usability and consists of over 7,000 tweets, no nonsense\\nYou may need to use a decoder other than UTF-8 if you want to see the emojis\\nContent\\nData consists of:\\n-Date\\n-Time\\n-Tweet_Text\\n-Type\\n-Media_Type\\n-Hashtags\\n-Tweet_Id\\n-Tweet_Url\\n-twt_favourites_IS_THIS_LIKE_QUESTION_MARK\\n-Retweets\\nI scrapped this from someone on reddit',\n",
       " 'Context\\nAWS spot Instances allow users to bid on spare server capacity. You set a bid threshold for an instance that is usually upwards of 30% cheaper than standard on-demand AWS instances. You can save a lot of money with AWS spot instances.\\nData Content\\nI pulled this data from the AWS CLI with the describe-spot-price-history command. I took a lot of time to acquire and transform, which is why I decided to provide it here.\\nThere are various time periods per region (I acquired all that I could). The columns are all fairly self-evident. Please comment if you have any questions about the data or columns.\\nThe data includes the following column fields:\\nprice: the current Spot price\\ndatetime: the date and time\\ninstance_type: the Spot instance type\\nos: the Spot instance operating system\\nregion: the region and availability zone (AZ) for the Spot instance\\nInspiration\\nWhile AWS spot instances are significantly cheaper than on-demand instances, there is only one problem with spot instances: once the spot market price of an instance exceeds the bid threshold you purchased an instance for, the instance is terminated and given to others with higher bids. So while hourly server costs are cheaper, your server is liable to terminate without notice. But, there is a difference between regions and spot pricing. Sometimes there is an arbitrage between regions, and some regions have more stable prices than others (fewer price spikes). If you can find which region/AZ is most stable, you can worry less about your instance terminating without notice.\\nI started collecting this data because I wanted answers to two questions:\\nWhich region/AZ is historically cheapest for instance X\\nWhich region/AZ is historically most stable for instance X\\nWe could also use this data to predict which regions are likely to stay under a certain $ Spot price, which would allow you to say with some amount of certainty whether a SPOT instance lasts the next [6,12,18]+ hours.',\n",
       " \"This dataset contains the solution and the top 20 team's submissions for Facebook's 5th recruiting competition on predicting checkin places.\\nHere's how an ensemble model leveraging the top results would have performed in the competition:\",\n",
       " 'Context\\nThis dataset contains oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. This data was collected with the Tropical Atmosphere Ocean (TAO) array, which consists of nearly 70 moored buoys spanning the equatorial Pacific, measuring oceanographic and surface meteorological variables critical for improved detection, understanding and prediction of seasonal-to-interannual climate variations originating in the tropics.\\nContent\\nThe data consists of the following variables: date, latitude, longitude, zonal winds (west<0, east>0), meridional winds (south<0, north>0), relative humidity, air temperature, sea surface temperature and subsurface temperatures down to a depth of 500 meters. Data taken from the buoys from as early as 1980 for some locations. Other data that was taken in various locations are rainfall, solar radiation, current levels, and subsurface temperatures.\\nThe latitude and longitude in the data showed that the bouys moved around to different locations. The latitude values stayed within a degree from the approximate location. Yet the longitude values were sometimes as far as five degrees off of the approximate location.\\nThere are missing values in the data. Not all buoys are able to measure currents, rainfall, and solar radiation, so these values are missing dependent on the individual buoy. The amount of data available is also dependent on the buoy, as certain buoys were commissioned earlier than others.\\nAll readings were taken at the same time of day.\\nAcknowledgement\\nThis dataset is part of the UCI Machine Learning Repository, and the original source can be found here. The original owner is the NOAA Pacific Marine Environmental Laboratory.\\nInspiration\\nHow can the data be used to predict weather conditions throughout the world?\\nHow do the variables relate to each other?\\nWhich variables have a greater effect on the climate variations?\\nDoes the amount of movement of the buoy effect the reliability of the data?',\n",
       " 'Context\\nThis dataset contains data behind the story, The Dallas Shooting Was Among The Deadliest For Police In U.S. History. The data are scraped from ODMP and capture information on all tracked on-duty police officer deaths in the U.S. broken down by cause from 1971 until 2016.\\nContent\\nThis dataset tags every entry as human or canine. There are 10 variables:\\nperson\\ndept: Department\\neow: End of watch\\ncause: Cause of death\\ncause_short: Shortened cause of death\\ndate: Cleaned EOW\\nyear: Year from EOW\\ncanine\\ndept_name\\nstate\\nInspiration\\nUsing the data, can you determine the temporal trend of police officer deaths by cause? By state? By department?\\nAcknowledgements\\nThe primary source of data is the Officer Down Memorial Page (ODMP), started in 1996 by a college student who is now a police officer and who continues to maintain the database. The original data and code can be found on the FiveThirtyEight GitHub.',\n",
       " \"This dataset contains geolocation information for thousands of Twitter users during natural disasters in their area.\\nAbstract\\n(from original paper)\\nNatural disasters pose serious threats to large urban areas, therefore understanding and predicting human movements is critical for evaluating a population’s vulnerability and resilience and developing plans for disaster evacuation, response and relief. However, only limited research has been conducted into the effect of natural disasters on human mobility. This study examines how natural disasters influence human mobility patterns in urban populations using individuals’ movement data collected from Twitter. We selected fifteen destructive cases across five types of natural disaster and analyzed the human movement data before, during, and after each event, comparing the perturbed and steady state movement data. The results suggest that the power-law can describe human mobility in most cases and that human mobility patterns observed in steady states are often correlated with those in perturbed states, highlighting their inherent resilience. However, the quantitative analysis shows that this resilience has its limits and can fail in more powerful natural disasters. The findings from this study will deepen our understanding of the interaction between urban dwellers and civil infrastructure, improve our ability to predict human movement patterns during natural disasters, and facilitate contingency planning by policymakers.\\nAcknowledgments\\nThe original journal article for which this dataset was collected:\\nWang Q, Taylor JE (2016) Patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. PLoS ONE 11(1): e0147299. http://dx.doi.org/10.1371/journal.pone.0147299\\nThe Dryad page that this dataset was downloaded from:\\nWang Q, Taylor JE (2016) Data from: Patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.88354\\nThe Data\\nThis dataset contains the following fields:\\ndisaster.event: the natural disaster during which the observation was collected. One of:\\n-- one of:\\n--- *01_Wipha*, *02_Halong*, *03_Kalmaegi*, *04_Rammasun_Manila* (typhoons)\\n--- *11_Bohol*, *12_Iquique*, *13_Napa* (earthquakes)\\n--- *21_Norfolk*, *22_Hamburg*, *23_Atlanta* (winter storms)\\n--- *31_Phoenix*, *32_Detroit*, *33_Baltimore* (thunderstorms)\\n--- *41_AuFire1*, *42_AuFire2* (wildfires)\\nuser.anon: an anonymous user id; unique within each disaster event\\nlatitude: latitude of user's tweet\\nlongitude.anon: longitude of user's tweet; shifted to preserve anonymity\\ntime: the date and time of the tweet\",\n",
       " \"About this Dataset\\nDalia Research conducted the first representative poll on basic income across Europe in the Spring of 2016. The results, first presented together with NEOPOLIS at the Future of Work conference in Zurich, showed that two thirds of Europeans would vote for basic income. Dalia's basic income poll is now an annual survey, and the first wave of results from 2016 are now being made public. Although Dalia's latest research on basic income is not yet public, you can visit here to see the results from the most recent Spring 2017 survey.\\nThe study was conducted by Dalia Research in April 2016 on public opinion across 28 EU Member States. The sample of n=9.649 was drawn across all 28 EU Member States, taking into account current population distributions with regard to age (14-65 years), gender and region/country.\\nEnjoy perusing the dataset and exploring interesting connections between demographics and support for basic income.\",\n",
       " 'Context\\nConditions that could be caused by smoking resulted in 1.7 million admissions to hospitals in England, for adults aged 35 and over, in 2014-2015 -- an average of 4,700 admissions per day! These figures refer to admissions with a primary diagnosis of a disease that can be caused by smoking, but for which smoking may or may not have actually been the cause.\\nContent\\nThe Statistics on Smoking in England report aims to present a broad picture of health issues relating to smoking in England and covers topics such as smoking prevalence, habits, behaviours, and attitudes, smoking-related health issues and mortality, and associated costs.\\nAcknowledgements\\nThis report contains data and information previously published by the Health and Social Care Information Centre (HSCIC), Department of Health, the Office for National Statistics, and Her Majesty’s Revenue and Customs.',\n",
       " 'This is the data behind the Rhythm of Food visualisation by Moritz Stefaner. It shows seasonal food searches in different food types around the world since 2004. The data is indexed, with 0 being the least and 100 being the highest search interest.\\nFind out more here: http://rhythm-of-food.net/',\n",
       " \"History\\nI made the database from the fragments of my own photos of flowers. The images are selected to reflect the flowering features of these plant species.\\nContent\\nThe content is very simple: 210 images (128x128x3) with 10 species of flowering plants and the file with labels flower-labels.csv. Photo files are in the .png format and the labels are the integers.\\nLabel => Name\\n0 => phlox; 1 => rose; 2 => calendula; 3 => iris; 4 => leucanthemum maximum; 5 => bellflower; 6 => viola; 7 => rudbeckia laciniata (Goldquelle); 8 => peony; 9 => aquilegia.\\nAcknowledgements\\nAs an owner of this database, I have published it for absolutely free using by any site visitor.\\nUsage\\nAccurate classification of plant species with a small number of images isn't a trivial task. I hope this set can be interesting for training skills in this field. A wide spectrum of algorithms can be used for classification.\",\n",
       " \"Context\\nData Set with the football matches of the Spanish league of the 1st and 2nd division from the 1970-71 to 2016-17 season, has been created with the aim of opening a line of research in the Machine Learning, for the prediction of results (1X2) of football matches.\\nContent\\nThis file contains information about a football matches with the follow features:\\n4808,1977-78,1,8,Rayo Vallecano,Real Madrid,3,2,30/10/1977,247014000\\nid (4808): Unique identifier of football match\\nseason (1977-78): Season in which the match was played\\ndivision (1): División in which the match was played (1st '1', 2nd '2')\\nround (8): round in which the match was played\\nlocalTeam (Rayo Vallecano): Local Team name\\nvisitorTeam (Real Madrid): Visitor Team name\\nlocalGoals (3): Goals scored by the local team\\nvisitorGoals (2): Goals scored by the visitor team\\nfecha (30/10/1977): Date in which the match was played\\ndate (247014000): Timestamp in which the match was played\\nAcknowledgements\\nScraping made from:\\nhttp://www.bdfutbol.com\\nhttp://www.resultados-futbol.com\",\n",
       " 'Context\\nA small subset of dataset of product reviews from Amazon Kindle Store category.\\nContent\\n5-core dataset of product reviews from Amazon Kindle Store category from May 1996 - July 2014. Contains total of 982619 entries. Each reviewer has at least 5 reviews and each product has at least 5 reviews in this dataset.\\nColumns\\nasin - ID of the product, like B000FA64PK\\nhelpful - helpfulness rating of the review - example: 2/3.\\noverall - rating of the product.\\nreviewText - text of the review (heading).\\nreviewTime - time of the review (raw).\\nreviewerID - ID of the reviewer, like A3SPTOKDG7WBLN\\nreviewerName - name of the reviewer.\\nsummary - summary of the review (description).\\nunixReviewTime - unix timestamp.\\nAcknowledgements\\nThis dataset is taken from Amazon product data, Julian McAuley, UCSD website. http://jmcauley.ucsd.edu/data/amazon/\\nLicense to the data files belong to them.\\nInspiration\\nSentiment analysis on reviews.\\nUnderstanding how people rate usefulness of a review/ What factors influence helpfulness of a review.\\nFake reviews/ outliers.\\nbest rated product IDs, or similarity between products based on reviews alone (not the best idea ikr).\\nAny other interesting analysis.',\n",
       " 'Context:\\nWord embeddings define the similarity between two words by the normalised inner product of their vectors. The matrices in this repository place languages in a single space, without changing any of these monolingual similarity relationships. When you use the resulting multilingual vectors for monolingual tasks, they will perform exactly the same as the original vectors.\\nFacebook recently open-sourced word vectors in 89 languages. However these vectors are monolingual; meaning that while similar words within a language share similar vectors, translation words from different languages do not have similar vectors. In this dataset are 78 matrices, which can be used to align the majority of the fastText languages in a single space.\\nContents:\\nThis repository contains 78 matrices, which can be used to align the majority of the fastText languages in a single space.\\nThis dataset was obtained by first getting the 10,000 most common words in the English fastText vocabulary, and then using the Google Translate API to translate these words into the 78 languages available. This vocabulary was then split in two, assigning the first 5000 words to the training dictionary, and the second 5000 to the test dictionary. The alignment procedure is discribed in this blog. It takes two sets of word vectors and a small bilingual dictionary of translation pairs in two languages; and generates a matrix which aligns the source language with the target. Sometimes Google translates an English word to a non-English phrase, in these cases we average the word vectors contained in the phrase. To place all 78 languages in a single space, every matrix is aligned to the English vectors (the English matrix is the identity). You can find more information on this dataset in the authors’ GitHub repository, here.\\nAcknowledgements:\\nThis dataset was produced by Samuel Smith, David Turban, Steven Hamblin and Nils Hammerly. If you use this repository, please cite: Offline bilingual word vectors, orthogonal transformations and the inverted softmax. Samuel L. Smith, David H. P. Turban, Steven Hamblin and Nils Y. Hammerla. ICLR 2017 (conference track)\\nInspiration:\\nCan you use the word embeddings in this dataset to cluster languages into their families?\\nCan you create a visualization of the relationship between words for similar concepts across languages?',\n",
       " 'Context:\\nLinked data: “Linked open data is linked data that is open content. In computing, linked data (often capitalized as Linked Data) is a method of publishing structured data so that it can be interlinked and become more useful through semantic queries. It builds upon standard Web technologies such as HTTP, RDF and URIs, but rather than using them to serve web pages for human readers, it extends them to share information in a way that can be read automatically by computers. This enables data from different sources to be connected and queried.” -- “Linked Open Data” on Wikipedia\\nAnime: “Anime is a Japanese term for hand-drawn or computer animation. The word is the abbreviated pronunciation of \"animation\" in Japanese, where this term references all animation. Outside Japan, anime is used to refer specifically to animation from Japan or as a Japanese-disseminated animation style often characterized by colorful graphics, vibrant characters and fantastical themes.” -- “Anime” on Wikipedia\\nThis dataset is a linked open dataset that contains information on 391706 anime titles.\\nContent:\\nThis dataset contains two files. The first is the native N-Triples format, which is suitable for tasks. The second is a .csv containing three columns:\\nAnime: the title of the anime\\nConcept: the concept\\nValue: the value of the concept for that anime\\nThe .csv is not a true linked data dataset, since it has removed many of the relevant URL’s. However, it should prove easier for data analysis.\\nAcknowledgements:\\nThis dataset has been collected and maintained by Pieter Heyvaert. It is © Between Our Worlds and reproduced here under an MIT license. You can find more information on this dataset and the most recent version here.\\nInspiration:\\nMany anime have summaries, under the “description” concept. Can you use these to identify common themes in anime? What about training an anime description generator?\\nCan you plot the number of titles released over time? Has the rate of anime production increased or decreased over time?',\n",
       " 'Context:\\nIn the United States, animal bites are often reported to law enforcement (such as animal control). The main concern with an animal bite is that the animal may be rabid. This dataset includes information on over 9,000 animal bites which occurred near Louisville, Kentucky from 1985 to 2017 and includes information on whether the animal was quarantined after the bite occurred and whether that animal was rabid.\\nContent:\\nAttributes of animal bite incidents reported to and investigated by Louisville Metro Department of Public Health and Wellness. Personal/identifying data has been removed. This dataset is a single .csv with the following fields.\\nbite_date: The date the bite occurred\\nSpeciesIDDesc: The species of animal that did the biting\\nBreedIDDesc: Breed (if known)\\nGenderIDDesc: Gender (of the animal)\\ncolor: color of the animal\\nvaccination_yrs: how many years had passed since the last vaccination\\nvaccination_date: the date of the last vaccination\\nvictim_zip: the zipcode of the victim\\nAdvIssuedYNDesc: whether advice was issued\\nWhereBittenIDDesc: Where on the body the victim was bitten\\nquarantine_date: whether the animal was quarantined\\nDispositionIDDesc: whether the animal was released from quarantine\\nhead_sent_date: the date the animal’s head was sent to the lab\\nrelease_date: the date the animal was released\\nResultsIDDesc: results from lab tests (for rabies)\\nAcknowledgements:\\nAttributes of animal bite incidents reported to and investigated by Louisville Metro Department of Public Health and Wellness. This data is in the public domain.\\nInspiration:\\nWhich animals are most likely to bite humans?\\nAre some dog breeds more likely to bite?\\nWhat factors are most strongly associated with a positive rabies ID?',\n",
       " \"Content\\nThis is the Better Life Index for 2017 gathered from the OECD stats page. Grouping labels have been removed and the row for units of measurment for each column has been removed with the units added to the end of each column label as such: (Percentage: 'as pct'; Ratio: 'as rat'; US Dollar: 'in usd'; Average score: 'as avg score'; Years: 'in years'; Micrograms per cubic metre: 'in ugm3'; Hours: 'in hrs'). Also, although included in the report, Brazil, Russia, and South Africa are non-OECD economies at the time of reporting\\nAcknowledgements\\nOECD stats page For full index and others please visit: http://stats.oecd.org/Index.aspx?DataSetCode=BLI\",\n",
       " 'Context:\\nThe Bureau of Labor Statistics defines the Consumer Price Index (CPI) as “a statistical measure of change, over time, of the prices of goods and services in major expenditure groups--such as food, housing, apparel, transportation, and medical care--typically purchased by urban consumers. Essentially, it compares the cost of a sample of goods and services in a specific month relative to the cost of the same \"market basket\" in an earlier reference period.\\nMake sure to read the cu.txt for more descriptive summaries on each data file and how to use the unique identifiers.\\nContent:\\nThis dataset was collected June 27th, 2017 and may not be up-to-date.\\nThe revised CPI introduced by the BLS in 1998 includes indexes for two populations; urban wage earners and clerical workers (CW), and all urban consumers (CU). This dataset covers all urban consumers (CU).\\nThe Consumer Price Index (CPI) is a statistical measure of change, over time, of the prices of goods and services in major expenditure groups--such as food, housing, apparel, transportation, and medical care--typically purchased by urban consumers. Essentially, it compares the cost of a sample \"market basket\" of goods and services in a specific month relative to the cost of the same \"market basket\" in an earlier reference period. This reference period is designated as the base period.\\nAs a result of the 1998 revision, both the CW and the CU utilize updated expenditure weights based upon data tabulated from three years (1982, 1983, and 1984) of the Consumer Expenditure Survey and incorporate a number of technical improvements, including an updated and revised item structure.\\nTo construct the two indexes, prices for about 100,000 items and data on about 8,300 housing units are collected in a sample of 91 urban places. Comparison of indexes for individual CMSA\\'s or cities show only the relative change over time in prices between locations. These indexes cannot be used to measure interarea differences in price levels or living costs.\\nSummary Data Available: U.S. average indexes for both populations are available for about 305 consumer items and groups of items. In addition, over 100 of the indexes have been adjusted for seasonality. The indexes are monthly with some beginning in 1913. Semi-annual indexes have been calculated for about 100 items for comparison with semi-annual areas mentioned below. Semi-annual indexes are available from 1984 forward.\\nArea indexes for both populations are available for 26 urban places. For each area, indexes are published for about 42 items and groups. The indexes are published monthly for three areas, bimonthly for eleven areas, and semi-annually for 12 urban areas.\\nRegional indexes for both populations are available for four regions with about 55 items and groups per region. Beginning with January 1987, indexes are monthly, with some beginning as early as 1966. Semi-annual indexes have been calculated for about 42 items for comparison with semi-annual areas mentioned above. Semi-annual indexes have been calculated for about 42 items in the 27 urban places for comparison with semi-annual areas.\\nCity-size indexes for both populations are available for three size classes with about 55 items and groups per class. Beginning with January 1987, indexes are monthly and most begin in 1977. Semi-annual indexes have been calculated for about 42 items for comparison with semi-annual areas mentioned below.\\nRegion/city-size indexes for both populations are available cross classified by region and city-size class. For each of 13 cross calculations, about 42 items and groups are available. Beginning with January 1987, indexes are monthly and most begin in 1977. Semi-annual indexes have been calculated for about 42 items in the 26 urban places for comparison with semi-annual areas.\\nFrequency of Observations: U.S. city average indexes, some area indexes, and regional indexes, city-size indexes, and region/city-size indexes for both populations are monthly. Other area indexes for both populations are bimonthly or semi-annual.\\nAnnual Averages: Annual averages are available for all unadjusted series in the CW and CU.\\nBase Periods: Most indexes have a base period of 1982-1984 = 100. Other indexes, mainly those which have been added to the CPI program with the 1998 revision, are based more recently. The base period value is 100.0, except for the \"Purchasing Power\" values (AAOR and SAOR) where the base period value is 1.000.\\nData Characteristics: Indexes are stored to one decimal place, except for the \"Purchasing Power\" values which are stored to three decimal places.\\nReferences: BLS Handbook of Methods, Chapter 17, \"Consumer Price Index\", BLS Bulletin 2285, April 1988.\\nAcknowledgements:\\nThis dataset was taken directly from the U.S. Bureau of Labor Statistics website at http://www.bls.gov/data/ and converted to CSV format.\\nInspiration:\\nThe Bureau of Labor Statistics has done a great job of providing this source of information for the public to explore. You can use this information to compare the cost of living in urban areas around the United States. What are the top 10 most expensive places to live? Which cities have the most expensive snacks or college textbooks? Coffee? Beer?',\n",
       " \"The objective of the BRFSS is to collect uniform, state-specific data on preventive health practices and risk behaviors that are linked to chronic diseases, injuries, and preventable infectious diseases in the adult population. Factors assessed by the BRFSS include tobacco use, health care coverage, HIV/AIDS knowledge or prevention, physical activity, and fruit and vegetable consumption. Data are collected from a random sample of adults (one per household) through a telephone survey.\\nThe Behavioral Risk Factor Surveillance System (BRFSS) is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.\\nContent\\nEach year contains a few hundred columns. Please see one of the annual code books for complete details.\\nThese CSV files were converted from a SAS data format using pandas; there may be some data artifacts as a result.\\nIf you like this dataset, you might also like the data for 2001-2010.\\nAcknowledgements\\nThis dataset was released by the CDC. You can find the original dataset and additional years of data here.\",\n",
       " \"Context\\nThe Project Tycho database was named after the Danish nobleman Tycho Brahe, who is known for his detailed astronomical and planetary observations. Tycho was not able to use all of his data for breakthrough discoveries, but his assistant Johannes Kepler used Tycho's data to derive the laws of planetary motion. Similarly, this project aims to advance the availablity of large scale public health data to the worldwide community to accelerate advancements in scientific discovery and technological progress.\\nContent\\nThe Project Tycho database (level one) includes standardized counts at the state level for smallpox, polio, measles, mumps, rubella, hepatitis A, and whooping cough from weekly National Notifiable Disease Surveillance System (NNDSS) reports for the United States. The time period of data varies per disease somewhere between 1916 and 2010. The records include cases and incidence rates per 100,000 people based on historical population estimates. These data have been used by investigators at the University of Pittsburgh to estimate the impact of vaccination programs in the United States, recently published in the New England Journal of Medicine.\\nAcknowledgements\\nThe Project Tycho database was digitized and standardized by a team at the University of Pittsburgh, including Professor Wilbert van Panhuis, MD, PhD, Professor John Grefenstette, PhD, and Dean Donald Burke, MD.\",\n",
       " \"Context\\nThe GeoNames geographical database contains over 10 million geographical names and consists of over 9 million unique features with 2.8 million populated places and 5.5 million alternate names. All features are categorized into one out of nine feature classes and further subcategorized into one out of 645 feature codes.\\nContent\\nThe main 'geoname' table has the following fields :\\ngeonameid : integer id of record in geonames database\\nname : name of geographical point (utf8) varchar(200)\\nasciiname : name of geographical point in plain ascii characters, varchar(200)\\nalternatenames : alternatenames, comma separated, ascii names automatically transliterated, convenience attribute from alternatename table, varchar(10000)\\nlatitude : latitude in decimal degrees (wgs84)\\nlongitude : longitude in decimal degrees (wgs84)\\nfeature class : see http://www.geonames.org/export/codes.html, char(1)\\nfeature code : see http://www.geonames.org/export/codes.html, varchar(10)\\ncountry code : ISO-3166 2-letter country code, 2 characters\\ncc2 : alternate country codes, comma separated, ISO-3166 2-letter country code, 200 characters\\nadmin1 code : fipscode (subject to change to iso code), see exceptions below, see file admin1Codes.txt for display names of this code; varchar(20)\\nadmin2 code : code for the second administrative division, a county in the US, see file admin2Codes.txt; varchar(80)\\nadmin3 code : code for third level administrative division, varchar(20)\\nadmin4 code : code for fourth level administrative division, varchar(20)\\npopulation : bigint (8 byte int)\\nelevation : in meters, integer\\ndem : digital elevation model, srtm3 or gtopo30, average elevation of 3''x3'' (ca 90mx90m) or 30''x30'' (ca 900mx900m) area in meters, integer. srtm processed by cgiar/ciat.\\ntimezone : the iana timezone id (see file timeZone.txt) varchar(40)\\nmodification date : date of last modification in yyyy-MM-dd format\\nAdminCodes:\\nMost adm1 are FIPS codes. ISO codes are used for US, CH, BE and ME. UK and Greece are using an additional level between country and fips code. The code '00' stands for general features where no specific adm1 code is defined. The corresponding admin feature is found with the same countrycode and adminX codes and the respective feature code ADMx.\\nfeature classes:\\nA: country, state, region,...\\nH: stream, lake, ...\\nL: parks,area, ...\\nP: city, village,...\\nR: road, railroad\\nS: spot, building, farm\\nT: mountain,hill,rock,...\\nU: undersea\\nV: forest,heath,...\\nAcknowledgements\\nData Sources: http://www.geonames.org/data-sources.html\",\n",
       " \"Context\\nHealth care in the United States is provided by many distinct organizations. Health care facilities are largely owned and operated by private sector businesses. 58% of US community hospitals are non-profit, 21% are government owned, and 21% are for-profit. According to the World Health Organization (WHO), the United States spent more on healthcare per capita ($9,403), and more on health care as percentage of its GDP (17.1%), than any other nation in 2014. Many different datasets are needed to portray different aspects of healthcare in US like disease prevalences, pharmaceuticals and drugs, Nutritional data of different food products available in US. Such data is collected by surveys (or otherwise) conducted by Centre of Disease Control and Prevention (CDC), Foods and Drugs Administration, Center of Medicare and Medicaid Services and Agency for Healthcare Research and Quality (AHRQ). These datasets can be used to properly review demographics and diseases, determining start ratings of healthcare providers, different drugs and their compositions as well as package informations for different diseases and for food quality. We often want such information and finding and scraping such data can be a huge hurdle. So, Here an attempt is made to make available all US healthcare data at one place to download from in csv files.\\nContent\\nNhanes Survey (National Health and Nutrition Examination Survey) - The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation. The NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel. The diseases, medical conditions, and health indicators to be studied include: Anemia, Cardiovascular disease, Diabetes, Environmental exposures, Eye diseases, Hearing loss, Infectious diseases, Kidney disease, Nutrition, Obesity, Oral health, Osteoporosis, Physical fitness and physical functioning, Reproductive history and sexual behavior, Respiratory disease (asthma, chronic bronchitis, emphysema), Sexually transmitted diseases, Vision. 10000 individuals are surveyed to represent US statistics. Five files in this datasets represent current recent Nhanes data -\\n*Nhanes_2005_2006.csv*\\n*Nhanes_2007_2008.csv*\\n*Nhanes_2009_2010.csv*\\n*Nhanes_2011_2012.csv*\\n*Nhanes_2013_2014.csv*\\nData fields' description -\\nNhanes_2005_2006.csv - Demographic, Dietary, Examinations, Laboratory\\nNhanes_2007_2008.csv - Demographic, Dietary, Examinations, Laboratory\\nNhanes_2009_2010.csv - Demographic, Dietary, Examinations, Laboratory\\nNhanes_2011_2012.csv - Demographic, Dietary, Examinations, Laboratory\\nNhanes_2013_2014.csv - Demographic, Dietary, Examinations, Laboratory\\nUS Drugs datasets - FDA provides a database for searching all the published drugs and all the unpublished drugs on their website, This database provides all the information about package of drugs and compositions of drugs their NDC codes. Description of variables for this datasets are as follows -\\n*Drugs_product (current and unfinished)*\\nPRODUCTID - Id of the product\\nPRODUCTNDC - National drug code of the product\\nPRODUCTTYPENAME - Type of the product\\nPROPRIETARYNAME - Proprietary name of the product\\nPROPRIETARYNAMESUFFIX - Proprietary name Suffix\\nNONPROPRIETARYNAME - Non- proprietary (common name) of the product\\nDOSAGEFORMNAME - Dosage information\\nROUTENAME - Route of taking drugs (Oral / Injections)\\nSTARTMARKETINGDATE - Date on which marketing for the drug has started\\nENDMARKETINGDATE - Date on which the marketing for the drug has stopped\\nMARKETINGCATEGORYNAME - Marketing category name\\nAPPLICATIONNUMBER - Application number for registering drug\\nLABELERNAME - Labeler name\\nSUBSTANCENAME - Names of the substances in drug\\nACTIVE_NUMERATOR_STRENGTH - Strength of the drug\\nACTIVE_INGRED_UNIT - Unit of strength\\nPHARM_CLASSES - Pharmaceutical class of the drugs\\nDEASCHEDULE - DEA schedule\\nDrugs Package (current and unfinished)\\nPRODUCTID - Id of the product\\nPRODUCTNDC - National drug code of the product\\nNDCPACKAGECODE National drug code of the package\\nPACKAGEDESCRIPTION - description of the p[ackage\\nNutritions Data from USDA - Whenever we buy a packaged food product, we find the nutritional fact written on it. United States Department of Agriculture Agricultural Research Service’s Food composition database. This database contains all kinds food products available in US and provides description of their nutritions. This dataset is web scrapped and converted into a csv file. Variables are self-explanatory names yet the descriptions can be found at this link - variables descriptions -( All values are per 100 grams) -\\nData fields' description -\\nNDB_No - Nutrition database number\\nShrt_Desc - Short description\\nWater_(g) - water in grams per 100 grams\\nEnerg_Kcal - Energy in Kcal\\nProtein_(g) - Protein\\nLipid_Tot_(g) - Total Lipid\\nAsh_(g) - Ash\\nCarbohydrt_(g) - Carbohydrate, by difference\\nFiber_TD_(g) - Fiber, total dietary\\nSugar_Tot_(g) - Total Sugars\\nCalcium_(mg) - Calcium\\nIron_(mg) - Iron\\nMagnesium_(mg) - Magnesium\\nPhosphorus_(mg) - Phosphorus\\nPotassium_(mg) - Potassium\\nZinc_(mg) - Zinc\\nCopper_(mg) - Copper\\nManganese_(mg) - Manganese\\nSelenium_(æg) - Selenium\\nVit_C_(mg) - Vitamin C, total ascorbic acid\\nThiamin_(mg) - Thiamin\\nRiboflavin_(mg) - Riboflavin\\nNiacin_(mg) - Niacin\\nPanto_Acid_(mg) - Pantothenic acid\\nVit_B6_(mg) - Vitamin B6\\nFolate_Tot_(æg) - Folate, total\\nFolic_Acid_(æg) - Folic acid\\nFood_Folate_(æg) - Folate, food\\nFolate_DFE_(æg) - Folate, DFE\\nCholine_Tot_ (mg) - Choline, total\\nVit_B12_(æg) - Vitamin B-12\\nVit_A_IU - Vitamin A, IU\\nVit_A_RAE - Vitamin A, RAE\\nRetinol_(æg) - Retinol\\nAlpha_Carot_(æg) - Carotene, alpha\\nBeta_Carot_(æg) - Carotene, beta\\nBeta_Crypt_(æg) - Cryptoxanthin, beta\\nLycopene_(æg) - Lycopene\\nLut+Zea_ (æg) - Lutein + zeaxanthin\\nVit_E_(mg) - Vitamin E (alpha-tocopherol)\\nVit_D_æg - Vitamin D (D2 + D3)\\nVit_D_IU - Vitamin D\\nVit_K_(æg) - Vitamin K (phylloquinone)\\nFA_Sat_(g) - Fatty acids, total saturated\\nFA_Mono_(g) - Fatty acids, total monounsaturated\\nFA_Poly_(g) - Fatty acids, total polyunsaturated\\nCholestrl_(mg) - Cholesterol\\nGmWt_1 - gram weight 1\\nGmWt_Desc1 gram weight 1 descriptions\\nGmWt_2 - gram weight 2\\nGmWt_Desc2 - gram weight 2 description\\nStar rating of health care plans with HOS-CAHPS measures - HOS CAHPS survey measures are the base of determining star rating of healthcare plan. Files related to star rating have two types of measures which are used to determine star rating of the healthcare plans - Part C and Part D. Part C is has three type of information 1. Chronic conditions (disease) 2. Tests and Vaccines 3. Member experience with healthcare plans. All variables starting with C01 to C32 are related to part C of the surveys. Similarly Part D of the survey is related to Drugs plans customer services. In data variables starting with D01 to D15 is related to part D. Surveys such as HOS CAHPS etc contains questions whose final standing results into C01 to C32, and D01 to D15 measures. Dataset has two star rating and measurements data released in fall 2015 and Spring 2016. Files description -\\nStar_rating_fall/spring_2015_C_cutoff.csv - Contains information about different cut off used in determining star rating of part C measures.\\nStar_rating_fall/spring_2016_D_cutoff - Contains information about different cut off used in determining star rating of part D measures.\\nStar_rating_fall/spring_domain.csv - Contains information about domain rating of plans\\nStar_rating_fall/spring_high_performing_plans.csv - List of high performing plans\\nStar_rating_fal/spring_low_performing_plans.csv - List of low performing plans\\nStar_rating_fall/spring_master_data.csv - Contains information on all the measures of all plans\\nStar_rating_fall/spring_plans_final_star_rating.csv - Having information of star rating of healthcare plans\\nDescription -\\nCONTRACT_ID - Healthcare plan id\\nOrganization Type - Type of the organizer - employer/demo/local cpp etc\\nContract Name - Name of the contract\\nOrganization Marketing Name - Self explanatory\\nParent Organization - Healthcare provider\\nHD1: Staying Healthy: Screenings, Tests and Vaccines (domain)\\nC01: Breast Cancer Screening\\nC02: Colorectal Cancer Screening\\nC03: Annual Flu Vaccine\\nC04: Improving or Maintaining Physical Health\\nC05: Improving or Maintaining Mental Health\\nC06: Monitoring Physical Activity\\nC07: Adult BMI Assessment\\nHD2: Managing Chronic (Long Term) Conditions (domain)\\nC08: Special Needs Plan (SNP) Care Management\\nC09: Care for Older Adults – Medication Review\\nC10: Care for Older Adults – Functional Status Assessment\\nC11: Care for Older Adults – Pain Assessment\\nC12: Osteoporosis Management in Women who had a Fracture\\nC13: Diabetes Care – Eye Exam\\nC14: Diabetes Care – Kidney Disease Monitoring\\nC15: Diabetes Care – Blood Sugar Controlled\\nC16: Controlling Blood Pressure\\nC17: Rheumatoid Arthritis Management\\nC18: Reducing the Risk of Falling\\nC19: Plan All-Cause Readmissions\\nHD3: Member Experience with Health Plan (domain)\\nC20: Getting Needed Care\\nC21: Getting Appointments and Care Quickly\\nC22: Customer Service\\nC23: Rating of Health Care Quality\\nC24: Rating of Health Plan\\nC25: Care Coordination\\nHD4: Member Complaints and Changes in the Health Plan's Performance (domain)\\nC26: Complaints about the Health Plan\\nC27: Members Choosing to Leave the Plan\\nC28: Beneficiary Access and Performance Problems\\nC29: Health Plan Quality Improvement\\nHD5: Health Plan Customer Service (domain)\\nC30: Plan Makes Timely Decisions about Appeals\\nC31: Reviewing Appeals Decisions\\nC32: Call Center – Foreign Language Interpreter and TTY Availability\\nDD1: Drug Plan Customer Service\\nD01: Call Center – Foreign Language Interpreter and TTY Availability\\nD02: Appeals Auto–Forward\\nD03: Appeals Upheld\\nDD2: Member Complaints and Changes in the Drug Plan’s Performance\\nD04: Complaints about the Drug Plan\\nD05: Members Choosing to Leave the Plan\\nD06: Beneficiary Access and Performance Problems\\nD07: Drug Plan Quality Improvement\\nDD3: Member Experience with the Drug Plan\\nD08: Rating of Drug Plan\\nD09: Getting Needed Prescription Drugs\\nDD4: Drug Safety and Accuracy of Drug Pricing\\nD10: MPF Price Accuracy\\nD11: High Risk Medication\\nD12: Medication Adherence for Diabetes Medications\\nD13: Medication Adherence for Hypertension (RAS antagonists)\\nD14: Medication Adherence for Cholesterol (Statins)\\nD15: MTM Program Completion Rate for CMR\\nSNP - Are they offering special plans\\nSanction Deduction - If sanction is deducted from last survey to this survey\\n2016 Part C Summary - 2016 Part C rating\\n2016 Part D Summary - 2016 Part D rating\\n2016 Overall - 2016 Overall star rating of the plan\\nRated-as - Category name\\nHighest rating - category -C/D/Overall for which rating is high\\nRating - Star rating of the plan\\nAcknowledgements\\nI have collected these files from various data websites and data sources listed below -\\nNhanes - from CDS's National Health and Nutrition Examination Survey. Link\\nDrugs' dataset - from FDA drug database. link\\nNutritions' dataset - USDA Food composition databsase. link\\nStar rating dataset - CMS website. link\\nInspiration\\nThese datasets are used for hundreds of publications per year worldwide. Link\",\n",
       " \"Content\\nThe dataset consists of two files, training and validation. Each folder contains 10 subforders labeled as n0~n9, each corresponding a species form Wikipedia's monkey cladogram. Images are 400x300 px or larger and JPEG format (almost 1400 images). Images were downloaded with help of the googliser open source code.\\nLabel mapping:\\n> Label, Latin Nama\\n> n0, alouatta_palliata\\n> n1, erythrocebus_patas\\n> n2, cacajao_calvus\\n> n3, macaca_fuscata\\n> n4, cebuella_pygmea\\n> n5, cebus_capucinus\\n> n6, mico_argentatus\\n> n7, saimiri_sciureus\\n> n8, aotus_nigriceps\\n> n9, trachypithecus_johnii\\nFor more information on the monkey species and number of images per class make sure to check monkey_labels.txt file.\\nAim\\nThis dataset is intended as a test case for fine-grain classification tasks, perhaps best used in combination with transfer learning. Hopefully someone can help us expand the number of classes or number of images.\\nAcknowledgements\\nThanks to Romain Renard for his help with the code implementation. Also, thanks to Gustavo Montoya, Jacky Zhang and Sofia Loaiciga for their help with the dataset curation.\\nNotes\\nSome demo code for usage of the dataset in combination with Keras can be found in this repo.\",\n",
       " 'Charlottesville, Virgina\\nCharlottesville is home to a statue of Robert E. Lee which is slated to be removed. (For those unfamiliar with American history, Robert E. Lee was a US Army general who defected to the Confederacy during the American Civil War and was considered to be one of their best military leaders.) While many Americans support the move, believing the main purpose of the Confederacy was to defend the institution of slavery, many others do not share this view. Furthermore, believing Confederate symbols to be merely an expression of Southern pride, many have not taken its planned removal lightly.\\nAs a result, many people--including white nationalists and neo-Nazis--have descended to Charlottesville to protest its removal. This in turn attracted many counter-protestors. Tragically, one of the counter-protestors--Heather Heyer--was killed and many others injured after a man intentionally rammed his car into them. In response, President Trump blamed \"both sides\" for the chaos in Charlottesville, leading many Americans to denounce him for what they see as a soft-handed approach to what some have called an act of \"domestic terrorism.\"\\nThis dataset below captures the discussion--and copious amounts of anger--revolving around this past week\\'s events.\\nThe Data\\nDescription\\nThis data set consists of a random sample of 50,000 tweets per day (in accordance with the Twitter Developer Agreement) of tweets mentioning Charlottesville or containing \"#charlottesville\" extracted via the Twitter Streaming API, starting on August 15. The files were copied from a large Postgres database containing--currently--over 2 million tweets. Finally, a table of tweet counts per timestamp was created using the whole database (not just the Kaggle sample). The data description PDF provides a full summary of the attributes found in the CSV files.\\nNote: While the tweet timestamps are in UTC, the cutoffs were based on Eastern Standard Time, so the August 16 file will have timestamps ranging from 2017-08-16 4:00:00 UTC to 2017-08-17 4:00:00 UTC.\\nFormat\\nThe dataset is available as either separate CSV files or a single SQLite database.\\nLicense\\nI\\'m releasing the dataset under the CC BY-SA 4.0 license. Furthermore, because this data was extracted via the Twitter Streaming API, its use must abide by the Twitter Developer Agreement. Most notably, the display of individual tweets should satisfy these requirements. More information can be found in the data description file, or on Twitter\\'s website.\\nAcknowledgements\\nObviously, I would like to thank Twitter for providing a fast and reliable streaming service. I\\'d also like to thank the developers of the Python programming language, psycopg2, and Postgres for creating amazing software with which this data set would not exist.\\nImage Credit\\nThe banner above is a personal modification of these images:\\nEvan Nesterak: Image Source Image License\\nWikipedia user Cville Dog Image Source\\nThe Associated Press Image Source\\nInspiration\\nI almost removed the header \"inspiration\" from this section, because this is a rather sad and dark data set. However, this is preciously why this is an important data set to analyze. Good history books have never shied away from unpleasant events, and never should we.\\nThis data set provides a rich opportunity for many types of research, including:\\nNatural language processing\\nSentiment analysis\\nData visualization\\nFurthermore, given the political nature of this dataset, there are a lot of social science questions that can potentially be answered, or at least piqued, by this data.',\n",
       " \"Context:\\nThere are over 7,000 human languages in the world. The World Atlas of Language Structures (WALS) contains information on the structure of 2,679 of them. It also includes information about where languages are used. WALS is widely-cited and used in the linguistics research community.\\nContent:\\nThe World Atlas of Language Structures (WALS) is a large database of structural (phonological, grammatical, lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors. The atlas provides information on the location, linguistic affiliation and basic typological features of a great number of the world's languages\\nWALS Online is a publication of the (Max Planck Institute for Evolutionary Anthropology)[http://www.eva.mpg.de/]. It is a separate publication, edited by Dryer, Matthew S. & Haspelmath, Martin (Leipzig: Max Planck Institute for Evolutionary Anthropology, 2013) The main programmer is Robert Forkel.\\nThis dataset includes three files:\\nsource.bib: A BibTex file with all of the sources cited in the dataset in it\\nlanguage.csv: A file with a list of all the languages included in WALS\\nwals-data.csv: A file containing information on the features associated with each individual language\\nAcknowledgements:\\nThis dataset is licensed under a Creative Commons Attribution 4.0 International License .\\nThe World Atlas of Language Structures was edited by Matthew Dryer and Martin Haspelmath. If you use this data in your work, please include the following citation:\\nDryer, Matthew S. & Haspelmath, Martin (eds.) 2013. The World Atlas of Language Structures Online. Leipzig: Max Planck Institute for Evolutionary Anthropology. (Available online at http://wals.info, Accessed on September 7, 2017.)\\nInspiration:\\nThis dataset was designed to make interactive maps of language features. Can you make an interactive map that shows different linguistic features? You might find it helpful to use Leaflet (for R) or Plotly (for Python). This blog post is a great resource to help you get started.\\nThere’s a lot of discussion of “linguistic universals” in linguistics. These are specific features that every language (should) have. Can you identify any features that you think may be universals from this dataset?\\nYou may also like:\\nAtlas of Pidgin and Creole Language Structures: Information on 76 Creole and Pidgin Languages\\nWorld Language Family Map\\nThe Sign Language Analyses (SLAY) Database\",\n",
       " 'General Info\\nThis is a set of just over 20,000 games collected from a selection of users on the site Lichess.org, and how to collect more. I will also upload more games in the future as I collect them. This set contains the:\\nGame ID;\\nRated (T/F);\\nStart Time;\\nEnd Time;\\nNumber of Turns;\\nGame Status;\\nWinner;\\nTime Increment;\\nWhite Player ID;\\nWhite Player Rating;\\nBlack Player ID;\\nBlack Player Rating;\\nAll Moves in Standard Chess Notation;\\nOpening Eco (Standardised Code for any given opening, list here);\\nOpening Name;\\nOpening Ply (Number of moves in the opening phase)\\nFor each of these separate games from Lichess. I collected this data using the Lichess API, which enables collection of any given users game history. The difficult part was collecting usernames to use, however the API also enables dumping of all users in a Lichess team. There are several teams on Lichess with over 1,500 players, so this proved an effective way to get users to collect games from.\\nPossible Uses\\nLots of information is contained within a single chess game, let alone a full dataset of multiple games. It is primarily a game of patterns, and data science is all about detecting patterns in data, which is why chess has been one of the most invested in areas of AI in the past. This dataset collects all of the information available from 20,000 games and presents it in a format that is easy to process for analysis of, for example, what allows a player to win as black or white, how much meta (out-of-game) factors affect a game, the relationship between openings and victory for black and white and more.',\n",
       " 'Context\\nThe task is to predict whether an image is an advertisement (\"ad\") or not (\"nonad\").\\nContent\\nThere are 1559 columns in the data.Each row in the data represent one image which is tagged as ad or nonad in the last column.column 0 to 1557 represent the actual numerical attributes of the images\\nAcknowledgements\\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\\nHere is a BiBTeX citation as well:\\n@misc{Lichman:2013 , author = \"M. Lichman\", year = \"2013\", title = \"{UCI} Machine Learning Repository\", url = \"http://archive.ics.uci.edu/ml\", institution = \"University of California, Irvine, School of Information and Computer Sciences\" } https://archive.ics.uci.edu/ml/citation_policy.html',\n",
       " 'Context\\nThis corpus contains 5001 female names and 2943 male names, sorted alphabetically, one per line created by Mark Kantrowitz and redistributed in NLTK.\\nThe names.zip file includes\\nREADME: The readme file.\\nfemale.txt: A line-delimited list of words.\\nmale.txt: A line-delimited list of words.\\nLicense/Usage\\nNames Corpus, Version 1.3 (1994-03-29)\\nCopyright (C) 1991 Mark Kantrowitz\\nAdditions by Bill Ross\\n\\nThis corpus contains 5001 female names and 2943 male names, sorted\\nalphabetically, one per line.\\n\\nYou may use the lists of names for any purpose, so long as credit is\\ngiven in any published work. You may also redistribute the list if you\\nprovide the recipients with a copy of this README file. The lists are\\nnot in the public domain (I retain the copyright on the lists) but are\\nfreely redistributable.  If you have any additions to the lists of\\nnames, I would appreciate receiving them.\\n\\nMark Kantrowitz <mkant+@cs.cmu.edu>\\nhttp://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\\nInspiration\\nThis corpus is used for the text classification chapter in the NLTK book.',\n",
       " 'Context:\\nIdentification of adverse drug reactions (ADRs) during the post-marketing phase is one of the most important goals of drug safety surveillance. Spontaneous reporting systems (SRS) data, which are the mainstay of traditional drug safety surveillance, are used for hypothesis generation and to validate the newer approaches. The publicly available US Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) data requires substantial curation before they can be used appropriately, and applying different strategies for data cleaning and normalization can have material impact on analysis results.\\nContent:\\nWe provide a curated and standardized version of FAERS removing duplicate case records, applying standardized vocabularies with drug names mapped to RxNorm concepts and outcomes mapped to SNOMED-CT concepts, and pre-computed summary statistics about drug-outcome relationships for general consumption. This publicly available resource, along with the source code, will accelerate drug safety research by reducing the amount of time spent performing data management on the source FAERS reports, improving the quality of the underlying data, and enabling standardized analyses using common vocabularies.\\nAcknowledgements:\\nData available from this source.\\nWhen using this data, please cite the original publication:\\nBanda JM, Evans L, Vanguri RS, Tatonetti NP, Ryan PB, Shah NH (2016) A curated and standardized adverse drug event resource to accelerate drug safety research. Scientific Data 3: 160026. http://dx.doi.org/10.1038/sdata.2016.26\\nAdditionally, please cite the Dryad data package:\\nBanda JM, Evans L, Vanguri RS, Tatonetti NP, Ryan PB, Shah NH (2016) Data from: A curated and standardized adverse drug event resource to accelerate drug safety research. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.8q0s4\\nInspiration:\\nThis is a large-ish dataset (~4.5 gb uncompressed), so try out your batch processing skills in a Kernel\\nWhat groups of drugs are most risky?\\nWhat medical conditions are most at risk to drug-associated risks?',\n",
       " 'Context\\nThis dataset is a snapshot of the OpenPowerlifting database as of February 2018. OpenPowerlifting is an organization which tracks meets and competitor results in the sport of powerlifting, in which competitors complete to lift the most weight for their class in three separate weightlifting categories.\\nContent\\nThis dataset includes two files. meets.csv is a record of all meets (competitions) included in the OpenPowerlifting database. competitors.csv is a record of all competitors who attended those meets, and the stats and lifts that they recorded at them.\\nFor more on how this dataset was collected, see the OpenPowerlifting FAQ.\\nAcknowledgements\\nThis dataset is republished as-is from the OpenPowerlifting source.\\nInspiration\\nHow much influence does overall weight have on lifting capacity?\\nHow big of a difference does gender make? What is demographic of lifters more generally?',\n",
       " 'Context\\nThe IMDB Movies Dataset contains information about 14,762 movies. Information about these movies was downloaded with wget for the purpose of creating a movie recommendation app. The data was preprocessed and cleaned to be ready for machine learning applications.\\nContent\\ntitle,\\nwordsInTitle,\\nurl,\\nimdbRating,\\nratingCount,\\nduration,\\nyear,\\ntype,\\nnrOfWins,\\nnrOfNominations,\\nnrOfPhotos,\\nnrOfNewsArticles,\\nnrOfUserReviews,\\nnrOfGenre,\\nThe rest of the fields are dummy (0/1) variables indicating if the movie has the given genre:\\nAction,\\nAdult,\\nAdventure,\\nAnimation,\\nBiography,\\nComedy,\\nCrime,\\nDocumentary,\\nDrama,\\nFamily,\\nFantasy,\\nFilmNoir,\\nGameShow,\\nHistory,\\nHorror,\\nMusic,\\nMusical,\\nMystery,\\nNews,\\nRealityTV,\\nRomance,\\nSciFi,\\nShort,\\nSport,\\nTalkShow,\\nThriller,\\nWar,\\nWestern\\nPast Research\\nMovie Recommendation App (Learning to Rank)\\nhttps://github.com/orgesleka/filmempfehlung,\\nInspiration:\\nMovie Recommendation App: https://github.com/orgesleka/filmempfehlung\\nLearning to Rank: https://play.google.com/store/apps/details?id=de.leka.orges.filmempfehlung&hl=de',\n",
       " 'Context\\nThe dataset has been downloaded from a BuzzFeed news article that was posted on Jan 15, 2017. The link to the original source can be checked in the Acknowledgements section.\\nThe authors have created a database of more than 1500 people/organization who have a connection with the Trump family, his top advisors or his cabinet picks.\\nThe dataset can help us to capture how policy decisions may be impacted by these varied connections.\\nContent\\nYou have three datasets to play with.\\nPerson_Person.csv: Each row represents a connection between a person and another person (eg. Charles Kushner and Alan Hammer)\\nPerson_Org.csv: Each row represents a connection between a person and an organization (eg. 401 NORTH WABASH VENTURE LLC. and Donald J. Trump)\\nOrg_Org.csv: Each row represents a connection between an organization and another organization (eg. TRUMP COMMERCIAL CHICAGO LLC and 401 NORTH WABASH VENTURE LLC. )\\nAll the three files are in the following format:\\nColumn1: Person or Organization (A)\\nColumn2: Person or Organization (B)\\nColumn3: Connection between (A) and (B)\\nColumn4: Source url from which the connection is derived\\nAcknowledgements\\nSource: https://www.buzzfeed.com/johntemplon/help-us-map-trumpworld\\nInspiration\\nhttps://github.com/BuzzFeedNews/trumpworld\\nThis is an incomplete database, and you are free to add more connections.',\n",
       " 'Context\\nAbstract: Surveys for more than 9,500 households were conducted in the growing seasons 2002/2003 or 2003/2004 in eleven African countries: Burkina Faso, Cameroon, Ghana, Niger and Senegal in western Africa; Egypt in northern Africa; Ethiopia and Kenya in eastern Africa; South Africa, Zambia and Zimbabwe in southern Africa. Households were chosen randomly in districts that are representative for key agro-climatic zones and farming systems. The data set specifies farming systems characteristics that can help inform about the importance of each system for a country’s agricultural production and its ability to cope with short- and long-term climate changes or extreme weather events. Further it informs about the location of smallholders and vulnerable systems and permits benchmarking agricultural systems characteristics.\\nContent\\nThe data file contains survey data collected from different families and has 9597 rows that represent the households and 1753 columns with details about the households. The questionnaire was organized into seven sections and respondents were asked to relate the information provided to the previous 12 months’ farming season. There are too many columns to describe here, however they are described in detail in this paper: https://www.nature.com/articles/sdata201620?WT.ec_id=SDATA-201605\\nQuestionnaire.pdf: This file contains the questionnaire used, a description for each variable name and the question ID.\\nSurveyManual.pdf: This file gives further information on the household questionnaire, the research design and surveying. It was produced for the team leaders and interviewers in the World Bank/GEF project.\\nAdaptationCoding.pdf: This file describes codes for variables ‘ad711’ to ‘ad7625’ from section VII of the questionnaire on adaptation options.\\nThere is also some description in how the data was collected in Survey.pdf.\\nAcknowledgements\\nWaha, Katharina; Zipf, Birgit; Kurukulasuriya, Pradeep; Hassan, Rashid (2016): An agricultural survey for more than 9,500 African households. figshare. https://doi.org/10.6084/m9.figshare.c.1574094\\nhttps://www.nature.com/articles/sdata201620?WT.ec_id=SDATA-201605\\nThe original DTA file was converted to CSV\\nInspiration\\nThis dataset contains a huge amount of information related to farming households in Africa. Data like these are important for studying the impact of global warming on African agriculture and farming families.',\n",
       " \"Context\\nI wanted to find a better way to provide live traffic updates. We dont all have access to the data from traffic monitoring sensors or whatever gets uploaded from people's smart phones to Apple, Google etc plus I question how accurate the traffic congestion is on Google Maps or other apps. So I figured that since buses are also in the same traffic and many buses stream their GPS location and other data live, that would be an ideal source for traffic data. I investigated the data streams available from many bus companies around the world and found MTA in NYC to be very reliable.\\nContent\\nThis dataset is from the NYC MTA buses data stream service. In roughly 10 minute increments the bus location, route, bus stop and more is included in each row. The scheduled arrival time from the bus schedule is also included, to give an indication of where the bus should be (how much behind schedule, or on time, or even ahead of schedule).\\nData for the entire month of June 2017 is included.\\nDue to space limitations on Kaggle for datasets, only selected bus routes have been included.\\nAcknowledgements\\nData is recorded from the MTA SIRI Real Time data feed and the MTA GTFS Schedule data.\\nInspiration\\nI want to see what exploratory & discovery people come up with from this data. Feel free to download this dataset for your own use however I would appreciate as many Kernals included on Kaggle as we can get.\\nBased on the interest this generates I plan to collect more data for subsequent months down the track.\",\n",
       " \"Reddit is a social network which divide topics into so called 'subreddits'.\\nIn subreddit 'worldnews', news of the whole world are published. The dataset contains following columns: time_created - a Unix timestamp of the submission creation date date_created - creation time in %Y-%m-%d up_votes - how often the submission was upvoted down_votes - how often the submission was downvoted title - the title of the submission over_18 - if the submission is for mature persons author - the reddit username of the author subreddit - this is always 'worldnews'\\nWith the dataset, you can estimate several things in contrast to world politics and special events.\",\n",
       " \"About the Missing Migrants Data\\nThis data is sourced from the International Organization for Migration. The data is part of a specific project called the Missing Migrants Project which tracks deaths of migrants, including refugees , who have gone missing along mixed migration routes worldwide. The research behind this project began with the October 2013 tragedies, when at least 368 individuals died in two shipwrecks near the Italian island of Lampedusa. Since then, Missing Migrants Project has developed into an important hub and advocacy source of information that media, researchers, and the general public access for the latest information.\\nWhere is the data from?\\nMissing Migrants Project data are compiled from a variety of sources. Sources vary depending on the region and broadly include data from national authorities, such as Coast Guards and Medical Examiners; media reports; NGOs; and interviews with survivors of shipwrecks. In the Mediterranean region, data are relayed from relevant national authorities to IOM field missions, who then share it with the Missing Migrants Project team. Data are also obtained by IOM and other organizations that receive survivors at landing points in Italy and Greece. In other cases, media reports are used. IOM and UNHCR also regularly coordinate on such data to ensure consistency. Data on the U.S./Mexico border are compiled based on data from U.S. county medical examiners and sheriff’s offices, as well as media reports for deaths occurring on the Mexico side of the border. Estimates within Mexico and Central America are based primarily on media and year-end government reports. Data on the Bay of Bengal are drawn from reports by UNHCR and NGOs. In the Horn of Africa, data are obtained from media and NGOs. Data for other regions is drawn from a combination of sources, including media and grassroots organizations. In all regions, Missing Migrants Projectdata represents minimum estimates and are potentially lower than in actuality.\\nUpdated data and visuals can be found here: https://missingmigrants.iom.int/\\nWho is included in Missing Migrants Project data?\\nIOM defines a migrant as any person who is moving or has moved across an international border or within a State away from his/her habitual place of residence, regardless of\\n    (1) the person’s legal status; \\n    (2) whether the movement is voluntary or involuntary; \\n    (3) what the causes for the movement are; or \\n    (4) what the length of the stay is.[1]\\nMissing Migrants Project counts migrants who have died or gone missing at the external borders of states, or in the process of migration towards an international destination. The count excludes deaths that occur in immigration detention facilities, during deportation, or after forced return to a migrant’s homeland, as well as deaths more loosely connected with migrants’ irregular status, such as those resulting from labour exploitation. Migrants who die or go missing after they are established in a new home are also not included in the data, so deaths in refugee camps or housing are excluded. This approach is chosen because deaths that occur at physical borders and while en route represent a more clearly definable category, and inform what migration routes are most dangerous. Data and knowledge of the risks and vulnerabilities faced by migrants in destination countries, including death, should not be neglected, rather tracked as a distinct category.\\nHow complete is the data on dead and missing migrants?\\nData on fatalities during the migration process are challenging to collect for a number of reasons, most stemming from the irregular nature of migratory journeys on which deaths tend to occur. For one, deaths often occur in remote areas on routes chosen with the explicit aim of evading detection. Countless bodies are never found, and rarely do these deaths come to the attention of authorities or the media. Furthermore, when deaths occur at sea, frequently not all bodies are recovered - sometimes with hundreds missing from one shipwreck - and the precise number of missing is often unknown. In 2015, over 50 per cent of deaths recorded by the Missing Migrants Project refer to migrants who are presumed dead and whose bodies have not been found, mainly at sea.\\nData are also challenging to collect as reporting on deaths is poor, and the data that does exist are highly scattered. Few official sources are collecting data systematically. Many counts of death rely on media as a source. Coverage can be spotty and incomplete. In addition, the involvement of criminal actors in incidents means there may be fear among survivors to report deaths and some deaths may be actively covered-up. The irregular immigration status of many migrants, and at times their families as well, also impedes reporting of missing persons or deaths.\\nThe varying quality and comprehensiveness of data by region in attempting to estimate deaths globally may exaggerate the share of deaths that occur in some regions, while under-representing the share occurring in others.\\nWhat can be understood through this data?\\nThe available data can give an indication of changing conditions and trends related to migration routes and the people travelling on them, which can be relevant for policy making and protection plans. Data can be useful to determine the relative risks of irregular migration routes. For example, Missing Migrants Project data show that despite the increase in migrant flows through the eastern Mediterranean in 2015, the central Mediterranean remained the more deadly route. In 2015, nearly two people died out of every 100 travellers (1.85%) crossing the Central route, as opposed to one out of every 1,000 that crossed from Turkey to Greece (0.095%). From the data, we can also get a sense of whether groups like women and children face additional vulnerabilities on migration routes.\\nHowever, it is important to note that because of the challenges in data collection for the missing and dead, basic demographic information on the deceased is rarely known. Often migrants in mixed migration flows do not carry appropriate identification. When bodies are found it may not be possible to identify them or to determine basic demographic information. In the data compiled by Missing Migrants Project, sex of the deceased is unknown in over 80% of cases. Region of origin has been determined for the majority of the deceased. Even this information is at times extrapolated based on available information – for instance if all survivors of a shipwreck are of one origin it was assumed those missing also came from the same region.\\nThe Missing Migrants Project dataset includes coordinates for where incidents of death took place, which indicates where the risks to migrants may be highest. However, it should be noted that all coordinates are estimates.\\nWhy collect data on missing and dead migrants?\\nBy counting lives lost during migration, even if the result is only an informed estimate, we at least acknowledge the fact of these deaths. What before was vague and ill-defined is now a quantified tragedy that must be addressed. Politically, the availability of official data is important. The lack of political commitment at national and international levels to record and account for migrant deaths reflects and contributes to a lack of concern more broadly for the safety and well-being of migrants, including asylum-seekers. Further, it drives public apathy, ignorance, and the dehumanization of these groups.\\nData are crucial to better understand the profiles of those who are most at risk and to tailor policies to better assist migrants and prevent loss of life. Ultimately, improved data should contribute to efforts to better understand the causes, both direct and indirect, of fatalities and their potential links to broader migration control policies and practices.\\nCounting and recording the dead can also be an initial step to encourage improved systems of identification of those who die. Identifying the dead is a moral imperative that respects and acknowledges those who have died. This process can also provide a some sense of closure for families who may otherwise be left without ever knowing the fate of missing loved ones.\\nIdentification and tracing of the dead and missing\\nAs mentioned above, the challenge remains to count the numbers of dead and also identify those counted. Globally, the majority of those who die during migration remain unidentified. Even in cases in which a body is found identification rates are low. Families may search for years or a lifetime to find conclusive news of their loved one. In the meantime, they may face psychological, practical, financial, and legal problems.\\nUltimately Missing Migrants Project would like to see that every unidentified body, for which it is possible to recover, is adequately “managed”, analysed and tracked to ensure proper documentation, traceability and dignity. Common forensic protocols and standards should be agreed upon, and used within and between States. Furthermore, data relating to the dead and missing should be held in searchable and open databases at local, national and international levels to facilitate identification.\\nFor more in-depth analysis and discussion of the numbers of missing and dead migrants around the world, and the challenges involved in identification and tracing, read our two reports on the issue, Fatal Journeys: Tracking Lives Lost during Migration (2014) and Fatal Journeys Volume 2, Identification and Tracing of Dead and Missing Migrants\\nContent\\nThe data set records incidents of missing persons and deaths of migrants\\ncolumns in the data:\\nID - unique key documenting incident\\nCause of Death - reason for death\\nRegion of Origin\\nNationality\\nMissing Persons - counts\\nDead - counts of deaths\\nIncident Region - region where incident was recorded\\nDate - the date when the incident was recorded. Note the data set includes records from 2014 to June 2017\\nLatitude - spatial coordinates\\nLongitude - spatial coordinates\\nAcknowledgements\\nThis data set was created by the International Organization for Migration.\\nhttps://www.iom.int/about-iom\\nEstablished in 1951, IOM is the leading inter-governmental organization in the field of migration and works closely with governmental, intergovernmental and non-governmental partners.\\nWith 166 member states, a further 8 states holding observer status and offices in over 100 countries, IOM is dedicated to promoting humane and orderly migration for the benefit of all. It does so by providing services and advice to governments and migrants.\\nIOM works to help ensure the orderly and humane management of migration, to promote international cooperation on migration issues, to assist in the search for practical solutions to migration problems and to provide humanitarian assistance to migrants in need, including refugees and internally displaced people.\\nThe IOM Constitution recognizes the link between migration and economic, social and cultural development, as well as to the right of freedom of movement.\\nIOM works in the four broad areas of migration management:\\nMigration and development\\nFacilitating migration\\nRegulating migration\\nForced migration.\\nIOM activities that cut across these areas include the promotion of international migration law, policy debate and guidance, protection of migrants' rights, migration health and the gender dimension of migration.\\nStart a new kernel\",\n",
       " 'Context\\nThis datasheet is an extension of the job of \"Murder Accountability Project\". In this datasheet is included a vectorial file of states to make easier the labour of geographical plotting.\\nContent\\nThe Murder Accountability Project is the most complete database of homicides in the United States currently available. This dataset includes murders from the FBI\\'s Supplementary Homicide Report from 1976 to the present and Freedom of Information Act data on more than 22,000 homicides that were not reported to the Justice Department. This dataset includes the age, race, sex, ethnicity of victims and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used.\\nAcknowledgements\\nThe data was compiled and made available by the Murder Accountability Project, founded by Thomas Hargrove.\\nInspiration\\nCan you develop an algorithm to detect serial killer activity?',\n",
       " 'Content\\nAge-adjusted Death Rates for Selected Major Causes of Death: United States, 1900-2013\\nAge adjusting rates\\nis a way to make fairer comparisons between groups with different age distributions. For example, a county having a higher percentage of elderly people may have a higher rate of death or hospitalization than a county with a younger population, merely because the elderly are more likely to die or be hospitalized. (The same distortion can happen when comparing races, genders, or time periods.) Age adjustment can make the different groups more comparable. A \"standard\" population distribution is used to adjust death and hospitalization rates. The age-adjusted rates are rates that would have existed if the population under study had the same age distribution as the \"standard\" population. Therefore, they are summary measures adjusted for differences in age distributions.\\nAcknowledgements\\nScrap data from data.gov',\n",
       " \"Context\\nThis dataset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227,428 check-ins in New York city and 573,703 check-ins in Tokyo. Each check-in is associated with its time stamp, its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories). This dataset is originally used for studying the spatial-temporal regularity of user activity in LBSNs.\\nContent\\nThis dataset includes long-term (about 10 months) check-in data in New York city and Tokyo collected from Foursquare from 12 April 2012 to 16 February 2013. It contains two files in tsv format. Each file contains 8 columns, which are:\\nUser ID (anonymized)\\nVenue ID (Foursquare)\\nVenue category ID (Foursquare)\\nVenue category name (Fousquare)\\nLatitude\\nLongitude\\nTimezone offset in minutes (The offset in minutes between when this check-in occurred and the same time in UTC)\\nUTC time\\nThe file dataset_TSMC2014_NYC.txt contains 227428 check-ins in New York city. The file dataset_TSMC2014_TKY.txt contains 537703 check-ins in Tokyo.\\nAcknowledgements\\nThis dataset is acquired from here\\nFollowing is the citation of the dataset author's paper:\\nDingqi Yang, Daqing Zhang, Vincent W. Zheng, Zhiyong Yu. Modeling User Activity Preference by Leveraging User Spatial Temporal Characteristics in LBSNs. IEEE Trans. on Systems, Man, and Cybernetics: Systems, (TSMC), 45(1), 129-142, 2015. PDF\\nInspiration\\nOne of the questions that I am trying to answer is if there is a pattern in users' checkin behaviour. For example, if it's a Friday evening, what all places they might be interested to visit.\",\n",
       " 'nan',\n",
       " 'Context\\nIf you use this data, please be sure to give credit to Witten, et. al., since it is their data set.\\nCervical cancer tumor vs matched control data. Data set is gene expression profiling data from tumor and matched normal samples (29 each). The data are the raw read counts (not normalized) from sequencing of microRNA. This is not my data, but was published by:\\nWitten, D., et al. (2010) Ultra-high throughput sequencing-based small RNA discovery and discrete statistical biomarker analysis in a collection of cervical tumours and matched controls. BMC Biology, 8:58\\nContent\\nThe rows are each micro RNA name and the columns are the sample names (N=normal, T=tumor). The values are raw read counts.\\nAcknowledgements\\nWitten, D., et al. (2010) Ultra-high throughput sequencing-based small RNA discovery and discrete statistical biomarker analysis in a collection of cervical tumours and matched controls. BMC Biology, 8:58\\nInspiration\\nUse this data to practice making predictive models from machine learning/deep learning algorithms on gene expression profiling data.',\n",
       " 'Context\\nPredict whether or not a horse can survive based upon past medical conditions.\\nNoted by the \"outcome\" variable in the data.\\nContent\\nAll of the binary representation have been converted into the words they actually represent. However, a fuller description is provided by the data dictionary (datadict.txt).\\nThere are a lot of NA\\'s in the data. This is the real struggle here. Try to find a way around it through imputation or other means.\\nAcknowledgements\\nThis dataset was originally published by the UCI Machine Learning Database: http://archive.ics.uci.edu/ml/datasets/Horse+Colic',\n",
       " 'Context\\nThis dataset contains information on pesticide residues in food. The U.S. Department of Agriculture (USDA) Agricultural Marketing Service (AMS) conducts the Pesticide Data Program (PDP) every year to help assure consumers that the food they feed themselves and their families is safe. Ultimately, if EPA determines a pesticide is not safe for human consumption, it is removed from the market.\\nThe PDP tests a wide variety of domestic and imported foods, with a strong focus on foods that are consumed by infants and children. EPA relies on PDP data to conduct dietary risk assessments and to ensure that any pesticide residues in foods remain at safe levels. USDA uses the data to better understand the relationship of pesticide residues to agricultural practices and to enhance USDA’s Integrated Pest Management objectives. USDA also works with U.S. growers to improve agricultural practices.\\nContent\\nWhile the original 2015 MS Access database can be found [here (https://www.ams.usda.gov/datasets/pdp/pdpdata), the data has been transferred to a SQLite database for easier, more open use. The database contains two tables, Sample Data and Results Data. Each sampling includes attributes such as extraction method, the laboratory responsible for the test, and EPA tolerances among others. These attributes are labeled with codes, which can be referenced in PDF format here, or integrated into the database using the included csv files.\\nInspiration\\nWhat are the most common types of pesticides tested in this study?\\nDo certain states tend to use one particular pesticide type over another?\\nDoes pesticide type correspond more with crop type or location (state)?\\nAre any produce types found to have higher pesticide levels than assumed safe by EPA standards?\\nBy combining databases from several years of PDP tests, can you see any trends in pesticide use?\\nAcknowledgement\\nThis dataset is part of the USDA PDP yearly database, and the original source can be found here.',\n",
       " \"About This Data\\nThis is a list of over 18,000 restaurants in the US that serve vegetarian or vegan food provided by Datafiniti's Business Database. The dataset includes address, city, state, business name, business categories, menu data, phone numbers, and more.\\nWhat You Can Do With This Data\\nYou can use this data to determine the most vegetarian and vegan-friendly cities in the US. E.g.:\\nHow many restaurants in each metro area offers vegetarian options?\\nWhich metros among the 25 most popular metro areas have the most and least vegetarian restaurants per 100,000 residents?\\nWhich metros with at least 10 vegetarian restaurants have the most vegetarian restaurants per 100,000 residents?\\nHow many restaurants in each metro area offers vegan options?\\nWhich metros among the 25 most popular metro areas have the most and least vegan restaurants per 100,000 residents?\\nWhich metros with at least 10 vegan restaurants have the most vegan restaurants per 100,000 residents?\\nWhich cuisines are served the most at vegetarian restaurants?\\nData Schema\\nA full schema for the data is available in our support documentation.\\nAbout Datafiniti\\nDatafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.\\nWant More?\\nYou can get more data like this by joining Datafiniti or requesting a demo.\",\n",
       " 'Context\\nThis dataset contains the prevalence and trends of health care access/coverage for 1995-2010. Percentages are weighted to population characteristics. Data are not available if it did not meet Behavioral Risk Factor Surveillance System (BRFSS) stability requirements. For more information on these requirements, as well as risk factors and calculated variables, see the Technical Documents and Survey Data for a specific year - http://www.cdc.gov/brfss/annual_data/annual_data.htm.\\nContent\\nThis dataset has 7 variables:\\nYear\\nState\\nYes\\nNo\\nCategory\\nCondition\\nLocation 1\\nAcknowledgements\\nThe original dataset can be found here.\\nRecommended citation: Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System. Atlanta, Georgia: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, [appropriate year].\\nInspiration\\nHow does health care coverage change over time?\\nDoes health care access differ by state?',\n",
       " \"The Gender Statistics database is a comprehensive source for the latest sex-disaggregated data and gender statistics covering demography, education, health, access to economic opportunities, public life and decision-making, and agency.\\nThe Data\\nThe data is split into several files, with the main one being Data.csv. The Data.csv contains all the variables of interest in this dataset, while the others are lists of references and general nation-by-nation information.\\nData.csv contains the following fields:\\nData.csv\\nCountry.Name: the name of the country\\nCountry.Code: the country's code\\nIndicator.Name: the name of the variable that this row represents\\nIndicator.Code: a unique id for the variable\\n1960 - 2016: one column EACH for the value of the variable in each year it was available\\nThe other files\\nI couldn't find any metadata for these, and I'm not qualified to guess at what each of the variables mean. I'll list the variables for each file, and if anyone has any suggestions (or, even better, actual knowledge/citations) as to what they mean, please leave a note in the comments and I'll add your info to the data description.\\nCountry-Series.csv\\nCountryCode\\nSeriesCode\\nDESCRIPTION\\nCountry.csv\\nCountry.Code\\nShort.Name\\nTable.Name\\nLong.Name\\n2-alpha.code\\nCurrency.Unit\\nSpecial.Notes\\nRegion\\nIncome.Group\\nWB-2.code\\nNational.accounts.base.year\\nNational.accounts.reference.year\\nSNA.price.valuation\\nLending.category\\nOther.groups\\nSystem.of.National.Accounts\\nAlternative.conversion.factor\\nPPP.survey.year\\nBalance.of.Payments.Manual.in.use\\nExternal.debt.Reporting.status\\nSystem.of.trade\\nGovernment.Accounting.concept\\nIMF.data.dissemination.standard\\nLatest.population.census\\nLatest.household.survey\\nSource.of.most.recent.Income.and.expenditure.data\\nVital.registration.complete\\nLatest.agricultural.census\\nLatest.industrial.data\\nLatest.trade.data\\nLatest.water.withdrawal.data\\nFootNote.csv\\nCountryCode\\nSeriesCode\\nYear\\nDESCRIPTION\\nSeries-Time.csv\\nSeriesCode\\nYear\\nDESCRIPTION\\nSeries.csv\\nSeries.Code\\nTopic\\nIndicator.Name\\nShort.definition\\nLong.definition\\nUnit.of.measure\\nPeriodicity\\nBase.Period\\nOther.notes\\nAggregation.method\\nLimitations.and.exceptions\\nNotes.from.original.source\\nGeneral.comments\\nSource\\nStatistical.concept.and.methodology\\nDevelopment.relevance\\nRelated.source.links\\nOther.web.links\\nRelated.indicators\\nLicense.Type\\nAcknowledgements\\nThis dataset was downloaded from The World Bank's Open Data project. The summary of the Terms of Use of this data is as follows:\\nYou are free to copy, distribute, adapt, display or include the data in other products for commercial and noncommercial purposes at no cost subject to certain limitations summarized below.\\nYou must include attribution for the data you use in the manner indicated in the metadata included with the data.\\nYou must not claim or imply that The World Bank endorses your use of the data by or use The World Bank’s logo(s) or trademark(s) in conjunction with such use.\\nOther parties may have ownership interests in some of the materials contained on The World Bank Web site. For example, we maintain a list of some specific data within the Datasets that you may not redistribute or reuse without first contacting the original content provider, as well as information regarding how to contact the original content provider. Before incorporating any data in other products, please check the list: Terms of use: Restricted Data.\\n-- [ed. note: this last is not applicable to the Gender Statistics database]\\nThe World Bank makes no warranties with respect to the data and you agree The World Bank shall not be liable to you in connection with your use of the data.\\nThis is only a summary of the Terms of Use for Datasets Listed in The World Bank Data Catalogue. Please read the actual agreement that controls your use of the Datasets, which is available here: Terms of use for datasets. Also see World Bank Terms and Conditions.\",\n",
       " 'Context\\nThe emergence in the United States of large-scale “megaregions” centered on major metropolitan areas is a phenomenon often taken for granted in both scholarly studies and popular accounts of contemporary economic geography.\\nThis dataset comes from a paper (Nelson & Rae, 2016. An Economic Geography of the United States: From Commutes to Megaregions) that uses a data set of more than 4,000,000 commuter flows as the basis for an empirical approach to the identification of such megaregions.\\nContent\\nThis dataset consists of two files: one contains the commuting data, and one is a gazetteer describing the population and locations of the census tracts referred to by the commuting data. The fields Ofips and Dfips (FIPS codes for the originating and destination census tracts, respectively) in commute_data.csv refer to the GEOID field in census_tracts_2010.csv.\\ncommute_data\\nThis file contains information on over 4 million commute flows. It has the following fields:\\nOfips: the full FIPS code for the origin census tract of an individual flow line\\n*Dfips *: the full FIPS code for the destination census tract of an individual flow line\\nOstfips: the FIPS code for the origin state of an individual flow line\\nOctfips: the FIPS code for the origin county of an individual flow line\\nOtrfips: the FIPS code for the destination census tract of an individual flow line\\nDstfips: the FIPS code for the destination state of an individual flow line\\nDctfips: the FIPS code for the destination county of an individual flow line\\nDtrfips: the FIPS code for the destination census tract of an individual flow line\\nFlow: the total number of commuters associated with this individual point to point flow line (i.e. the total number of journeys to work)\\nMoe: margin of error of the Flow value above\\nLenKM: length of each flow line, in Kilometers\\nESTDIVMOE: the Flow value divided by the Margin of Error of the estimate\\ncensus_tracts_2010\\nThis file contains the following fields, which represent information about different U.S. Census Tracts:\\nUSPS: United States Postal Service State Abbreviation\\nGEOID: Geographic Identifier - fully concatenated geographic code (State FIPS and County FIPS)\\nANSICODE: American National Standards Institute code\\nNAME: Name\\nPOP10: 2010 Census population count.\\nHU10: 2010 Census housing unit count.\\nALAND: Land Area (square meters) - Created for statistical purposes only.\\nAWATER: Water Area (square meters) - Created for statistical purposes only.\\nALAND_SQMI: Land Area (square miles) - Created for statistical purposes only.\\nAWATER_SQMI: Water Area (square miles) - Created for statistical purposes only.\\nINTPTLAT: Latitude (decimal degrees) First character is blank or \"-\" denoting North or South latitude respectively.\\nINTPTLONG: Longitude (decimal degrees) First character is blank or \"-\" denoting East or West longitude respectively.\\nAcknowledgements\\nThis dataset comes from the following article:\\nNelson & Rae, 2016. An Economic Geography of the United States: From Commutes to Megaregions\\nThe full dataset (in GIS shapefile format) can be found on figshare here',\n",
       " \"Content\\nThe Smithsonian Institution's Global Volcanism Program (GVP) documents Earth's volcanoes and their eruptive history over the past 10,000 years. The GVP reports on current eruptions from around the world and maintains a database repository on active volcanoes and their eruptions. The GVP is housed in the Department of Mineral Sciences, part of the National Museum of Natural History, on the National Mall in Washington, D.C.\\nThe GVP database includes the names, locations, types, and features of more than 1,500 volcanoes with eruptions during the Holocene period (approximately the last 10,000 years) or exhibiting current unrest.\",\n",
       " 'Content\\nThis dataset consists of digitized paper mission reports from WWII. Each record includes the date, conflict, geographic location, and other data elements to form a live-action sequence of air warfare from 1939 to 1945. The records include U.S. and Royal Air Force data, in addition to some Australian, New Zealand and South African air force missions.\\nAcknowledgements\\nLt Col Jenns Robertson of the US Air Force developed the Theater History of Operations Reports (THOR) and posted them online after receiving Department of Defense approval.',\n",
       " 'Context\\nTIME\\'s Person of the Year hasn\\'t always secured his or her place in the history books, but many honorees remain unforgettable: Gandhi, Khomeini, Kennedy, Elizabeth II, the Apollo 8 astronauts Anders, Borman and Lovell. Each has left an indelible mark on the world.\\nTIME\\'s choices for Person of the Year are often controversial. Editors are asked to choose the person or thing that had the greatest impact on the news, for good or ill — guidelines that leave them no choice but to select a newsworthy, not necessarily praiseworthy, cover subject. Controversial choices have included Adolf Hitler (1938), Joseph Stalin (1939, 1942), and Ayatullah Khomeini (1979).\\nTIME\\'s choices for Person of the Year are often politicians and statesmen. Eleven American presidents, from FDR to George W. Bush, have graced the Person of the Year cover, many of them more than once. As commander in chief of one of the world\\'s greatest nations, it\\'s hard not to be a newsmaker.\\nContent\\nThis dataset includes a record for every Time Magazine cover which has honored an individual or group as \"Men of the Year\", \"Women of the Year\", or (as of 1999) \"Person of the Year\".\\nAcknowledgements\\nThe data was scraped from Time Magazine\\'s website.\\nInspiration\\nWho has been featured on the magazine cover the most times? Did any American presidents not receive the honor for their election victory? How has the selection of Person of the Year changed over time? Have the magazine\\'s choices become more or less controversial?',\n",
       " \"Abstract\\nThis dataset provides a collection of vital signals and reference blood pressure values acquired from 26 subjects that can be used for the purpose of non-invasive cuff-less blood pressure estimation.\\nSource\\nCreators: Amirhossein Esmaili, Mohammad Kachuee, Mahdi Shabany Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran Date: October 2017\\nRelevant Information\\nThis dataset is to be used for research methods trying to estimate blood pressure in a cuff-less non-invasive manner. For each subject in this dataset, phonocardiogram (PCG), electrocardiogram (ECG), and photoplethysmogram (PPG) signals are acquired. Alongside the acquisition of the signals per subject, a number of reference BPs are measured. Here, a signal from a force-sensing resistor (FSR), placed under the cuff of the BP reference device, is used to distinguish exact moments of reference BP measurements, which are corresponding to the inflation and deflation of the cuff. The signal from FSR is also included in our dataset. For each subject, age, weight, and height are also recorded.\\nAttribute Information\\nIn the dataset, corresponding to each subject there is a “.json” file. In each file, we have the following attributes:\\n“UID”: Subject number\\n“age”: Age of the subject\\n“weight”: Weight of the subject (Kg)\\n“height”: Height of the subject (cm)\\n“data_PCG”: Acquired PCG signal from chest (Fs = 1 KHz)\\n“data_ECG”: Acquired ECG signal (Fs = 1 KHz)\\n“data_PPG”: Acquired PPG signal from fingertip (Fs = 1 KHz)\\n“data_FSR”: Acquired FSR signal (Fs = 1 KHz)\\n“data_BP”: reference systolic blood pressure (SBP) and diastolic blood pressure (DBP) values acquired from the subjects. To distinguish the time instances in the signals in which SBP and DBP values are measured, FSR signal can be used. For more details, please refer to our paper.\\nPlease note that the FSR signal should be inverted prior to any analysis to reflect the instantaneous cuff pressure. Also, the moment of each reference device measurement is approximately the time at which the cuff pressure drops with a considerable negative slope.\\nRelevant Papers\\nA. Esmaili, M. Kachuee, M. Shabany, Nonlinear Cuffless Blood Pressure Estimation of Healthy Subjects Using Pulse Transit Time and Arrival Time, IEEE Transactions on Instrumentation and Measurement, 2017.\\nA. Esmaili, M. Kachuee, M. Shabany, Non-invasive Blood Pressure Estimation Using Phonocardiogram, IEEE International Symposium on Circuits and Systems (ISCAS'17), 2017.\\nCitation Request\\nA. Esmaili, M. Kachuee, M. Shabany, Nonlinear Cuffless Blood Pressure Estimation of Healthy Subjects Using Pulse Transit Time and Arrival Time, IEEE Transactions on Instrumentation and Measurement, 2017.\",\n",
       " \"Can you beat the market?\\nHorse racing has always intrigued me - not so much from the point of view as a sport, but more from the view of it as a money market. Inspired by the pioneers of computerised horse betting, I'm sharing this dataset in the hope of finding more data scientists willing to take up the challenge and find new ways of exploiting it!\\nAs always, the goal for most of us is to find information in the data that can be used to generate profit, usually by finding information that has not already been considered by the other players in the game. But I'm always interested in finding new uses for the data, whatever they may be.\\nHorse racing is a huge business in Hong Kong, which has two race tracks in a city that is only 1,104 square km. The betting pools are bigger than all US racetracks combined, which means that the opportunity is unlimited for those who are successful.\\nSo are you up for it?\\nContent\\nThe data was obtained from various free sources and is presented in CSV format. Personally-identifiable information, such as horse and jockey names, has not been included. However these should have no relevance to the purpose of this dataset, which is purely for experimental use.\\nThere are two files:\\nraces.csv\\nEach line describes the condition of an individual race.\\nrace_id - unique identifier for the race\\ndate - date of the race, in YYYY-MM-DD format (note that the dates given have been obscured and are not the real ones, although the durations between each race should be correct)\\nvenue - a 2-character string, representing which of the 2 race courses this race took place at: ST = Shatin, HV = Happy Valley\\nrace_no - race number of the race in the day's meeting\\nconfig - race track configuration, mostly related to the position of the inside rail. For more details, see the HKJC website.\\nsurface - a number representing the type of race track surface: 1 = dirt, 0 = turf\\ndistance - distance of the race, in metres\\ngoing - track condition. For more details, see the HKJC website.\\nhorse_ratings - the range of horse ratings that may participate in this race\\nprize - the winning prize, in HK Dollars\\nrace_class - a number representing the class of the race\\nsec_time1 - time taken by the leader of the race to reach the end of the end of the 1st sectional point (sec)\\nsec_time2 - time taken by the leader of the race to reach the end of the 2nd sectional point (sec)\\nsec_time3 - time taken by the leader of the race to reach the end of the 3rd sectional point (sec)\\nsec_time4 - time taken by the leader of the race to reach the end of the 4th sectional point, if any (sec)\\nsec_time5 - time taken by the leader of the race to reach the end of the 5th sectional point, if any (sec)\\nsec_time6 - time taken by the leader of the race to reach the end of the fourth sectional point, if any (sec)\\nsec_time7 - time taken by the leader of the race to reach the end of the fourth sectional point, if any (sec)\\ntime1 - time taken by the leader of the race in the 1st section only (sec)\\ntime2 - time taken by the leader of the race in the 2nd section only (sec)\\ntime3 - time taken by the leader of the race in the 3rd section only (sec)\\ntime4 - time taken by the leader of the race in the 4th section only, if any (sec)\\ntime5 - time taken by the leader of the race in the 5th section only, if any (sec)\\ntime6 - time taken by the leader of the race in the 6th section only, if any (sec)\\ntime7 - time taken by the leader of the race in the 7th section only, if any (sec)\\nplace_combination1 - placing horse no (1st)\\nplace_combination2 - placing horse no (2nd)\\nplace_combination3 - placing horse no (3rd)\\nplace_combination4 - placing horse no (4th)\\nplace_dividend1 - placing dividend paid (for place_combination1)\\nplace_dividend2 - placing dividend paid (for place_combination2)\\nplace_dividend3 - placing dividend paid (for place_combination2)\\nplace_dividend4 - placing dividend paid (for place_combination2)\\nwin_combination1 - winning horse no\\nwin_dividend1 - winning dividend paid (for win_combination1)\\nwin_combination2 - joint winning horse no, if any\\nwin_dividend2 - winning dividend paid (for win_combination2, if any)\\nruns.csv\\nEach line describes the characteristics of one horse run, in one of the races given in races.csv.\\nrace_id - unique identifier for the race\\nhorse_no - the number assigned to this horse, in the race\\nhorse_id - unique identifier for this horse\\nresult - finishing position of this horse in the race\\nwon - whether horse won (1) or otherwise (0)\\nlengths_behind - finishing position, as the number of horse lengths behind the winner\\nhorse_age - current age of this horse at the time of the race\\nhorse_country - country of origin of this horse\\nhorse_type - sex of the horse, e.g. 'Gelding', 'Mare', 'Horse', 'Rig', 'Colt', 'Filly'\\nhorse_rating - rating number assigned by HKJC to this horse at the time of the race\\nhorse_gear - string representing the gear carried by the horse in the race. An explanation of the codes used may be found on the HKJC website.\\ndeclared_weight - declared weight of the horse and jockey, in lbs\\nactual_weight - actual weight carried by the horse, in lbs\\ndraw - post position number of the horse in this race\\nposition_sec1 - position of this horse (ranking) in section 1 of the race\\nposition_sec2 - position of this horse (ranking) in section 2 of the race\\nposition_sec3 - position of this horse (ranking) in section 3 of the race\\nposition_sec4 - position of this horse (ranking) in section 4 of the race, if any\\nposition_sec5 - position of this horse (ranking) in section 5 of the race, if any\\nposition_sec6 - position of this horse (ranking) in section 6 of the race, if any\\nbehind_sec1 - position of this horse (lengths behind leader) in section 1 of the race\\nbehind_sec2 - position of this horse (lengths behind leader) in section 2 of the race\\nbehind_sec3 - position of this horse (lengths behind leader) in section 3 of the race\\nbehind_sec4 - position of this horse (lengths behind leader) in section 4 of the race, if any\\nbehind_sec5 - position of this horse (lengths behind leader) in section 5 of the race, if any\\nbehind_sec6 - position of this horse (lengths behind leader) in section 6 of the race, if any\\ntime1 - time taken by the horse to pass through the 1st section of the race (sec)\\ntime2 - time taken by the horse to pass through the 2nd section of the race (sec)\\ntime3 - time taken by the horse to pass through the 3rd section of the race (sec)\\ntime4 - time taken by the horse to pass through the 4th section of the race, if any (sec)\\ntime5 - time taken by the horse to pass through the 5th section of the race, if any (sec)\\ntime6 - time taken by the horse to pass through the 6th section of the race, if any (sec)\\nfinish_time - finishing time of the horse in this race (sec)\\nwin_odds - win odds for this horse at start of race\\nplace_odds - place (finishing in 1st, 2nd or 3rd position) odds for this horse at start of race\\ntrainer_id - unique identifier of the horse's trainer at the time of the race\\njockey_id - unique identifier of the jockey riding the horse in this race\\nAcknowledgements\\nNone of this research would have even started without me standing on the shoulders of giants such as William Benter, Ruth Bolton and Randall Chapman and many others who have published the results of their research.\\nInspiration\\nIt is probably not going to be enough to just take this dataset and feed it into Google Cloud Machine Learning, Azure MI, etc... but let me know if you find otherwise!\\nQuestions that need to be answered include:\\nFeature engineering - what features are needed and how best to estimate them from the data given?\\nModelling - what kind of model works best? Maybe more than one model?\\nOther data - is there any other data needed, apart from that given in this data set?\",\n",
       " \"As a cytogeneticist studying some features of the chromosomes (their telomeres, structural anomalies ...) you need to take pictures from chromosomal preparations fixed on glass slides . Unfortunately, like the sticks in the mikado game, sometime a chromosome or more can fall on an other one yielding overlapping chromosomes in the image, the plague of cytogeneticists. Before computers and images processing with photography, chromosomes were cut from a paper picture and then classified (at least two paper pictures were required), with computers and quantitative analysis, automatic methods were developped to overcome this problem (Lunsteen & Piper, Minaee et al.)\\nThis dataset modelizes overlapping chromosomes. Couples of non overlapping chromosomes were combined by relative translations and rotations to generate two kind of images:\\na grey scaled image of the two overlapping chromosomes combining a DAPI stained chromosome and the labelling of the telomeres (showing the chromosomes extremities)\\na label image (the ground truth) were the value equal to 3 labels the pixels (displayed in red) belonging to the overlapping domain.\\nThe images were saved in a numpy array as an hdf5 file. The following minimalist python 2 code can load the data (assuming that the unzipped data are in the same folder than the code) :\\nimport h5py\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\n\\n#h5f = h5py.File('overlapping_chromosomes_examples.h5','r')\\nh5f = h5py.File('LowRes_13434_overlapping_pairs.h5','r')\\npairs = h5f['dataset_1'][:]\\nh5f.close()\\n\\ngrey = pairs[220,:,:,0]\\nmask = pairs[220,:,:,1]\\n#%matplotlib inline\\nplt.subplot(121)\\nplt.imshow(grey)\\nplt.title('max='+str(grey.max()))\\nplt.subplot(122)\\nplt.imshow(mask)\\nI hope this dataset to be suitable to apply supervised learning methods, possibly similar to segnet or its implementation with keras.\",\n",
       " 'Thinking of making a move to the lovely Twin Cities? First check out this dataset (curtesy of Open Data Minneapolis) before you pack your bags for the \"Little Apple.\" The datasets included contain information about 311 calls and crimes committed between 2010 to 2016 which will help you convince your friends, family, and loved ones that Minneapolis is the place to be (or not). Snow plow noise complaints be darned!',\n",
       " 'Patterns in the Brazilian congress voting behavior\\nThe Brazilian Government House of Representatives maintains a public database, that contains legislative information since 1970. One type of information that is available are the records of bills. For each bill, the database gives a list of votes choices, state and party of each deputy, and a list of details about the bill itself like type, year, text of proposal, benches orientations and situation (a bill can be voted more than one time, in this work we will treat each votation as a single one). We retrieved more than 100000 bills (propList), where less than 1% was voted (propVotList) until November 2016.\\nOur objective is detect regularity patterns of legislative behavior, institutional arrangements, and legislative outcome.\\nRaw data from: http://www2.camara.leg.br/transparencia/dados-abertos/dados-abertos-legislativo/webservices/proposicoes-1/proposicoes',\n",
       " 'Context\\nThe Iowa Department of Commerce requires that every store that sells alcohol in bottled form for off-the-premises consumption must hold a class \"E\" liquor license (an arrangement typical of most of the state alcohol regulatory bodies). All alcoholic sales made by stores registered thusly with the Iowa Department of Commerce are logged in the Commerce department system, which is in turn published as open data by the State of Iowa.\\nContent\\nThis dataset contains information on the name, kind, price, quantity, and location of sale of sales of individual containers or packages of containers of alcoholic beverages.\\nThis dataset is relatively straightforward, but one source of further information on the contents of the data is this Gist.\\nAcknowledgements\\nThis data was originally published by the State of Iowa here and has been republished as-is on Kaggle.\\nInspiration\\nThis data is probably a representative sample of sale activity for alcohol in the United States, and can be used to answer many questions thereof, like: how much alcohol is sold and consumed in the United States? What kind? What are the most popular brands and labels? What are the most popular mixers? What is the distribution of prices paid in-store? Etcetera.',\n",
       " 'Context\\nThis dataset contains 2.6 million words from Urban Dictionary, including their definitions and votes in CSV format.\\nSource: https://www.urbandictionary.com\\nTo lookup a word in the dataset via urban dictionary api: http://api.urbandictionary.com/v0/define?defid={word_id}\\nWarning that this dataset contains a lot of profanity and racial slurs.\\nContent\\nRows: 2,606,522\\nColumn 1: word_id - for usage in urban dictionary api\\nColumn 2: word - the text being defined\\nColumn 3: up_votes - thumbs up count as of may 2016\\nColumn 4: down_votes - thumbs down count as of may 2016\\nColumn 5: author - hash of username of submitter\\nColumn 6: definition - text with possible utf8 chars, double semi-colon denotes a newline\\nAcknowledgements\\nRemixed from data posted anonymously on reddit.\\nhttps://archive.org/details/UrbanDictionary1999-May2016DefinitionsCorpus\\nInspiration\\nModified and cleaned the original source which is in a very un-friendly format.',\n",
       " 'Grant Red Book (GRB) Full Text\\nContext\\nEvery Tuesday, the US Patent and Trademark Office (USPTO) issues approximately 6,000 patent grants (patents) and posts the full text of the patents online. These patent grant documents contain much of the supporting details for a given patent. From this data, we can track trends in innovation across industries.\\nFrequency: Weekly (Tuesdays)\\nPeriod: 10/11/2016\\nContent\\nThe fields include patent number, series code and application number, type of patent, filing date, title, issue date, applicant information, inventor information, assignee(s) at time of issue, foreign priority information, related US patent documents, classification information (IPCR, CPC, US), US and foreign references, attorney, agent or firm/legal representative, examiner, citations, Patent Cooperation Treaty (PCT) information, abstract, specification, and claims.\\nInspiration\\nHow many times will you find “some assembly required”? What inventions are at the cutting edge of machine learning? To answer these questions and any others you may have about this catalog of knowledge, fork the kernel Library of Inventions and Innovations which demonstrates how to work with XML files in Python.\\nAcknowledgements\\nThe USPTO owns the dataset. These files are a subset and concatenation of the Patent Grant Data/XML Version 4.5 ICE (Grant Red Book). Because of the concatenation of the individual XML documents, these files will not parse successfully or open/display by default in Internet Explorer. They also will not import into MS Excel. Each XML document within the file should have one start tag and one end tag. Concatenation creates a file that contains 6,000 plus start/end tag combinations. If you take one document out of the Patent Grant Full Text file and place it in a directory with the correct DTD and then double click that individual document, Internet Explorer will parse/open the document successfully. NOTE: You may receive a warning about Active X controls. NOTE: All Patent Grant Full Text files will open successfully in MS Word; NotePad; WordPad; and TextPad.\\nLicense\\nCreative Commons - Public Domain Mark 1.0',\n",
       " 'What is the SherLock dataset?\\nA long-term smartphone sensor dataset with a high temporal resolution. The dataset also offers explicit labels capturing the to activity of malwares running on the devices. The dataset currently contains 10 billion data records from 30 users collected over a period of 2 years and an additional 20 users for 10 months (totaling 50 active users currently participating in the experiment).\\nThe primary purpose of the dataset is to help security professionals and academic researchers in developing innovative methods of implicitly detecting malicious behavior in smartphones. Specifically, from data obtainable without superuser (root) privileges. However, this dataset can be used for research in domains that are not strictly security related. For example, context aware recommender systems, event prediction, user personalization and awareness, location prediction, and more. The dataset also offers opportunities that aren\\'t available in other datasets. For example, the dataset contains the SSID and signal strength of the connected WiFi access point (AP) which is sampled once every second, over the course of many months.\\nTo gain full free access to the SherLock Dataset, follow these two steps:\\n1) Read, complete and sign the license agreement. The general restrictions are:\\n-The license lasts for 3 years, afterwhich the data must be deleted.\\n-Do not share the data with those who are not bound by the license agreement.\\n-Do not attempt to de-anonymize the individuals (volunteers) who have contributed the data.\\n-Any of your publication that benefit from the SherLock project must cite the following article: Mirsky, Yisroel, et al. \"SherLock vs Moriarty: A Smartphone Dataset for Cybersecurity Research.\" Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security. ACM, 2016.\\n2)Send the scanned document as a PDF to bgu.sherlock@gmail.com and provide a gmail account to share a google drive folder with.\\nMore information can be found here, or in this publication (download link).\\nA 2 week data sample from a single user is provided on this Kaggle page. To access the full dataset for free, please visit our site. Note: The format of the sample dataset may differ from the full dataset.',\n",
       " \"This is the Google Search interest data that powers the Visualisation Searching For Health. Google Trends data allows us to see what people are searching for at a very local level. This visualization tracks the top searches for common health issues in the United States, from Cancer to Diabetes, and compares them with the actual location of occurrences for those same health conditions to understand how search data reflects life for millions of Americans.\\nHow does search interest for top health issues change over time? From 2004–2017, the data shows that search interest gradually increased over the past few years. Certain regions show a more significant increase in search interest than others. The increase in search activity is greatest in the Midwest and Northeast, while the changes are noticeably less dramatic in California, Texas, and Idaho. Are people generally becoming more aware of health conditions and health risks?\\nThe search interest data was collected using the Google Trends API. The visualisation also brings in incidences of each condition so they can be compared. The health conditions were hand-selected from the Community Health Status Indicators (CHSI) which provides key indicators for local communities in the United States. The CHSI dataset includes more than 200 measures for each of the 3,141 United States counties. More information about the CHSI can be found on healthdata.gov.\\nMany striking similarities exist between searches and actual conditions—but the relationship between the Obesity and Diabetes maps stands out the most. “There are many risk factors for type 2 diabetes such as age, race, pregnancy, stress, certain medications, genetics or family history, high cholesterol and obesity. However, the single best predictor of type 2 diabetes is overweight or obesity. Almost 90% of people living with type 2 diabetes are overweight or have obesity. People who are overweight or have obesity have added pressure on their body's ability to use insulin to properly control blood sugar levels, and are therefore more likely to develop diabetes.” —Obesity Society via obesity.org\",\n",
       " 'This dataset contains 16,000 images of four shapes; square, star, circle, and triangle. Each image is 200x200 pixels.\\nThe data was collected using a Garmin Virb 1080p action camera. The shapes were cut from poster board, and then painted green. I held each shape in view of the camera for two minutes. While the camera was recording the shape, I moved the shape around and rotated it.\\nThe four videos were then processed using OpenCV in Python. Using colorspaces, the green shape is cropped out of the image and resized to 200x200 pixels. The data is arranged into four folders; square, circle, triangle, and star. The images are labeled 0.png, 1.png, etc...\\nA fifth video was taken with all of the shapes in the frame. This fifth video is for testing purposes. The goal is to classify the shapes in the test video using a model created with the training data. These classifications were made using a model made in Keras.\\nHow is this different than the MINST handwritten digits dataset? There are 10 classes in the MINST dataset and 4 in this shapes dataset. The images in this data set are rotated, and the digits in the MINST data set are not.',\n",
       " 'The datasets provides data of annual nominal catches of more than 200 species of fish and shellfish in the Northeast Atlantic region, which are officially submitted by 20 International Council for the Exploration of the Sea (ICES) member countries between 2006 and 2014.',\n",
       " \"Context\\nThis dataset contains data about the earthquakes that hit the center of Italy between August and November 2016. For some simple visualizations of this dataset you can checkout this post.\\nContent\\nThe dataset contains events from 2016-08-24 to 2016-11-30. It's a single .csv file with the following header:\\nTime,Latitude,Longitude,Depth/Km,Magnitude\\nThe dataset contains 8087 rows (8086 of data + 1 of header)\\nAcknowledgements\\nThe dataset was collected from this real-time updated list from the Italian Earthquakes National Center.\\nInspiration\\nI hope that someone in the kaggle community will find this dataset interesting to analyze and/or visualize.\",\n",
       " 'Context\\nThe data set includes information about different leagues in different sports (Basketball and Soccer) all around the world, as well as some basic facts about each country, regarding the home advantage phenomenon in sports.\\nContent\\nThe data is comprised of 3 data sets:\\nThe home and away performance of 8365 soccer teams from a few dozens of countries during the years 2010-2016.\\nThe home and away performance of 1216 NBA teams during the years 1968-2010\\nGeneral facts about 88 countries including soccer data such as their FIFA rank, the average attendance of soccer matches and the Home Advatnage Factor of the leagure\\nAcknowledgement\\nThe soccer data was scraped from here: http://footballdatabase.com/competitions-index\\nThe NBA data was scraped from NBA.com.\\nThe world facts were copied from Wikipedia.',\n",
       " \"Context\\nThere are many kinds of birds: pigeons, ducks, ostriches, penguins... Some are good at flying, others can't fly but run fast. Some swim under water, others wading in shallow pool.\\nAccording to their living environments and living habits, birds are classified into different ecological groups. There are 8 ecological groups of birds:\\nSwimming Birds\\nWading Birds\\nTerrestrial Birds\\nRaptors\\nScansorial Birds\\nSinging Birds\\nCursorial Birds (not included in dataset)\\nMarine Birds (not included in dataset)\\nFirst 6 groups are main and are covered by this dataset.\\nApparently, birds belong to different ecological groups have different appearances: flying birds have strong wings and wading birds have long legs. Their living habits are somewhat reflected in their bones' shapes. As data scientists we may think of examining the underlying relationship between sizes of bones and ecological groups , and recognising birds' ecological groups by their bones' shapes.\\nContent\\nThere are 420 birds contained in this dataset. Each bird is represented by 10 measurements (features):\\nLength and Diameter of Humerus\\nLength and Diameter of Ulna\\nLength and Diameter of Femur\\nLength and Diameter of Tibiotarsus\\nLength and Diameter of Tarsometatarsus\\nAll measurements are continuous float numbers (mm) with missing values represented by empty strings. The skeletons of this dataset are collections of Natural History Museum of Los Angeles County. They belong to 21 orders, 153 genera, 245 species.\\nEach bird has a label for its ecological group:\\nSW: Swimming Birds\\nW: Wading Birds\\nT: Terrestrial Birds\\nR: Raptors\\nP: Scansorial Birds\\nSO: Singing Birds\\nAcknowledgements\\nThis dataset is provided by Dr. D. Liu of Beijing Museum of Natural History.\\nInspiration\\nThis dataset is a 420x10 size continuous values unbalanced multi-class dataset. What can be done include:\\nData Visualisation\\nStatical Analysis\\nSupervised Classification\\nUnsupervised Clustering\\nLicense\\nPlease do not publish or cite this dataset in research papers or other public publications.\",\n",
       " 'From Wikipedia:\\n\"The 500 Greatest Albums of All Time\" is a 2003 special issue of American magazine Rolling Stone, and a related book published in 2005. The lists presented were compiled based on votes from selected rock musicians, critics, and industry figures, and predominantly feature American and British music from the 1960s and the 1970s.\\nIn 2012, Rolling Stone published a revised edition of the list drawing on the original and a later survey of albums in the 2000s. It was made available in \"bookazine\" format on newsstands in the US from April 27 to July 25. The new list contained 38 albums not present in the previous one, 16 of them released after 2003.\\nI took the albums from MusicBrainz but the genres weren\\'t listed. I wrote a Python script to get the genres and subgenres of each album from the Discogs API.\\nThe data collected are:\\nPosition on the list\\nYear of release\\nAlbum name\\nArtist name\\nGenre name\\nSubgenre name\\nSome of the genres/subgenres may not be entirely correct - Discogs seems to not consider some of the smaller genres. Let me know if there are any glaring issues and I\\'ll try to fix them.',\n",
       " \"Context\\nThe WTA (Women's Tennis Association) is the principal organizing body of women's professional tennis, it governs its own tour worldwide. On its website, it provides a lot of data about the players as individuals as well the tour matches with results and the current rank during it.\\nLuckily for us, Jeff Sackmann scraped the website and collected everything from there and put in a nice way into easily consumable datasets.\\nOn Jeff's GitHub account you can find a lot more data about tennis!\\nContent\\nThe dataset present here is directly downloaded from the source, no alteration on the data was made, the files were only placed in subdirectories so one can easily locate them.\\nIt covers statistics of players registered on the WTA, the matches that happened on each tour by year, with results, as well some qualifying matches for the tours.\\nAs a reminder, you may not find all data of the matches prior to 2006, so be warned when working with those sets.\\nAcknowledgements\\nThanks to Jeff Sackmann for maintaining such collection and making it public!\\nAlso, a thank you for WTA for collecting those stats and making them accessible to anyone on their site.\\nInspiration\\nHere are some things to start:\\nWhich player did the most rapidly climb the ranks through the years?\\nDoes the rank correlates with the money earn by the player?\\nWhat can we find about the age?\\nThere is some deterministic factor to own the match?\",\n",
       " \"Context\\nThis dataset contains results from the 2015 and 2013 elections in Israel. Results are given by voting booths (of comparable sizes of 0-800) and not by settlements (which are very varied - think Tel Aviv compared to a small kibbutz).\\nContent\\nThe first seven columns are information about each settlement and voting booth, and from the eighth to the end is the number of votes each party has received in each booth.\\nAcknowledgements\\nThis data is freely available at http://votes20.gov.il/ and http://www.votes-19.gov.il/nationalresults, I just translated the column headers into English. Settlement names are translated according to this key from the Central Bureau of Statistics, which uses the same settlement_code as the election results.\\nInspiration\\nPersonally, I've viewed this dataset in order to map out the relationships between different parties (i.e which are 'closer', which are more 'central'). This question is significant in Israel, where the composition of the parliament is determined almost directly by the popular vote (e.g a party with 25% of the total proper votes will recieve 25% of seats in parliament), but the government is formed by a coalition of parties (so the head of the largest party in parliament will not necessarily be the Prime Minister).\",\n",
       " 'https://www.youtube.com/watch?v=A8syQeFtBKc\\nContext\\nThe Stanford Mass Shootings in America (MSA) is a dataset released under Creative Commons Attribution 4.0 international license by the Stanford Geospatial Center. While not an exhaustive collection of mass shootings, it is a high-quality dataset ranging from 1966 to 2016 with well-defined methodology, definitions and source URLs for user validation.\\nThis dataset can be used to validate other datasets, such as us-mass-shootings-last-50-years, which contains more recent data, or conduct other analysis, as more information is provided.\\nContent\\nThis dataset contains data by the MSA project both from it\\'s website and from it\\'s Github account. The difference between the two sources is only on the data format (i.e. .csv versus .geojson for the data, or .csv versus .pdf for the dictionary).\\nmass_shooting_events_stanford_msa_release_06142016\\nContains a nonexaustive list of US Mass Shootings from 1966 to 2016 in both .csv and .geojson formats.\\ndictionary_stanford_msa_release_06142016\\nContains the data dictionary in .csv and .pdf formats. Note the .pdf format provides an easier way to visualize sub-fields.\\nNote the data was reproduced here without any modifications other than file renaming for clarity, the content is the same as in the source.\\nThe following sections are reproduced from the dataset creators website. For more details, please see the source.\\nProject background\\nThe Stanford Mass Shootings of America (MSA) data project began in 2012, in reaction to the mass shooting in Sandy Hook, CT. In our initial attempts to map this phenomena it was determined that no comprehensive collection of these incidents existed online. The Stanford Geospatial Center set out to create, as best we could, a single point repository for as many mass shooting events as could be collected via online media. The result was the Stanford MSA.\\nWhat the Stanford MSA is\\nThe Stanford MSA is a data aggregation effort. It is a curated set of spatial and temporal data about mass shootings in America, taken from online media sources. It is an attempt to facilitate research on gun violence in the US by making raw data more accessible.\\nWhat the Stanford MSA is not\\nThe Stanford MSA is not a comprehensive, longitudinal research project. The data collected in the MSA are not investigated past the assessment for inclusion in the database. The MSA is not an attempt to answer specific questions about gun violence or gun laws.\\nThe Stanford Geospatial Center does not provide analysis or commentary on the contents of this database or any derivatives produced with it.\\nData collection methodology\\nThe information collected for the Stanford MSA is limited to online resources. An initial intensive investigation was completed looking back over existing online reports to fill in the historic record going back to 1966. Contemporary records come in as new events occur and are cross referenced against a number of online reporting sources. In general a minimum of three corroborating sources are required to add the full record into the MSA (as many as 6 or 7 sources may have been consulted in many cases). All sources for each event are listed in the database.\\nDue to the time involved in vetting the details of any new incident, there is often a 2 to 4 week lag between a mass shooting event and its inclusion in the public release database.\\nIt is important to note the records in the Stanford MSA span a time from well before the advent of online media reporting, through its infancy, to the modern era of web based news and information resources. Researchers using this database need to be aware of the reporting bias these changes in technology present. A spike in incidents for recent years is likely due to increased online reporting and not necessarily indicative of the rate of mass shootings alone. Researchers should look at this database as a curated collection of quality checked data regarding mass shootings, and not an exhaustive research data set itself. Independent verification and analysis will be required to use this data in examining trends in mass shootings over time.\\nDefinition of Mass Shooting\\nThe definition of mass shooting used for the Stanford database is 3 or more shooting victims (not necessarily fatalities), not including the shooter. The shooting must not be identifiably gang, drug, or organized crime related.\\nAcknowledgements\\nThe Stanford Mass Shootings in America (MSA) is a dataset released under Creative Commons Attribution 4.0 international license by the Stanford Geospatial Center.\\nHow to cite the MSA\\nThe Stanford MSA is released under a Creative Commons Attribution 4.0 international license. Please cite the MSA as “Stanford Mass Shootings in America, courtesy of the Stanford Geospatial Center and Stanford Libraries”.\\nInspiration\\nThere is already a great number of interesting datasets in Kaggle surrounding the subject of Mass Shootings, however, little has been done leveraging information from multiple sources. Can you see a story among them? Can we learn anything, for example, comparing the different sources by city or state?\\nFrom a bigger picture\\nLeading Causes of Death in US: https://www.kaggle.com/cdc/mortality\\nGun Violence Database: https://www.kaggle.com/gunviolencearchive/gun-violence-database\\nGun Deaths in US: https://www.kaggle.com/hakabuk/gun-deaths-in-the-us\\nHomicide Reports: https://www.kaggle.com/murderaccountability/homicide-reports\\nGlobal Terrorism Database: https://www.kaggle.com/START-UMD/gtd\\nCrime Rates in America: https://www.kaggle.com/marshallproject/crime-rates\\n\"Prevention?\\nFirearms Provisions in US States: https://www.kaggle.com/jboysen/state-firearms\\nTrial and Terror: https://www.kaggle.com/jboysen/trial-and-terror\\nConnecticut Inmates Waiting trial: https://www.kaggle.com/Connecticut-open-data/connecticut-inmates-awaiting-trial\\nAre there warning signs?\\nMental Health in Tech Survey: https://www.kaggle.com/osmi/mental-health-in-tech-2016/data (Not directly related but can be used to make a parenthesis about mental health being an issue in our surroundings).',\n",
       " 'Context\\nThis dataset includes the median list price divided by the square footage of a 1-bedroom home for a select number of neighborhoods around the United States.\\nContent\\nWhen available, data includes median price per square foot on a monthly basis between January 2010 and September 2016.\\nSelected neighborhoods include:\\nUpper East Side, New York, NY\\nSpring Valley, Las Vegas, NV\\nHollywood, Los Angeles, CA\\nWilliamsburg, New York, NY\\nHarlem, New York, NY\\nEnterprise, Las Vegas,NV\\nDowntown, San Jose, CA\\nSheepshead Bay, New York, NY\\nForest Hills, New York, NY\\nJackson Heights, New York, NY\\nGramercy, New York, NY\\nFlagami, Miami, FL\\nDowntown, Memphis, TN\\nChelsea, New York, NY\\nOak Lawn, Dallas, TX\\nGreater Uptown, Houston, TX\\nSouth Loop, Chicago, IL\\nMakiki-Lower Punchbowl-Tantalus, Honolulu, HI\\nDowntown, Los Angeles, CA\\nCapitol Hill, Seattle, WA\\nClinton, New York, NY\\nAlexandria West, Alexandria, VA\\nFinancial District, New York, NY\\nFlatiron District, New York, NY\\nLandmark-Van Dom, Alexandria, VA\\nFlamingo Lummus, Miami Beach, FL\\nWinchester, Las Vegas, NV\\nBrickell, Miami, FL\\nWaikiki, Honolulu, HI\\nBack Bay, Boston, MA\\nSutton Place, New York, NY\\nand several others\\nInspiration\\nWhat neighborhoods have the most expensive real estate per square foot? Least expensive?\\nWhich neighborhoods and/or cities have the fastest growth rates in price?\\nAre there any neighborhoods that remain relatively steady in price?\\nGiven that this metric is listing price per square foot, is there a similar dataset that could help you compare median square footage in a 1-bedroom home across neighborhoods?\\nAcknowledgement\\nThis dataset is part of Zillow Data, and the original source can be found here, under the Neighborhoods link.',\n",
       " 'Context\\nAs part of the House Intelligence Committee investigation into how Russia may have influenced the 2016 US Election, Twitter released the screen names of almost 3000 Twitter accounts believed to be connected to Russia’s Internet Research Agency, a company known for operating social media troll accounts. Twitter immediately suspended these accounts, deleting their data from Twitter.com and the Twitter API. A team at NBC News including Ben Popken and EJ Fox was able to reconstruct a dataset consisting of a subset of the deleted data for their investigation and were able to show how these troll accounts went on attack during key election moments. This dataset is the body of this open-sourced reconstruction.\\nFor more background, read the NBC news article publicizing the release: \"Twitter deleted 200,000 Russian troll tweets. Read them here.\"\\nContent\\nThis dataset contains two CSV files. tweets.csv includes details on individual tweets, while users.csv includes details on individual accounts.\\nTo recreate a link to an individual tweet found in the dataset, replace user_key in https://twitter.com/user_key/status/tweet_id with the screen-name from the user_key field and tweet_id with the number in the tweet_id field.\\nFollowing the links will lead to a suspended page on Twitter. But some copies of the tweets as they originally appeared, including images, can be found by entering the links on web caches like archive.org and archive.is.\\nAcknowledgements\\nIf you publish using the data, please credit NBC News and include a link to this page. Send questions to ben.popken@nbcuni.com.\\nInspiration\\nWhat are the characteristics of the fake tweets? Are they distinguishable from real ones?',\n",
       " 'Context\\nThis is Kaggle\\'s Facial Keypoint Detection dataset that is uploaded in order to allow kernels to work on it, as was also requested by a fellow kaggler in this discussion thread.\\nContent\\nThe dataset contains 7049 facial images and up to 15 keypoints marked on them.\\nThe keypoints are in the facial_keypoints.csv file.\\nThe image are in the face_images.npz file.\\nLook at the exploration script for code that reads and presents the dataset.\\nAcknowledgements\\nThis is a kaggle dataset, so all acknowledgements are to kaggle.\\nFrom the original dataset acknowledgements : \"The data set for this competition was graciously provided by Dr. Yoshua Bengio of the University of Montreal\"\\nInspiration\\nI hope the dataset can serve as a decent deep learning repository of code.',\n",
       " 'Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nThis dataset contains the complete detail about the Prison and various characteristics of inmates. This will help to understand better about prison system in India.\\nContent\\nDetails of Jail wise population of prison inmates\\nDetails about the list of jails in India at the end of year 2015.\\nJail category wise population of inmates.\\nCapacity of jails by inmate population.\\nAge group, nationality and gender wise population of inmates.\\nReligion and gender wise population of inmates.\\nCaste and gender wise population of inmates.\\nEducation standards of inmates.\\nDomicile of inmates.\\nIncidence of recidivism.\\nRehabilitation of prisoners.\\nDistribution of sentence periods of convicts in various jails by sex and age-groups.\\nDetails of under trial prisoners by the type of IPC (Indian Penal Code) offences.\\nDetails of convicts by the type of IPC (Indian Penal Code) offences.\\nDetails of SLL (special & local law) Crime headwise distribution of inmates who convicted\\nDetails of SLL (special & local law) Crime head wise distribution of inmates under trial\\nDetails of educational facilities provided to prisoners.\\nDetails of Jail breaks, group clashes and firing in jail (Tranquility).\\nDetails of wages per day to convicts.\\nDetails of Prison inmates trained under different vocational training.\\nDetails of capital punishment (death sentence) and life imprisonment.\\nDetails of prison inmates escaped.\\nDetails of prison inmates released.\\nDetails of Strength of officials\\nDetails of Total Budget and Actual Expenditure during the year 2015-16.\\nDetails of Budget\\nDetails of Expenditure\\nDetails of Expenditure on inmates\\nDetails of Inmates suffering from mental ilness\\nDetails of Period of detention of undertrials\\nDetails of Number of women prisoners with children\\nDetails of Details of inmates parole during the year\\nDetails of Value of goods produced by inmates\\nDetails of Number of vehicles available\\nDetails of Training of Jail Officers\\nDetails of Movements outside jail premises\\nDetails of Details of electronic equipment used in prison\\nInspiration\\nThere are many questions about Indian prison with this dataset. Some of the interesting questions are\\nPercentage of jails over crowded. Is there any change in percentage over time?\\nHow many percentage of inmates re-arrested?\\nWhich state/u.t pay more wages to the inmates?\\nWhich state/u.t has more capital punishment/life imprisonment inmates?\\nInmates gender ratio per state\\nAcknowledgements\\nNational Crime Records Bureau (NCRB), Govt of India has shared this dataset under Govt. Open Data License - India. NCRB has also shared prison data on their website.',\n",
       " \"Context\\nThe data is of National Stock Exchange of India. The data is compiled to felicitate Machine Learning, without bothering much about Stock APIs.\\nContent\\nThe data is of National Stock Exchange of India's stock listings for each trading day of 2016 and 2017. A brief description of columns. SYMBOL: Symbol of the listed company. SERIES: Series of the equity. Values are [EQ, BE, BL, BT, GC and IL] OPEN: The opening market price of the equity symbol on the date. HIGH: The highest market price of the equity symbol on the date. LOW: The lowest recorded market price of the equity symbol on the date. CLOSE: The closing recorded price of the equity symbol on the date. LAST: The last traded price of the equity symbol on the date. PREVCLOSE: The previous day closing price of the equity symbol on the date. TOTTRDQTY: Total traded quantity of the equity symbol on the date. TOTTRDVAL: Total traded volume of the equity symbol on the date. TIMESTAMP: Date of record. TOTALTRADES: Total trades executed on the day. ISIN: International Securities Identification Number.\\nAcknowledgements\\nAll data is fetched from NSE official site. https://www.nseindia.com/\\nInspiration\\nThis dataset is compiled to felicitate Machine learning on Stocks.\",\n",
       " 'About the Data\\nThe data used in this project is real and is based on the collection of over 20 years. The total number of record in this dataset is roughly around 120 million rows and the size of the data is approximately 12GB. The data consists of flight arrival and departure details for all commercial flights within the USA, from October 1987 to April 2008. This is a large dataset. There are around 29 attributes.\\nHow to get the data? The data originally comes from http://stat-computing.org/dataexpo/2009/the-data.html\\nYou can download the data for each year by clicking the appropriate link in the above website (Remember the size is going to be more than 12GB).\\n(i) Problem Statement (a) Check the skewness of Distance travelled by airlines. (b) Calculate the mean, median and quantiles of the distance travelled by US Airlines (US). (c) Check the standard deviation of distance travelled by American Airlines (AA). (d) Draw a boxplot of UniqueCarrier with Distance. (e) Draw the direction of relationship between ArrDelay and DepDelay by drawing a scatterplot.\\n(ii) Problem Statement\\n(a) What is the probability that a flight which is landing/taking off is “WN” Airlines (marginal probability) (b) What is the probability that a flight which is landing/taking off is either “WN” or “AA” Airlines (disjoint events) (c) What is the joint probability that a flight is both “WN” and travels less than 600 miles (joint probability) (d) What is the conditional probability that the flight travels less than 2500 miles given that the flight is “AA” Airlines (conditional probability) (e) What is the joint probability of a flight getting cancelled and is supposed to travel less than 2500 miles given that the flight is “AA” Airlines (joint + conditional probability)\\n(iii) Problem Statement\\n(a) Suppose arrival delays of flights belonging to “AA” are normally distributed with mean 15 minutes and standard deviation 3 minutes. If the “AA” plans to announce a scheme where it will give 50% cash back if their flights are delayed by 20 minutes, how much percentage of the trips “AA” is supposed to loose this money. (Hint: pnorm) (b) Assume that 65% of flights are diverted due to bad weather through the Weather System. What is the probability that in a random sample of 10 flights, 6 are diverted through the Weather System. (Hint: dbinorm) (c) Do linear regression between the Arrival Delay and Departure Delay of the flights. (d) Find out the confidence interval of the fitted linear regression line. (e) Perform a multiple linear regression between the Arrival Delay along with the Departure Delay and Distance travelled by flights.',\n",
       " \"Context\\nGitHub is the leader in hosting open source projects. For those who are not familiar with open source projects, a group of developers share and contribute to common code to develop software. Example open source projects include, Chromium (which makes Google Chrome), WordPress, and Hadoop. Open source projects are said to have disrupted the software industry (2008 Kansas Keynote).\\nContent\\nFor this study, I crawled the leader in hosting open source projects, GitHub.com and extracted a list of the top starred open source projects. On GitHub, a user may choose the star a repository representing that they “like” the project. For each project, I gathered the repository username or Organization the project resided in, the repository name, a description, the last updated date, the language of the project, the number of stars, any tags, and finally the url of the project.\\nAcknowledgements\\nThis data wouldn't be available if it weren't for GitHub. An example micro-study can be found at The Concept Center\",\n",
       " 'Context\\nTweets containing Hurricane Harvey from the morning of 8/25/2017. I hope to keep this updated if computer problems do not persist.\\n*8/30 Update This update includes the most recent tweets tagged \"Tropical Storm Harvey\", which spans from 8/20 to 8/30 as well as the properly merged version of dataset including Tweets from when Harvey before it was downgraded back to a tropical storm.\\nInspiration\\nWhat are the popular tweets?\\nCan we find popular news stories from this?\\nCan we identify people likely staying or leaving, and is there a difference in sentiment between the two groups?\\nIs it possible to predict popularity with respect to retweets, likes, and shares?',\n",
       " \"Context & Content\\nThis dataset features the salaries of 874 nhl players for the 2016/2017 season. I have randomly split the players into a training (612 players) and test (262 players) populations. There are 151 predictor columns (described in column legend section, if you're not familiar with hockey the meaning of some of these may be a bit cryptic!) as well as a leading column with the players 2016/2017 annual salary. For the test population the actual salaries have been broken off into a separate .csv file.\\nAcknowledgements\\nRaw excel sheet was acquired http://www.hockeyabstract.com/\\nInspiration\\nCan you build a model to predict NHL player's salaries? What are the best predictors of how much a player will make?\\nColumn Legend\\nAcronym - Meaning\\n%FOT - Percentage of all on-ice faceoffs taken by this player.\\n+/- - Plus/minus\\n1G - First goals of a game\\nA/60 - Events Against per 60 minutes, defaults to Corsi, but can be set to another stat\\nA1 - First assists, primary assists\\nA2 - Second assists, secondary assists\\nBLK% - Percentage of all opposing shot attempts blocked by this player\\nBorn - Birth date\\nC.Close - A player shot attempt (Corsi) differential when the game was close\\nC.Down - A player shot attempt (Corsi) differential when the team was trailing\\nC.Tied - A player shot attempt (Corsi) differential when the team was tied\\nC.Up - A player shot attempt (Corsi) differential when the team was in the lead\\nCA - Shot attempts allowed (Corsi, SAT) while this player was on the ice\\nCap Hit - The player's cap hit\\nCBar - Crossbars hit\\nCF - The team's shot attempts (Corsi, SAT) while this player was on the ice\\nCF.QoC - A weighted average of the Corsi percentage of a player's opponents\\nCF.QoT - A weighted average of the Corsi percentage of a player's linemates\\nCHIP - Cap Hit of Injured Player is games lost to injury multiplied by cap hit per game\\nCity - City of birth\\nCntry - Country of birth\\nDAP - Disciplined aggression proxy, which is hits and takeaways divided by minor penalties\\nDFA - Dangerous Fenwick against, which is on-ice unblocked shot attempts weighted by shot quality\\nDFF - Dangerous Fenwick for, which is on-ice unblocked shot attempts weighted by shot quality\\nDFF.QoC - Quality of Competition metric based on Dangerous Fenwick, which is unblocked shot attempts weighted for shot quality\\nDftRd - Round in which the player was drafted\\nDftYr - Year drafted\\nDiff - Events for minus event against, defaults to Corsi, but can be set to another stat\\nDiff/60 - Events for minus event against, per 60 minutes, defaults to Corsi, but can be set to another stat\\nDPS - Defensive point shares, a catch-all stats that measures a player's defensive contributions in points in the standings\\nDSA - Dangerous shots allowed while this player was on the ice, which is rebounds plus rush shots\\nDSF - The team's dangerous shots while this player was on the ice, which is rebounds plus rush shots\\nDZF - Shifts this player has ended with an defensive zone faceoff\\ndzFOL - Faceoffs lost in the defensive zone\\ndzFOW - Faceoffs win in the defensive zone\\ndzGAPF - Team goals allowed after faceoffs taken in the defensive zone\\ndzGFPF - Team goals scored after faceoffs taken in the defensive zone\\nDZS - Shifts this player has started with an defensive zone faceoff\\ndzSAPF - Team shot attempts allowed after faceoffs taken in the defensive zone\\ndzSFPF - Team shot attempts taken after faceoffs taken in the defensive zone\\nE+/- - A player's expected +/-, based on his team and minutes played\\nENG - Empty-net goals\\nExp dzNGPF - Expected goal differential after faceoffs taken in the defensive zone, based on the number of them\\nExp dzNSPF - Expected shot differential after faceoffs taken in the defensive zone, based on the number of them\\nExp ozNGPF - Expected goal differential after faceoffs taken in the offensive zone, based on the number of them\\nExp ozNSPF - Expected shot differential after faceoffs taken in the offensive zone, based on the number of them\\nF.Close - A player unblocked shot attempt (Fenwick) differential when the game was close\\nF.Down - A player unblocked shot attempt (Fenwick) differential when the team was trailing\\nF.Tied - A player unblocked shot attempt (Fenwick) differential when the team was tied\\nF.Up - A player unblocked shot attempt (Fenwick) differential when the team was in the lead. Not the best acronym.\\nF/60 - Events For per 60 minutes, defaults to Corsi, but can be set to another stat\\nFA - Unblocked shot attempts allowed (Fenwick, USAT) while this player was on the ice\\nFF - The team's unblocked shot attempts (Fenwick, USAT) while this player was on the ice\\nFirst Name -\\nFO% - Faceoff winning percentage\\nFO%vsL - Faceoff winning percentage against lefthanded opponents\\nFO%vsR - Faceoff winning percentage against righthanded opponents\\nFOL - The team's faceoff losses while this player was on the ice\\nFOL.Close - Faceoffs lost when the score was close\\nFOL.Down - Faceoffs lost when the team was trailing\\nFOL.Up - Faceoffs lost when the team was in the lead\\nFovsL - Faceoffs taken against lefthanded opponents\\nFovsR - Faceoffs taken against righthanded opponents\\nFOW - The team's faceoff wins while this player was on the ice\\nFOW.Close - Faceoffs won when the score was close\\nFOW.Down - Faceoffs won when the team was trailing\\nFOW.Up - Faceoffs won when the team was in the lead\\nG - Goals\\nG.Bkhd - Goals scored on the backhand\\nG.Dflct - Goals scored with deflections\\nG.Slap - Goals scored with slap shots\\nG.Snap - Goals scored with snap shots\\nG.Tip - Goals scored with tip shots\\nG.Wrap - Goals scored with a wraparound\\nG.Wrst - Goals scored with a wrist shot\\nGA - Goals allowed while this player was on the ice\\nGame - Game Misconduct penalties\\nGF - The team's goals while this player was on the ice\\nGP - Games Played\\nGrit - Defined as hits, blocked shots, penalty minutes, and majors\\nGS - The player's combined game score\\nGS/G - The player's average game score\\nGVA - The team's giveaways while this player was on the ice\\nGWG - Game-winning goals\\nGWG - Game-winning goals\\nHA - The team's hits taken while this player was on the ice\\nHand - Handedness\\nHF - The team's hits thrown while this player was on the ice\\nHopFO - Opening faceoffs taken at home\\nHopFOW - Opening faceoffs won at home\\nHt - Height\\niBLK - Shots blocked by this individual\\niCF - Shot attempts (Corsi, SAT) taken by this individual\\niDS - Dangerous shots taken by this player, the sum of rebounds and shots off the rush\\niFF - Unblocked shot attempts (Fenwick, USAT) taken by this individual\\niFOL - Faceoff losses by this individual\\niFOW - Faceoff wins by this individual\\niGVA - Giveaways by this individual\\niHA - Hits taken by this individual\\niHDf - The difference in hits thrown by this individual minus those taken\\niHF - Hits thrown by this individual\\niMiss - Individual shots taken that missed the net.\\nInjuries - List of types of injuries incurred, if any\\niPEND - Penalties drawn by this individual\\niPenDf - The difference in penalties drawn minus those taken\\niPENT - Penalties taken by this individual\\nIPP% - Individual points percentage, which is on-ice goals for which this player had the goal or an assist\\niRB - Rebound shots taken by this individual\\niRS - Shots off the rush taken by this individual\\niSCF - All scoring chances taken by this individual\\niSF - Shots on goal taken by this individual\\niTKA - Takeaways by this individual\\nixG - Expected goals (weighted shots) for this individual, which is shot attempts weighted by shot location\\nLast Name -\\nMaj - Major penalties taken\\nMatch - Match penalties\\nMGL - Games lost due to injury\\nMin - Minor penalties taken\\nMisc - Misconduct penalties\\nNat - Nationality\\nNGPF - Net Goals Post Faceoff. A differential of all goals within 10 seconds of a faceoff, relative to expectations set by the zone in which they took place\\nNHLid - NHL player id useful when looking at the raw data in game files\\nNMC - What kind of no-movement clause this player's contract has, if any\\nNPD - Net Penalty Differential is the player's penalty differential relative to a player of the same position with the same ice time per manpower situation\\nNSPF - Net Shots Post Faceoff. A differential of all shot attempts within 10 seconds of a faceoff, relative to expectations set by the zone in which they took place\\nNZF - Shifts this player has ended with a neutral zone faceoff\\nnzFOL - Faceoffs lost in the neutral zone\\nnzFOW - Faceoffs won in the neutral zone\\nnzGAPF - Team goals allowed after faceoffs taken in the neutral zone\\nnzGFPF - Team goals scored after faceoffs taken in the neutral zone\\nNZS - Shifts this player has started with a neutral zone faceoff\\nnzSAPF - Team shot attempts allowed after faceoffs taken in the neutral zone\\nnzSFPF - Team shot attempts taken after faceoffs taken in the neutral zone\\nOCA - Shot attempts allowed (Corsi, SAT) while this player was not on the ice\\nOCF - The team's shot attempts (Corsi, SAT) while this player was not on the ice\\nODZS - Defensive zone faceoffs that occurred without this player on the ice\\nOFA - Unblocked shot attempts allowed (Fenwick, USAT) while this player was not on the ice\\nOFF - The team's unblocked shot attempts (Fenwick, USAT) while this player was not on the ice\\nOGA - Goals allowed while this player was not on the ice\\nOGF - The team's goals while this player was not on the ice\\nONZS - Neutral zone faceoffs that occurred without this player on the ice\\nOOZS - Offensive zone faceoffs that occurred without this player on the ice\\nOpFO - Opening faceoffs taken\\nOpFOW - Opening faceoffs won\\nOppCA60 - A weighted average of the shot attempts (Corsi, SAT) the team allowed per 60 minutes of a player's opponents\\nOppCF60 - A weighted average of the shot attempts (Corsi, SAT) the team generated per 60 minutes of a player's opponents\\nOppFA60 - A weighted average of the unblocked shot attempts (Fenwick, USAT) the team allowed per 60 minutes of a player's opponents\\nOppFF60 - A weighted average of the unblocked shot attempts (Fenwick, USAT) the team generated per 60 minutes of a player's opponents\\nOppGA60 - A weighted average of the goals the team allowed per 60 minutes of a player's opponents\\nOppGF60 - A weighted average of the goals the team scored per 60 minutes of a player's opponents\\nOppSA60 - A weighted average of the shots on goal the team allowed per 60 minutes of a player's opponents\\nOppSF60 - A weighted average of the shots on goal the team generated per 60 minutes of a player's opponents\\nOPS - Offensive point shares, a catch-all stats that measures a player's offensive contributions in points in the standings\\nOSA - Shots on goal allowed while this player was not on the ice\\nOSCA - Scoring chances allowed while this player was not on the ice\\nOSCF - The team's scoring chances while this player was not on the ice\\nOSF - The team's shots on goal while this player was not on the ice\\nOTF - Shifts this player started with an on-the-fly change\\nOTG - Overtime goals\\nOTOI - The amount of time this player was not on the ice.\\nOver - Shots that went over the net\\nOvrl - Where the player was drafted overall\\nOxGA - Expected goals allowed (weighted shots) while this player was not on the ice, which is shot attempts weighted by location\\nOxGF - The team's expected goals (weighted shots) while this player was not on the ice, which is shot attempts weighted by location\\nOZF - Shifts this player has ended with an offensive zone faceoff\\nozFO - Faceoffs taken in the offensive zone\\nozFOL - Faceoffs lost in the offensive zone\\nozFOW - Faceoffs won in the offensive zone\\nozGAPF - Team goals allowed after faceoffs taken in the offensive zone\\nozGFPF - Team goals scored after faceoffs taken in the offensive zone\\nOZS - Shifts this player has started with an offensive zone faceoff\\nozSAPF - Team shot attempts allowed after faceoffs taken in the offensive zone\\nozSFPF - Team shot attempts taken after faceoffs taken in the offensive zone\\nPace - The average game pace, as estimated by all shot attempts per 60 minutes\\nPass - An estimate of the player's setup passes (passes that result in a shot attempt)\\nPct% - Percentage of all events produced by this team, defaults to Corsi, but can be set to another stat\\nPDO - The team's shooting and save percentages added together, times a thousand\\nPEND - The team's penalties drawn while this player was on the ice\\nPENT - The team's penalties taken while this player was on the ice\\nPIM - Penalties in minutes\\nPosition - Positions played. NHL source listed first, followed by those listed by any other source.\\nPost - Times hit the post\\nPr/St - Province or state of birth\\nPS - Point shares, a catch-all stats that measures a player's contributions in points in the standings\\nPSA - Penalty shot attempts\\nPSG - Penalty shot goals\\nPTS - Points. Goals plus all assists\\nPTS/60 - Points per 60 minutes\\nQRelCA60 - Shot attempts allowed per 60 minutes relative to how others did against the same competition\\nQRelCF60 - Shot attempts per 60 minutes relative to how others did against the same competition\\nQRelDFA60 - Weighted unblocked shot attempts (Dangeorus Fenwick) allowed per 60 minutes relative to how others did against the same competition\\nQRelDFF60 - Weighted unblocked shot attempts (Dangeorus Fenwick) per 60 minutes relative to how others did against the same competition\\nRBA - Rebounds allowed while this player was on the ice. Two very different sources.\\nRBF - The team's rebounds while this player was on the ice. Two very different sources.\\nRelA/60 - The player's A/60 relative to the team when he's not on the ice\\nRelC/60 - Corsi differential per 60 minutes relative to his team\\nRelC% - Corsi percentage relative to his team\\nRelDf/60 - The player's Diff/60 relative to the team when he's not on the ice\\nRelF/60 - The player's F/60 relative to the team when he's not on the ice\\nRelF/60 - Fenwick differential per 60 minutes relative to his team\\nRelF% - Fenwick percentage relative to his team\\nRelPct% - The players Pct% relative to the team when he's not on the ice\\nRelZS% - The player's zone start percentage when he's on the ice relative to when he's not.\\nRopFO - Opening faceoffs taken at home\\nRopFOW - Opening faceoffs won at home\\nRSA - Shots off the rush allowed while this player was on the ice\\nRSF - The team's shots off the rush while this player was on the ice\\nS.Bkhd - Backhand shots\\nS.Dflct - Deflections\\nS.Slap - Slap shots\\nS.Snap - Snap shots\\nS.Tip - Tipped shots\\nS.Wrap - Wraparound shots\\nS.Wrst - Wrist shots\\nSA - Shots on goal allowed while this player was on the ice\\nSalary - The player's salary\\nSCA - Scoring chances allowed while this player was on the ice\\nSCF - The team's scoring chances while this player was on the ice\\nsDist - The average shot distance of shots taken by this player\\nSF - The team's shots on goal while this player was on the ice\\nSH% - The team's (not individual's) shooting percentage when the player was on the ice\\nSOG - Shootout Goals\\nSOGDG - Game-deciding shootout goals\\nSOS - Shootout Shots\\nStatus - This player's free agency status\\nSV% - The team's save percentage when the player was on the ice\\nTeam -\\nTKA - The team's takeaways while this player was on the ice\\nTMCA60 - A weighted average of the shot attempts (Corsi, SAT) the team allowed per 60 minutes of a player's linemates\\nTMCF60 - A weighted average of the shot attempts (Corsi, SAT) the team generated per 60 minutes of a player's linemates\\nTMFA60 - A weighted average of the unblocked shot attempts (Fenwick, USAT) the team allowed per 60 minutes of a player's linemates\\nTMFF60 - A weighted average of the unblocked shot attempts (Fenwick, USAT) the team generated per 60 minutes of a player's linemates\\nTMGA60 - A weighted average of the goals the team allowed per 60 minutes of a player's linemates\\nTMGF60 - A weighted average of the goals the team scored per 60 minutes of a player's linemates\\nTMSA60 - A weighted average of the shots on goal the team allowed per 60 minutes of a player's linemates\\nTMSF60 - A weighted average of the shots on goal the team generated per 60 minutes of a player's linemates\\nTmxGF - A weighted average of a player's linemates of the expected goals the team scored\\nTmxGA - A weighted average of a player's linemates of the expected goals the team allowed\\nTMGA - A weighted average of a player's linemates of the goals the team scored\\nTMGF - A weighted average of a player's linemates of the goals the team allowed\\nTOI - Time on ice, in minutes, or in seconds (NHL)\\nTOI.QoC - A weighted average of the TOI% of a player's opponents.\\nTOI.QoT - A weighted average of the TOI% of a player's linemates.\\nTOI/GP - Time on ice divided by games played\\nTOI% - Percentage of all available ice time assigned to this player.\\nWide - Shots that went wide of the net\\nWt - Weight\\nxGA - Expected goals allowed (weighted shots) while this player was on the ice, which is shot attempts weighted by location\\nxGF - The team's expected goals (weighted shots) while this player was on the ice, which is shot attempts weighted by location\\nxGF.QoC - A weighted average of the expected goal percentage of a player's opponents\\nxGF.QoT - A weighted average of the expected goal percentage of a player's linemates\\nZS% - Zone start percentage, the percentage of shifts started in the offensive zone, not counting neutral zone or on-the-fly changes\",\n",
       " 'Context\\nMovehub city ranking as published on http://www.movehub.com/city-rankings\\nContent\\nmovehubqualityoflife.csv\\nCities ranked by\\nMovehub Rating: A combination of all scores for an overall rating for a city or country.\\nPurchase Power: This compares the average cost of living with the average local wage.\\nHealth Care: Compiled from how citizens feel about their access to healthcare, and its quality.\\nPollution: Low is good. A score of how polluted people find a city, includes air, water and noise pollution.\\nQuality of Life: A balance of healthcare, pollution, purchase power, crime rate to give an overall quality of life score.\\nCrime Rating: Low is good. The lower the score the safer people feel in this city.\\nmovehubcostofliving.csv\\nUnit: GBP\\nCity\\nCappuccino\\nCinema\\nWine\\nGasoline\\nAvg Rent\\nAvg Disposable Income\\ncities.csv\\nCities to countries as parsed from Wikipedia https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/cityname:_A (A-Z)\\nAcknowledgements\\nMovehub\\nhttp://www.movehub.com/city-rankings\\nWikipedia\\nhttps://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/cityname:_A',\n",
       " 'This dataset contains 38,269 jokes of the question-answer form, obtained from the r/Jokes subreddit. The dataset contains a csv file, where a row contains a question (\"Why did the chicken cross the road\"), the corresponding answer (\"To get to the other side\") and a unique ID.\\nThe data comes from the end of 2016 all the way to 2008. The entries with a higher ID correspond to the ones submitted earlier.\\nAn example of what one might do with the data is build a sequence-to-sequence model where the input is a question and the output is an answer. Then, given a question, the model should generate a funny answer. This is what I did as the final project for my fall 2016 machine learning class. The project page can be viewed here.\\nDisclaimer: The dataset contains jokes that some may find inappropriate.\\nLicense\\nReleased under reddit\\'s API terms',\n",
       " \"Context\\nI am working on building a classifier that will examine today's 'top 40' and determine whether or not they are worthy of appearing on the next 'Now That's What I Call Music' album.\\nContent\\nThe dataset includes all 61 US released Now That's what I call Music tracklistings.\\nColumns are: volume_number - the album number corresponding with the volume. (ex: a value of 60 would represent the album 'Now That's What I Call Music Vol. 60)\\nartist - the name of the artist singing the track\\ntitle - the song name\\nnumber - the song's track number on it's album\\nduration - the song's length in seconds\\nAcknowledgements\\nThanks to Wikipedia contributors for maintaining this data!\\nImprovements\\nI am currently working on adding another csv file that contains this same data joined with each song's audio features from the Spotify Web API.\\nIn the future I would also be interested in scraping Now releases in other countries as well as the 'special' releases (ex: Now That's What I Call Christmas music, etc).\",\n",
       " 'Context\\nAgricuture Production in India from 2001-2014\\nContent\\nThis Dataset Describes the Agricuture Crops Cultivation/Production in india. This is from https://data.gov.in/ fully Licensed\\nAcknowledgements\\nThis Dataset can solves the problems of various crops Cultivation/production in india.\\nColumns\\ncrop:string, crop name Variety:string,crop subsidary name state: string,Crops Cultivation/production Place Quantity:Integer,no of Quintals/Hectars production:Integer,no of years Production Season:DateTime,medium(no of days),long(no of days) Unit:String , Tons Cost:Integer, cost of cutivation and Production Recommended Zone:String ,place(State,Mandal,Village)\\nInspiration\\nAcross The Globe India Is The Second Largest Country having People more than 1.3 Billion. Many People Are Dependent On The Agricuture And it is the Main Resource. In Agricuturce Cultivation/Production Having More Problems. I want to solve the Big problem in india and usefull to many more people',\n",
       " 'Context\\nThese are all the flights tracked by the National Civil Aviation Agency, in Brazil, from January 2015 to August 2017. I plan on keeping this dataset updated and on gradually adding data from previous years as well.\\nContent\\nThe dataset is in portuguese so I had to remove some characters that were not supported on Kaggle. You can see the translation for the columns in the main dataset description.\\n\"Nao Identificado\" means \"Unidentified\".\\nUF means the State where the airport is located. For every airport not located in Brazil, the value is N/I.\\nFeel free to ask me if you need translation of any other word.',\n",
       " \"Context\\n1K dataset of speckled pharmaceutical pills. Using a CNN to extract features and create binary hash code, these pills can be retrieved from a mobile device for remote identification. Every pill can be tracked using a mobile phone app.\\nContent\\n1 K pharmaceutical pills jpeg images that have been convoluted by: rotations, grey scale, noise, non-pill\\nAcknowledgements\\nSpecial thanks for Funding and support of Microsoft - Paul DeBaun and NWCadence- Steve Borg\\nInspiration\\nThe Pill Crisis in America 1) Fake Fentanyl - killing young people  2) Opioid Abuse - killing all ages of people  3) Fake Online Drugs - killing unknown numbers 4) Non-Compliance - killing older people\\nNon-Compliance  up to 90% of diabetics don't take their meds enough to benefit \\nUp to 75% of hypertensive patients do not adhere to their medicine \\nLess than 27% depressed patients adhere to their medication\\n41-59% of mentally ill take their meds infrequently or not at all\\n33% of patients with schizophrenia don’t take their medicine at all\",\n",
       " \"Context\\n(U) My purpose is to analyze Amazon Web Services (AWS) honeypot data for any trends and/or correlations that could possibly be used in predictive cyber threat vectors. I spent a lot of time looking for data sets and most of the ones I found had no documentation and the data was hard to interpret just from the file. This data is well formatted and straight forward.\\nContent\\n(U) The AWS Honeypot Database is an open-source database including information on cyber attacks/attempts.\\n(U) Data has 451,581 data points collected from 9:53pm on 3 March 2013 to 5:55am on 8 September 2013.\\nAcknowledgements\\nhttp://datadrivensecurity.info/blog/pages/dds-dataset-collection.html Jay Jacobs & Bob Rudis\\nInspiration\\nYour data will be in front of the world's largest data science community. What questions do you want to see answered?\",\n",
       " 'Context\\nThis data set is focused on WLAN fingerprint positioning technologies and methodologies (also know as WiFi Fingerprinting). It was the official database used in the IPIN2015 competition.\\nMany real world applications need to know the localization of a user in the world to provide their services. Therefore, automatic user localization has been a hot research topic in the last years. Automatic user localization consists of estimating the position of the user (latitude, longitude and altitude) by using an electronic device, usually a mobile phone. Outdoor localization problem can be solved very accurately thanks to the inclusion of GPS sensors into the mobile devices. However, indoor localization is still an open problem mainly due to the loss of GPS signal in indoor environments. Although, there are some indoor positioning technologies and methodologies, this database is focused on WLAN fingerprint-based ones (also know as WiFi Fingerprinting).\\nAlthough there are many papers in the literature trying to solve the indoor localization problem using a WLAN fingerprint-based method, there still exists one important drawback in this field which is the lack of a common database for comparison purposes. So, UJIIndoorLoc database is presented to overcome this gap.\\nThe UJIIndoorLoc database covers three buildings of Universitat Jaume I (http://www.uji.es) with 4 or more floors and almost 110.000m2. It can be used for classification, e.g. actual building and floor identification, or regression, e.g. actual longitude and latitude estimation. It was created in 2013 by means of more than 20 different users and 25 Android devices. The database consists of 19937 training/reference records (trainingData.csv file) and 1111 validation/test records (validationData.csv file)\\nThe 529 attributes contain the WiFi fingerprint, the coordinates where it was taken, and other useful information.\\nEach WiFi fingerprint can be characterized by the detected Wireless Access Points (WAPs) and the corresponding Received Signal Strength Intensity (RSSI). The intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM. The positive value 100 is used to denote when a WAP was not detected. During the database creation, 520 different WAPs were detected. Thus, the WiFi fingerprint is composed by 520 intensity values.\\nThen the coordinates (latitude, longitude, floor) and Building ID are provided as the attributes to be predicted.\\nThe particular space (offices, labs, etc.) and the relative position (inside/outside the space) where the capture was taken have been recorded. Outside means that the capture was taken in front of the door of the space.\\nInformation about who (user), how (android device & version) and when (timestamp) WiFi capture was taken is also recorded.\\nContent\\nAttributes 001 to 520 (WAP001-WAP520): Intensity value for WAP001. Negative integer values from -104 to 0 and +100. Positive value 100 used if WAP001 was not detected.\\nAttribute 521 (Longitude): Longitude. Negative real values from -7695.9387549299299000 to -7299.786516730871000\\nAttribute 522 (Latitude): Latitude. Positive real values from 4864745.7450159714 to 4865017.3646842018.\\nAttribute 523 (Floor): Altitude in floors inside the building. Integer values from 0 to 4.\\nAttribute 524 (BuildingID): ID to identify the building. Measures were taken in three different buildings. Categorical integer values from 0 to 2.\\nAttribute 525 (SpaceID): Internal ID number to identify the Space (office, corridor, classroom) where the capture was taken. Categorical integer values.\\nAttribute 526 (RelativePosition): Relative position with respect to the Space (1 - Inside, 2 - Outside in Front of the door). Categorical integer values.\\nAttribute 527 (UserID): User identifier (see below). Categorical integer values.\\nAttribute 528 (PhoneID): Android device identifier (see below). Categorical integer values.\\nAttribute 529 (Timestamp): UNIX Time when the capture was taken. Integer value.\\nRelevent Paper\\nMore information can be found in this paper:\\nJoaquín Torres-Sospedra, Raúl Montoliu, Adolfo Martínez-Usó, Tomar J. Arnau, Joan P. Avariento, Mauri Benedito-Bordonau, Joaquín Huerta. UJIIndoorLoc: A New Multi-building and Multi-floor Database for WLAN Fingerprint-based Indoor Localization Problems. In Proceedings of the Fifth International Conference on Indoor Positioning and Indoor Navigation, 2014.\\nAvailable at: http://www.ipin2014.org/wp/pdf/4A-3.pdf\\nIf your are going to use this dataset in your research, please cite this paper\\nAcknowledgements\\nThe dataset was created by:\\nJoaquín Torres-Sospedra, Raul Montoliu, Adolfo Martínez-Usó, Tomar J. Arnau, Joan P. Avariento, Mauri Benedito-Bordonau, Joaquín Huerta, Yasmina Andreu, óscar Belmonte, Vicent Castelló, Irene Garcia-Martí, Diego Gargallo, Carlos Gonzalez, Nadal Francisco, Josep López, Ruben Martínez, Roberto Mediero, Javier Ortells, Nacho Piqueras, Ianisse Quizán, David Rambla, Luis E. Rodríguez, Eva Salvador Balaguer, Ana Sanchís, Carlos Serra, and Sergi Trilles.\\nInspiration\\nThe objective is to estimate the building, floor and coordinates (latitude and longitude) of the 1111 samples included in the validation set. Since the real values of the building, floor and coordinates are also included, it is posible to determine the localization error.\\nThe formula used in the IPIN2015 competition was the mean of the localization error of each sample. The localization error of each sample can be estimated as follows:\\nError = building_penality * building_error + floor_penality * floor_error + coordinates_error\\nwhere:\\nbuilding_error is 1 if the estimated building is not equal to the real one. 0 otherwise\\nfloor_error is 1 if the estimated floor is not equal to the real one. 0 otherwise\\ncoordinates_error is sqrt( (estimated_latitude - real_latitude)^2 + (estimated_longitude-real_longitude)^2)\\nIn the IPIN2015 competition building_penalty and floor_penalty where set to 50 and 4 meters, respectively.',\n",
       " 'Content\\nExecutive orders are official documents, numbered consecutively, through which the President of the United States manages the operations of the federal government. The text of executive orders appears in the daily Federal Register as each executive order is signed by the President and received by the Office of the Federal Register. The total number of Executive orders issued by each administration includes number-and-letter designated orders, such as 9577-A, 9616-A, etc.\\nAcknowledgements\\nThe data was compiled and published by the National Archives as executive order disposition tables, available for Franklin D. Roosevelt and later presidents.',\n",
       " \"Context\\nWe should definitely stop hate crimes. Let's use data science to stop them. This is mainly classification but any other approach is welcome. Example; prediction for next possible hate crime.\\nContent\\n3700 rows of CSV from Google Trend. Headline, date, location, URL.\\nAcknowledgements\\nSpecial thanks to ; http://googletrends.github.io/data/\\nInspiration\\nMedia data is mainly NLP CSV. We have to come up with other ways to add the value from it.\",\n",
       " 'Context:\\nRestaurant inspections for permitted food establishments in NYC. Restaurants are graded on A-F scale with regular visits by city health department.\\nContent:\\nDataset includes address, cuisine description, inspection date, type, action, violation code and description(s). Data covers all of NYC and starts Jan 1, 2010-Aug 29, 2017.\\nAcknowledgements:\\nData was collected by the NYC Department of Health and is available here.\\nInspiration:\\nCan you predict restaurant closings?\\nAre certain violations more prominent in certain neighborhoods? By cuisine?\\nWho gets worse grades--chain restaurants or independent establishments?',\n",
       " \"Context\\nIn the past, the ancient city of Krakow was known as the capital of Poland. In 2000, it became known as the official European Capital of Culture. Now, it is known for having some of the most polluted air in Europe... In a World Health Organiation (WHO) study Krakow has been rated amongst the most polluted in the world. In the report, city was ranked 8th among 575 cities for levels of PM2.5 and 145th among 1100 cities for levels of PM10. Hazardous air quality is a common problem particularly during the colder months when many residents use solid fuels (mostly coal) for household heating. Air pollution in Krakow poses a significant danger to human health and life. Krakow's poisoned air includes amongst other things: particulate matter, benzo(a)pyrene and nitrogen dioxide. The state-run network of monitoring stations consists of 8 monitoring stations in Krakow. We decided to go step further - to build network of low-cost air quality sensors that can be deployed across entire city. The first step in the fight against smog is to identify areas of problem and to raise awareness among residents and the authorities. It is very important to create a network of sensors – only then you can check the actual conditions in various areas of the city. The technology enables real-time monitoring of air quality via map.airly.eu, so the information about the air in a specific location is easily accessible and always up to date.\\nContent\\nThis dataset consists air quality data (the concentrations of particulate matter PM1, PM2.5 and PM10, temperature, air pressure and humidity) from 2017 generated by network of 56 low-cost sensors located in Krakow, Poland. Each had its own location (6 of them where replaced during this time period and have almost the same latitude and longitude). Measurements are grouped in 12 files, one for each month. Resolution of data is 1 hour.\\nKnown issues: - PM1 is not calibrated and therefore can be bigger than PM2.5 - PM2.5 can be bigger than PM10 within the limits of measurement error - for the first two months humidity and temperature were not calibrated and therefore can show inaccurate values\\nAcknowledgements\\nThe data was generated by Airly network - the project is still in its beginning stage, but over 1000 sensors have already been implemented in Poland. Airly is a startup definitely worth watching, especially for citizens of the most polluted cities. After all, it’s clean air we all want to breathe.\\nInspiration\\nI think that this dataset offers some great opportunities for predictive models and data visualization. Airly's goal is to develop an effective forecast and monitoring of air quality, employ Artificial Intelligence and utilise data from extensive sensor network. If anyone has any ideas, breakthroughs or other interesting models please post them.\\nSome questions worth exploring: - What are the best prediction models based on extensive sensor network - statistical or numerical forecast? - How weather affects air quality? - How much pollution comes from cars, factories and coal-fired power plants?\",\n",
       " \"Context\\nExperimental data used to create regression models of appliances energy use in a low energy building.\\nContent\\nThe data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters).\\nAcknowledgements\\nLuis Candanedo, luismiguel.candanedoibarra '@' umons.ac.be, University of Mons (UMONS)\\nLuis M. Candanedo, Veronique Feldheim, Dominique Deramaix, Data driven prediction models of energy use of appliances in a low-energy house, Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788, [Web Link].\\nInspiration\\nData used include measurements of temperature and humidity sensors from a wireless network, weather from a nearby airport station and recorded energy use of lighting fixtures. data filtering to remove non-predictive parameters and feature ranking plays an important role with this data. Different statistical models could be developed over this dataset. Highlights: The appliances energy consumption prediction in a low energy house is the dataset content Weather data from a nearby station was found to improve the prediction.\\nPressure, air temperature and wind speed are important parameters in the prediction.\\nData from a WSN that measures temperature and humidity increase the pred. accuracy.\\nFrom the WSN, the kitchen, laundry and living room data ranked high in importance.\",\n",
       " \"Data Set Information:\\nThe main goal of this data set is providing clean and valid signals for designing cuff-less blood pressure estimation algorithms. The raw electrocardiogram (ECG), photoplethysmograph (PPG), and arterial blood pressure (ABP) signals are originally collected from the physionet.org and then some preprocessing and validation performed on them. (For more information about the process please refer to our paper)\\nAttribute Information:\\nThis database consists of a cell array of matrices, each cell is one record part. In each matrix each row corresponds to one signal channel:\\n1: PPG signal, FS=125Hz; photoplethysmograph from fingertip\\n2: ABP signal, FS=125Hz; invasive arterial blood pressure (mmHg)\\n3: ECG signal, FS=125Hz; electrocardiogram from channel II\\nRelevant Papers:\\nM. Kachuee, M. M. Kiani, H. Mohammadzade, M. Shabany, Cuff-Less High-Accuracy Calibration-Free Blood Pressure Estimation Using Pulse Transit Time, IEEE International Symposium on Circuits and Systems (ISCAS'15), 2015.\\nA. Goldberger, L. Amaral, L. Glass, J. Hausdorff, P. Ivanov, R. Mark, J.Mietus, G. Moody, C. Peng and H. Stanley, â€œPhysiobank, physiotoolkit, and physionet components of a new research resource for complex physiologic signals,â€\\x9d Circulation, vol. 101, no. 23, pp. 215â€“220, 2000.\\nCitation Request:\\nIf you found this data set useful please cite the following:\\nM. Kachuee, M. M. Kiani, H. Mohammadzade, M. Shabany, Cuff-Less High-Accuracy Calibration-Free Blood Pressure Estimation Using Pulse Transit Time, IEEE International Symposium on Circuits and Systems (ISCAS'15), 2015.\\nM. Kachuee, M. M. Kiani, H. Mohammadzadeh, M. Shabany, Cuff-Less Blood Pressure Estimation Algorithms for Continuous Health-Care Monitoring, IEEE Transactions on Biomedical Engineering, 2016.\",\n",
       " \"Content\\nThis dataset contains the username of any reddit account that has left at least one comment, and their number of comments.\\nThis data was grabbed in December 2017 from the Reddit comments dataset hosted on Google BigQuery. It should be current up to November 2017.\\nQuick stats\\n26 million users\\n8 million have left only a single comment\\n13 million (50%) have left no more than 5 comments\\n42,000 usernames demand something via PM (e.g. PM_ME_PIX_OF_UR_CAT, PM_me_your_successes, PMmeyourRGB, and lots of less wholesome ones)\\nAcknowledgements\\nThanks to /u/Stuck_In_the_Matrix, who collected and maintains the original comments dataset.\\nInspiration\\nWhat words commonly appear in Reddit usernames?\\nCan you identify frequently occurring username 'recipes' using clustering techniques?\\nWhat numbers most commonly appear as suffixes in Reddit usernames?\\nCan you train a generative language model to output new usernames based on this dataset?\",\n",
       " 'Arabic Handwritten Digits Dataset\\nAbstract\\nIn recent years, handwritten digits recognition has been an important area due to its applications in several fields. This work is focusing on the recognition part of handwritten Arabic digits recognition that face several challenges, including the unlimited variation in human handwriting and the large public databases. The paper provided a deep learning technique that can be effectively apply to recognizing Arabic handwritten digits. LeNet-5, a Convolutional Neural Network (CNN) trained and tested MADBase database (Arabic handwritten digits images) that contain 60000 training and 10000 testing images. A comparison is held amongst the results, and it is shown by the end that the use of CNN was leaded to significant improvements across different machine-learning classification algorithms.\\nThe Convolutional Neural Network was trained and tested MADBase database (Arabic handwritten digits images) that contain 60000 training and 10000 testing images. Moreover, the CNN is giving an average recognition accuracy of 99.15%.\\nContext\\nThe motivation of this study is to use cross knowledge learned from multiple works to enhancement the performance of Arabic handwritten digits recognition. In recent years, Arabic handwritten digits recognition with different handwriting styles as well, making it important to find and work on a new and advanced solution for handwriting recognition. A deep learning systems needs a huge number of data (images) to be able to make a good decisions.\\nContent\\nThe MADBase is modified Arabic handwritten digits database contains 60,000 training images, and 10,000 test images. MADBase were written by 700 writers. Each writer wrote each digit (from 0 -9) ten times. To ensure including different writing styles, the database was gathered from different institutions: Colleges of Engineering and Law, School of Medicine, the Open University (whose students span a wide range of ages), a high school, and a governmental institution. MADBase is available for free and can be downloaded from (http://datacenter.aucegypt.edu/shazeem/) .\\nAcknowledgements\\nCNN for Handwritten Arabic Digits Recognition Based on LeNet-5 http://link.springer.com/chapter/10.1007/978-3-319-48308-5_54 Ahmed El-Sawy, Hazem El-Bakry, Mohamed Loey Proceedings of the International Conference on Advanced Intelligent Systems and Informatics 2016 Volume 533 of the series Advances in Intelligent Systems and Computing pp 566-575\\nInspiration\\nCreating the proposed database presents more challenges because it deals with many issues such as style of writing, thickness, dots number and position. Some characters have different shapes while written in the same position. For example the teh character has different shapes in isolated position.\\nArabic Handwritten Characters Dataset\\nhttps://www.kaggle.com/mloey1/ahcd1\\nBenha University\\nhttp://bu.edu.eg/staff/mloey\\nhttps://mloey.github.io/',\n",
       " \"Free Code Camp is an open source community where you learn to code and build projects for nonprofits.\\nWe surveyed more than 20,000 people who started coding within the past 5 years. We reached them through the twitter accounts and email lists of various organizations that help people learn to code.\\nOur goal was to understand these people's motivations in learning to code, how they're learning to code, their demographics, and their socioeconomic background.\\nWe've written in depth about this dataset here: https://medium.freecodecamp.com/we-asked-20-000-people-who-they-are-and-how-theyre-learning-to-code-fff5d668969\",\n",
       " 'Context\\nWe all know that there is relationship between tides and Moon. What if we try to find the match between the position of objects in our Solar System and Earthquakes? So I prepared the dataset to answer the question.\\nContent\\nThe dataset has information about Earthquakes (magnitude 6.1+) occur between yyyy.mm.dd: 1986.05.04 to 2016.05.04 and position (and other params) of Solar System planets + Sun and Moon at the time when the specific Earthquake occured Each row has: 1) info about the specific Earthquake: date and time, where it occur: latitude and longitude, magnitude, place (it is useless field since we have latitude and longitude but I left the filed just to have humanreading meaning e.g. 7km SW of Ueki, Japan) 2) Planets, Moon and Sun information (position and etc) relatively latitude and longitude of place and time of the Earthquake.\\nAcknowledgements\\nTha dataset has two sources of information. Both ones are free and available for public. 1) Earthquakes info: The USGS Earthquake Hazards Program https://earthquake.usgs.gov/ 2) Solar System objects info: NGINOV Astropositions: http://api.nginov note: NGINOV uses a bit specific calculation of azimuth (as far as i know more often the one calculates from geographicall north). The note from NGINOV: \"An astronomical azimuth is expressed in degree or schedules arcs. Taking as a reference point, geographically south of the place of observation and a dextrorotatory angular progression.\" note: I am not quite strong in Astronomic stuff. I expressed the idea about possible relationship and wrote a simple app to match and merge the info from two data sources into the dataset which possibly help to answer the question.\\nInspiration\\nMay be there is a relationship between the position and other params of objects in our Solar System and Earthquakes? Hope someone will find the match! :) Good luck!!',\n",
       " 'Data Dictionary:\\nTitle: Credit data\\nSource: Credit One Bank\\nNumber of Instances: 5000\\nName of Dataset: Analysis_of_Default\\nNumber of Attributes: 20 (7 numerical, 13 categorical)\\nAttribute description\\nAttribute 1: (Qualitative / Categorical) Status of existing checking account A11: ... < 0 USD A12: 0 <= ... < 10000 USD A13: ... >= 10000 USD A14: no checking account\\nAttribute 2: (numerical) Duration in month\\nAttribute 3: (Qualitative / Categorical) Credit history A30: no credits taken/all credits paid back duly A31: all credits at this bank paid back duly A32: existing credits paid back duly till now A33: delay in paying off in the past A34:critical account/other credits existing(not at this bank)\\nAttribute 4: (Qualitative / Categorical) Purpose A40: car (new) A41: car (used) A42: furniture/equipment A43: radio/television A44: domestic appliances A45: repairs A46: education A47: (vacation - does not exist?) A48: retraining A49: business A410: others\\nAttribute 5: (numerical) Credit amount\\nAttribute 6: (Qualitative / Categorical) Savings account/bonds A61: ... < 1000 USD A62: 1000 <= ... < 5000 USD A63: 5000 <= ... < 10000 USD A64: .. >= 10000 USD A65: unknown/ no savings account\\nAttribute 7: (Qualitative / Categorical) Present employment since A71: unemployed A72: ... < 1 year A73: 1 <= ... < 4 years\\nA74: 4 <= ... < 7 years A75: .. >= 7 years\\nAttribute 8: (numerical) Installment rate in percentage of disposable income\\nAttribute 9: (Qualitative / Categorical) Personal status and sex A91: male : divorced/separated A92: female: divorced/separated/married A93: male : single A94: male : married/widowed A95: female: single\\nAttribute 10: (Qualitative / Categorical) Other debtors / guarantors A101: none A102: co-applicant A103: guarantor\\nAttribute 11: (numerical) Present residence since\\nAttribute 12: (Qualitative / Categorical) Property A121: real estate A122: if not A121: building society savings agreement/ life insurance A123: if not A121/A122: car or other, not in attribute 6 A124: unknown / no property\\nAttribute 13: (numerical) Age in years\\nAttribute 14: (Qualitative / Categorical) Other installment plans A141: bank A142: stores A143: none\\nAttribute 15: (Qualitative / Categorical) Housing A151: rent A152: own A153: for free\\nAttribute 16: (numerical) Number of existing credits at this bank\\nAttribute 17: (Qualitative / Categorical) Job A171: unemployed/ unskilled - non-resident A172: unskilled - resident A173: skilled employee / official A174: management/ self-employed/ highly qualified employee/ officer\\nAttribute 18: (numerical) Number of people being liable to provide maintenance for\\nAttribute 19: (Qualitative / Categorical) Telephone A191: none A192: yes, registered under the customer’s name\\nAttribute 20: (Qualitative / Categorical) foreign worker A201: yes A202: no\\nDefault on Payment due\\n1 (Defaulted) 0 (No Default)',\n",
       " 'Why?\\nLast year a redditor created a survey to collect demographic data on the subreddit /r/ForeverAlone. Since then they have deleted their account but they left behind their data set.\\nColumns are below:\\nContent\\nTimestamp\\nDateTime\\nWhat is your Gender?\\nString\\nWhat is your sexual orientation?\\nString\\nHow old are you?\\nDateTime\\nWhat is your level of income?\\nDateTime\\nWhat is your race?\\nString\\nHow would you describe your body/weight?\\nString\\nAre you a virgin?\\nString\\nIs prostitution legal where you live?\\nString\\nWould you pay for sex?\\nString\\nHow many friends do you have IRL?\\nDateTime\\nDo you have social anxiety/phobia?\\nString\\nAre you depressed?\\nString\\nWhat kind of help do you want from others? (Choose all that apply)\\nString\\nHave you attempted suicide?\\nString\\nEmployment Status: Are you currently…?\\nString\\nWhat is your job title?\\nString\\nWhat is your level of education?\\nString\\nWhat have you done to try and improve yourself? (Check all that apply)',\n",
       " \"'champsdata.csv' and runnerupsdata.csv'\\n'champs.csv' contains game-by-game team totals for the championship team from every finals game between 1980 and 2017. 'runnerups.csv' contains game-by-game team totals for the runner-up team from every finals game between 1980 and 2017. The 1980 NBA Finals was the first Finals series since the NBA added the three point line.\\nContent\\nThe data was scrapped from basketball-reference.com.\\nVariables in 'champs.csv' and 'runnerups.csv'\\nYear: The year the series was played\\nTeam: The name of the team.\\nWin: 1 = Win. 0 = Loss\\nHome: 1 = Home team. 0 = Away team.\\nGame: Game #\\nMP - Total minutes played. Equals 240 (48x5=240) if game did not go to overtime. MP>240 if game went to overtime.\\nFG - Field goals made\\nFGA - Field goal attempts\\nFGP - Field Goal Percentage\\nTP - 3 Point Field Goals Made\\nTPA - Three point attempts\\nTPP - three point percentage\\nFT - Free throws made\\nFTA - Free throws attempted\\nFTP - Free throw percentage\\nORB - Offensive rebounds\\nDRB - Defensive rebounds\\nTRB - Total rebounds\\nAST - Assists\\nSTL - Steals\\nBLK - Blocks\\nTOV - Turnovers\\nPF - Personal fouls\\nPTS - points scored\\nDatasets created from 'champsionsdata.csv' and 'runnerupsdata.csv'\\nThe R code that I used to make the three files listed below can be found here\\n'champs_and_runner_ups_series_averages.csv'\\nThis data frame contains series averages for the champion and runnerup each year.\\n'champs_series_averages.csv'\\nThis data frame contains series averages for just the champion each year.\\n'runner_ups_series_averages.csv'\\nThis data frame contains decade-by-decade averages for champions and runners up.\",\n",
       " 'Context:\\nShort Message Service (SMS) messages are short messages sent from one person to another from their mobile phones. They represent a means of personal communication that is an important communicative artifact in our current digital era. This dataset contains SMS messages that were collected from users who knew they were participating in a research project and that their messages would be shared publicly. This dataset contains two SMS messages in two languages: Singapore English and Mandarin Chinese.\\nContent:\\nThis is a corpus of SMS (Short Message Service) messages collected for research at the Department of Computer Science at the National University of Singapore. This dataset consists of 67,093 SMS messages taken from the corpus on Mar 9, 2015. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The data collectors opportunistically collected as much metadata about the messages and their senders as possible, so as to enable different types of analyses.\\nAcknowledgements:\\nThis corpus was collected by Tao Chen and Min-Yen Kan. If you use this data, please cite the following paper:\\nTao Chen and Min-Yen Kan (2013). Creating a Live, Public Short Message Service Corpus: The NUS SMS Corpus. Language Resources and Evaluation, 47(2)(2013), pages 299-355. URL: https://link.springer.com/article/10.1007%2Fs10579-012-9197-9\\nInspiration:\\nThis dataset contains a lot of short, informal texts and is ideal for trying your hand at various natural language processing tasks. There’s also a lot of information about the messages which might reveal interesting insights. Here are some ideas to get you started:\\nThis dataset contains Singapore English. How well do tools trained on other varieties of English, like stemmers or part of speech taggers, work on it?\\nWhat time of day are most SMS messages sent? Is this different for the English and Mandarin datasets?\\nUnlike English, Mandarin does not have spaces between words, which can be made up of several characters. Can you build or implement a system for word identification?',\n",
       " 'Data Sources\\nThis dataset compiles data from the following Massachusetts Department of Education reports:\\nEnrollment by Grade\\nEnrollment by Selected Population\\nEnrollment by Race/Gender\\nClass Size by Gender and Selected Populations\\nTeacher Salaries\\nPer Pupil Expenditure\\nGraduation Rates\\nGraduates Attending Higher Ed\\nAdvanced Placement Participation\\nAdvanced Placement Performance\\nSAT Performance\\nMCAS Achievement Results\\nAccountability Report\\nIn each case, the data is the latest available data as of August 2017.\\nData Dictionary\\nThe data dictionary lists the report from which each field is sourced. It also includes the original field names - minor changes have been made to make the field names easier to understand. Data definitions can be found on the About the Data section of the MA DOE website.\\nQuestions\\nWhat contributes to differences in schools outcomes?\\nAre there meaningful regional differences within MA?\\nWhich schools do well despite limited resources?',\n",
       " 'Emotion expression is an essential part of human interaction. The same text can hold different meanings when expressed with different emotions. Thus understanding the text alone is not enough for getting the meaning of an utterance. Acted and natural corpora have been used to detect emotions from speech. Many speech databases for different languages including English, German, Chinese, Japanese, Russian, Italian, Swedish and Spanish exist for modeling emotion recognition. Since there is no reported reference of an available Arabic corpus, we decided to collect the first Arabic Natural Audio Dataset (ANAD) to recognize discrete emotions.\\nEmbedding an effective emotion detection feature in speech recognition system seems a promising solution for decreasing the obstacles faced by the deaf when communicating with the outside world. There exist several applications that allow the deaf to make and receive phone calls normally, as the hearing-impaired individual can type a message and the person on the other side hears the words spoken, and as they speak, the words are received as text by the deaf individual. However, missing the emotion part still makes these systems not hundred percent reliable. Having an effective speech to text and text to speech system installed in their everyday life starting from a very young age will hopefully replace the human ear. Such systems will aid deaf people to enroll in normal schools at very young age and will help them to adapt better in classrooms and with their classmates. It will help them experience a normal childhood and hence grow up to be able to integrate within the society without external help.\\nEight videos of live calls between an anchor and a human outside the studio were downloaded from online Arabic talk shows. Each video was then divided into turns: callers and receivers. To label each video, 18 listeners were asked to listen to each video and select whether they perceive a happy, angry or surprised emotion. Silence, laughs and noisy chunks were removed. Every chunk was then automatically divided into 1 sec speech units forming our final corpus composed of 1384 records.\\nTwenty five acoustic features, also known as low-level descriptors, were extracted. These features are: intensity, zero crossing rates, MFCC 1-12 (Mel-frequency cepstral coefficients), F0 (Fundamental frequency) and F0 envelope, probability of voicing and, LSP frequency 0-7. On every feature nineteen statistical functions were applied. The functions are: maximum, minimum, range, absolute position of maximum, absolute position of minimum, arithmetic of mean, Linear Regression1, Linear Regression2, Linear RegressionA, Linear RegressionQ, standard Deviation, kurtosis, skewness, quartiles 1, 2, 3 and, inter-quartile ranges 1-2, 2-3, 1-3. The delta coefficient for every LLD is also computed as an estimate of the first derivative hence leading to a total of 950 features.\\nI would have never reached that far without the help of my supervisors. I warmly thank and appreciate Dr. Rached Zantout, Dr. Lama Hamandi, and Dr. Ziad Osman for their guidance, support and constant supervision.',\n",
       " 'Context\\nIn Daily Fantasy Sports (DFS) contests, contestants construct a virtual lineup of players that score points based on their real-world performances. Unlike in season-long Fantasy Sports contests,in DFS contestants submit a new lineup for each set of games. DFS contests are held for several professional sports leagues, including the National Football League (NFL), National Basketball League (NBA), and National Hockey League (NHL). The leading DFS sites today are DraftKings and Fanduel, which control approximately 90% of the $3B DFS market.\\nThere are three primary types of DFS games: Head-to-Heads (H2Hs), Double-Ups, and Guaranteed Prize Pools (GPPs). In H2H games, two contestants play for a single cash prize. In Double-Up games, a pool of contestants compete to place in the top 50% of lineups, which are awarded twice the entry fee. In GPPs, a pool of contestants compete for a fixed prize structure that tends to be very top heavy; some contests payout hundreds of thousands of dollars to the top finisher.\\nOver the last year, I have developed a winning system for daily fantasy football and baseball contests. Building this system from scratch was a fantastic compliment to the things I learned as a student, from machine learning and optimization to optimal learning and game theory. I hope others can join me in researching daily fantasy basketball and perhaps get involved with the burgeoning world of daily fantasy sports.\\nContent\\nThis dataset contains 20 days of DraftKings NBA contest data scraped between 2017-11-27 and 2017-12-28. For DraftKings NBA daily fantasy basketball contest rules, see https://www.draftkings.com/help/rules/nba.\\nFormat:\\nOne folder per day\\nOne folder per contest for a given day\\nSalary file (“DKSalaries.csv”), payout structure file (“payout_structure.csv”), and contest results file (“contest-standings.csv”) for a given contest. Column headers in each files are pretty self-explanatory.\\nSome additional files (e.g. “players.csv”, “covariance_mat_unfiltered.csv”, “hist_fpts_mat.csv”) for a given contest. These files were for my personal research, feel free to use or ignore.\\n“projections” folder contains projections data for each player from rotogrinders and daily fantasy nerd, labeled by date.\\n“contests.csv” contains information about each contest, e.g. entry fee, slate, and contest size.\\nAcknowledgements\\nThank you to my friend from college, Michael Chiang, for contributions to this project.\\nInspiration\\nA few ideas to get started:\\nWhat kind of position \"stacks\" tend to maximize correlation within a lineup?\\nHow can you minimize correlation between lineups, such that you maximize your chances of winning a GPP?\\nWhat are the tendencies of some of the top DFS pros?\\nCan you improve rotogrinders and daily fantasy nerd player projections?\\nCan you predict which players are undervalued (i.e. high fantasy points / salary ratio)?\\nCan you predict the ownership percentage for each player in a given contest?',\n",
       " \"Context\\nThe Million Song Dataset (MSD) is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This is a subset of the MSD and contains audio features of songs with the year of the song. The purpose being to predict the release year of a song from audio features.\\nContent\\nThe owners recommend that you split the data like this to avoid the 'producer effect' by making sure no song from a given artist ends up in both the train and test set.\\ntrain: first 463,715 examples\\ntest: last 51,630 examples\\nField descriptions:\\nThe first value is the year (target), ranging from 1922 to 2011.\\nThen there are 90 attributes\\nTimbreAverage[1-12]\\nTimbreCovariance[1-78]\\nThese features were extracted from the 'timbre' features from The Echo Nest API. The authors took the average and covariance over all 'segments' and each segment was described by a 12-dimensional timbre vector.\\nAcknowledgements\\nOriginal dataset: Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The Million Song Dataset. In Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2011), 2\\nSubset downloaded from: https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\\nInspiration\\nUse this dataset to predict the years that each song was released based on it's audio features\",\n",
       " 'Context:\\nCarbon Monoxide (CO) is a colorless, odorless gas that can be harmful when inhaled in large amounts. CO is released when something is burned. The greatest sources of CO to outdoor air are cars, trucks and other vehicles or machinery that burn fossil fuels. A variety of items in your home such as unvented kerosene and gas space heaters, leaking chimneys and furnaces, and gas stoves also release CO and can affect air quality indoors.\\nContent:\\nThe daily summary file contains data for every monitor (sampled parameter) in the Environmental Protection Agency (EPA) database for each day. This file will contain a daily summary record that is:\\nThe aggregate of all sub-daily measurements taken at the monitor.\\nThe single sample value if the monitor takes a single, daily sample (e.g., there is only one sample with a 24-hour duration). In this case, the mean and max daily sample will have the same value.\\nWithin the data file you will find these fields: 1. State Code: The Federal Information Processing Standards (FIPS) code of the state in which the monitor resides.\\nCounty Code: The FIPS code of the county in which the monitor resides.\\nSite Num: A unique number within the county identifying the site.\\nParameter Code: The AQS code corresponding to the parameter measured by the monitor.\\nPOC: This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site.\\nLatitude: The monitoring site’s angular distance north of the equator measured in decimal degrees.\\nLongitude: The monitoring site’s angular distance east of the prime meridian measured in decimal degrees.\\nDatum: The Datum associated with the Latitude and Longitude measures.\\nParameter Name: The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants.\\nSample Duration: The length of time that air passes through the monitoring device before it is analyzed (measured). So, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).\\nPollutant Standard: A description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.)\\nDate Local: The calendar date for the summary. All daily summaries are for the local standard day (midnight to midnight) at the monitor.\\nUnits of Measure: The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations.\\nEvent Type: Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question, the data will have multiple records for each monitor.\\nObservation Count: The number of observations (samples) taken during the day.\\nObservation Percent: The percent representing the number of observations taken with respect to the number scheduled to be taken during the day. This is only calculated for monitors where measurements are required (e.g., only certain parameters).\\nArithmetic Mean: The average (arithmetic mean) value for the day.\\n1st Max Value: The highest value for the day.\\n1st Max Hour: The hour (on a 24-hour clock) when the highest value for the day (the previous field) was taken.\\nAQI: The Air Quality Index for the day for the pollutant, if applicable.\\nMethod Code: An internal system code indicating the method (processes, equipment, and protocols) used in gathering and measuring the sample. The method name is in the next column.\\nMethod Name: A short description of the processes, equipment, and protocols used in gathering and measuring the sample.\\nLocal Site Name: The name of the site (if any) given by the State, local, or tribal air pollution control agency that operates it.\\nAddress: The approximate street address of the monitoring site.\\nState Name: The name of the state where the monitoring site is located.\\nCounty Name: The name of the county where the monitoring site is located.\\nCity Name: The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas.\\nCBSA Name: The name of the core bases statistical area (metropolitan area) where the monitoring site is located.\\nDate of Last Change: The date the last time any numeric values in this record were updated in the AQS data system.\\nAcknowledgements:\\nThese data come from the EPA and are current up to May 1, 2017. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/epa.\\nInspiration:\\nBreathing air with a high concentration of CO reduces the amount of oxygen that can be transported in the bloodstream to critical organs like the heart and brain. At very high levels, which are possible indoors or in other enclosed environments, CO can cause dizziness, confusion, unconsciousness and death. Very high levels of CO are not likely to occur outdoors. However, when CO levels are elevated outdoors, they can be of particular concern for people with some types of heart disease. These people already have a reduced ability for getting oxygenated blood to their hearts in situations where the heart needs more oxygen than usual. They are especially vulnerable to the effects of CO when exercising or under increased stress. In these situations, short-term exposure to elevated CO may result in reduced oxygen to the heart accompanied by chest pain also known as angina.',\n",
       " \"About This Data\\nThis is a list of over 7,000 breweries and brewpubs in the USA provided by Datafiniti's Business Database. The dataset includes the category, name, address, city, state, and more for each listing.\\nWhat You Can Do With This Data\\nYou can use this geographical and categorical information for business locations to determine which cities and states have the most breweries. E.g.:\\nWhat is the number of breweries in each state?\\nWhat are the cities with the most breweries per person?\\nWhat are the states with the most breweries per person?\\nWhat are the top cities for breweries?\\nWhat are the top states for breweries?\\nWhat industry categories are typically grouped with breweries?\\nData Schema\\nA full schema for the data is available in our support documentation.\\nAbout Datafiniti\\nDatafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.\\nWant More?\\nYou can get more data like this by joining Datafiniti or requesting a demo.\",\n",
       " \"Context\\nSharing economy and vacation rentals are among the hottest topics that has touched millions of lives across the globe. Airbnb has been instrumental in this space and currently operating in more than 191 countries. Hence, it'd be good idea to analyze this data and uncover insights.\\nContent\\nDataset contains more than 18,000 property listings from Texas, United Staes. Given below are the data fields:\\nRate per night Number of bedrooms City Joining month and year Longitude Latitude Property description Property title Property URL\\nThe Airbnb data was extracted by PromptCloud’s Data-as-a-Service solution.\\nInitial Analysis\\nThe following article covers spatial data visualization and topic modelling of the description text: http://www.kdnuggets.com/2017/08/insights-data-mining-airbnb.html\\nInspiration\\nSome of the interesting analysis are related to spatial mapping and text mining of the description text apart from the exploratory analysis.\",\n",
       " \"Context\\nThe Federal Reserve sets interest rates to promote conditions that achieve the mandate set by the Congress — high employment, low and stable inflation, sustainable economic growth, and moderate long-term interest rates. Interest rates set by the Fed directly influence the cost of borrowing money. Lower interest rates encourage more people to obtain a mortgage for a new home or to borrow money for an automobile or for home improvement. Lower rates encourage businesses to borrow funds to invest in expansion such as purchasing new equipment, updating plants, or hiring more workers. Higher interest rates restrain such borrowing by consumers and businesses.\\nContent\\nThis dataset includes data on the economic conditions in the United States on a monthly basis since 1954. The federal funds rate is the interest rate at which depository institutions trade federal funds (balances held at Federal Reserve Banks) with each other overnight. The rate that the borrowing institution pays to the lending institution is determined between the two banks; the weighted average rate for all of these types of negotiations is called the effective federal funds rate. The effective federal funds rate is determined by the market but is influenced by the Federal Reserve through open market operations to reach the federal funds rate target. The Federal Open Market Committee (FOMC) meets eight times a year to determine the federal funds target rate; the target rate transitioned to a target range with an upper and lower limit in December 2008. The real gross domestic product is calculated as the seasonally adjusted quarterly rate of change in the gross domestic product based on chained 2009 dollars. The unemployment rate represents the number of unemployed as a seasonally adjusted percentage of the labor force. The inflation rate reflects the monthly change in the Consumer Price Index of products excluding food and energy.\\nAcknowledgements\\nThe interest rate data was published by the Federal Reserve Bank of St. Louis' economic data portal. The gross domestic product data was provided by the US Bureau of Economic Analysis; the unemployment and consumer price index data was provided by the US Bureau of Labor Statistics.\\nInspiration\\nHow does economic growth, unemployment, and inflation impact the Federal Reserve's interest rates decisions? How has the interest rate policy changed over time? Can you predict the Federal Reserve's next decision? Will the target range set in March 2017 be increased, decreased, or remain the same?\",\n",
       " 'Context\\nOne of the main challenges in any marketplace business is achieving the balance between demand and supply. At WUZZUF we optimize for demand, relevance and quality while connecting employers with the matching applicants, and recommending relevant jobs to the job seekers.\\nContent\\nThe dataset includes:\\nWuzzuf_Job_Posts_Sample : a sample of jobs posted on WUZZUF during 2014-2016.\\nWuzzuf_Applications_Sample : the corresponding applications (Excluding some entries).\\nNote: The jobs are mainly in Egypt but other locations are included.\\nExploration Ideas\\nThere are several areas to explore, including, but not limited to:\\nCorrelations between different features\\nSalaries trends\\nInsights about supply/demand\\nGrowth opportunities\\nData quality',\n",
       " 'This data and analysis originally provided information for the February 5, 2017, Los Angeles Times story \"Californians are paying billions for power they don\\'t need\" by documenting California\\'s glut of power and the increasing cost to consumers. It also underpins a complementary interactive graphic. The data are drawn from the Energy Information Administration, a branch of the United States government.\\nAcknowledgements\\nData and analysis originally published by Ryan Menezes and Ben Welsh on the LA Times Data Desk GitHub.',\n",
       " 'This data set will contain the results from all the 2016 kitefoil races. This allows analysis to be done including the calculation of world rankings.',\n",
       " 'Pokemon Go type, latitude, longitude, and despawn times - July 26 to July 28.\\nThis was an early mined dataset from Pokemon Go. Representing a fairly large spatial area within a thin time frame. Mostly useful in identifying potential spatial patterns in pokemon spawns.',\n",
       " 'Context\\nPeople have been making music for tens of thousands of years [1]. Today, making music is easier and more accessible than ever before. The technological developments of the last few decades allow people to simulate playing every imaginable instrument on their computers. Audio sequencers enable users to arrange their songs on a time line, sample by sample. Digital audio workstations (DAWs) ship with virtual instruments and synthesizers which allow users to virtually play a whole band or orchestra in their bedrooms.\\nOne challenge in working with DAWs is organizing samples and recordings in a structured way; so, users can easily access them. In addition to their own recordings, many users download samples. Browsing through sample collections to find the perfect sound is time consuming and may impede the user\\'s creative flow [2]. On top of this, manually naming and tagging recordings is a time-consuming and tedious task, so not many users do [3]. The consequence is that finding the right sound at the right moment becomes a challenging problem [4].\\nModeling the relationship between the acoustic content and semantic descriptions of sounds could allow users to retrieve sounds using text queries. This dataset was collected to support research on content-based audio retrieval systems, focused on sounds used in creative context.\\nContent\\nThis dataset was collected from Freesound [5] in June 2016. It contains the frame-based MFCCs of about 230,000 sounds and the associated tags.\\nsounds.json: Sound metadata originally downloaded from the Freesound API. This file includes the id, associated tags, links to previews, and links to an analysis_frames file, which contains frame-based low-level features, for each sound.\\npreprocessed_tags.csv: Preprocessed tags. Contains only tags which are associated to at least 0.01% of sounds. Moreover, tags were split on hyphens and stemmed. Tags containing numbers and short tags with less than three characters were removed.\\nqueries.csv: An aggregated query-log of real user-queries against the Freesound database, collected between May 11 and November 24 in 2016.\\npreprocessed_queries.csv Queries were preprocessed in the same way tags were preprocessed.\\n*_mfccs.csv.bz2: The original MFCCs for each sound, extracted from the URL provided in the analysis_frames field of sounds.json, split across ten files.\\ncb_{512|1024|2048|4096}_sparse.pkl: Codebook representation of sounds saved as sparse pd.DataFrame. The first-order and second-order derivatives of the 13 MFCCs were appended to the MFCC feature vectors of each sound. All frames were clustered using K-Means (Mini-Batch K-Means) to find {512|1024|2048|4096} cluster centers. Each frame was, then, assigned to its closest cluster center and the counts used to represent a sound as a single {512|1024|2048|4096}-dimensional vector.\\nAcknowledgements\\nThanks to the Music Technology Group of the Universitat Pompeu Fabra in Barcelona for creating and maintaining the Freesound [5] database and for providing the aggregated query-logs.\\nInspiration\\nWho can create the best content-based audio retrieval system measured by precision-at-k for values of k in {1, ..., 20} and mean average precision.\\nGetting started\\nHere\\'s the accompanying GitHub repository: https://github.com/dschwertfeger/cbar\\nReferences\\n[1] N. L. Wallin and B. Merker, The Origins of Music. MIT Press, 2001.\\n[2] M. Csikszentmihalyi, Flow: The Psychology of Optimal Experience. New York: Harper Perennial Modern Classics, 2008.\\n[3] E. Pampalk, A. Rauber, and D. Merkl, \"Content-based organization and visualization of music archives\", in Proceedings of the tenth ACM international conference on Multimedia, 2002, pp. 570–579.\\n[4] T. Bertin-Mahieux, D. Eck, and M. Mandel, \"Automatic tagging of audio: The state-of-the-art\", Machine audition: Principles, algorithms and systems, pp. 334–352, 2010.\\n[5] F. Font, G. Roma, and X. Serra, \"Freesound technical demo\", 2013, pp. 411–412.',\n",
       " \"In the pursuit of any goal, the first step is invariably data collection. As put up on the OpenAI blog, writing a program which can write other programs is an incredibly important problem.\\nThis dataset collects publicly available information from the Codechef site's practice section to provide about 1000 problem statements and a little over 1 million solutions in total to these problems in various languages.\\nThe ultimate aim is to allow a program to learn program generation in any language to satisfy a given problem statement.\",\n",
       " \"Context\\nI've uploaded a dataset previously that contains Paradise Papers, Panama Papers, Bahamas and Offshore Leaks. I've been getting a lot of requests to upload Paradise papers alone to make it less confusing for my fellow data scientists. Here it is, the complete cache of Paradise Papers released so far. I will keep updating it.\\nThe Paradise Papers is a cache of some 13GB of data that contains 13.4 million confidential records of offshore investment by 120,000 people and companies in 19 tax jurisdictions (Tax Heavens - an awesome video to understand this); that was published by the International Consortium of Investigative Journalists (ICIJ) on November 5, 2017. Subsequent data was released on November 20, 2017. Here is a brief video about the leak. The people include Queen Elizabeth II, the President of Columbia (Juan Manuel Santos), Former Prime Minister of Pakistan (Shaukat Aziz), U.S Secretary of Commerce (Wilbur Ross) and many more. According to an estimate by the Boston Consulting Group, the amount of money involved is around $10 trillion. The leak contains many famous companies, including Facebook, Apple, Uber, Nike, Walmart, Allianz, Siemens, McDonald’s and Yahoo.\\nIt also contains a lot of U. S President Donald Trump allies including Rax Tillerson, Wilbur Ross, Koch Brothers, Paul Singer, Sheldon Adelson, Stephen Schwarzman, Thomas Barrack and Steve Wynn etc. The complete list of Politicians involve is available here.\\nI am calling all data scientists to help me stop the corruption and reveal the patterns and linkages invisible for the untrained eye.\\nContent\\nThe data is the effort of more than 100 journalists from 60+ countries\\nThe original data is available under creative common license and can be downloaded from this link.\\nI will keep updating the datasets with more leaks and data as it’s available\\nAcknowledgements\\nInternational Consortium of Investigative Journalists (ICIJ)\\nInspiration\\nSome ideas worth exploring:\\nHow many companies and individuals are there in all of the leaks data\\n\\nHow many countries involved\\n\\nTotal money involved\\n\\nWhat is the biggest best tax heaven\\n\\nCan we compare the corruption with human development index and make an argument that would correlate corruption with bad conditions in that country\\n\\nWho are the biggest cheaters and where they live\\n\\nWhat role Fortune 500 companies play in this game\\nI need your help to make this world corruption free in the age of NLP and Big Data\",\n",
       " 'The GSS gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes. Hundreds of trends have been tracked since 1972. In addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years.\\nThe GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.\\nAltogether the GSS is the single best source for sociological and attitudinal trend data covering the United States. It allows researchers to examine the structure and functioning of society in general as well as the role played by relevant subgroups and to compare the United States to other nations. (Source)\\nThis dataset is a csv version of the Cumulative Data File, a cross-sectional sample of the GSS from 1972-current.',\n",
       " 'History\\nI made the database from my own photos of Russian lowercase letters written by hand.\\nContent\\nThe GitHub repository with examples\\nGitHub\\nThe main dataset (letters.zip)\\n1650 (50x33) color images (32x32x3) with 33 letters and the file with labels letters.txt.\\nPhoto files are in the .png format and the labels are integers and values.\\nAdditional letters.csv file.\\nThe file LetterColorImages.h5 consists of preprocessing images of this set: image tensors and targets (labels)\\nThe additional dataset (letters2.zip)\\n5940 (180x33) color images (32x32x3) with 33 letters and the file with labels letters2.txt.\\nPhoto files are in the .png format and the labels are integers and values.\\nAdditional letters2.csv file.\\nThe file LetterColorImages2.h5 consists of preprocessing images of this set: image tensors and targets (labels)\\nLetter Symbols => Letter Labels\\nа=>1, б=>2, в=>3, г=>4, д=>5, е=>6, ё=>7, ж=>8, з=>9, и=>10, й=>11, к=>12, л=>13, м=>14, н=>15, о=>16, п=>17, р=>18, с=>19, т=>20, у=>21, ф=>22, х=>23, ц=>24, ч=>25, ш=>26, щ=>27, ъ=>28, ы=>29, ь=>30, э=>31, ю=>32, я=>33\\nBackground Images => Background Labels\\nstriped=>0, gridded=>1, no background=>2\\nAcknowledgements\\nAs an owner of this database, I have published it for absolutely free using by any site visitor.\\nUsage\\nClassification, image generation, etc. in a case of handwritten letters with a small number of images are useful exercises.\\nImprovement\\nThere are lots of ways for increasing this set and the machine learning algorithms applying to it. For example: add the same images but written by other person or add capital letters.',\n",
       " \"Context:\\nThis dataset contains information obtained from an impact sensor within a Taekwondo chest protector. Participants were asked to perform various Taekwondo techniques on this chest protector for analysis of sensor readings.\\nContent:\\nData was obtained from 6 participants performing 4 different Taekwondo techniques – Roundhouse/Round Kick, Back Kick, Cut Kick & Punch. Participant details are summarized in Table 1. The table is organized in ascending order according to participant weight/experience level.\\nIn the file 'Taekwondo_Technique_Classification_Stats.csv’, the data is organized as follows. The rows display:\\nTechnique – Roundhouse/Round Kick (R), Back Kick (B), Cut Kick (C), Punch (P)\\nParticipant ID – P1, P2, P3, P4, P5, P6\\nTrial # – For each Technique, each participant performed a total of 5 trials\\nSensor Readings – Data shows the ADC readings obtained from a 12-bit ADC connected to the sensor (not listed as Voltage but can be converted to it)\\nThe columns identify type of technique, participant, trial # and showcase the sensor readings. There are a total of 115 columns of sensor readings. Each participant performed 5 trials for each type of technique with hard intensity. The only exception is that Participant 6 (P6) did not perform Back Kicks.\\nAcknowledgements:\\nThe dataset was collected at a local Taekwondo school. We would like to thank the instructor and students for taking their time to participate in our data collection!\\nPast Research:\\nPrevious work to classify Taekwondo techniques included using a butter-worth low pass filter on Matlab and performing integration around the maximum signal peak. The goal was to observe a pattern in the resulting integration values to determine impact intensity for each type of technique.\\nInspiration:\\nMain analysis questions:\\n1) Determine impact intensity that is proportional to the participant’s weight/experience level\\nIdeally, impact intensity should increase with the increasing participant weight/experience level or ID (Participant ID in Table 1 corresponds to ascending weight/experience level)\\n2) Classify or distinguish between types of impact (Round Kick, Back Kick, Cut Kick or Punch)\\nEach Taekwondo technique usually has a unique waveform to be identified\",\n",
       " \"Context\\nI'm going straight to the point: I'm obsessed with Steven Wilson. I can't help it, I love his music. And I need more music with similar (almost identical) style. So, what I'm trying to solve here is, how to find songs that match SW's style with almost zero error?\\nI'm aware that Spotify gives you recommendations, like similar artists and such. But that's not enough -- Spotify always gives you varied music. Progressive rock is a very broad genre, and I just want those songs that sound very, very similar to Steven Wilson or Porcupine Tree.\\nBTW, Porcupine Tree was Steven Wilson's band, and they both sound practically the same. I made an analysis where I checked their musical similarities.\\nContent\\nI'm using the Spotify web API to get the data. They have an amazingly rich amount of information, especially the audio features.\\nThis repository has 5 datasets:\\nStevenWilson.csv: contains Steven Wilson discography (65 songs)\\nPorcupineTree.csv: 65 Porcupine Tree songs\\nComplete Steven Wilson.csv: a merge between the past two datasets (Steven Wilson + Porcupine Tree)\\nTrain.csv: 200 songs used to train KNN. 100 are Steven Wilson songs and the rest are totally different songs\\nTest.csv: 100 songs that may or may not be like Steven Wilson's. I picked this songs from various prog rock playlists and my Discover Weekly from Spotify.\\nAlso, so far I've made two kernels:\\nComparing Steven Wilson and Porcupine Tree\\nFinding songs that match SW's style using K-Nearest Neighbors\\nData\\nThere are 21 columns in the datasets.\\nNumerical: this columns were scraped using get_audio_features from the Spotify API.\\nacousticness: a confidence measure from 0.0 to 1.0 of whether the track is acoustic; 1.0 represents high confidence the track is acoustic\\ndanceability: it describes how suitable a track is for dancing; a value of 0.0 is least danceable and 1.0 is most danceable\\nduration_ms: the duration of the track in milliseconds\\nenergy: a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity\\ninstrumentalness: predicts whether a track contains no vocals; values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0\\nliveness: detects the presence of an audience in the recording; 1.0 represents high confidence that the track was performed live\\nloudness: the overall loudness of a track in decibels (dB)\\nspeechiness: detects the presence of spoken words in a track; the more exclusively speech-like the recording (e.g. talk show), the closer to 1.0 the attribute value\\ntempo: the overall estimated tempo of a track in beats per minute (BPM)\\nvalence: a measure from 0.0 to 1.0 describing the musical positiveness; tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\\nCategorical: these features are categories represented as numbers.\\nkey: the musical key the track is in. e.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on\\nmode: mode indicates the modality (major or minor); major is represented by 1 and minor is 0\\ntime_signature: an estimated overall time signature of a track; it is a notational convention to specify how many beats are in each bar (or measure). e.g. 4/4, 4/3, 3/4, 8/4 etc.\\nStrings: these fields are mostly useless (except for name, album, artist and lyrics)\\nid: the Spotify ID of the song\\nname: name of the song\\nalbum: album of the song\\nartist: artist of the song\\nuri: the Spotify URI of the song\\ntype: the type of the Spotify object\\ntrack_href: the Spotify API link of the song\\nanalysis_url: the URL used for getting the audio features\\nlyrics: lyrics of the song in lower case\\nFuture\\nEver been obsessed with a song? an album? an artist? I'm planning on building a web app that solves this. It will help you find music extremely similar to other.\",\n",
       " 'Context\\nAnalyze the world\\'s largest rock climbing logbook!\\nWho\\'s the biggest downgrader? Is it better to be short, tall, or average height? How many years does it take the average climber to send her first 5.12? After countless crag debates over these and similar topics, I set out to find the answers. Now you can prove statistically why that dude who tagged your multi-year project \"Soft\" is wrong. (Sorry, he might actually be right.)\\nContent\\nI used Python3 to build a web-scraper to collect all of the user and ascent information from the world\\'s largest rock climbing logbook, https://www.8a.nu/. I actually ended up scraping their beta site, https://beta.8a.nu/, as it provided well-formed JSON objects. The scraper dumps all of the data into an SQLite database. Check out https://github.com/dcohen21/8a.nu-Scraper for more information about the project. This dataset was collected on 9/13/2017.\\nThe database consists of four tables: User, Ascent, Method, and Grade.\\nAcknowledgements\\nThanks to Andrew Cassidy (https://github.com/andrewcassidy) for the idea and mentorship. Thanks to Jens Larssen and 8a.nu for creating the logbook and maintaining a thriving community.\\nInspiration\\nThe sky\\'s the limit!',\n",
       " 'The dataset is approximately 1,400 cleaned and standardized product listings from Dream Market\\'s \"Cocaine\" category. It was collected with web-scraping and text extraction techniques in July 2017.\\nExtracted features for each listing include:\\nproduct_title\\nships_from_to\\nquantity in grams\\nquality\\nbtc_price\\nvendor details\\nshipping dummy variables (true/false columns)\\nFor further details on the creation of this dataset and what it contains, see the blog post here: https://medium.com/thought-skipper/dark-market-regression-calculating-the-price-distribution-of-cocaine-from-market-listings-10aeff1e89e0',\n",
       " 'Context\\nNowadays everybody is talking about cryptocurrencies and chances are this phenomenon will keep growing in the future. Not only Bitcoin and Ethereum, but also other cryptos are slowly attracting more and more investments. Here I have included all the available historical data of the top 50 cryptocurrencies listed on coinmarketcap.com (which at the time of writing is listing 1296 cryptocurrencies in total). Data is available at 1-day interval from when the crypto was listed on coinmarketcap.com up to 17 November 2017.\\nGood luck!\\nContent\\nAs specified above, all the data was retrieved from coinmarketcap.com for the top 50 (at the time of writing) cryptocurrencies. Everytime marketcap was not available (listed as \\'-\\', usually during the first days after the launch) has been replaced with a \"0\"\\nFiles are organized as follows: Date (dd/mm/yyyy) | Open ($) | High ($) | Low ($) | Close ($) | 24-hrs Volume ($) | MarketCap ($)\\nThis dataset includes (with starting date):\\nardor.csv (23/07/2016) ark.csv (22/03/2017) attention-token-of-media.csv (04/10/2017) augur.csv (27/10/2015) basic-attention-token.csv (01/06/2017) binance-coin.csv (25/07/2017) bitcoin-cash.csv (23/07/2017) bitcoin.csv (28/04/2013) bitcoindark.csv (16/07/2014) bitconnect.csv (20/10/2017) bitshares.csv (21/07/2014) byteball.csv (27/12/2017) bytecoin-bcn.csv (17/06/2014) cardano.csv (01/10/2017) dash.csv (14/02/2014) decred.csv (10/02/2016) digixdao.csv (18/04/2016) dogecoin.csv (15/12/2013) eos.csv (01/07/2017) ethereum-classic.csv (24/07/2016) ethereum.csv (07/08/2015) factom.csv (06/10/2015) gas.csv (06/07/2017) golem.csv (19/10/2017) hshare.csv (20/08/2017) iota.csv (13/06/2017) komodo.csv (05/02/2017) kyber-network.csv (24/09/2017) lisk.csv (06/04/2016) litecoin.csv (28/04/2013) maidsafecoin.csv (28/04/2014) monacoin.csv (20/03/2014) monero.csv (21/05/2014) nem.csv (01/04/2015) neo.csv (09/09/2016) omisego.csv (14/07/2017) pivx.csv (13/02/2016) populous.csv (11/07/2017) qtum.csv (24/05/2017) ripple.csv (04/08/2013) salt.csv (29/09/2017) steem.csv (18/04/2016) stellar.csv (05/08/2014) stratis.csv (12/08/2016) tenx.csv (27/06/2017) tether.csv (25/02/2015) veritaseum.csv (08/06/2017) vertcoin.csv (20/01/2014) waves.csv (02/06/2016) zcash.csv (29/10/2016)\\nI will keep this collection up to date as much as I can. Please let me know if you are interested in additional cryptos.\\nAcknowledgements and Inspiration\\nIf these files exist is thanks to CoinMarketCap (coinmarketcap.com) which is an awesome database of cryptocurrencies (and makes an awesome homepage).\\nMany have tried, few have succeeded. Can you predict tomorrow\\'s price? What data patterns do you recognise? Can\\'t wait to see what code/results you have to share! Comment below and please upvote this if you like it.',\n",
       " 'Context\\nWhat’s the best (or at least the most popular) Halloween candy? That was the question this dataset was collected to answer. Data was collected by creating a website where participants were shown presenting two fun-sized candies and asked to click on the one they would prefer to receive. In total, more than 269 thousand votes were collected from 8,371 different IP addresses.\\nContent\\ncandy-data.csv includes attributes for each candy along with its ranking. For binary variables, 1 means yes, 0 means no. The data contains the following fields:\\nchocolate: Does it contain chocolate?\\nfruity: Is it fruit flavored?\\ncaramel: Is there caramel in the candy?\\npeanutalmondy: Does it contain peanuts, peanut butter or almonds?\\nnougat: Does it contain nougat?\\ncrispedricewafer: Does it contain crisped rice, wafers, or a cookie component?\\nhard: Is it a hard candy?\\nbar: Is it a candy bar?\\npluribus: Is it one of many candies in a bag or box?\\nsugarpercent: The percentile of sugar it falls under within the data set.\\npricepercent: The unit price percentile compared to the rest of the set.\\nwinpercent: The overall win percentage according to 269,000 matchups.\\nAcknowledgements:\\nThis dataset is Copyright (c) 2014 ESPN Internet Ventures and distributed under an MIT license. Check out the analysis and write-up here: The Ultimate Halloween Candy Power Ranking. Thanks to Walt Hickey for making the data available.\\nInspiration:\\nWhich qualities are associated with higher rankings?\\nWhat’s the most popular candy? Least popular?\\nCan you recreate the 538 analysis of this dataset?',\n",
       " 'The Hockey Database is a collection of historical statistics from men\\'s professional hockey teams in North America.\\nNote that as of v1, this dataset is missing a few files, due to Kaggle restrictions on the number of individual files that can be uploaded. The missing files will be noted in the description below.\\nThe Data\\nThe dataset contains the following tables (all are csv):\\nMaster: Names and biographical information\\nScoring: Scoring statistics\\nScoringSup: Supplemental scoring statistics. Missing in v1\\nScoringSC: Scoring for Stanley Cup finals, 1917-18 through 1925-26\\nScoringShootout: Scoring statistics for shootouts\\nGoalies: Goaltending statistics\\nGoaliesSC: Goaltending for Stanley Cup finals, 1917-18 through 1925-26\\nGoaliesShootout: Goaltending statistics for shootouts\\nAwardsPlayers: Player awards, trophies, postseason all-star teams\\nAwardsCoaches: Coaches awards, trophies, postseason all-star teams\\nAwardsMisc: Miscellaneous awards. Missing in v1\\nCoaches: Coaching statistics\\nTeams: Team regular season statistics\\nTeamsPost: Team postseason statistics\\nTeamsSC: Team Stanley Cup finals statistics, 1917-18 through 1925-26\\nTeamsHalf: First half / second half standings, 1917-18 through 1920-21\\nTeamSplits: Team home/road and monthly splits\\nTeamVsTeam: Team vs. team results\\nSeriesPost: Postseason series\\nCombinedShutouts: List of combined shutouts.\\nabbrev: Abbreviations used in Teams and SeriesPost tables\\nHOF: Hall of Fame information\\nDescriptions of the individual fields in each file can be found in the file\\'s description.\\nCopyright Notice\\nThe Hockey Databank project allows for free usage of its data, including the production of a commercial product based upon the data, subject to the terms outlined below.\\n1) In exchange for any usage of data, in whole or in part, you agree to display the following statement prominently and in its entirety on your end product:\\n\"The information used herein was obtained free of charge from and is copyrighted by the Hockey Databank project. For more information about the Hockey Databank project please visit http://sports.groups.yahoo.com/group/hockey-databank\"\\n2) Your usage of the data constitutes your acknowledgment, acceptance, and agreement that the Hockey Databank project makes no guarantees regarding the accuracy of the data supplied, and will not be held responsible for any consequences arising from the use of the information presented.\\nAcknowledgments\\nThis dataset was downloaded from the hockey database at Open Source Sports. The original acknowledgments are as follows:\\nA variety of sources were consulted while constructing this database. These are listed below in no particular order.\\nBooks:\\nNational Hockey League Guide (various years)\\nNational Hockey League Official Record Book (1982-83 and 1983-84)\\nNational Hockey League Official Guide & Record Book (1984-85 to present)\\nThe Stanley Cup Records and Statistics (various years)\\nWorld Hockey Association Media Guide (various years)\\nWHA Schedule & Statistics (1974-75)\\nThe Sporting News Hockey Guide (various years)\\nOfficial NHL Record Book 1917-64\\nThe Complete Historical and Statistical Reference to the World Hockey Association 1972-1979, by Scott Surgent; Xaler Press (7th edition, 2004; 8th edition, 2008)\\nTotal Hockey; Total Sports Publishing (1st edition, 1998; 2nd edition, 2000)\\nThe Encyclopedia of Hockey, by Robert A. Styer; A.S. Barnes (2nd edition, 1973)\\nThe Hockey Encyclopedia, by Stan Fischler and Shirley Walton Fischler; Macmillan (1983)\\nThe Trail of the Stanley Cup (Vol. 1, 2, and 3), by Charles L. Coleman\\nPeriodicals:\\nThe Sporting News\\nOn-line sources:\\nESPN.com: http://www.espn.com/nhl/statistics\\nFind A Grave: http://www.findagrave.com\\nThe Goaltender Home Page (Doug Norris): http://hockeygoalies.org\\nHistory Of NHL Trades: http://nhltradeshistory.blogspot.com\\nHockey Research Association: http://www.hockeyresearch.com/stats\\nHockey-Reference.com (Justin Kubatko): http://www.hockey-reference.com\\nHockey Summary Project: http://sports.groups.yahoo.com/group/hockey_summary_project/, http://hsp.flyershistory.com (previously at http://www.shrpsports.com/hsp)\\nInternet Hockey Database (Ralph Slate): http://www.hockeydb.com\\nLegends of Hockey.net (Hockey Hall of Fame): http://www.legendsofhockey.net/html/search.htm\\nLostHockey.com: http://www.losthockey.com\\nNational Hockey League: http://www.nhl.com\\nNHL Hockey Shootout Statistics: http://jeays.net/shootout/index.htm\\nNHL Shootouts: http://www.nhlshootouts.com\\nNorth American Pro Hockey: http://www.ottawavalleyonline.com/sites/tomking_01/index.html\\nPuckerings: http://www.puckerings.com\\nSociety for International Hockey Research: http://www.sihrhockey.org\\nThe Sports Network: http://www.sportsnetwork.com\\nUSA Today hockey stats archive: http://www.usatoday.com/sports/hockey, http://www.usatoday.com/sports/hockey/archive.htm\\nYahoo Sports: http://sports.yahoo.com/nhl\\nThanks to the following individuals:\\nRalph Dinger (NHL Publishing / Dan Diamond and Associates) has confirmed a number of corrections to errors found in the NHL\\'s official statistics. Thanks also to Justin Kubatko of hockey-reference.com for a number of discussions in this area.\\nMorey Holzman provided information on Lloyd Cook\\'s 1921-22 goaltending appearance.\\nStu McMurray provided correct 1917-18 scoring statistics, including GWG.\\nDoug Norris provided corrected 1984-85 statistics for Rick St. Croix.\\nPaul Reeths created the Hall of Fame table, and provided updates for the Coaches table\\nOther contributors include Roger Brewer, Mike Burton, Eric Hornick, and Claude Paradis.\\nAn acknowledgement is also given to the team led by Sean Forman and Sean Lahman that has developed and maintained the Lahman baseball database. This database follows the same general design.',\n",
       " 'Context\\nThe Los Angeles City Controller Office releases payroll information for all city employees on a quarterly basis since 2013.\\nContent\\nData includes department titles, job titles, projected annual salaries (with breakdowns of quarterly pay), bonuses, and benefits information.\\nInspiration\\nHow do benefits and salaries differ for employees across departments and titles? Are there any unusually large differences between lowest and highest employee salaries?\\nHow have salaries changed over the past three years?\\nHave the costs of benefits changed dramatically since the passing of the Affordable Care Act?\\nWhat is the most common government role in Los Angeles?',\n",
       " 'Context\\nThe Uniform Crime Reporting (UCR) Program has been the starting place for law enforcement executives, students of criminal justice, researchers, members of the media, and the public at large seeking information on crime in the nation. The program was conceived in 1929 by the International Association of Chiefs of Police to meet the need for reliable uniform crime statistics for the nation. In 1930, the FBI was tasked with collecting, publishing, and archiving those statistics.\\nToday, four annual publications, Crime in the United States, National Incident-Based Reporting System, Law Enforcement Officers Killed and Assaulted, and Hate Crime Statistics are produced from data received from over 18,000 city, university/college, county, state, tribal, and federal law enforcement agencies voluntarily participating in the program. The crime data are submitted either through a state UCR Program or directly to the FBI’s UCR Program.\\nThis dataset focuses on the crime rates and law enforcement employment data in the state of California.\\nContent\\nCrime and law enforcement employment rates are separated into individual files, focusing on offenses by enforcement agency, college/university campus, county, and city. Categories of crimes reported include violent crime, murder and nonnegligent manslaughter, rape, robbery, aggravated assault, property crime, burglary, larceny-theft, motor vehicle damage, and arson. In the case of rape, data is collected for both revised and legacy definitions. In some cases, a small number of enforcement agencies switched definition collection sometime within the same year.\\nAcknowledgements\\nThis dataset originates from the FBI UCR project, and the complete dataset for all 2015 crime reports can be found here.\\nInspiration\\nWhat are the most common types of crimes in California? Are there certain crimes that are more common in a particular place category, such as a college/university campus, compared to the rest of the state?\\nHow does the number of law enforcement officers compare to the crime rates of a particular area? Is the ratio similar throughout the state, or do certain campuses, counties, or cities have a differing rate?\\nHow does the legacy vs. refined definition of rape differ, and how do the rape counts compare? If you pulled the same data from FBI datasets for previous years, can you see a difference in rape rates over time?',\n",
       " \"Context\\nWe all know of the Roman empire, but what about its emperors specifically?\\nContent\\nHere, you will find information on each of the emperors of the Roman empire, which lasted between 26 BC and 395 AD. Specifically, you can use data on their:\\nNames\\nDate of birth\\nCity and Province of birth\\nDate of death\\nMethod of accession to power\\nDate of accession to power\\nDate of end of reign\\nCause of death\\nIdentity of killer\\nDynasty\\nEra\\nPhoto\\nAcknowledgements\\nThis dataset was provided by Zonination, who made it available on Wikipedia. See his repository on Github\\nInspiration\\nWhat kind of trend can you find in these emperors' lives and reigns? What aspects of them allowed them to live longer?\",\n",
       " \"Path of Exile League statistic\\nData contains stats of 59000 players, from 4th August of 2017 and before now.\\nContent\\nOne file with 12 data sections. One league - Harbinger, but 4 different types of divisions:\\nHarbinger\\nHardcore Harbinger\\nSSF Harbinger\\nSSF Harbinger HC\\nEach division has own ladder with leaders\\nAcknowledgements\\nI found this data at the pathofstats.com as a JSON format and exported at CVS. Data have been collecting by this API, and free to use for interested people. If GGG or pathofstats.com don't want to share it here, please contact me and it will be removed.\\nAs I could understand, it is simple data for people, who are new at data science and want to have practice.\\nQuestions for participants\\nA total number of players in each division, usage of each class in descending order.\\nSome of the players streaming their game (twitch colum). Do they play better than people, who does not?\\nPredict chance to be at top 30 in each division, if we are Necromancer. With and without stream.\\nAverage number of finished challanges for each division, show division with highest and lowest average challanges.\\nShow dependency between level and class of died characters. Only for HC divisions.\",\n",
       " 'Context:\\nOver 1.5 billions pounds of pumpkin are grown annually in the United States. Where are they sold, and for how much?\\nThis dataset contains prices for which pumpkins were sold at selected U.S. cities’ terminal markets. Prices are differentiated by the commodities’ growing origin, variety, size, package and grade.\\nContent:\\nThis dataset contains terminal market prices for different pumpkin crops in 13 cities in the United States from September 24, 2016 to September 30, 2017. In keeping with the structure of the original source data, information on each city has been uploaded as a separate file.\\nAtlanta, GA\\nBaltimore, MD\\nBoston, MA\\nChicago, IL\\nColumbia, SC\\nDallas, TX\\nDetroit, MI\\nLos Angeles, CA\\nMiami, FL\\nNew York, NY\\nPhiladelphia, PA\\nSan Francisco, CA\\nSaint Louis, MO\\nData for each city includes the following columns (although not all information is available for every city)\\nCommodity Name: Always pumpkin, since this is a pumpkin-only dataset\\nCity Name: City where the pumpkin was sold\\nType\\nPackage\\nVariety\\nSub Variety\\nGrade: In the US, usually only canned pumpkin is graded\\nDate: Date of sale (rounded up to the nearest Saturday)\\nLow Price\\nHigh Price\\nMostly Low\\nMostly High\\nOrigin: Where the pumpkins were grown\\nOrigin District\\nItem Size\\nColor\\nEnvironment\\nUnit of Sale\\nQuality\\nCondition\\nAppearance\\nStorage\\nCrop\\nRepack: Whether the pumpkin has been repackaged before sale\\nTrans Mode\\nAcknowledgements:\\nThis dataset is based on Specialty Crops Terminal Markets Standard Reports distributed by the United States Department of Agriculture. Up-to-date reports can be generated here. This data is in the public domain.\\nInspiration:\\nWhich states produce the most pumpkin?\\nWhere are pumpkin prices highest?\\nHow does pumpkin size relate to price?\\nWhich pumpkin variety is the most expensive? Least expensive?',\n",
       " \"Context\\nThis dataset is an aggregated count of all crimes committed in France, broken down by month and category.\\nContent\\nThis data was aggregated by the French national government and published online on the French Open Data Portal. It is a combination of records kept by both local and national police forces. It's important to note that the name of the categories of crime are in French!\\nAcknowledgements\\nThis data is a part of a larger group of Excel files published by the French Goverment on the French Open Data Portal. It has been converted to a single CSV file before uploading here.\\nInspiration\\nThis is a simple time series dataset that can be probed for trends in the underlying types of crimes committed. Is petty theft more or less popular today than it was ten years ago? How much variation is there in the amount of robberies year-to-year? Can you normalize the growth in the number of crimes against the growth in the number of people? How do crimes committed here differ from those committed in, say, Los Angeles?\",\n",
       " 'Sample Sales Data, Order Info, Sales, Customer, Shipping, etc., Used for Segmentation, Customer Analytics, Clustering and More. Inspired for retail analytics. This was originally used for Pentaho DI Kettle, But I found the set could be useful for Sales Simulation training.\\nOriginally Written by María Carina Roldán, Pentaho Community Member, BI consultant (Assert Solutions), Argentina. This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License. Modified by Gus Segura June 2014.',\n",
       " 'Build the proposed optimized pricing model: Determine the largest or key value driver from the data Build price segments using product characteristics, distribution channel, behavior and demographic customer characteristics. Proposed new Pricing strategy bases of customer segmentation. Identify segments where we can lower the rates and as well as highlight segments where it is underpriced without impacting the profitability',\n",
       " \"Context\\nThis is a dataset that I built by scraping the United States Department of Labor's Bureau of Labor Statistics. I was looking for county-level unemployment data and realized that there was a data source for this, but the data set itself hadn't existed yet, so I decided to write a scraper and build it out myself.\\nContent\\nThis data represents the Local Area Unemployment Statistics from 1990-2016, broken down by state and month. The data itself is pulled from this mapping site:\\nhttps://data.bls.gov/map/MapToolServlet?survey=la&map=county&seasonal=u\\nFurther, the ever-evolving and ever-improving codebase that pulled this data is available here:\\nhttps://github.com/jayrav13/bls_local_area_unemployment\\nAcknowledgements\\nOf course, a huge shoutout to bls.gov and their open and transparent data. I've certainly been inspired to dive into US-related data recently and having this data open further enables my curiosities.\\nInspiration\\nI was excited about building this data set out because I was pretty sure something similar didn't exist - curious to see what folks can do with it once they run with it! A curious question I had was surrounding Unemployment vs 2016 Presidential Election outcome down to the county level. A comparison can probably lead to interesting questions and discoveries such as trends in local elections that led to their most recent election outcome, etc.\\nNext Steps\\nVersion 1 of this is as a massive JSON blob, normalized by year / month / state. I intend to transform this into a CSV in the future as well.\",\n",
       " 'Context\\nThe data set includes information when the free throw was taken during the game, who took the shot and if it went in or not.\\nContent\\nThe data was scraped from ESPN.com. One example site is: http://www.espn.com/nba/playbyplay?gameId=261229030',\n",
       " 'Weather data Barajas Airport, Madrid, between 1997 and 2015. Gathered web https://www.wunderground.com/ The Weather Company, LLC\\nFields:\\nMax TemperatureC\\nMean TemperatureC\\nMin TemperatureC\\nDew PointC\\nMeanDew PointC\\nMin DewpointC\\nMax Humidity\\nMean Humidity\\nMin Humidity\\nMax Sea Level PressurehPa\\nMean Sea Level PressurehPa\\nMin Sea Level PressurehPa\\nMax VisibilityKm\\nMean VisibilityKm\\nMin VisibilitykM\\nMax Wind SpeedKm/h\\nMean Wind SpeedKm/h\\nMax Gust SpeedKm/h\\nPrecipitationmm\\nCloudCover\\nEvents\\nWindDirDegrees\\nScript for download the data:\\n#!/bin/bash\\n\\nLOCAL=\\'weather_madrid\\'\\nSTATION=\\'LEMD\\'\\nINI=1997\\nEND=2015\\nFILE=${LOCAL}_${STATION}_${INI}_${END}.csv\\nSITE=\\'airport\\'\\n\\necho \"CET,Max TemperatureC,Mean TemperatureC,Min TemperatureC,Dew PointC,MeanDew PointC,Min DewpointC,Max Humidity, Mean Humidity, Min Humidity, Max Sea Level PressurehPa, Mean Sea Level PressurehPa, Min Sea Level PressurehPa, Max VisibilityKm, Mean VisibilityKm, Min VisibilitykM, Max Wind SpeedKm/h, Mean Wind SpeedKm/h, Max Gust SpeedKm/h,Precipitationmm, CloudCover, Events,WindDirDegrees\" > ${FILE}\\n\\nfor YEAR in $(seq ${INI} ${END})\\ndo\\n   echo \"Year $YEAR\"\\n   wget \"https://www.wunderground.com/history/${SITE}/${STATION}/${YEAR}/1/1/CustomHistory.html?dayend=31&monthend=12&yearend=${YEAR}&req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=&format=1\" -O \"${LOCAL}_${YEAR}.csv\"\\n   tail -n +3 ${LOCAL}_${YEAR}.csv > ${LOCAL}_${YEAR}_1.csv\\n   sed \\'s/<br\\\\ \\\\/>//g\\' ${LOCAL}_${YEAR}_1.csv >> ${FILE}\\n   rm ${LOCAL}_${YEAR}.csv ${LOCAL}_${YEAR}_1.csv\\ndone',\n",
       " 'nan',\n",
       " \"Why?\\nThe NFL, ESPN, and many others have their own Quarterback rating system. Can you create your own? How many points does a QB contribute to a given game? And, with MVP trophy season coming up, who really stands out as an MVP and who is carried by their team?\\nQB Stats\\nThis is scraped from footballdb.com using Pandas' read_html function. This dataset contains every regular season NFL game and every NFL passer (including non-quarterbacks) from 1996 to 2016. Individual years are available for the past 10 years, and all 21 years are in QBStats_all. In addition to the traditional stats, the total points from the game have been appended to the stats. Win/Loss is up and coming, but is not a priority at the moment since a QB cannot control how well the defense stops the opposing offense.\\nContent\\nInside you'll find:\\nQuarterback Name (qb)\\nAttempts (att)\\nCompletions (cmp)\\nYards (yds)\\nYards per Attempt (ypa)\\nTouchdowns (td)\\nInterceptions (int)\\nLongest Throw (lg)\\nSacks (sack)\\nLoss of Yards (loss)\\nThe NFL's Quarterback Rating for the game (rate)\\nTotal points scored in the game (game_points)\\nHome or Away Game (home_away)\\nYear (year)\\nImportant Note:\\nBecause of the way that these were scraped, the the game week is not supplied. However, the games are all in oldest to most recent which would allow for some time-series analysis.\\nAdditionally:\\nFeel free to make any requests for additional information. But due to the time that it takes to scrape 21 years of NFL stats, it will likely take a while before I finish updating the dataset.\\nAcknowledgements\\nI would very much like to thank footballdb.com for not blacklisting me after numerous scrapes and potential future scrapes for information on other positions.\",\n",
       " 'Context\\nReuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI, originally High Energy Solar Spectroscopic Imager or HESSI) is a NASA solar flare observatory. It is the sixth mission in the Small Explorer program, selected in October 1997 and launched on 5 February 2002. Its primary mission is to explore the physics of particle acceleration and energy release in solar flares. HESSI was renamed to RHESSI on 29 March 2002 in honor of Reuven Ramaty, a pioneer in the area of high energy solar physics. RHESSI is the first space mission named after a NASA scientist. RHESSI was built by Spectrum Astro for Goddard Space Flight Center and is operated by the Space Sciences Laboratory in Berkeley, California. The principal investigator from 2002 to 2012 was Robert Lin, who was succeeded by Säm Krucker.\\nuseful links: https://en.wikipedia.org/wiki/Reuven_Ramaty_High_Energy_Solar_Spectroscopic_Imager https://hesperia.gsfc.nasa.gov/hessi/objectives.htm\\nContent\\nRamaty High Energy Solar Spectroscopic Imager (RHESSI)\\nNotes: Note that only events with non-zero position and energy range not equal to 3-6 keV are confirmed as solar sources. Events which have no position and show up mostly in the front detectors, but were not able to be imaged are flagged as \"PS\".\\nEvents which do not have valid position are only confirmed to be non-solar if the NS flag is set.\\nPeak Rate: peak counts/second in energy range 6-12 keV, averaged over active collimators, including background.\\nTotal Counts: counts in energy range 6-12 keV integrated over duration of flare summed over all subcollimators, including background.\\nEnergy: the highest energy band in which the flare was observed. Electron Kev (kilo electron volt) https://en.wikipedia.org/wiki/Electronvolt\\nRadial Distance: distance from Sun center\\nQuality Codes: Qn, where n is the total number of data gap, SAA, particle, eclipse or decimation flags set for event. n ranges from 0 to 11. Use care when analyzing the data when the quality is not zero.\\nActive_Region: A number for the closest active region, if available\\nradial_offset: the offset of the flare position from the spin axis of the spacecraft in arcsec. This is used i spectroscopy.\\npeak_c/s: peak count rate in corrected counts.\\nFlare Flag Codes: a0 - In attenuator state 0 (None) sometime during flare a1 - In attenuator state 1 (Thin) sometime during flare a2 - In attenuator state 2 (Thick) sometime during flare a3 - In attenuator state 3 (Both) sometime during flare An - Attenuator state (0=None, 1=Thin, 2=Thick, 3=Both) at peak of flare DF - Front segment counts were decimated sometime during flare DR - Rear segment counts were decimated sometime during flare ED - Spacecraft eclipse (night) sometime during flare EE - Flare ended in spacecraft eclipse (night) ES - Flare started in spacecraft eclipse (night) FE - Flare ongoing at end of file FR - In Fast Rate Mode FS - Flare ongoing at start of file GD - Data gap during flare GE - Flare ended in data gap GS - Flare started in data gap MR - Spacecraft in high-latitude zone during flare NS - Non-solar event PE - Particle event: Particles are present PS - Possible Solar Flare; in front detectors, but no position Pn - Position Quality: P0 = Position is NOT valid, P1 = Position is valid Qn - Data Quality: Q0 = Highest Quality, Q11 = Lowest Quality SD - Spacecraft was in SAA sometime during flare SE - Flare ended when spacecraft was in SAA SS - Flare started when spacecraft was in SAA\\nAcknowledgements\\nWhat is a solar flare?\\nA Solar flare is the rapid release of a large amount of energy stored in the solar atmosphere. During a flare, gas is heated to 10 to 20 million degrees Kelvin (K) and radiates soft X rays and longer-wavelength emission. Unable to penetrate the Earth\\'s atmosphere, the X rays can only be detected from space. Instruments on Skylab, SMM, the Japanese/US Yohkoh mission and other spacecraft have recorded many flares in X rays over the last twenty years or so. Ground-based observatories have recorded the visible and radio outputs. These data form the basis of our current understanding of a solar flare. But there are many possible mechanisms for heating the gas, and observations to date have not been able to differentiate between them.\\nHESSI\\'s new approach\\nResearchers believe that much of the energy released during a flare is used to accelerate, to very high energies, electrons (emitting primarily X-rays) and protons and other ions (emitting primarily gamma rays). The new approach of the HESSI mission is to combine, for the first time, high-resolution imaging in hard X-rays and gamma rays with high-resolution spectroscopy, so that a detailed energy spectrum can be obtained at each point of the image.\\nThis new approach will enable researchers to find out where these particles are accelerated and to what energies. Such information will advance understanding of the fundamental high-energy processes at the core of the solar flare problem. https://hesperia.gsfc.nasa.gov/hessi/objectives.htm\\nInspiration\\nExplore,\\nKnow something new,\\nPredict the solar flare,\\nRespect the Sun and value it and\\nTake care of the environments.\\nThanks',\n",
       " \"This isn't a dataset, it is a collection of kernels written on Kaggle that use no data at all.\",\n",
       " \"The 2015 American Community Survey Public Use Microdata Sample\\nContext\\nThe American Community Survey (ACS) is an ongoing survey that provides vital information on a yearly basis about our nation and its people. Information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year.\\nFrequency: Annual\\nPeriod: 2015\\nPWGTP (Weights)\\nPlease note. Each record is weighted with PWGTP. For accurate analysis, these weights need to be applied. Reference Getting Started 'Python' for a simple kernel on how this field gets used. Or, click on the image below to see how this can be done in R (see code in this kernel).\\nThe Data Dictionary can be found here, but you'll need to scroll down to the PERSON RECORD section.\\nContent\\nThrough the ACS, we know more about jobs and occupations, educational attainment, veterans, whether people own or rent their home, and other topics. Public officials, planners, and entrepreneurs use this information to assess the past and plan the future. When you respond to the ACS, you are doing your part to help your community plan hospitals and schools, support school lunch programs, improve emergency services, build bridges, and inform businesses looking to add jobs and expand to new markets, and more. The data dictionary can be found here.\\nInspiration\\nKernels created using the 2014 ACS and 2013 ACS can serve as excellent starting points for working with the 2015 ACS. For example, the following analyses were created using ACS data:\\nWork arrival times and earnings in the USA\\nInequality in STEM careers\\nAcknowledgements\\nThe American Community Survey (ACS) is administered, processed, researched and disseminated by the U.S. Census Bureau within the U.S. Department of Commerce.\",\n",
       " 'Context\\nProject Aristo at the Allen Institute for Artificial Intelligence (AI2) is focused on building a system that acquires and stores a vast amount of knowledge in computable form, then applies this knowledge to answer a variety of science questions from standardized exams for students in multiple grade levels. We are inviting the wider AI research community to work on this grand challenge with us by providing this dataset of student science assessment questions.\\nContent\\nThese are English language questions that span several grade levels as indicated in the files. Each question is a 4-way multiple choice structure. Some of these questions include a diagram, either as part of the question text, as an answer option, or both. The diagrams are represented in the text with filenames that correspond to the diagram file itself in the companion folder. These questions come pre-split into Train, Development, and Test sets.\\nThe data set includes the following fields:\\nquestionID: a unique identifier for the question\\noriginalQuestionID: the question number on the test\\ntotalPossiblePoints: how many points the question is worth\\nAnswerKey: the correct answer option\\nisMultipleChoiceQuestion: 1 = multiple choice, 0 = other\\nincludesDiagram: 1 = includes diagram, 0 = other\\nexamName: the source of the exam\\nschoolGrade: grade level\\nyear: year the source exam was published\\nquestion: the question itself\\nsubject: Science\\ncategory: Test, Train, or Dev (data comes pre-split into these categories)\\nEvaluation\\nAI2 has made available Aristo mini, a light-weight question answering system that can quickly evaluate science questions with an evaluation web server and provided baseline solvers. You can extend the provided solvers with your own implementation to try out new approaches and compare results.\\nAcknowledgements\\nThe Aristo project team at AI2 compiled this dataset and we use it actively in our research. For a description of the motivations and intention for this data, please see:\\nClark, Peter. “Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!” AAAI (2015).',\n",
       " 'Context\\nThe main reason for making this dataset is the publication of the paper: Learning from Simulated and Unsupervised Images through Adversarial Training and the idea of the SimGAN. The dataset and kernels should make it easier to get started making SimGAN networks and testing them out and comparing them to other approaches like KNN, GAN, InfoGAN and the like.\\nSource\\nThe synthetic images were generated with the windows version of UnityEyes http://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/tutorial.html\\nThe real images were taken from https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/, which can be cited like this: Appearance-based Gaze Estimation in the Wild, X. Zhang, Y. Sugano, M. Fritz and A. Bulling, Proc. of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June, p.4511-4520, (2015).\\nChallenges\\nEnhancement\\nOne of the challenges (as covered in the paper) is enhancing the simulated images by using the real images. One possible approach is using the SimGAN which is implemented for reference in one of the notebooks. There are a number of other approaches (pix2pix, CycleGAN) which could have interesting results.\\nGaze Detection\\nThe synthetic dataset has the gaze information since it was generated by UnityEyes with a predefined look-vector. The overview notebook covers what this vector means and how each component can be interpreted. It would be very useful to have a simple, quick network for automatically generating this look vector from an image',\n",
       " 'Context\\nThis is FiveThirtyEight\\'s Congress Trump Score. As the website itself puts it, it\\'s \"an updating tally of how often every member of the House and the Senate votes with or against the president\".\\nContent\\nThere are two tables: cts and votes. The first one has summary information for every congressperson: their name, their state, their Trump Score, Trump\\'s share of the votes in the election, etc. The second one has information about every vote each congressperson has cast: their vote, Trump\\'s position on the issue, etc.\\nThe data was extracted using R. The code is available as a package on github.\\nAcknowledgements\\nThe data is 100% collected and maintained by FiveThirtyEight. They are awesome.',\n",
       " 'The United States Drought Monitor collects weekly data on drought conditions around the U.S.\\nAcknowledgements\\nAll data was downloaded from the United States Drought Monitor webpage.\\nThe U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the University of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and Atmospheric Administration. Map courtesy of NDMC-UNL.\\nThe Data\\nThe data contains weekly observations about the extent and severity of drought in each county of the United States. The dataset contains the following fields:\\nreleaseDate: when this data was released on the USDM website\\nFIPS: the FIPS code for this county\\ncounty: the county name\\nstate: the state the county is in\\nNONE: percentage of the county that is not in drought\\nD0: percentage of the county that is in abnormally dry conditions\\nD1: percentage of the county that is in moderate drought\\nD2: percentage of the county that is in severe drought\\nD3: percentage of the county that is in extreme drought\\nD4: percentage of the county that is in exceptional drought\\nvalidStart: the starting date of the week that these observations represent\\nvalidEnd: the ending date of the week that these observations represent\\ndomStatisticFormatID: seems to always be 1\\nNote: the drought categories are cumulative: if an area is in D3, then it is also in D2, D1, and D0. This means that, for every observation, D4 <= D3 <= D2 <= D1 <= D0.\\nCounty Info\\nTo make some analyses slightly easier, I\\'ve also included *county_info_2016.csv*, which contains physical size information about each county. This file contains the following fields:\\nUSPS: United States Postal Service State Abbreviation\\nGEOID: FIPS code\\nANSICODE: American National Standards Institute code\\nNAME: Name\\nALAND: Land Area (square meters) - Created for statistical purposes only\\nAWATER: Water Area (square meters) - Created for statistical purposes only\\nALAND_SQMI: Land Area (square miles) - Created for statistical purposes only\\nAWATER_SQMI: Water Area (square miles) - Created for statistical purposes only\\nINTPTLAT: Latitude (decimal degrees) First character is blank or \"-\" denoting North or South latitude respectively\\nINTPTLONG: Longitude (decimal degrees) First character is blank or \"-\" denoting East or West longitude respectively',\n",
       " \"Baseball Databank is a compilation of historical baseball data in a convenient, tidy format, distributed under Open Data terms.\\nThis version of the Baseball databank was downloaded from Sean Lahman's website.\\nNote that as of v1, this dataset is missing a few tables because of a restriction on the number of individual files that can be added. This is in the process of being fixed. The missing tables are Parks, HomeGames, CollegePlaying, Schools, Appearances, and FieldingPost.\\nThe Data\\nThe design follows these general principles. Each player is assigned a unique number (playerID). All of the information relating to that player is tagged with his playerID. The playerIDs are linked to names and birthdates in the MASTER table.\\nThe database is comprised of the following main tables:\\nMASTER - Player names, DOB, and biographical info\\nBatting - batting statistics\\nPitching - pitching statistics\\nFielding - fielding statistics\\nIt is supplemented by these tables:\\nAllStarFull - All-Star appearances\\nHallofFame - Hall of Fame voting data\\nManagers - managerial statistics\\nTeams - yearly stats and standings\\nBattingPost - post-season batting statistics\\nPitchingPost - post-season pitching statistics\\nTeamFranchises - franchise information\\nFieldingOF - outfield position data\\nFieldingPost- post-season fielding data\\nManagersHalf - split season data for managers\\nTeamsHalf - split season data for teams\\nSalaries - player salary data\\nSeriesPost - post-season series information\\nAwardsManagers - awards won by managers\\nAwardsPlayers - awards won by players\\nAwardsShareManagers - award voting for manager awards\\nAwardsSharePlayers - award voting for player awards\\nAppearances - details on the positions a player appeared at\\nSchools - list of colleges that players attended\\nCollegePlaying - list of players and the colleges they attended\\nDescriptions of each of these tables can be found attached to their associated files, below.\\nAcknowledgments\\nThis work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. For details see: http://creativecommons.org/licenses/by-sa/3.0/\\nPerson identification and demographics data are provided by Chadwick Baseball Bureau (http://www.chadwick-bureau.com), from its Register of baseball personnel.\\nPlayer performance data for 1871 through 2014 is based on the Lahman Baseball Database, version 2015-01-24, which is Copyright (C) 1996-2015 by Sean Lahman.\\nThe tables Parks.csv and HomeGames.csv are based on the game logs and park code table published by Retrosheet. This information is available free of charge from and is copyrighted by Retrosheet. Interested parties may contact Retrosheet at http://www.retrosheet.org.\",\n",
       " 'Context\\nThe datasets are from a companion website for the book Modeling Online Auctions, by Wolfgang Jank and Galit Shmueli (Wiley and Sons, ISBN: 978-0-470-47565-2, July 2010).\\nContent\\nThe datasets contain eBay auction information on Cartier wristwatches, Palm Pilot M515 PDAs, Xbox game consoles, and Swarowski beads.\\nauction.csv includes 9 variables:\\nauctionid: unique identifier of an auction\\nbid: the proxy bid placed by a bidder\\nbidtime: the time in days that the bid was placed, from the start of the auction\\nbidder: eBay username of the bidder\\nbidderrate: eBay feedback rating of the bidder\\nopenbid: the opening bid set by the seller\\nprice: the closing price that the item sold for (equivalent to the second highest bid + an increment)\\nitem: auction item\\nauction_type\\nswarovski.csv includes 5 variables:\\nSeller\\nBidder\\nWeight\\nBidder.Volume\\nSeller.Volume\\nAcknowledgements\\nThe original dataset can be found here.\\nInspiration\\nSome ideas worth exploring:\\nFor each item, what is the relationship between bids, bid time, and the closing price? Does this differ by length of the auction, opening bid, or by bidder rating?',\n",
       " 'Context\\nThe election of Donald Trump has taken the world by surprise and is fuelling populist movements in Europe, e.g. in Italy, Austria and France. Understanding populism and assessing the impact of the “Trump effect” on Europe is a tremendous challenge, and Dalia wants to help pool brainpower to find answers.\\nThe goal is to find out where the next wave of populism could hit in Europe by comparing and contrasting US and EU voter profiles, opinions of Trump vs Clinton voters, Brexit vs. Bremain voters, and future expectations.\\nContent\\nExpanding Dalia’s quarterly \"EuroPulse\" omnibus survey to the USA, Dalia has conducted a representative survey with n=11.283 respondents across all 28 EU member countries and n=1.052 respondents from the United States of America. To find out where the next wave of populism could hit Europe, Dalia’s survey traces commonalities in social and political mindsets (like authoritarianism, prejudice, open-mindedness, xenophobia, etc.), voting behaviour and socio-demographic profiles on both sides of the Atlantic.\\nInspiration\\nThe sources of our inspirations are many, but to name a few who influenced the way we asked questions: we were very inspired by the \\'angry voter\\' profile laid out by Douglas Rivers, the influence of political and moral attitudes pointed out by Jonathan Haidt and the profile of \"America\\'s forgotten working class\" by J. D. Vance.\\nResearchers should apply the necessary logic, caution and diligence when analysing and interpreting the results.',\n",
       " 'Context\\nA referendum was held on the 23 June 2016 to decide whether the United Kingdom should remain a member of the European Union or leave. Approximately 52%, or more than 17 million people, voted to leave the EU. The referendum turnout was 72.2%, with more than 33.5 million votes cast.\\nContent\\nThe Electoral Commission published the results of the EU referendum by district and region after the vote. The Office of National Statistics provided the population demographics by district from the 2011 United Kingdom Census.',\n",
       " 'Content\\nThis dataset includes annual county-level pesticide use estimates for 423 pesticides (active ingredients) applied to agricultural crops grown in the contiguous United States. Two different methods were used to estimate a range of pesticide use for all states except California. Both low and high estimate methods incorporated proprietary surveyed rates for United States Department of Agriculture Crop Reporting Districts, but the estimates differed in how they treated situations when a district was surveyed and pesticide use was not reported. Low estimates assumed zero use in the district for that pesticide; however, high estimates treated the unreported use of pesticides as missing data and estimated the pesticide usage from neighboring locations within the same region.\\nAcknowledgements\\nData for the state of California was provided by the 2014 Department of Pesticide Regulation Pesticide Use Report. The 2015 report is not yet available.',\n",
       " 'Content\\nThe U. S. Fire Administration tracks and collects information on the causes of on-duty firefighter fatalities that occur in the United States. We conduct an annual analysis to identify specific problems so that we may direct efforts toward finding solutions that will reduce firefighter fatalities in the future.\\nAcknowledgements\\nThis study of firefighter fatalities would not have been possible without members of individual fire departments, chief fire officers, fire service organizations, the National Fire Protection Association, and the National Fallen Firefighters Foundation.',\n",
       " 'NFL Football Stats\\nMy family has always been serious about fantasy football. I\\'ve managed my own team since elementary school. It\\'s a fun reason to talk with each other on a weekly basis for almost half the year.\\nEver since I was in 8th grade I\\'ve dreamed of building an AI that could draft players and choose lineups for me. I started off in Excel and have since worked my way up to more sophisticated machine learning. The one thing that I\\'ve been lacking is really good data, which is why I decided to scrape pro-football-reference.com for all recorded NFL player data.\\nFrom what I\\'ve been able to determine researching, this is the most complete public source of NFL player stats available online. I scraped every NFL player in their database going back to the 1940s. That\\'s over 25,000 players who have played over 1,000,000 football games.\\nThe scraper code can be found here. Feel free to user, alter, or contribute to the repository.\\nThe data was scraped 12/1/17-12/4/17\\nI finished in last place this year in fantasy football, so hopefully this data will help me improve my performance next year. Only 8 months until draft day!\\nMy ultimate goal is to create an AI that ranks players every week, which could be used to set lineups and draft players. I\\'m also interested in predicting the winners of games. If you have any ideas or would like to collaborate, please contact me!\\nThe data is broken into two parts. There is a players table where each player has been asigned an ID and a game stats table that has one entry per game played. These tables can be linked together using the player ID.\\nPlayer Profile Fields\\nPlayer ID: The assigned ID for the player.\\nName: The player\\'s full name.\\nPosition: The position the player played abbreviated to two characters. If the player played more than one position, the position field will be a comma-separated list of positions (i.e. \"hb,qb\").\\nHeight: The height of the player in feet and inches. The data format is -. So 6-5 would be six feet and five inches tall.\\nWeight: The weight of the player in pounds.\\nCurrent Team: The three-letter code of the team the player plays for. This is null if they are not currently active.\\nBirth Date: The day, month, and year the player was born. This is null if unknown.\\nBirth Place: The city, state or city, country the player was born in. This is null if unknown.\\nDeath Date: The day, month, and year the player died. This is null if they are still alive.\\nCollege: The name of the college they played football at. This is null if they did not play football in college.\\nHigh School: the city, state or city, country the player went to high school. This is null if the player didn\\'t go to high school or if the school is unknown.\\nDraft Team: The three letter code of the team that drafted the player. This is null if the player was not drafted.\\nDraft Position: The draft position number the player was taken. Again, null if the player was not drafted.\\nDraft Round: The round of the draft the player was drafted in. Null if the player was not drafted.\\nDraft Position: The position the player was drafted at as a two-letter code. Null if the player was not drafted.\\nDraft Year: The year the player was drafted. Null if the player was not drafted.\\nCurrent Salary Cap Hit: The player\\'s current salary hit for their current team. Null if the player is not currently active on a team.\\nHall of Fame Induction Year: The year the player was inducted into the NFL Hall of Fame. Null if the player has not been inducted into the HOF yet.\\nGame Stats Fields\\nNote that if there are games missing in the season for a player (i.e. the player has logs for games 1, 2, 3, 5, 6,...), then they didn\\'t play in game 4 because of injury, suspension, etc.\\nGame Info:\\nPlayer ID: The assigned ID for the player.\\nYear: The year the game took place.\\nDate: The date the game took place.\\nGame Number: The number of the game when all games in a season are numbered sequentially.\\nAge: The age of the player when the game was played. This is in the format -. So 22-344 would be 22 years and 344 days old.\\nTeam: The three-letter code of the team the player played for.\\nGame Location: One of H, A, or N. H=Home, A=Away, and N=Neutral.\\nOpponent: The three-letter code of the team the game was played against.\\nPlayer Team Score: The score of the team the player played for.\\nOpponent Score: The score of the team the player played against. You can use this field and the last field to determine if the player\\'s team won.\\nPassing Stats:\\nPassing Attempts: The number of passes thrown by the player.\\nPassing Completions: The number of completions thrown by the player.\\nPassing Yards: The number of passing yards thrown by the player.\\nPassing Rating: The NFL passer rating for the player in that game.\\nPassing Touchdowns: The number of passing touchdowns the player threw.\\nPassing Interceptions: The number of interceptions the player threw.\\nPassing Sacks: The number of times the player was sacked.\\nPassing Sacks Yards Lost: The cumulative yards lost from the player being sacked.\\nRushing Stats:\\nRushing Attempts: The number of times the the player attempted a rush.\\nRushing Yards: The number of yards the player rushed for.\\nRushing Touchdowns: The number of touchdowns the player rushed for.\\nReceiving Stats:\\nReceiving Targets: The number of times the player was thrown to.\\nReceiving Receptions: The number of times the player caught a pass thrown to them.\\nReceiving Yards: The number of yards the player gained through receiving.\\nReceiving Touchdowns: The number of touchdowns scored through receiving.\\nKick/Punt Return Stats\\nKick Return Attempts: The number of times the player attempted to return a kick.\\nKick Return Yards: The cumulative number of yards the player returned kicks for.\\nKick Return Touchdowns: The number of touchdowns the player scored through kick returns.\\nPunt Return Attempts: The number of times the player attempted to return a punt.\\nPunt Return Yards: The cumulative number of yards the player returned punts for.\\nPunt Return Touchdowns: The number of touchdowns the player scored through punt returns.\\nKick/Punt Stats\\nPoint After Attempts: The number of PAs the player attempted kicking.\\nPoint After Makes: The number of PAs the player made.\\nField Goal Attempts: The number of field goals the player attempted.\\nField Goal Makes: The number of field goals the player made.\\nDefense Stats\\nSacks: The number of sacks the player got.\\nTackles: The number of tackles the player got.\\nTackle Assists: The number of tackles the player assisted on.\\nInterceptions: The number of times the player intercepted the ball.\\nInterception Yards: The number of yards the player gained after interceptions.\\nInterception Touchdowns: The number of touchdowns the player scored after interceptions.\\nSafeties: The number of safeties the player caused.\\nFutute Improvements\\nFormat data in an SQLite database\\nExtract Team IDs to make relating players across teams and games easier\\nResolve Game IDs to make relating players in a given game easier\\nScrape college data (there are links on the website that shouldn\\'t be too difficult to scrape)\\nFigure out another method of scraping some additional data that isn\\'t available on pro-football-reference.com, such as fumbles, passes defended, etc.\\nResolve blocking stats back to lineman based on the team they played for and the QB\\'s sack stats for that game.\\nContributing\\nIf you would like to contribute, please feel free to put up a PR or reach out to me with ideas. I would love to collaborate with some fellow football fans on this project.\\nIf you\\'re interested in collaborating, the repository can be found here.\\nConnect with me\\nIf you\\'d like to collaborate on a project, learn more about me, or just say hi, feel free to contact me using any of the social channels listed below.\\nPersonal Website\\nEmail\\nLinkedIn\\nTwitter\\nMedium\\nQuora\\nHackerNews\\nReddit\\nKaggle\\nInstagram\\n500px',\n",
       " \"Context\\nWikipedia, the world's largest encyclopedia, is a crowdsourced open knowledge project and website with millions of individual web pages. This dataset is a grab of the title of every article on Wikipedia as of September 20, 2017.\\nContent\\nThis dataset is a simple newline (\\\\n) delimited list of article titles. No distinction is made between redirects (like Schwarzenegger) and actual article pages (like Arnold Schwarzenegger).\\nAcknowledgements\\nThis dataset was created by scraping Special:AllPages on Wikipedia. It was originally shared here.\\nInspiration\\nWhat are common article title tokens? How do they compare against frequent words in the English language?\\nWhat is the longest article title? The shortest?\\nWhat countries are most popular within article titles?\",\n",
       " 'Context\\nWhat distinguishes the great from the good, the remembered from the accomplished, and the genius from the merely brilliant? Scrapping English Wikipedia, Joseph Philleo has cleaned and compiled a database of more than 8,500 famous mathematicians for the Kaggle data science community to analyze and better understand.\\nInspiration\\nWhat are the common characteristics of famous mathematicians?\\nHow old do they live, which fields do they work in, where are they born, and where do they live?\\nCan you predict which mathematicians will win a Fields Medal, join the Royal Society, or secure tenure at Harvard?',\n",
       " \"All BPD data on Open Baltimore is preliminary data and subject to change. The information presented through Open Baltimore represents Part I victim based crime data. The data do not represent statistics submitted to the FBI's Uniform Crime Report (UCR); therefore any comparisons are strictly prohibited. For further clarification of UCR data, please visit http://www.fbi.gov/about-us/cjis/ucr/ucr. Please note that this data is preliminary and subject to change. Prior month data is likely to show changes when it is refreshed on a monthly basis. All data is geocoded to the approximate latitude/longitude location of the incident and excludes those records for which an address could not be geocoded. Any attempt to match the approximate location of the incident to an exact address is strictly prohibited.\\nAcknowledgements\\nThis dataset was kindly made available by the City of Baltimore. You can find the original dataset, which is updated regularly, here.\",\n",
       " \"This dataset contains hourly estimates of an area's energy potential for 1986-2015 as a percentage of a power plant's maximum output.\\nThe overall scope of EMHIRES is to allow users to assess the impact of meteorological and climate variability on the generation of solar power in Europe and not to mime the actual evolution of wind power production in the latest decades. For this reason, the hourly wind power generation time series are released for meteorological conditions of the years 1986-2015 (30 years) without considering any changes in the wind installed capacity. Thus, the installed capacity considered is fixed as the one installed at the end of 2015. For this reason, data from EMHIRES should not be compared with actual power generation data other than referring to the reference year 2015.\\nContent\\nThe data is available at both the national level and the NUTS 2 level. The NUTS 2 system divides the EU into 276 statistical units.\\nPlease see the manual for the technical details of how these estimates were generated.\\nThis product is intended for policy analysis over a wide area and is not the best for estimating the output from a single system. Please don't use it commercially.\\nAcknowledgements\\nThis dataset was kindly made available by the European Commission's STETIS program. You can find the original dataset here.\\nInspiration\\nHow clean is the dataset?\\nWhat does a typical year look like? One common approach is to stitch together 12 months of raw data, using the 12 most typical months per this ISO standard.\\nCan you identify more useful geographical areas for this sort of analysis, such as valleys that would share similar wind patterns?\\nIf you like\\nIf you like this dataset, you might also enjoy: - 30 years of European solar - Google's Project Sunroof data\",\n",
       " 'Context\\nThis corpus consists of truthful and deceptive hotel reviews of 20 Chicago hotels. The data is described in two papers according to the sentiment of the review. In particular, we discuss positive sentiment reviews in [1] and negative sentiment reviews in [2]. While we have tried to maintain consistent data preprocessing procedures across the data, there are differences which are explained in more detail in the associated papers. Please see those papers for specific details.\\nContent\\nThis corpus contains:\\n400 truthful positive reviews from TripAdvisor (described in [1])\\n400 deceptive positive reviews from Mechanical Turk (described in [1])\\n400 truthful negative reviews from Expedia, Hotels.com, Orbitz, Priceline, TripAdvisor and Yelp (described in [2])\\n400 deceptive negative reviews from Mechanical Turk (described in [2])\\nEach of the above datasets consist of 20 reviews for each of the 20 most popular Chicago hotels (see [1] for more details). The files are named according to the following conventions: Directories prefixed with fold correspond to a single fold from the cross-validation experiments reported in [1] and [2].\\nHotels included in this dataset\\naffinia: Affinia Chicago (now MileNorth, A Chicago Hotel)\\nallegro: Hotel Allegro Chicago - a Kimpton Hotel\\namalfi: Amalfi Hotel Chicago\\nambassador: Ambassador East Hotel (now PUBLIC Chicago)\\nconrad: Conrad Chicago\\nfairmont: Fairmont Chicago Millennium Park\\nhardrock: Hard Rock Hotel Chicago\\nhilton: Hilton Chicago\\nhomewood: Homewood Suites by Hilton Chicago Downtown\\nhyatt: Hyatt Regency Chicago\\nintercontinental: InterContinental Chicago\\njames: James Chicago\\nknickerbocker: Millennium Knickerbocker Hotel Chicago\\nmonaco: Hotel Monaco Chicago - a Kimpton Hotel\\nomni: Omni Chicago Hotel\\npalmer: The Palmer House Hilton\\nsheraton: Sheraton Chicago Hotel and Towers\\nsofitel: Sofitel Chicago Water Tower\\nswissotel: Swissotel Chicago\\ntalbott: The Talbott Hotel\\nReferences\\n[1] M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011. Finding Deceptive Opinion Spam by Any Stretch of the Imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.\\n[2] M. Ott, C. Cardie, and J.T. Hancock. 2013. Negative Deceptive Opinion Spam. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\\nAcknowledgements\\nIf you use any of this data in your work, please cite the appropriate associated paper (described above). Please direct questions to Myle Ott (myleott@cs.cornell.edu).',\n",
       " 'Context:\\nFraudulent e-mails contain criminally deceptive information, usually with the intent of convincing the recipient to give the sender a large amount of money. Perhaps the best known type of fraudulent e-mails is the Nigerian Letter or “419” Fraud.\\nContent:\\nThis dataset is a collection of more than 2,500 \"Nigerian\" Fraud Letters, dating from 1998 to 2007.\\nThese emails are in a single text file. Each e-mail has a header which includes the following information:\\nReturn-Path: address the email was sent from\\nX-Sieve: the X-Sieve host (always cmu-sieve 2.0)\\nMessage-Id: a unique identifier for each message\\nFrom: the message sender (sometimes blank)\\nReply-To: the email address to which replies will be sent\\nTo: the email address to which the e-mail was originally set (some are truncated for anonymity)\\nDate: Date e-mail was sent\\nSubject: Subject line of e-mail\\nX-Mailer: The platform the e-mail was sent from\\nMIME-Version: The Multipurpose Internet Mail Extension version\\nContent-Type: type of content & character encoding\\nContent-Transfer-Encoding: encoding in bits\\nX-MIME-Autoconverted: the type of autoconversion done\\nStatus: r (read) and o (opened)\\nAcknowledgements:\\nIf you use this collection of fraud email in your research, please include the following citation in any resulting papers:\\nRadev, D. (2008), CLAIR collection of fraud email, ACL Data and Code Repository, ADCR2008T001, http://aclweb.org/aclwiki\\nInspiration:\\nThis dataset contains fraudulent e-mails sent over a period of years. Has the language used in fraudulent E-mails changed over time?\\nAre there any words or phrases that are particularly common in this type of e-mail? (You might compare it with the Enron email corpus, linked below)\\nRelated datasets:\\nhttps://www.kaggle.com/wcukierski/enron-email-dataset\\nhttps://www.kaggle.com/uciml/sms-spam-collection-dataset',\n",
       " 'Context:\\nWhen children are born they don’t know any words. By the time they’re three, most children know 200 words or more. These words aren’t randomly selected from the language they’re learning, however. A two-year-old is much more likely to know the word “bottle” than the word “titration”. What words do children learn first, and what qualities do those words have? This dataset was collected to explore this question.\\nContent:\\nThe main dataset includes information for 732 Norwegian words. A second table also includes measures of how frequently each word is used in Norwegian, both on the internet (as observed in the Norwegian Web as Corpus dataset) and when an adult is talking to a child. The latter is commonly called “child directed speech” and is abbreviated as “CDS”.\\nMain data\\nID_CDI_I: Word ID from the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories, version 1\\nID_CDI_II: Word ID from the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories, version 2\\nWord_NW: The word in Norwegian\\nWord_CDI: The form of the word found in the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories\\nTranslation: the English translation of the Norwegian word\\nAoA: how old a child generally is was when they this this word, in months (Estimated from the MacArthur-Bates Communicative Development Inventories)\\nVSoA: how many other words a child generally knows when they learn this word (rounded up to the nearest 10)\\nLex_cat: the specific part of speech of the word\\nBroad_lex: the broad part of speech of the word\\nFreq: a measure of how commonly this word occurs in Norwegian\\nCDS_Freq: a measure of how commonly this word occurs when a Norwegian adult is talking to a Norwegian child\\nNorwegian CDS Frequency\\nWord_CDI: The word from, as found in the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories\\nTranslation: The English translation of the Norwegian word\\nFreq_NoWaC: How often this word is used on the internet\\nFreq_CDS: How often this word is used when talking to children (based on two Norwegian CHILDES corpora)\\nAcknowledgements:\\nThis dataset was collected by Pernille Hansen. If you use this data, please cite the following paper:\\nHansen (2016). What makes a word easy to acquire? The effects of word class, frequency, imageability and phonological neighbourhood density on lexical development. First Language. Advance online publication. doi: 10.1177/0142 723716679956 http://dx.doi.org/10.1177/0142723716679956\\nInspiration:\\nHow well can you predict which words a child will learn first?\\nAre some sounds or letters found more often than chance in words learned early?\\nCan you build topic models on earlier-acquired and later-acquired words? Which topics are over-represented in words learned very early?',\n",
       " 'Context:\\nWord vectors, also called word embeddings, are a multi-dimensional representation of words based on which words are used in similar contexts. They can capture some elements of words’ meanings. For example, documents which use a lot of words that are clustered together in a vector space representation are more likely to be on similar topics.\\nWord vectors are very computationally intensive to train, and the vectors themselves will vary based on the documents or corpora they are trained on. For these reasons, it is often convenient to use word vectors which have been pre-trained rather than training them from scratch for each project.\\nContent:\\nThis dataset contains 1,000,653 word embeddings of dimension 300 trained on the Spanish Billion Words Corpus. These embeddings were trained using word2vec.\\nParameters for Embeddings Training:\\nWord embeddings were trained using the following parameters:\\nThe selected algorithm was the skip-gram model with negative-sampling.\\nThe minimum word frequency was 5.\\nThe amount of “noise words” for the negative sampling was 20.\\nThe 273 most common words were downsampled.\\nThe dimension of the final word embedding was 300.\\nThe original corpus had the following amount of data:\\nA total of 1420665810 raw words.\\nA total of 46925295 sentences.\\nA total of 3817833 unique tokens.\\nAfter the skip-gram model was applied, filtering of words with less than 5 occurrences as well as the downsample of the 273 most common words, the following values were obtained:\\nA total of 771508817 raw words.\\nA total of 1000653 unique tokens.\\nAcknowledgements:\\nThis dataset was created by Cristian Cardellino. If you use this dataset in your work, please reference the following citation:\\nCristian Cardellino: Spanish Billion Words Corpus and Embeddings (March 2016), http://crscardellino.me/SBWCE/\\nInspiration:\\nWord vector representations are widely-used in natural language processing tasks.\\nCan you improve an existing part of speech tagger in Spanish by using word vectors? You might find it helpful to check out this paper.\\nCan you use these word embeddings to improve existing parsers for Spanish? This paper outlines some approaches for this.\\nThere has been quite a bit of work recently on how word embeddings might encode implicit gender bias. For example, this paper show how embeddings can capture stereyoptyes about career fields and gender. However, this recent paper suggests that for languages with grammatical gender (like Spanish), grammatical gender is more influential than gender bias. Do these word embeddings support that claim?\\nYou may also like:\\nGloVe: Global Vectors for Word Representation. Pre-trained English word vectors from Wikipedia 2014 + Gigaword 5\\n20 Million Word Spanish Corpus. The Spanish Language portion of the Wikicorpus (v 1.0)',\n",
       " \"Context\\nWe are building a data set that can be used for building useful reports, understanding the difference between data and information, and multivariate analysis. The data set we are building is similar to that used in several academic reports and what may be found in ERP HR subsystems.\\nWe will update the sample data set as we gain a better understanding of the data elements using the calculations that exist in scholarly journals. Specifically, we will use the correlation tables to rebuild the data sets.\\nContent\\nThe fields represent a fictitious data set where a survey was taken and actual employee metrics exist for a particular organization. None of this data is real.\\nAcknowledgements\\nWe wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\\nPrabhjot Singh contributed a portion of the data (the columns on the right before the survey data was added). https://www.kaggle.com/prabhjotindia https://www.kaggle.com/prabhjotindia/visualizing-employee-data/data\\nAbout this Dataset Why are our best and most experienced employees leaving prematurely? Have fun with this database and try to predict which valuable employees will leave next. Fields in the dataset include:\\nSatisfaction Level Last evaluation Number of projects Average monthly hours Time spent at the company Whether they have had a work accident Whether they have had a promotion in the last 5 years Departments Salary\\nInspiration\\nYour data will be in front of the world's largest data science community. What questions do you want to see answered?\",\n",
       " \"Context\\nStock market data -- and particularly intraday price data -- can be very expensive to buy. To help more people gain access to it, here I provide daily as well as intraday price and volume data for all U.S.-based stocks and ETFs trading on the NYSE, NASDAQ, and NYSE MKT.\\nContent\\nThe dataset (last updated 12/06/2017) is presented in CSV format as follows:\\nIntraday data: Date,Time,Open,High,Low,Close,Volume,OpenInt\\nDaily data: Date,Open,High,Low,Close,Volume,OpenInt\\nAcknowledgements\\nThe dataset belongs to me. I’m sharing it here for free. You may do with it as you wish.\\nInspiration\\nMany have tried, but most have failed, to predict the stock market's ups and downs. Can you do any better?\",\n",
       " 'Context\\nI collected this data from incubators and accelerators to find out what they have been talking about in 2017.\\nContent\\nThe data contains the twitter usernames of various organizations and tweets for the year 2017 collected on 28th Dec 2017.\\nAcknowledgements\\nMuch appreciation to @emmanuelkens for helping in thinking through this\\nInspiration\\nI am very curious to find out what the various organizations have been talking about in 2017. I would also like to find out the most popular organization by tweets and engagement. I am also curious to find out if there is any relationship between the number of retweets a tweet gets and the time of day it was posted!',\n",
       " \"Context\\nThis database is managed by the US Environmental Protection Agency and contains information reported annually by some industry groups as well as federal facilities. Each year, companies across a wide range of industries (including chemical, mining, paper, oil and gas industries) that produce more than 25,000 pounds or handle more than 10,000 pounds of a listed toxic chemical must report it to the TRI. The TRI threshold was initially set at 75,000 pounds annually. If the company treats, recycles, disposes, or releases more than 500 pounds of that chemical into the environment (as opposed to just handling it), then they must provide a detailed inventory of that chemical's inventory.\\nContent\\nThere are roughly 100 columns in this dataset; please see the tri_basic_data_file_format_v15.pdf for details. You may also wish to consult factors_to_consider_6.15.15_final.pdf for general background about interpreting the data.\\nI've merged all of the TRI basic data files into a single large csv. You will probably need to process it in batches or use a tool like Dask to stay within kernel memory limits.\\nPlease note that the 2016 data remains preliminary at the time of this release.\\nAcknowledgements\\nThis dataset was released by the US EPA. You can find the original dataset, more detailed versions of the data, and a great deal of background information here: https://www.epa.gov/toxics-release-inventory-tri-program/tri-data-and-tools\\nInspiration\\nThe EPA runs an annual university contest. Their list of previous winners contains a lot of great ideas that people have had for this dataset in the past. The 2017 competition is already over, but you can find the rules here.\",\n",
       " \"Context\\nWeb data extraction or web scraping can be a great business tool for trend spotting via media monitoring. Essentially, leading media outlets can be tracked to unveil the top buzzwords and the number of mentions companies (including their products) garner over specific time period. We wanted to apply this method to understand the tech landscape and its coverage in 2017. Hence, we deployed PromptCloud’s in-house web crawler to extract the article titles from two popular outlets (TechCrunch and VentureBeat) and performed text mining on the dataset to uncover the top buzzwords, companies and products.\\nContent\\nThe dataset contains following 3 fields:\\nURL\\nTitle\\nDate of publication\\nAcknowledgements\\nThis dataset was created by using PromptCloud's in-house web scraping service.\\nInspiration\\nInitial Analysis can be found here. It includes the following findings:\\nTop companies/products that were covered by media over the year\\nTop tech trends over the year\",\n",
       " 'The Freight Analysis Framework (FAF) integrates data from a variety of sources to create a comprehensive picture of freight movement among states and major metropolitan areas by all modes of transportation. Starting with data from the 2012 Commodity Flow Survey (CFS) and international trade data from the Census Bureau, FAF incorporates data from agriculture, extraction, utility, construction, service, and other sectors. FAF version 4 (FAF4) provides estimates for tonnage (in thousand tons) and value (in million dollars) by regions of origin and destination, commodity type, and mode. Data are available for the base year of 2012, the recent years of 2013 - 2015, and forecasts from 2020 through 2045 in 5-year intervals.\\nInspiration\\nThis dataset should be great for map-based visualizations.',\n",
       " \"Context\\nThe online job market is a good indicator of overall demand for labor in an economy. This dataset consists of 19,000 job postings from 2004 to 2015 posted on CareerCenter, an Armenian human resource portal. Since postings are text documents and tend to have similar structures, text mining can be used to extract features like posting date, job title, company name, job description, salary, and more. Postings that had no structure or were not job-related were removed. The data was originally scraped from a Yahoo! mailing group.\\nInspiration\\nStudents, job seekers, employers, career advisors, policymakers, and curriculum developers use online job postings to explore the nature of today's dynamic labor market. This dataset can be used to:\\nUnderstand the demand for certain professions, job titles, or industries\\nIdentify skills that are most frequently required by employers, and how the distribution of necessary skills changes over time\\nHelp education providers with curriculum development\\nAcknowledgements\\nThe data collection and initial research were funded by the American University of Armenia’s research grant (2015).\\nHabet Madoyan, CEO at Datamotus, compiled this dataset and has granted us permission to republish. The republished dataset is identical to the original dataset, which can be found here. Datamotus also published a report detailing the text mining techniques used, plus analyses and visualizations of the data.\",\n",
       " 'Content\\nThis dataset documents all United Nations General Assembly votes since its establishment in 1946. The data is broken into three different files: the first lists each UN resolution, subject, and vote records; the second records individual member state votes per resolution; and the third provides an annual summary of member state voting records with affinity scores and an ideal point estimate in relation to the United States.\\nAcknowledgements\\nThe UN General Assembly voting data was compiled and published by Professor Erik Voeten of Georgetown University.',\n",
       " 'Context\\nWhat did the expansion of the London Underground, the world’s first underground railway which opened in 1863, look like? What about the transportation system in your home city? Citylines collects data on transportation lines across the world so you can answer questions like these and more.\\nContent\\nThis dataset, originally shared and updated here, includes transportation line data from a number of cities from around the world including London, Berlin, Mexico City, Barcelona, Washington D.C., and others covering many thousands of kilometers of lines.\\nInspiration\\nYou can explore geometries to generate maps and even see how lines have changed over time based on historical records. Want to include shapefiles with your analysis? Simply publish a shapefile dataset here and then create a new kernel (R or Python script/notebook), adding your shapefile as an additional datasource.',\n",
       " 'The Financial Statement Data Sets below provide numeric information from the face financials of all financial statements. This data is extracted from exhibits to corporate financial reports filed with the Commission using eXtensible Business Reporting Language (XBRL). As compared to the more extensive Financial Statement and Notes Data Sets, which provide the numeric and narrative disclosures from all financial statements and their notes, the Financial Statement Data Sets are more compact.\\nThe information is presented without change from the \"as filed\" financial reports submitted by each registrant. The data is presented in a flattened format to help users analyze and compare corporate disclosure information over time and across registrants. The data sets also contain additional fields including a company\\'s Standard Industrial Classification to facilitate the data\\'s use.\\nContent\\nEach quarter\\'s data is stored as a json of the original text files. This was necessary to limit the overall number of files. The num.txt file will likely be of most interest.\\nAcknowledgements\\nThis dataset was kindly made available by the SEC. You can find the original dataset, which is updated quarterly, here.',\n",
       " 'We are updating this dataset everymonth. Access updated data for every month here\\nContext\\nThe dataset is proprietary data of Yun Solutions, collected from Beta Testing phase. The dataset contains sensor and OBD data for over 4 months and around 30 vehicles.\\nContent\\nThe Dataset contains Vehicle telematics and Driving data. Metadata is explained in the file description on the data tab.\\nAcknowledgements\\nWe look forward to analysis on the data.\\nInspiration\\nWe want to make this data accessible for learning and analysis.',\n",
       " 'Context\\nMost countries of the world define poverty as a lack of money. Yet poor people themselves consider their experience of poverty much more broadly. A person who is poor can suffer from multiple disadvantages at the same time – for example they may have poor health or malnutrition, a lack of clean water or electricity, poor quality of work or little schooling. Focusing on one factor alone, such as income, is not enough to capture the true reality of poverty.\\nMultidimensional poverty measures can be used to create a more comprehensive picture. They reveal who is poor and how they are poor – the range of different disadvantages they experience. As well as providing a headline measure of poverty, multidimensional measures can be broken down to reveal the poverty level in different areas of a country, and among different sub-groups of people.\\nContent\\nOPHI researchers apply the AF method and related multidimensional measures to a range of different countries and contexts. Their analyses span a number of different topics, such as changes in multidimensional poverty over time, comparisons in rural and urban poverty, and inequality among the poor. For more information on OPHI’s research, see our working paper series and research briefings.\\nOPHI also calculates the Global Multidimensional Poverty Index MPI, which has been published since 2010 in the United Nations Development Programme’s Human Development Report. The Global MPI is an internationally-comparable measure of acute poverty covering more than 100 developing countries. It is updated by OPHI twice a year and constructed using the AF method.\\nThe Alkire Foster (AF) method is a way of measuring multidimensional poverty developed by OPHI’s Sabina Alkire and James Foster. Building on the Foster-Greer-Thorbecke poverty measures, it involves counting the different types of deprivation that individuals experience at the same time, such as a lack of education or employment, or poor health or living standards. These deprivation profiles are analysed to identify who is poor, and then used to construct a multidimensional index of poverty (MPI). For free online video guides on how to use the AF method, see OPHI’s online training portal.\\nTo identify the poor, the AF method counts the overlapping or simultaneous deprivations that a person or household experiences in different indicators of poverty. The indicators may be equally weighted or take different weights. People are identified as multidimensionally poor if the weighted sum of their deprivations is greater than or equal to a poverty cut off – such as 20%, 30% or 50% of all deprivations.\\nIt is a flexible approach which can be tailored to a variety of situations by selecting different dimensions (e.g. education), indicators of poverty within each dimension (e.g. how many years schooling a person has) and poverty cut offs (e.g. a person with fewer than five years of education is considered deprived).\\nThe most common way of measuring poverty is to calculate the percentage of the population who are poor, known as the headcount ratio (H). Having identified who is poor, the AF method generates a unique class of poverty measures (Mα) that goes beyond the simple headcount ratio. Three measures in this class are of high importance:\\nAdjusted headcount ratio (M0), otherwise known as the MPI: This measure reflects both the incidence of poverty (the percentage of the population who are poor) and the intensity of poverty (the percentage of deprivations suffered by each person or household on average). M0 is calculated by multiplying the incidence (H) by the intensity (A). M0 = H x A.\\nFind out about other ways the AF method is used in research and policy.\\nAdditional data here.\\nAcknowledgements\\nAlkire, S. and Robles, G. (2017). “Multidimensional Poverty Index Summer 2017: Brief methodological note and results.” OPHI Methodological Note 44, University of Oxford.\\nAlkire, S. and Santos, M. E. (2010). “Acute multidimensional poverty: A new index for developing countries.” OPHI Working Papers 38, University of Oxford.\\nAlkire, S. Jindra, C. Robles, G. and Vaz, A. (2017). ‘Multidimensional Poverty Index – Summer 2017: brief methodological note and results’. OPHI MPI Methodological Notes No. 44, Oxford Poverty and Human Development Initiative, University of Oxford.\\nInspiration\\nWhich countries exhibit the largest subnational disparities in MPI?\\nWhich countries have high per-capita incomes yet still rank highly in MPI?',\n",
       " 'Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nWhen India got independence from British in 1947 the literacy rate was 12.2% and as per the recent census 2011 it is 74.0%. Although it looks an accomplishment, still many people are there without access to education.\\nIt would be interesting to know the current status of the Indian education system.\\nContent\\nThis dataset contains district and state wise Indian primary and secondary school education data for 2015-16.\\nGranularity: Annual\\nList of files:\\n2015_16_Districtwise.csv ( 680 observations and 819 variables )\\n2015_16_Statewise_Elementary.csv ( 36 observations and 816 variables )\\n2015_16_Statewise_Secondary.csv ( 36 observations and 630 variables )\\nAcknowledgements\\nMinistry of Human Resource Development (DISE) has shared the dataset here and also published some reports.\\nInspiration\\nThis dataset provides the complete information about primary and secondary education. There are many inferences can be made from this dataset. There are few things I would like to understand from this dataset.\\nDrop out ratio in primary and secondary education. (Govt. has made law that every child under age 14 should get free compulsary education.)\\nVarious factors affecting examination results of the students.\\nWhat are all the factors that makes the difference (in literacy rate) between Kerala and Bihar?\\nWhat could be done to improve the female literacy rate and literacy rate in rural area?',\n",
       " 'Context\\nInformation reproduced from the National Archives:\\n\"The Vietnam Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of 58,220 U.S. military fatal casualties of the Vietnam War. These records were transferred into the custody of the National Archives and Records Administration in 2008. The earliest casualty record contains a date of death of June 8, 1956, and the most recent casualty record contains a date of death of May 28, 2006. The Defense Casualty Analysis System Extract Files were created by the Defense Manpower Data Center (DMDC) of the Office of the Secretary of Defense. The records correspond to the Vietnam Conflict statistics on the DMDC web site, which is accessible online at https://www.dmdc.osd.mil/dcas/pages/main.xhtml .\\nA full series description for the Defense Casualty Analysis System (DCAS) Extract Files is accessible online via the National Archives Catalog under the National Archives Identifier 2163536. The Vietnam Conflict Extract Data File is also accessible for direct download via the National Archives Catalog file-level description, National Archives Identifier 2240992. \"\\nContent\\nThe raw data files have been cleaned and labelled as best as I can with reference to the accompanying Supplemental Code Lists. Names and ID numbers have been removed out of respect and to provide anonymity.\\nAcknowledgements\\nData provided by The U.S. National Archives and Records Administration.\\nRaw data can be accessed via the following link: https://catalog.archives.gov/id/2240992\\nInspiration\\nBy cleaning the data I hope to give wider access to this resource.',\n",
       " 'Context\\nThis dataset was obtained from Facebook groups as part of my postgraduate thesis. The objective of the thesis was to extract posts from groups related to rare diseases and compare them with the Spanish association of rare diseases (FEDER). If you want to use this open dataset or the code you should cite our paper:\\nReguera, N., Subirats, L., Armayones, M. Mining Facebook data of people with rare diseases. IEEE Computer-Based Medical Systems (IEEE CBMS 2017), Thessaloniki, Greece, 22-24th June, 2017.\\nContent\\nThe file contains information 3917 records from 5 Facebook groups and was extracted using Netvizz. The posts were generated since each group started (as far as 2009) until the 26/11/2016.\\nThe content is as follows:\\ntype: Facebook\\'s post classification (e.g. photo, status, etc.)\\nby: either\"post_page_pageid\" (post by page) or \"post_user_pageid\" (post by user);\\npost_id: id of the post;\\npost_link: direct link to the post;\\npost_message: text of the post;\\npicture: the picture scraped from any link included with the post;\\nfull_picture: the picture scraped from any link included with the post (full size);\\nlink: link URL (if the post points to external content);\\nlink_domain: domain name of link;\\npost_published: publishing date\\npost_published_unix: publishing date as Unix timestamp (for easy conversion and ranking);\\npost_published_sql: publishing date in SQL format (some analysis tools prefer this);\\nlikes_count_fb: Facebook provided like count for posts;\\ncomments_count_fb: Facebook provided comment count for posts;\\nreactions_count_fb: Facebook provided reactions count for posts (includes likes);\\nshares_count_fb: Facebook provided share count for posts;\\nengagement_fb: sum of comment, reaction, and share counts;\\nAcknowledgements\\nI would like to thank my thesis directors, who guided me through all the process: Laia Subirats Maté and Manuel Armayones Ruiz.\\nInspiration\\nDuring my research (code available on https://github.com/natt77/UOC---TFP) several insights were found on the relation between the information posted on the gropus and the information in FEDER. Text analytics was performed together with sentiment analysis. It would be interesting to deepen the analysis, create models to predict engagement or any other action that can help improving the life quality of people with rare diseases.',\n",
       " 'Content\\nThe data comes from the Vancouver Open Data Catalogue. It was extracted on 2017-07-18 and it contains 530,652 records from 2003-01-01 to 2017-07-13. The original data set contains coordinates in UTM Zone 10 (columns X and Y). I also included Latitude and Longitude, which I converted using this spreadsheet that can be found here.\\nThere\\'s also a Google Trends data that shows how often a search-term is entered relative to the total search-volume. From Google Trends:\\n\"Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. Likewise a score of 0 means the term was less than 1% as popular as the peak.\"\\nOriginal data for search term \"crime\" location British Columbia: https://trends.google.com/trends/explore?date=2004-01-01%202017-06-30&geo=CA-BC&q=crime\\nAcknowledgements\\nPhoto By Charles de Jesus [CC BY 3.0 (http://creativecommons.org/licenses/by/3.0)], via Wikimedia Commons',\n",
       " 'Context\\nAn interview was videotaped to analyze the facial expressions of emotion. This interview lasts for 21:30 minutes.\\nContent\\nAn interview with a duration of 21:30 minutes was videotaped. Emotional facial expressions were analyzed every 0.12 seconds with FaceReader software. Emotions were neutral, joy, fear, anger, surprise, fear and contemp, positive-negative valences, arousal, degrees of head direction, facial action units, among others.\\nAcknowledgements\\nAcknowledgements to The Clinic of Borderline Personality Disorder for contributing the material, also to the Laboratory of Chronoecology and Human Ethology.\\nInspiration\\nHow does the direction of the gaze relate to the expressions of emotions?\\nFrom what score of any emotion is considered open, closed and / or neutral in the right eye, left eye, right eyebrow and left eyebrow?\\nIn addition to descriptive statistical analyzes, what other analyzes can be performed?',\n",
       " \"Context\\nRio de Janeiro is one of the most beautiful and famous city in the world. Unfortunately, it's also one of the most dangerous. For the last years, in a scenario of economical and political crisis in Brazil, the State of Rio de Janeiro was one of the most affected. Since 2006, the Instituto de Segurança Pública do Rio de Janeiro (Institue of Public Security of Rio de Janeiro State) publishes reports of each police station.\\nContent\\nThree datasets are available: BaseDPEvolucaoMensalCisp - Monthly evolution of statistics by police station PopulacaoEvolucaoMensalCisp - Monthly evolution of population covered by police station delegacias - Info about each police station\\nMost of the data are in Brazilian Portuguese because it was extracted directly from government sites.\\nAcknowledgements\\nThis dataset is provided by the Instituto de Segurança Pública. delegacias.csv was compiled by myself.\\nInspiration\\nWhat is the most unsafe city in Rio de Janeiro State? And the safest? Which events can be correlated with the numbers in dataset? (Elections, crisis...) How crime correlates with population?\",\n",
       " 'This dataset was created to as part of an ongoing research on what information about the human mind could be possibly hidden in word-word associations.\\nInspiration\\nHere are a few questions you might try to answer with this dataset:\\nHow well can we classify word pairs as originating from a specific online community?\\nWhat are the properties of the graph/network of word associations?\\nWhat are frequently used words? Are there differences among communities?\\nContent\\nThe data was scraped from 10 public internet forums. The data is anonymous (all usernames are converted to unique user IDs) and cleaned[script].\\nMost topics were still active at the time of scraping (june 2017), thus rescraping will result in a (slightly) bigger dataset.\\nThe dataset contains 4 columns, {author, word1, word2, source}, where author is the person who wrote the second word as reaction to the first word. A word can also be a phrase or (in some cases) a sentence.',\n",
       " 'The National Oceanic and Atmospheric Administration (NOAA) National Status and Trends (NS&T) Mussel Watch Program is a contaminant monitoring program that started in 1986. It is the longest running continuous contaminant monitoring program of its kind in the United States. Mussel Watch monitors the concentration of contaminants in bivalves (mussels and oysters) and sediments in the coastal waters of the U.S., including the Great Lakes, to monitor bivalve health and by extension the health of their local and regional environment.\\nMussel Watch consults with experts to determine appropriate contaminants to monitor; these include dichlorodiphenyltrichloroethane (DDT), polycyclic aromatic hydrocarbons (PAHs), and polychlorinated biphenyls (PCBs). As of 2008, Mussel Watch monitors approximately 140 analytes. In addition to the effects of contaminants, Mussel Watch is able to assess the effects of natural disasters, such as the 2005 Hurricane Katrina, and environmental disasters, such as the 2010 Deepwater Horizon oil spill. Data collected by Mussel Watch can also be used to monitor the effectiveness of coastal remediation. The Mussel Watch Program utilized its 20 years of monitoring data to effectively analyze the impacts of Hurricane Katrina and has affected regulatory decisions based on the data it has collected on bivalve parasites.\\nYou can find additional details about the history of the program here.\\nData Notes\\nThis version has been consolidated and lightly cleaned from its original format.\\nIt was not possible to acquire data for all sites in mussel watch while preparing the dataset.\\nThe pdf manuals are technically for specific sites and may not map perfectly to the data here. You can find manuals specific to each site here if need be.\\nAcknowledgements\\nThis dataset is the result of the work of generations of scientists working for NOAA. You can find the original data here.',\n",
       " \"Context\\nThe Leipzig Corpora Collection presents corpora in different languages using the same format and comparable sources. All data are available as plain text files and can be imported into a MySQL database by using the provided import script. They are intended both for scientific use by corpus linguists as well as for applications such as knowledge extraction programs.\\nContent\\nThis dataset contains 3 million sentences taken from newspaper texts in 2015. Non-sentences and foreign language material was removed. In addition to the sentences themselves, this dataset contains information on the frequency of each word. More information about the format and content of these files can be found here.\\nThe corpora are automatically collected from carefully selected public sources without considering in detail the content of the contained text. No responsibility is taken for the content of the data. In particular, the views and opinions expressed in specific parts of the data remain exclusively with the authors.\\nAcknowledgements\\nThis dataset is released under a CC-BY 4.0 license. If you use this dataset in your work, please cite the following paper:\\nD. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages. In: Proceedings of the 8th International Language Resources and Evaluation (LREC'12), 2012\",\n",
       " \"Context:\\nPubs, or public houses, are popular traditional British gathering places where alcohol and food is served.\\nContent:\\nThis dataset includes information on 51,566 pubs. This dataset contains the following columns:\\nfsa_id (int): Food Standard Agency's ID for this pub.\\nname (string)L Name of the pub\\naddress (string): Address fields separated by commas.\\npostcode (string): Postcode of the pub.\\neasting (int)\\nnorthing (int)\\nlatitude (decimal)\\nlongitude (decimal)\\nlocal_authority (string): Local authority this pub falls under.\\nAcknowledgements:\\nThe data was derived from the Food Standard Agency's Food Hygiene Ratings and the ONS Postcode Directory. The data is licensed under the Open Government Licence. (See the included .html file.)\\nInspiration:\\nYou could use this data as the basis for a real-life travelling salesman problem and plan the world’s longest pub crawl.\",\n",
       " 'Context:\\n“Russian is an East Slavic language and an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan and many minor or unrecognised territories. It is an unofficial but widely spoken language in Ukraine and Latvia, and to a lesser extent, the other countries that were once constituent republics of the Soviet Union and former participants of the Eastern Bloc.” -- “Russian Language” on Wikipedia\\nRussian has around 150 million native speakers and 110 million non-native speakers. Russian in written in Cyrillic script. This dataset is a morphologically, syntactically and semantically annotated corpus of texts in Russian, fully accessible to researchers and edited by users.\\nContent:\\nThis dataset is encoded in UTF-8. There are two files included in this dataset: the corpus and the dictionary. The corpus is in .json format, while the dictionary is in plain text.\\nDictionary\\nIn the dictionary, each entry is a lemma, presented with all of its tagged derivations. The tags depend on the part of speech of the lemma. Some examples are:\\nNouns: part of speech, animacy, gender & number, case\\nVerbs: Part of speech, aspect, transitivity, gender & number, person, tense, mood\\nAdjectives: part of speech (ADJF), gender, number, case\\nA Python script to convert the tags in this corpus to this set more commonly used in English-language linguistics can be found here.\\nSample dictionary entries:\\n1\\nЁЖ  NOUN,anim,masc sing,nomn\\nЕЖА NOUN,anim,masc sing,gent\\nЕЖУ NOUN,anim,masc sing,datv\\nЕЖА NOUN,anim,masc sing,accs\\nЕЖОМ    NOUN,anim,masc sing,ablt\\nЕЖЕ NOUN,anim,masc sing,loct\\nЕЖИ NOUN,anim,masc plur,nomn\\nЕЖЕЙ    NOUN,anim,masc plur,gent\\nЕЖАМ    NOUN,anim,masc plur,datv\\n\\n41\\nЁРНИЧАЮ VERB,impf,intr sing,1per,pres,indc\\nЁРНИЧАЕМ    VERB,impf,intr plur,1per,pres,indc\\nЁРНИЧАЕШЬ   VERB,impf,intr sing,2per,pres,indc\\nЁРНИЧАЕТЕ   VERB,impf,intr plur,2per,pres,indc\\nЁРНИЧАЕТ    VERB,impf,intr sing,3per,pres,indc\\nЁРНИЧАЮТ    VERB,impf,intr plur,3per,pres,indc\\nЁРНИЧАЛ VERB,impf,intr masc,sing,past,indc\\nЁРНИЧАЛА    VERB,impf,intr femn,sing,past,indc\\nЁРНИЧАЛО    VERB,impf,intr neut,sing,past,indc\\nЁРНИЧАЛИ    VERB,impf,intr plur,past,indc\\nЁРНИЧАЙ VERB,impf,intr sing,impr,excl\\nЁРНИЧАЙТЕ   VERB,impf,intr plur,impr,excl\\nCorpous\\nIn this corpus, each word has been grammatically tagged. You can access individual tokens using the following general path:\\nJSON > text > paragraphs > paragraph > [paragraph number] > sentence > [sentence number] > tokens > [token number]\\nEach token has: * A unique id number (@id) * The text of the token (@text) * information on the lemma (under “l”), including the id number of the lemma as found in the dictionary\\nYou can see an example of the token portion of the .json structure below:\\n             {\\n                \"@id\": 1714292,\\n                \"@text\": \"сват\",\\n                \"tfr\": {\\n                  \"@t\": \"сват\",\\n                  \"@rev_id\": 3754311,\\n                  \"v\": {\\n                    \"l\": {\\n                      \"@id\": 314741,\\n                      \"@t\": \"сват\",\\n                      \"g\": [\\n                        {\\n                          \"@v\": \"NOUN\"\\n                        },\\n                        {\\n                          \"@v\": \"anim\"\\n                        },\\n                        {\\n                          \"@v\": \"masc\"\\n                        },\\n                        {\\n                          \"@v\": \"sing\"\\n                        },\\n                        {\\n                          \"@v\": \"nomn\"\\n                        }\\n                      ]\\n                    }\\n                  }\\n            }\\n          }\\nAcknowledgements:\\nThis dataset was collected and annotated by, among others, Svetlana Alekseeva, Anastasia Bodrova, Victor Bocharov, Dmitry Granovsky, Irina Krylova, Maria Nikolaeva, Catherine Protopopova, Alexander Chuchunkov, Anastasia Shimorina, Vasily Alekseev, Natalia Ostapuk, Maria Stepanova and Alexey Surikov. The code used to collect and clean this data is available online.\\nIt is reproduced here under a CC-BY-SA license.\\nMore information on this corpus and its most recent version can be found here (in Russian.)',\n",
       " 'Context:\\nTraffic management is a critical concern for policymakers, and a fascinating data question. This ~2gb dataset contains daily volumes of traffic, binned by hour. Information on flow direction and sensor placement is also included.\\nContent:\\nTwo datasets are included:\\ndot_traffic_2015.txt.gz\\ndaily observation of traffic volume, divided into 24 hourly bins\\nstation_id, location information (geographical place), traffic flow direction, and type of road\\ndot_traffic_stations_2015.txt.gz\\ndeeper location and historical data on individual observation stations, cross-referenced by station_id\\nAcknowledgements:\\nThis dataset was compiled by the US Department of Transportation and available on Google BigQuery\\nInspiration:\\nWhere are the heaviest traffic volumes? By time of day? By type of road?\\nAny interesting seasonal patterns to traffic volumes?',\n",
       " 'Context\\nThis dataset is a list of people who have been involved in an accident in the city of Barcelona (Spain) from year 2010 till 2016. This data is managed by the Police in the city of Barcelona and includes several information described below.\\nContent\\nThis dataset is composed by 7 files, each one containing between 10k-12k lines.\\nEvery row contains several information, like the type of injury (slightly wounded, serious injuries or death). It includes a description of the person (driver, passenger or pedestrian), sex, age, location, etc...\\nImportant: This dataset is uploaded as it is, so it\\'s possible that some data in some rows is missing/not correct.\\nDescription of each column:\\nNúmero d\\'expedient: Case File Number\\nCodi districte: District code where the accident was. Barcelona is divided in several districts\\nNom districte: Name of the district\\nCodi barri: Hood code where the accident was. Every district in Barcelona has several hoods\\nNom barri: Name of the hood\\nCodi carrer: Street code (Every street has a code)\\nNom carrer: Name of the street\\nNum postal caption: Postal number of the street\\nDescripció dia setmana: Day of the week in text (written in Catalan)\\nDia setmana: Shortcode of the previous field (also in Catalan)\\nDescripció tipus dia: Description of the type of the day, it can be \"labor\" or \"festive\" (also in Catalan)\\nNK Any: Number of the year\\nMes de any: Number of the month (1-12)\\nNom mes: Name of the month (in Catalan)\\nDia de mes: Day of the month\\nDescripció torn: Type of round of the police. It can be \"Matí\" (Morning), \"Tarda\" (Evening) or \"Nit\" (Night)\\nHora de dia: Hour of the day (0-23)\\nDescripció causa vianant: Text in catalan. Describes the accident in case the victim is a pedestrian. If not, it says \"No és causa del vianant\"\\nDesc. Tipus vehicle implicat: Type of vehicle in the accident. Also in Catalan.\\nDescripció sexe: Sex of the victim. \"Home\" means man, \"Dona\" means woman.\\nDescripció tipus persona: Type of role in the accident. It describes if the victim is the pilot (Conductor ), passenger (Passatger), pedestrian (Vianant)\\nEdat: Age of the victim\\nDescripció victimització: Type of injury in Catalan (slightly wounded (Ferit lleu), serious injuries (Ferit greu) or death (Mort))\\nCoordenada UTM (Y): UTM coordinate Y\\nCoordenada UTM (X): UTM coordinate X\\nAs you can see, some columns could be removed and we wouldn\\'t loose information. My experience working with these files tells me that some rows have no correct data or no data at all. So, be careful!\\nAcknowledgements\\nThis data can be found in \"Open Data BCN - Barcelona\\'s City Hall Open Data Service\", which is the owner of the CSV files.\\nInspiration\\nI have uploaded this information here because I believe that data should be shared with everybody! So, do your own \"research\" and share it also! I\\'m always happy to get some feedback and help each other!',\n",
       " 'Context\\nAchieving the appropriate balance of intellectual property (IP) protection through patent litigation is critical to economic growth. Examining the interplay between US patent law and economic effect is of great interest to many stakeholders. Published in March 2017, this dataset is the most comprehensive public body of information on USPTO patent litigation.\\nContent\\nThe dataset covers over 74k cases across 52 years. Five different files (attorneys.csv, cases.csv, documents.csv, names.csv, pacer_cases.csv) detail the litigating parties, their attorneys, results, locations, and dates. The large documents.csv file covers more than 5 million relevant documents (a tool like split might be your friend here).\\nAcknowledgements\\nThis data was collected by the Office of the Chief Economist at the USPTO. Data was collected from both the Public Access to Court Electronics Records (PACER), as well as RECAP, an independent PACER repository. Further documentation available via this paper.\\nInspiration\\nPatent litigation is a tug of war between patent holders, competing parties using similar IP, and government policy. Which industries see the most litigation? Any notable changes over time? Is there a positive (or negative) correlation between litigation, and a company’s economic fortunes?\\nLicense\\nPublic Domain Mark 1.0 Also see source.',\n",
       " 'Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 4.7 million job listings) that was created by extracting data from Monster.com, a leading job board.\\nContent\\nThis dataset has following fields:\\ncountry\\ncountry_code\\ndate_added\\nhas_expired - Always false.\\njob_description - The primary field for this dataset, containing the bulk of the information on what the job is about.\\njob_title\\njob_type - The type of tasks and skills involved in the job. For example, \"management\".\\nlocation\\norganization\\npage_url\\nsalary\\nsector - The industry sector the job is in. For example, \"Medical services\".\\nAcknowledgements\\nThis dataset was created by PromptCloud\\'s in-house web-crawling service.\\nInspiration\\nWhat kinds of jobs titles correspond with what kinds of wages?\\nWhat can you learn about the Moster.com-based US job market based on analyzing the contents of the job descriptions?\\nHow do job descriptions different between different industry sectors?',\n",
       " \"Context:\\nStarbucks is an American coffee chain founded in Seattle. It serves both beverages and food.\\nContent:\\nThis dataset includes the nutritional information for Starbucks’ food and drink menu items. All nutritional information for drinks are for a 12oz serving size.\\nAcknowledgements:\\nFood composition data is in the public domain, but product names marked with ® or ™ remain the registered trademarks of Starbucks.\\nInspiration:\\nCan you train a Markov Chain to generate new Starbucks drink or food items?\\nCan you design an easy-to-interpret visualization for the nutrition of each item?\\nHow to Starbucks menu items compare to McDonald’s menu items (see link to dataset below) in terms of nutrition?\\nYou may also like:\\nNutrition Facts for McDonald's Menu\\nStarbucks Locations Worldwide\",\n",
       " \"Context\\nThe carbon dioxide record from Mauna Loa Observatory, known as the “Keeling Curve,” is the world’s longest unbroken record of atmospheric carbon dioxide concentrations. Scientists make atmospheric measurements in remote locations to sample air that is representative of a large volume of Earth’s atmosphere and relatively free from local influences.\\nContent\\nThis dataset includes a monthly observation of atmospheric carbon dioxide (or CO2) concentrations from the Mauna Loa Observatory (Hawaii) at a latitude of 19.5, longitude of -155.6, and elevation of 3397 meters.\\nColumns 1-3: Provide the date in the following redundant formats: year, month and decimal date\\nColumn 4: Monthly CO2 concentrations in parts per million (ppm) measured on the 08A calibration scale and collected at 24:00 hours on the fifteenth of each month.\\nColumn 5: The fifth column provides the same data after a seasonal adjustment, which involves subtracting from the data a 4-harmonic fit with a linear gain factor to remove the seasonal cycle from carbon dioxide measurements\\nColumn 6: The sixth column provides the data with noise removed, generated from a stiff cubic spline function plus 4-harmonic functions with linear gain\\nColumn 7: The seventh column is the same data with the seasonal cycle removed.\\nAcknowledgements\\nThe carbon dioxide data was collected and published by the University of California's Scripps Institution of Oceanography under the supervision of Charles David Keeling with support from the US Department of Energy, Earth Networks, and the National Science Foundation.\\nInspiration\\nHow have atmospheric carbon dioxide levels changed in the past sixty years? How do carbon dioxide concentrations change seasonally? What do you think causes this seasonal cycle? When will the carbon dioxide levels exceed 450 parts per million?\",\n",
       " 'Context\\nThis are the official datasets used on the Medicare.gov Hospital Compare Website provided by the Centers for Medicare & Medicaid Services. These data allow you to compare the quality of care at over 4,000 Medicare-certified hospitals across the country.\\nContent\\nDataset fields:\\nProvider ID\\nHospital Name\\nAddress\\nCity\\nState\\nZIP Code\\nCounty Name\\nPhone Number\\nHospital Type\\nHospital Ownership\\nEmergency Services\\nMeets criteria for meaningful use of EHRs\\nHospital overall rating\\nHospital overall rating footnote\\nMortality national comparison\\nMortality national comparison footnote\\nSafety of care national comparison\\nSafety of care national comparison footnote\\nReadmission national comparison\\nReadmission national comparison footnote\\nPatient experience national comparison\\nPatient experience national comparison footnote\\nEffectiveness of care national comparison\\nEffectiveness of care national comparison footnote\\nTimeliness of care national comparison\\nTimeliness of care national comparison footnote\\nEfficient use of medical imaging national comparison\\nEfficient use of medical imaging national comparison\\nAcknowledgements\\nDataset was downloaded from [https://data.medicare.gov/data/hospital-compare]\\nInspiration\\nIf you just broke your leg, you might need to use this dataset to find the best Hospital to get that fixed!',\n",
       " 'THE POLL\\nAs part of Cards Against Humanity Saves America, this poll is funded for one year of monthly public opinion polls. Cards Against Humanity is asking the American people about their social and political views, what they think of the president, and their pee-pee habits.\\nTo conduct their polls in a scientifically rigorous manner, they partnered with Survey Sampling International — a professional research firm — to contact a nationally representative sample of the American public. For the first three polls, they interrupted people’s dinners on both their cell phones and landlines, and a total of about 3,000 adults didn’t hang up immediately. They examined the data for statistically significant correlations which can be found here: https://thepulseofthenation.com/\\nContent\\nPolls are released each month (they are still polling so this will be updated each month)\\nRow one is the header and contains the questions\\nEach row is one respondent\\'s answers\\nQuestions in the Sep 2017 poll:\\nIncome\\nGender\\nAge\\nAge Range\\nPolitical Affiliation\\nDo you approve or disapprove of how Donald Trump is handling his job as president?\\nWhat is your highest level of education?\\nWhat is your race?\\nWhat is your marital status?\\nWhat would you say is the likelihood that your current job will be entirely performed by robots or computers within the next decade?\\nDo you believe that climate change is real and caused by people, real but not caused by people, or not real at all?\"\\nHow many Transformers movies have you seen?\\nDo you agree or disagree with the following statement: scientists are generally honest and are serving the public good.\\nDo you agree or disagree with the following statement: vaccines are safe and protect children from disease.\\n\"How many books, if any have you read in the past year?\"\\nDo you believe in ghosts?\\nWhat percentage of the federal budget would you estimate is spent on scientific research?\\n\"Is federal funding of scientific research too high too low or about right?\"\\nTrue or false: the earth is always farther away from the sun in the winter than in the summer.\\n\"If you had to choose: would you rather be smart and sad or dumb and happy?\"\\nDo you think it is acceptable or unacceptable to urinate in the shower?\\nQuestions from Oct 2017 poll\\nIncome\\nGender\\nAge\\nAge Range\\nPolitical Affiliation\\nDo you approve or disapprove of how Donald Trump is handling his job as president?\\nWhat is your highest level of education?\\nWhat is your race?\\nFrom what you have heard or seen do you mostly agree or mostly disagree with the beliefs of White Nationalists?\\nIf you had to guess what percentage of Republicans would say that they mostly agree with the beliefs of White Nationalists?\\nWould you say that you love America?\\nIf you had to guess, what percentage of Democrats would say that they love America?\\nDo you think that government policies should help those who are poor and struggling in America?\\nIf you had to guess, what percentage of Republicans would say yes to that question?\\nDo you think that most white people in America are racist?\\nIf you had to guess, what percentage of Democrats would say yes to that question?\\nHave you lost any friendships or other relationships as a result of the 2016 presidential election?\\nDo you think it is likely or unlikely that there will be a Civil War in the United States within the next decade?\\nHave you ever gone hunting?\\nHave you ever eaten a kale salad?\\nIf Dwayne \"The Rock\" Johnson ran for president as a candidate for your political party, would you vote for him?\\nWho would you prefer as president of the United States, Darth Vader or Donald Trump?\\nQuestions from Nov 2017 poll\\nIncome\\nGender\\nAge\\nAge Range\\nIn politics today, do you consider yourself a Democrat, a Republican or Independent?\\nWould you say you are liberal, conservative, or moderate?\\nWhat is your highest level of education? (High school or less, Some college, College degree, Graduate degree)\\nWhat is your race? (white, black, latino, asian, other)\\nDo you live in a city, suburb, or small town?\\nDo you approve, disapprove, or neither approve nor disapprove of how Donald Trump is handling his job as president?\\nDo you think federal funding for welfare programs in America should be increased, decreased, or kept the same?\\nDo you think poor black people are more likely to benefit from welfare programs than poor white people?\\nDo you think poor people in cities are more likely to benefit from welfare programs than poor people in small towns?\\nIf you had to choose, would you rather live in a more equal society or a more unequal society?\\nAcknowledgements\\nThese polls are from Cards Against Humanity Saves America and the raw data can be found here: https://thepulseofthenation.com/#future',\n",
       " 'Context\\nThis dataset contains 1.3 million Sarcastic comments from the Internet commentary website Reddit. The dataset was generated by scraping comments from Reddit (not by me :)) containing the \\\\s ( sarcasm) tag. This tag is often used by Redditors to indicate that their comment is in jest and not meant to be taken seriously, and is generally a reliable indicator of sarcastic comment content.\\nContent\\nData has balanced and imbalanced (i.e true distribution) versions. (True ratio is about 1:100). The corpus has 1.3 million sarcastic statements, along with what they responded to as well as many non-sarcastic comments from the same source.\\nLabelled comments are in the train-balanced-sarcasm.csv file.\\nAcknowledgements\\nThe data was gathered by: Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli for their article \"A Large Self-Annotated Corpus for Sarcasm\". The data is hosted here.\\nCitation:\\n@unpublished{SARC,\\n  authors={Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli},\\n  title={A Large Self-Annotated Corpus for Sarcasm},\\n  url={https://arxiv.org/abs/1704.05579},\\n  year=2017\\n}\\nAnnotation of files in the original dataset: readme.txt.\\nInspiration\\nPredicting sarcasm and relevant NLP features (e.g. subjective determinant, racism, conditionals, sentiment heavy words, \"Internet Slang\" and specific phrases).\\nSarcasm vs Sentiment\\nUnusual linguistic features such as caps, italics, or elongated words. e.g., \"Yeahhh, I\\'m sure THAT is the right answer\".\\nTopics that people tend to react to sarcastically',\n",
       " \"Context\\nThis dataset was originally published by the University of Zurich Robotics and Perception Group here. A sample of the data along with accompanying descriptions is provided here for research uses.\\nThis presents the world's first dataset recorded on-board a camera equipped Micro Aerial Vehicle (MAV) flying within urban streets at low altitudes (i.e., 5-15 meters above the ground). The 2 km dataset consists of time synchronized aerial high-resolution images, GPS and IMU sensor data, ground-level street view images, and ground truth data. The dataset is ideal to evaluate and benchmark appearance-based topological localization, monocular visual odometry, simultaneous localization and mapping (SLAM), and online 3D reconstruction algorithms for MAV in urban environments.\\nContent\\nThe entire dataset is roughly 28 gigabyte. We also provide a sample subset less than 200 megabyte, representing the first part of the dataset. You can download the entire dataset from this page.\\nThe dataset contains time-synchronized high-resolution images (1920 x 1080 x 24 bits), GPS, IMU, and ground level Google-Street-View images. The high-resolution aerial images were captured with a rolling shutter GoPro Hero 4 camera that records each image frame line by line, from top to bottom with a readout time of 30 millisecond. A summary of the enclosed files is given below.\\nThe data from the on-board barometric pressure sensor BarometricPressure.csv, accelerometer RawAccel.csv, gyroscope RawGyro.csv, GPS receiver OnbordGPS.csv, and pose estimation OnboardPose.csv is logged and timesynchronized using the clock of the PX4 autopilot board. The on-board sensor data was spatially and temporally aligned with the aerial images. The first column of every file contains the timestamp when the data was recorded expressed in microseconds. In the next columns the sensor readings are stored. The second column in OnbordGPS.csv encodes the identification number (ID) of every aerial image stored in the /MAV Images/ folder. The first column in GroundTruthAGL.csv is the ID of the aerial image, followed by the ground truth camera position of the MAV and the raw GPS data. The second column in GroundTruthAGM.csv is the ID of of the aerial image, followed by the ID of the first, second and third best match ground-level street view image in the /Street View Img/ folder.\\nGround Truth\\nTwo types of ground truth data are provided in order to evaluate and benchmark different vision-based localization algorithms. Firstly, appearance-based topological localization algorithms, that match aerial images to street level ones, can be evaluated in terms of precision rate and recall rate. Secondly, metric localization algorithms, that computed the ego-motion of the MAV using monocular visual SLAM tools, can be evaluated in terms of standard deviations from the ground truth path of the vehicle.\\nSee more details here.\\nPast Research\\nThe work listed below inspired the recording of this dataset. In these papers a much smaller dataset was used, that did not contain time synchronized GPS (except a small street segment ), IMU data and accurate metric ground truth. If you used this dataset, please send your paper to majdik (at) ifi (dot) uzh (dot) ch.\\nA.L. Majdik, D. Verda, Y. Albers-Schoenberg, D. Scaramuzza. Air-ground Matching: Appearance-based GPS-denied Urban Localization of Micro Aerial Vehicles Journal of Field Robotics, 2015.\\nA. L. Majdik, D. Verda, Y. Albers-Schoenberg, D. Scaramuzza Micro Air Vehicle Localization and Position Tracking from Textured 3D Cadastral Models IEEE International Conference on Robotics and Automation (ICRA), Hong Kong, 2014.\\nA. Majdik, Y. Albers-Schoenberg, D. Scaramuzza. MAV Urban Localization from Google Street View Data IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Tokyo, 2013.\\nLicense\\nThese datasets are released under the Creative Commons license (CC BY-NC-SA 3.0), which is free for non-commercial use (including research).\\nAcknowledgements\\nThis dataset was recorded with the help of Karl Schwabe, Mathieu Noirot-Cosson, and Yves Albers-Schoenberg. To record the dataset we used a Fotokite MAV offered to our disposal by Perspective Robotics AG—http://fotokite.com.\\nThis work was supported by the National Centre of Competence in Research Robotics (NCCR) through the Swiss National Science Foundation and by the Hungarian Scientific Research Fund (No. OTKA/NKFIH 120499).\",\n",
       " 'Context\\nEpilepsy is a group of neurological disorders characterized by epileptic seizures. Epileptic seizures are episodes that can vary from brief and nearly undetectable to long periods of vigorous shaking.\\nContent\\nThe original dataset consists of 5 different folders, each with 100 files, with each file representing a single subject/person. Each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time.\\nThe column y contains the category of the 178-dimensional input vector. Specifically y in {1, 2, 3, 4, 5}\\n5- eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open 4- eyes closed, means when they were recording the EEG signal the patient had their eyes closed 3- Yes they identify where the region of the tumor was in the brain and recording the EEG activity from the healthy brain area 2- They recorder the EEG from the area where the tumor was located\\nAll subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure\\n1- Recording of seizure activity\\nThe Explanatory variables X1, X2, ..., X178\\nEach 178-dimensional vector contained in a row, represents a randomly selected 1-second long sample picked from the single file. Recall that each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points.\\nAcknowledgements\\nAndrzejak RG, Lehnertz K, Rieke C, Mormann F, David P, Elger CE (2001) Indications of nonlinear deterministic and finite dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state, Phys. Rev. E, 64, 061907\\nInspiration\\nCan you help epileptic people live a better life ?',\n",
       " 'All recorded police reports as taken from https://data.seattle.gov/Public-Safety/Seattle-Police-Department-Police-Report-Incident/7ais-f98f',\n",
       " 'Bug triage.',\n",
       " 'This is a dataset of tweets from various active scientists and personalities ranging from Donald Trump and Hillary Clinton to Neil deGrasse Tyson. More are forthcoming.\\nThey were obtained through javascript scraping of the browser twitter timeline rather than a Tweepy python API or the twitter timeline API.\\nThe inspiration for this twitter dataset is comparing tweets in my own twitter analysis to find who tweets like whom, e.g. does Trump or Hillary tweet more like Kim Kardashian than one another?\\nThus, this goes further back in time than anything directly available from Twitter.\\nThe data is in JSON format rather than CSV, which will be forthcoming as well.\\nKim Kardashian, Adam Savage, BillNye, Neil deGrasse Tyson, Donald Trump, and Hillary Clinton have been collected up to 2016-10-14 Richard Dawkins, Commander Scott Kelly, Barack Obama, NASA, and The Onion, tweets up to 2016-10-15.\\nFor your own pleasure, with special thanks to the Trump Twitter Archive for providing some of the code, here is the JavaScript used to scrape tweets off of a timeline and output the results to the clipboard in JSON format:\\n1) Construct the query with from:TWITTERHANDLE since:DATE until:DATE\\n2) In the browser console set up automatic scrolling with: setInterval(function(){ scrollTo(0, document.body.scrollHeight) }, 2500)\\n3) Scrape the resulting timeline with: var allTweets = []; var tweetElements = document.querySelectorAll(\\'li.stream-item\\');\\nfor (var i = 0; i < tweetElements.length; i++) { try { var el = tweetElements[i]; var text = el.querySelector(\\'.tweet-text\\').textContent; allTweets.push({ id: el.getAttribute(\\'data-item-id\\'), date: el.querySelector(\\'.time a\\').textContent, text: text, link: el.querySelector(\\'div.tweet\\').getAttribute(\\'data-permalink-path\\'), retweet: text.indexOf(\\'\\\\\"@\\') == 0 && text.includes(\\':\\') ? true : false }); } catch(err) {} }; copy(allTweets);\\nHave fun!',\n",
       " \"Context\\nThere is a well-documented phenomenon of increased suicide rates among United States military veterans. One recent analysis, published in 2016, found the suicide rate amongst veterans to be around 20 per day. The widespread nature of the problem has resulted in efforts by and pressure on the United States military services to combat and address mental health issues in and after service in the country's armed forces.\\nIn 2013 News21 published a sequence of reports on the phenomenon, aggregating and using data provided by individual states to typify the nationwide pattern. This dataset is the underlying data used in that report, as collected by the News21 team.\\nContent\\nThe data consists of six files, one for each year between 2005 and 2011. Each year's worth of data includes the general population of each US state, a count of suicides, a count of state veterans, and a count of veteran suicides.\\nAcknowledgements\\nThis data was originally published by News21. It has been converted from an XLS to a CSV format for publication on Kaggle. The original data, visualizations, and stories can be found at the source.\\nInspiration\\nWhat is the geospatial pattern of veterans in the United States? How much more vulnerable is the average veteran to suicide than the average citizen? Is the problem increasing or decreasing over time?\",\n",
       " 'Context\\nThe dataset reflects the votes for the different areas in Germany for the 2017 federal elections. See German Federal Election 2017 for more details\\nContent\\nThe data was aquired from govdata.de, which is state website offering interesting datasets. The original dataset was not easy to use, therefore I did some reshaping without changing the data\\nAcknowledgements\\nThe dataset is originally from https://www.govdata.de/web/guest/apps/-/details/bundestagswahl-2017\\nInspiration\\nThe data is interesting as it reflects the votes for all areas in Germany',\n",
       " \"Comcast is notorious for terrible customer service and despite repeated promises to improve, they continue to fall short. Only last month (October 2016) the FCC fined them a cool $2.3 million after receiving over 1000 consumer complaints. After dealing with their customer service for hours yesterday, I wanted to find out more about others' experiences.\\nThis will serve as a repository of public customer complaints filed against Comcast as I scrape them from the web. The data should not only provide a fun source for analysis, but it will help to pin down just what is wrong with Comcast's customer service.\",\n",
       " \"Context\\nA collection of SLC End-of-Year Crime Reports geocoded to standard GPS coordinates.\\nContent\\n2016 Crime Statistics for Salt Lake City, UT. Includes:\\nCase Numbers\\nOffence Codes for categorization\\nDescriptions for context\\nIBR (National Incident-Based Reporting System Number)\\nOccurrence Date\\nReport Date\\nDay of the Week (1 = Monday, 7 = Sunday)\\nLocation (Addresses in SLC)\\nCity Council District\\nSLCPD Police Zones\\nSLCPD Grid\\nx_coordinate: note that this is based on epsg:32043 projections\\ny_coordinate: note that this is based on epsg:32043 projections\\nx_gps_coords (added by yours truly, converted to epsg:4326)\\ny_gps_coords (added by yours truly, converted to epsg:4326)\\nData Accuracy Notes\\nSome data wrangling will still likely be required to clean up null columns.\\nI went ahead and lowercased column names (and corrected a spelling mistake in the y-coordinate column).\\nepsg:32043 projections were converted to epsg:4326 projections using pyplot with distances preserved.\\nMultiple year munging performed here: https://github.com/octaflop/slcpd/blob/master/develop/2017-08-16-crunch.ipynb\\nStill awaiting dataset owner clarification of Calls vs Cases\\nAcknowledgements\\nTaken from the SLC Open Data Web Site.\\nThank you Dean Larson, the original dataset owner.\\nThank you to the City of Salt Lake government and the Utah.gov catalog for providing this data for public use.\\nThanks to the DAT Science EdEx course for inspiring me to take a look at my own city's crime stats.\\nThank you to the SLCPD for keeping Salt Lake City citizens safe and enforcing an internal discipline of open data-collection.\\nInspiration\\nCrime report locations by season?\\nCross Reference of city council districts\\nTime of day\\nOffence descriptions\\nMoving centroids based on time of day / season?\\nHoliday rowdiness?\\nComing Soon\\nFull 2016 reports (eta Spring 2017) ✔\\n3-year combined reports (eta Summer 2017) ✔\\n3-year combined cases vs calls (eta Summer 2017)\\nyear-by-year files (eta Fall 2017)\",\n",
       " 'Context\\nEvery Marathoner has a time goal in mind, and this is the result of all the training done in months of exercises. Long runs, Strides, Kilometers and phisical exercise, all add improvement to the result. Marathon time prediction is an art, generally guided by expert physiologists that prescribe the weekly exercises and the milestones to the marathon.\\nUnfortunately, Runners have a lot of distractions while preparing the marathon, work, family, illnes, and therefore each one of us arrives to the marathon with his own story. The \"simple\" approach is to look at data after the competition, the Leaderboard.\\nBut what if we could link the Marathon result to the training history of the Athlete? Could we find that \"non orthodox\" training plans give good results?\\nThe Athlete Training History\\nAs a start, I\\'ll take just two data from the Athlete History, easy to extract. Two meaningful data, the average km run during the 4 weeks before the marathon, and the average speed that the athlete has run these km.\\nMeaningful, because in the last month of the training I have the recap of all the previous months that brought me to the marathon.\\nEasy to extract, because I can go to Strava and I have a \"side-by-side\" comparison, myself and the reference athlete. I said easy, well, that\\'s not so easy, since I have to search every athlete and write down those numbers, the exact day the marathon happened, otherwise I will put in the average the rest days after the marathon.\\nI\\'ve set my future work in extracting more data and build better algorithms. Thank you for helping me to understand or suggest.\\nContent\\nid:\\nsimple counter\\nMarathon:\\nthe Marathon name where the data were extracted. I use the data coming out from Strava \"Side by side comparison\" and the data coming from the final marathon result\\nName:\\nThe athlete\\'s name, still some problems with UTF-8, I\\'ll fix that soon\\nCategory:\\nthe sex and age group of a runner - MAM Male Athletes under 40 years - WAM Women under 40 Years - M40 Male Athletes between 40 and 45 years\\nkm4week\\nThis is the total number of kilometers run in the last 4 weeks before the marathon, marathon included. If, for example, the km4week is 100, the athlete has run 400 km in the four weeks before the marathon\\nsp4week\\nThis is the average speed of the athlete in the last 4 training weeks. The average counts all the kilometers done, included the slow kilometers done before and after the training. A typic running session can be of 2km of slow running, then 12-14km of fast running, and finally other 2km of slow running. The average of the speed is this number, and with time this is one of the numbers that has to be refined\\ncross training:\\nIf the runner is also a cyclist, or a triathlete, does it counts? Use this parameter to see if the athlete is also a cross trainer in other disciplines\\nWall21: In decimal. The tricky field. To acknowledge a good performance, as a marathoner, I have to run the first half marathon with the same split of the second half. If, for example, I run the first half marathon in 1h30m, I must finish the marathon in 3h (for doing a good job). If I finish in 3h20m, I started too fast and I hit \"the wall\". My training history is, therefore, less valid, since I was not estimating my result\\nMarathon time:\\nIn decimal. This is the final result. Based on my training history, I must predict my expected Marathon time\\nCategory:\\nThis is an ancillary field. It gives some direction, so feel free to use or discard it. It groups in:\\n- A results under 3h\\n- B results between 3h and 3h20m\\n- C results between 3h20m and 3h40m\\n- D results between 3h40 and 4h\\nAcknowledgements\\nThank you to the main Athletes data sources, GARMIN and STRAVA\\nThe Goal of this Competition:\\nBased on my training history, I must predict my expected Marathon time. Which other relevant data could help me to be more precise? Heart rate, cadence, speed training, what else? And how could I get those data?',\n",
       " 'Context\\nEmergent.info was a major rumor tracker, created by veteran journalist Craig Silverman. It has been defunct for a while, but its well-structured format and well-documented content provides an opportunity for analyzing rumors on the web.\\nSnopes.com is one of the oldest rumors trackers on the web. Originally launched by Barbara and David Mikkelson, it is now run by a team of editors who investigate urban legends, myths, viral rumors and fake news. The investigators try to provide a detailed explanation for why they have chosen to confirm or debunk a rumor, often citing several web pages and other external sources.\\nPolitifact.com is a fact-checker that is focused on statements made by politicians and claims circulated by political campaigns, blogs and similar websites. Politifact\\'s labels range from \"true,\" to \"pants on fire!\"\\nContent\\nThis dataset consists of three files. One file is a collection of all webpages cited in Emergent.info, and the second is a collection of webpages cited in Snopes.com, and the third is a similar collection from Politifact.com. The webpages were often cited because they had started a rumor, shared a rumor, or debunked a rumor.\\nEmergent.info\\nEmergent.info often provides a clean timeline of the rumor\\'s propagation on the web, and identifies which page was for the rumor, which page was against it, and which page was simply observing it. Please refer to the image below to learn more about the fields in this dataset.\\nSnopes.com\\nThe structure of posts on Snopes.com is not as well-defined. Please refer to the image below to learn more about the fields in the Snopes dataset.\\nPolitifact.com\\nSimilar to Emergent.info, Politifact.com follows a well-structured format in reporting and documenting rumors. There is a sidebar on the right side of each page that lists all of the sources cited within the page. The top link is the likeliest to be the original source of the rumor. For this link, page_is_first_citation is set to true.\\nInspiration\\nI created this dataset in order to study domains that frequently start, propagate, or debunk rumors. By studying these domains and people who follow them, I hope to gain some insight into the dynamics of rumor propagation on the web, as well as social media.\\nNotes/Disclaimer\\nWhen using the Snopes dataset, please keep the following in mind:\\nIn addition to debunking rumors, Snopes.com occasionally reports news and other types of content. This collection only includes data from \"Fact Check\" posts on Snopes.\\nSnopes.com was launched years ago. Some of the older posts on the website do not follow the current format of the site, therefore some of the fields might be missing.\\nSnopes.com used to use a service named \"DoNotLink.com\" for citation purposes. That service is no longer active and as a result some of the links are missing from older posts on Snopes.\\nIn addition, some of the shortened links would time-out prior to resolution, in which case they would not be added to the dataset.\\nOccasionally, a website that has been cited has not maliciously started a rumor. For instance, Andy Borowitz is a humorist who writes for The New Yorker. His satirical column is sometimes mistaken for real news; as a result, The New Yorker may be cited as a source of fake news on Snopes.com. This does not mean that The New Yorker is a fake news website.\\nWhen using the Politifact dataset, please keep the following in mind:\\nThe data included in this dataset are collected from the \"truth-o-meter\" page of Politifact.com.\\nPolitifact often fact-checks statements made by politicians. Since this dataset is focused on websites, I have ignored all the posts in which the rumor was attributed to a person, a political party, a campaign, or an organization. Instead, I have only included rumors attributed explicitly to websites or blogs.\\nUseful Tips for Using the Snopes collection\\nAs opposed to the Emergent collection where each page is flagged with whether it was for or against a rumor, no such information is available for the Snopes dataset. To avoid manually labeling the data, you may use the following heuristics to identify which page started a rumor:\\nWebpages that are cited in the \"Examples\" section of a post are often \"observing\" the rumor, i.e. they have not started it, but they are repeating it. In the snopes.csv file, these webpages have been flagged as \"page_is_example.\"\\nWebpages that are cited in the \"Featured Image\" section of a post are often not related to the rumor. The editors on Snopes have simply extracted an image from those pages to embed in their posts. In the snopes.csv file, these webpages have been flagged as \"page_is_image_credit.\"\\nWebpages that are cited through a secondary service (such as archive.is) are likelier to be rumor-propagators. Editors do not link to them directly so that a record of their page is available, even if it is later deleted.\\nIf neither of these hints help, very often (but not always) the first link cited on the page (for which \"page_is_example\" and \"page_is_image_credit\" are false) is the link to a page that started the rumor. This link is identified by the \"page_is_first_citation\" field. Pages for which both \"page_is_first_citation\" and \"page_is_archived\" are true are very likely to be rumor propagators.\\nTo identify satirical websites that are mistaken for real news, it\\'s useful to inspect the way they are cited on Snopes. To demonstrate that a website contains satire or humor, Snopes writers often cite the \"about us\" page of the site. Therefore it\\'s useful to see which domains often contain a URI to their \"about\" page (e.g. \"http://politicops.com/about-us/\").',\n",
       " 'Context\\nSatellite imagery provides unique insights into various markets, including agriculture, defense and intelligence, energy, and finance. New commercial imagery providers, such as Planet and BlackSky, are using constellations of small satellites to exponentially increase the amount of images of the earth captured every day.\\nThis flood of new imagery is outgrowing the ability for organizations to manually look at each image that gets captured, and there is a need for machine learning and computer vision algorithms to help automate the analysis process.\\nThe aim of this dataset is to help address the difficult task of detecting the location of airplanes in satellite images. Automating this process can be applied to many issues including monitoring airports for activity and traffic patterns, and defense intelligence.\\nContinusouly updates will be made to this dataset as new Planet imagery released. Current images were collected as late as July 2017.\\nContent\\nProvided is a zipped directory planesnet.zip that contains the entire dataset as .png image chips. Each individual image filename follows a specific format: {label} __ {scene id} __ {longitude} _ {latitude}.png\\nlabel: Valued 1 or 0, representing the \"plane\" class and \"no-plane\" class, respectively.\\nscene id: The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene.\\nlongitude_latitude: The longitude and latitude coordinates of the image center point, with values separated by a single underscore.\\nThe dataset is also distributed as a JSON formatted text file planesnet.json. The loaded object contains data, label, scene_ids, and location lists.\\nThe pixel value data for each 20x20 RGB image is stored as a list of 1200 integers within the data list. The first 400 entries contain the red channel values, the next 400 the green, and the final 400 the blue. The image is stored in row-major order, so that the first 20 entries of the array are the red channel values of the first row of the image.\\nThe list values at index i in labels, scene_ids, and locations each correspond to the i-th image in the data list.\\nClass Labels\\nThe \"plane\" class includes 8000 images. Images in this class are near-centered on the body of a single airplane, with the majority of the plane\\'s wings, tail, and nose also visible. Examples of different aircraft sizes, orientations, and atmospheric collection conditions are included. Example images from this class are shown below.\\nThe \"no-plane\" class includes 24000 images. A third of these are a random sampling of different landcover features - water, vegetion, bare earth, buildings, etc. - that do not include any portion of an airplane. The next third are \"partial planes\" that contain a portion of an airplane, but not enough to meet the full definition of the \"plane\" class. The last third are \"confusers\" - chips with bright objects or strong linear features that resemble a plane - that have previously been mislabeled by machine learning models. Example images from this class are shown below.\\nAcknowledgements\\nSatellite imagery used to build PlanesNet is made available through Planet\\'s Open California dataset, which is openly licensed. As such, this dataset is also available under the same CC-BY-SA license. Users can sign up for a free Planet account to search, view, and download thier imagery and gain access to their API.',\n",
       " 'Context:\\nHalloween begins frenetic candy consumption that continues into the Christmas holidays and New Year’s Day, when people often make (usually short-lived) resolutions to lose weight. But all this consumption first needs production. The graph shows the relevant data from the industrial production index and its stunning seasonality\\nContent:\\nThe industrial production (IP) index measures the real output of all relevant establishments located in the United States, regardless of their ownership, but not those located in U.S. territories. This dataset tracks industrial production every month from January 1972 to August 2017.\\nAcknowledgements:\\nBoard of Governors of the Federal Reserve System (US), Industrial Production: Nondurable Goods: Sugar and confectionery product [IPG3113N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/IPG3113N, October 13, 2017.\\nInspiration:\\nCan you correct for the seasonality in this data?\\nWhich months have the highest candy production?\\nCan you predict production for September through December 2017?',\n",
       " 'Context\\nThe Canadian Disaster Database The Canadian Disaster Database (CDD) contains detailed disaster information on more than 1000 natural, technological and conflict events (excluding war) that have happened since 1900 at home or abroad and that have directly affected Canadians.\\nContent\\nData description copied from: https://www.publicsafety.gc.ca/cnt/rsrcs/cndn-dsstr-dtbs/index-en.aspx\\nDataset date range: 1900 - present\\nThe CDD tracks \"significant disaster events\" which conform to the Emergency Management Framework for Canada definition of a \"disaster\" and meet one or more of the following criteria:\\n10 or more people killed\\n100 or more people affected/injured/infected/evacuated or homeless\\nan appeal for national/international assistance\\nhistorical significance\\nsignificant damage/interruption of normal processes such that the community affected cannot recover on its own\\nThe database describes where and when a disaster occurred, the number of injuries, evacuations, and fatalities, as well as a rough estimate of the costs. As much as possible, the CDD contains primary data that is valid, current and supported by reliable and traceable sources, including federal institutions, provincial/territorial governments, non-governmental organizations and media sources.\\nData is updated and reviewed on a semi-annual basis.\\nData Field Description\\nDisaster Type The type of disaster (e.g. flood, earthquake, etc.) that occurred.\\nDate of Event The date a specific event took place.\\nSpecific Location The city, town or region where a specific event took place.\\nDescription of Event A brief description of a specific event, including pertinent details that may not be captured in other data fields (e.g. amount of precipitation, temperatures, neighbourhoods, etc.)\\nFatalities The number of people killed due to a specific event.\\nInjured/Infected The number of people injured or infected due to a specific event.\\nEvacuees The number of individuals evacuated by the government of Canada due to a specific event.\\nLatitude & Longitude The exact geographic location of a specific event.\\nProvince/Territory The province or territory where a specific event took place.\\nEstimated Total Cost A roll-up of all the costs listed within the financial data fields for a specific event.\\nDFAA Payments The amount, in dollars, paid out by Disaster Financial Assistance Arrangements (Public Safety Canada) due to a specific event.\\nInsurance Payments The amount, in dollars, paid out by insurance companies due to a specific event.\\nProvincial/Territorial Costs/Payments The amount, in dollars, paid out by a Province or Territory due to a specific event.\\nUtility Costs/Losses The amount of people whose utility services (power, water, etc.) were interrupted/affected by a specific event.\\nMagnitude A measure of the size of an earthquake, related to the amount of energy released.\\nOther Federal Institution Costs The amount, in dollars, paid out by other federal institutions.\\nAcknowledgements\\nData gathered from: http://cdd.publicsafety.gc.ca Terms of use for commercial and non-comerical reproduction: https://www.publicsafety.gc.ca/cnt/ntcs/trms-en.aspx\\nInspiration\\nThis dataset provides valuable insight to natural and non-natrual disasters which have affected Canada.\\nPossible explorations: * Where do different types of disasters occur more frequently? * Which Province / Location in Canada has been hit the hardest in terms of fatalities, number of injuries, estimated total cost, etc.?\\nSpatial-temporal correlations between natural/artifical distasters *\\nI think that this can be used to produce some interesting data visualizations. Some of the questions I look forward to answering include:\\nCan any spatial-temporal correlations between disasters be found in this dataset?\\nWhich locations in Canada have been hit the hardest, in terms of people injured, fatalities, financial impact, etc.',\n",
       " \"Context\\nIs the movie industry dying? is Netflix the new entertainment king? Those were the first questions that lead me to create a dataset focused on movie revenue and analyze it over the last decades. But, why stop there? There are more factors that intervene in this kind of thing, like actors, genres, user ratings and more. And now, anyone with experience (you) can ask specific questions about the movie industry, and get answers.\\nContent\\nThere are 6820 movies in the dataset (220 movies per year, 1986-2016). Each movie has the following attributes:\\nbudget: the budget of a movie. Some movies don't have this, so it appears as 0\\ncompany: the production company\\ncountry: country of origin\\ndirector: the director\\ngenre: main genre of the movie.\\ngross: revenue of the movie\\nname: name of the movie\\nrating: rating of the movie (R, PG, etc.)\\nreleased: release date (YYYY-MM-DD)\\nruntime: duration of the movie\\nscore: IMDb user rating\\nvotes: number of user votes\\nstar: main actor/actress\\nwriter: writer of the movie\\nyear: year of release\\nAcknowledgements\\nThis data was scraped from IMDb.\\nContribute\\nYou can contribute via GitHub.\",\n",
       " 'OpenSprayer.com\\nOpen Sprayer will hopefully be an open sourced autonomous land drone that will propel itself across the fields spraying weeds it can see with its mounted cameras. The project should involve a mix of mechanical engineering, classical software design and machine learning to achieve its goal. The project is meant to be a DIY effort to compete with the big companies like John Deere currently developing similar tech. The benefit of an open design is cheaper capital and maintenance cost. The ability to fix, update and repair your own sprayer would offer a great alternative to the usual high running costs of branded machines.\\nThe data set includes around 150 photos with annotations (Bounding box coordinates) locating the broad leaved docks. 70% were taken by me with the remaining being collected on google. I plan to update the images to better reflect the images that the sprayer drone will produce when operating. When the drone is running photos will most likely be taken from a height looking straight down at the ground, therefore the google images may be useless or not? Give me feedback and I can take more pictures to improve the dataset.',\n",
       " 'The 2014 American Community Survey Public Use Microdata Sample\\nContext\\nThe American Community Survey (ACS) is an ongoing survey that provides vital information on a yearly basis about our nation and its people. Information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year.\\nFrequency: Annual\\nPeriod: 2014\\nContent\\nThrough the ACS, we know more about jobs and occupations, educational attainment, veterans, whether people own or rent their home, and other topics. Public officials, planners, and entrepreneurs use this information to assess the past and plan the future. When you respond to the ACS, you are doing your part to help your community plan hospitals and schools, support school lunch programs, improve emergency services, build bridges, and inform businesses looking to add jobs and expand to new markets, and more. The data dictionary can be found here.\\nInspiration\\nKernels created using the 2013 ACS can serve as excellent starting points for working with the 2014 ACS. For example, the following analyses were created using ACS data:\\nWork arrival times and earnings in the USA\\nInequality in STEM careers\\nAcknowledgements\\nThe American Community Survey (ACS) is administered, processed, researched and disseminated by the U.S. Census Bureau within the U.S. Department of Commerce.',\n",
       " 'Full text of questions and answers from Cross Validated, the statistics and machine learning Q&A site from the Stack Exchange network.\\nThis is organized as three tables:\\nQuestions contains the title, body, creation date, score, and owner ID for each question.\\nAnswers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.\\nTags contains the tags on each question\\nFor space reasons only non-deleted and non-closed content are included in the dataset. The dataset contains questions up to 19 October 2016 (UTC).\\nLicense\\nAll Stack Exchange user contributions are licensed under CC-BY-SA 3.0 with attribution required.',\n",
       " \"Context\\nThe Vision Zero data contained in this layer pertain to parking violations issued by the District of Columbia's Metropolitan Police Department (MPD) and partner agencies with the authority. Parking violation locations are summarized ticket counts based on time of day, week of year, year, and category of violation. Data was originally downloaded from the District Department of Motor Vehicle's eTIMS meter work order management system.\\nContent\\nThis dataset contains 132,850 rows of:\\nOBJECTID (unique ID)\\nROWID_ DAY_OF_WEEK (text)\\nHOLIDAY (number)\\nWEEK_OF_YEAR (number)\\nMONTH_OF_YEAR (number)\\nISSUE_TIME (number)\\nVIOLATION_CODE (text)\\nVIOLATION_DESCRIPTION (text)\\nLOCATION (text)\\nRP_PLATE_STATE (text)\\nBODY_STYLE (text)\\nADDRESS_ID (number)\\nSTREETSEGID (number)\\nXCOORD (number)\\nYCOORD (number)\\nTICKET_ISSUE_DATE (date or time)\\nX\\nY\\nAcknowledgements\\nThe dataset is shared by DCGISopendata, and the original dataset and metadata can be found here.\\nInspiration\\nCan you use the dataset to determine:\\nWhich area has the highest concentration of parking violations? How about by type of violation?\\nDo areas with high concentration of parking violations change throughout the month of December? Can you identify any trends?\",\n",
       " 'The Greenhouse Gas (GHG) Inventory Data contains the most recently submitted information, covering the period from 1990 to the latest available year, to the extent the data have been provided. The GHG data contain information on anthropogenic emissions by sources and removals by sinks of the following GHGs (carbon dioxide (CO2), methane (CH4), nitrous oxide (N2O), hydrofluorocarbons (HFCs), perfluorocarbons (PFCs), unspecified mix of HFCs and PFCs, sulphur hexafluoride (SF6) and nitrogen triflouride (NF3)) that are not controlled by the Montreal Protocol.\\nGHG emission inventories are developed by Parties to the Convention using scientific and methodological guidance from the Intergovernmental Panel on Climate Change (IPCC), such as 2006 IPCC Guidelines for National Greenhouse Gas Inventories, Revised Guidelines for National Greenhouse Gas Inventories (1996), IPCC Good Practice Guidance and Uncertainty Management in National Greenhouse Gas Inventories (2000) and IPCC Good Practice Guidance on Land Use, Land-use Change and Forestry (2003). Last update in UNdata: 23 Mar 2017 with data released in Nov 2016.\\nAcknowledgements\\nThis dataset was kindly published by the United Nation on the UNData site. You can find the original dataset here.\\nLicense\\nPer the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference.',\n",
       " 'This data set comprises events for major league baseball, provided by http://retrosheet.org.\\n The information used here was obtained free of\\n charge from and is copyrighted by Retrosheet.  Interested\\n parties may contact Retrosheet at \"www.retrosheet.org\".\\nRoughly speaking an event is an outcome in a baseball game. This includes the end result of a plate appearance (strikeout, out in the field, hit, base on balls), events that occur within a plate appearance (stolen bases, caught stealing), and rare other occurrences. The retrosheet event data prior to 1955 are not complete. The data subsequent to 1988 include pitch counts while the data prior do not. The data here cover the years 1970-2015, in three divisions (1970-1992, 1993-2004, 2005-2015) that correspond, roughly, to distinct eras with different run-scoring environments. These data have specifically been obtained with a mix of the data dumps available at baseball heatmaps and with the py-retrosheet Python package, available on github.\\nI have augmented the data provided by retrosheet with some additional fields. Most substantively the rows include the wOBA value of the event, in the field woba_pts, and an estimated time stamp, in units of seconds since Jan. 1, 1900 (time_since_1900).\\nThe conversion from retrosheet files to sql and csv is done by the chadwick software. A detailed description of all of the fields is available on the documentation for chadwick, http://chadwick.sourceforge.net/doc/cwevent.html. In order to keep the file sizes down, I have limited the fields in this data set to a subset of the fields described in the chadwick documentation.\\nThe master.csv file is a subset of the Baseball Databank data and is released under a Creative Commons Attribution-ShareAlike 3.0 Unported License.https://creativecommons.org/licenses/by-sa/3.0/. More details are available on the Baseball Databank github https://github.com/chadwickbureau/baseballdatabank',\n",
       " 'Content\\nData Sources : - Crime (2016): https://www.icpsr.umich.edu/icpsrweb/ - Population (2013): https://census.gov',\n",
       " 'UCLA Housing dataset',\n",
       " 'This dataset is a result of my research production in machine learning and android security. The data were obtained by a process that consisted to create a binary vector of permissions used for each application analyzed {1=used, 0=no used}. Moreover, the samples of malware/benign were devided by \"Type\"; 1 malware and 0 non-malware.\\nOne important topic to work is to create a good set of malware, because it is difficult to find one updated and with a research work to support it .\\nIf your papers or other works use our dataset, please cite our COLCOM 2016 paper as follows. Urcuqui, C., & Navarro, A. (2016, April). Machine learning classifiers for android malware analysis. In Communications and Computing (COLCOM), 2016 IEEE Colombian Conference on (pp. 1-6). IEEE.\\nIf you need an article in english, you can download another of my works: Urcuqui, Christian., & Navarro, Andres. (2016). Framework for malware analysis in Android. Sistemas & Telemática, 14(37), 45-56.\\nccurcuqui@icesi.edu.co',\n",
       " \"Context\\nA few years ago, I investigated a Korean corpus in order to find the most frequent 1000 words. Subsequently, I asked native speakers to translate those words and their example sentences into English, Japanese, Spanish, and Indonesian. I've totally forgotten this data since then, but it flashed on me this might be helpful for some people. Undoubtedly, 1000 sentences are a pretty small corpus, but it is also true that parallel corpora are hard to get.\\nContent\\nIt's a csv file. As you expect, the first line is the heading.\\nID: Id of the headword. It is arranged by alphabetical order.\\nHEADWORD: 1000 most frequent Korean words.\\nPOS: Part of speech.\\nENGLISH: English meaning or equivalent.\\nJAPANESE: Japanese meaning or equivalent.\\nSPANISH: Spanish meaning or equivalent.\\nINDONESIAN: Indonesian meaning or equivalent.\\nEXAMPLE (KO): An example sentence\\nEXAMPLE (EN): English translation\\nEXAMPLE (JA): Japanese translation\\nEXAMPLE (ES): Spanish translation\\nEXAMPLE (ID): Indonesian translation\\nInspiration\\nFor now, I'm not sure how this small corpus can be used. Hopefully this will be helpful for some pilot linguistic project.\",\n",
       " \"Visit my GitHub for detailed description and code.\\nAlso, here's an App, a Recommendation System for Museum Selection, I created with this data set: http://216.230.228.88:3838/bootcamp006_project/Project5-Capstone/Museo/app/\\nInspiration for Further Analysis\\nPredict if a museum will be featured or not\\nDiscover the important factors that results in a higher rating\\nApply natural language processing to discover insights from review/quote/tag data\\nCluster museums based on review/quote polarity or subjectivity (sentiment analysis)\\nApply association rules to uncover relationship of different combinations of review tags\\nProcessed and Merged Data\\ntripadvisor_merged.csv: A file containing museum data collected from TripAdvisor including tag/ type/ review/ quote features\\nRaw Data Scraped from TripAdvisor (US/World)\\ntripadvisor_museum: general museum data scraped from TripAdvisor\\ntraveler_type: {'museum': ['Families','Couples','Solo','Business','Friends']}\\ntraveler_rating: {'museum': ['Excellent','Very good','Average','Poor','Terrible']}\\ntag_clouds: {'museum': ['tag 1', 'tag 2', 'tag 3' ...]}\\nreview_quote: {'museum': ['quote 1', 'quote 2', ... ,'quote 10']}\\nreview_content: {'museum': ['review 1', 'review 2', ... ,'review 10']}\\nmuseum_categories: {'museum': ['museum type 1','museum type 2', ...]}\",\n",
       " 'Context\\nData set contains the recorded number of dengue cases per 100,000 population per region of the Philippines from 2008 to 2016\\nContent\\nThis is a small data set that is a good starting point for beginners that wants to play around with small scale temporal and spatial data set\\nAcknowledgements\\nPublisher would like to thank the Department of Health of the Philippines for providing the raw data\\nInspiration\\nWhat is the trend of dengue cases in the Philippines? What region/s recorded the highest prevalence of dengue cases? In what specific years do we observe the highest dengue cases? When and where will a possible dengue outbreak occur?',\n",
       " 'Context\\nThis dataset contains the metadata of over 50,000 tweets from Christmas Eve and Christmas. We are hoping the data science and research community can use this to develop new and informative conclusions about this holiday season.\\nContent\\nWe acquired this data through a web crawler written in Java. The first field is the id of the tweet, and the second is the HTML metadata. We recommend using BeautifulSoup or another library to parse this data and extract information from each tweet.\\nInspiration\\nWe would especially like to see research on the use of emojis in tweets, the type of sentiment there is on Christmas (Maybe determine how grateful each country is), or some kind of demographic on the age or nationality of active Twitter users during Christmas.',\n",
       " 'Overview\\nOnly two major-party candidates are competing in the 2016 US presidential election, but there was tough competition to get to the general election. This dataset contains transcripts of every Democratic, Republican, and Republican Undercard debate held during the 2016 primary season.\\nThis dataset is meant to be a complement to Megan Risdal\\'s transcripts of the 2016 US Presidential (General Election) Debates.\\nSo you can now take all of the questions (who talks the most? who has a wider vocabulary?) that you answered in the general election debates, and apply the same procedures to see how your favorite (or least favorite) candidate has changed over time.\\nThe column names (and order) in this dataset are a superset of those found in the general election dataset, and non-speech annotations (such as \"(applause)\") in this dataset are also a superset. Kernels uploaded for the general election dataset should be compatible with this dataset as well; please let me know if you have any compatibility issues.\\nWhat in the world is an \"Undercard\" debate?\\nThe field of Republicans running for President in the primaries was (yuuuuge!) pretty big: 17 candidates threw their hat in the ring at one point or another. Debate organizers realized that having 17 people on stage (each with a set amount of time to answer a question / respond to a criticism / interrupt each other) would, in the best case, lead to a three-to-four-hour-long debate (and, in the worst case, lead to a chaotic shout-fest as candidates tried to talk over each other for three to four hours).\\nTo alleviate this issue, many of the Republican debate nights were split into two separate debates: the main debate with the top party contenders, aired live during primetime; and the Undercard debate, which typically aired a few hours earlier than the main debate.\\nThe criteria for a candidate to be allowed into the main debate (rather than the \"kids\\' table\" debate, as some pundits derisively called the Undercard event) varied a bit by event. Typically a poll showing of 1% in one of several specified polls was sufficient to gain admission to the Undercard. To get into the main debate, candidates had to either (a) be polling above a different, higher, percentage in specific polls, or (b) be among the top n Republican candidates in said polls.\\nThe details get a little thorny (certain debates had multiple criteria, of which candidates had to meet at least one), so I refer questions to the individual Undercard debate pages at the American Presidency Project for detailed criteria.\\nIn this dataset, the split between Republican debates is made in the Party column: Republican is used for the primetime main events, and Republican Undercard is used for the Undercard.\\nAcknowledgements\\nAll transcripts were scraped from the presidential debates page of the American Presidency Project at the University of California, Santa Barbara. Individual lines in the dataset contain links to the particular page for that debate.\\nUpdates\\nv2 contains a few minor changes related to normalizing parenthesized elements (non-speech) within the Text field, and adding a few lines of interruptions that persisted in the Text field\\nThe Data\\nThe fields are described more fully in the file description. This section describes the particular elements that can appear in the Speaker and Text fields.\\nWho\\'s Who?\\n(aka, the Speaker column)\\nThe primary debates had a ton of participants. This dataset contains utterances from 22 politicians and 49 moderators, not to mention the occasional audience laughter or 2-minute timer.\\nAlmost all Speaker columns contain either a single title-case name (listed below in the Participants and Moderators subsections) or a single upper-case word indicating non-speaker noise (listed below in the None-speaker Turns subsection); the exceptions to this are cases where a name is concatenated with a space and a parenthesized tag, as follows:\\nspkr (VIDEO): transcriptions of pre-recorded material of any of the candidates or moderators\\nspkr (TRANSLATED): in the Univision/Telemundo debate, some questions and answers are translated into English from the original Spanish; the transcript reflects the translations as spoken by a translator\\nNon-speaker Turns\\nThe non-speaker turns in the Speaker column are:\\nAUDIENCE: any laughter, booing, applause, etc. from the audience\\nCANDIDATES: cross-talk between candidates\\nOTHER: non-speaker, non-audience noise (such as commercial break, timer bell, national anthem, etc.)\\nQUESTION: a question from an audience member (or a prerecorded question)\\nUNKNOWN: cases where the transcriber could hear a phrase but could not determine who said it\\nHere are the various speakers who appear in the dataset:\\nCandidates\\nDemocratic\\nChafee: Former Governor Lincoln Chafee (RI)\\nClinton: Former Secretary of State Hillary Clinton\\nO\\'Malley: Former Governor Martin O\\'Malley (MD)\\nSanders: Senator Bernie Sanders (VT)\\nWebb: Former Senator Jim Webb (VA)\\nRepublican\\nBush: Former Governor Jeb Bush (FL)\\nCarson: Ben Carson\\nCruz: Senator Ted Cruz (TX)\\nKasich: Governor John Kasich (OH)\\nPaul: Senator Rand Paul (KY)\\nRubio: Senator Marco Rubio (FL)\\nTrump: Donald Trump\\nWalker: Governor Scott Walker (WI)\\nRepublican (Undercard ONLY)\\nGilmore: Former Governor Jim Gilmore (VA)\\nGraham: Senator Lindsey Graham (SC)\\nJindal: Governor Bobby Jindal (LA)\\nPataki: Former Governor George Pataki (NY)\\nPerry: Former Governor Rick Perry (TX)\\nSantorum: Former Senator Rick Santorum (PA)\\nRepublican (Main AND Undercard)\\nChristie: Governor Chris Christie (NJ)\\nFiorina: Carly Fiorina\\nHuckabee: Former Governor Mike Huckabee (AR)\\nAll candidates in a Python list for easy copy/paste\\n[\\'Bush\\', \\'Carson\\', \\'Chafee\\', \\'Christie\\', \\'Clinton\\', \\'Cruz\\', \\'Fiorina\\', \\'Gilmore\\', \\'Graham\\', \\'Huckabee\\', \\'Jindal\\', \\'Kasich\\', \"O\\'Malley\", \\'Pataki\\', \\'Paul\\', \\'Perry\\', \\'Rubio\\', \\'Sanders\\', \\'Santorum\\', \\'Trump\\', \\'Walker\\', \\'Webb\\']\\nModerators\\nNOTE: Some moderators are seen across various debates; in particular, the Republican main debates and undercard debates on a given day tend to share the same moderators. Some moderators are public figures who are seen only in videos (with the (VIDEO) tag).\\nModerators\\nArrarás: María Celeste Arrarás (Telemundo)\\nBaier: Bret Baier (Fox News)\\nBaker: Gerard Baker (The Wall Street Journal)\\nBartiromo: Maria Bartiromo (Fox Business Network)\\nBash: Dana Bash (CNN)\\nBlitzer: Wolf Blitzer (CNN)\\nCavuto: Neil Cavuto (Fox Business Network)\\nCooney: Kevin Cooney (CBS News)\\nCooper: Anderson Cooper (CNN)\\nCordes: Nancy Cordes (CBS News)\\nCramer: Jim Cramer (CNBC)\\nCuomo: Governor Andrew Cuomo (NY)\\nDickerson: John Dickerson (CBS News)\\nDinan: Stephen Dinan (Washington Times)\\nEpperson: Sharon Epperson (CNBC)\\nGarrett: Major Garrett (CBS News)\\nHam: Mary Katharine Ham (Hot Air)\\nHannity: Sean Hannity (Fox News)\\nHarwood: John Harwood (CNBC)\\nHemmer: Bill Hemmer (Fox News)\\nHewitt: Hugh Hewitt (Salem Radio Network)\\nHolt: Lester Holt (NBC News)\\nIfill: Gwen Ifill (PBS)\\nKelly: Megyn Kelly (Fox News)\\nLemon: Don Lemon (CNN)\\nLevesque: Neil Levesque (New Hampshire Institute of Politics)\\nLopez: Juan Carlos Lopez (CNN en Espanol)\\nLouis: Errol Louis (NY1)\\nMacCallum: Martha MacCallum (Fox News)\\nMaddow: Rachel Maddow (MSNBC)\\nMcelveen: Josh McElveen (WMUR-TV)\\nMitchell: Andrea Mitchell (NBC News)\\nMuir: David Muir (ABC News)\\nO\\'Reilly: Bill O\\'Reilly (Fox News)\\nObradovich: Kathie Obradovich (The Des Moines Register)\\nQuick: Becky Quick (CNBC)\\nQuintanilla: Carl Quintanilla (CNBC)\\nRaddatz: Martha Raddatz (ABC News)\\nRamos: Jorge Ramos (Univision)\\nRegan: Trish Regan (Fox Business Network)\\nSalinas: María Elena Salinas (Univision)\\nSantelli: Rick Santelli (CNBC)\\nSeib: Gerald Seib (The Wall Street Journal)\\nStrassel: Kimberly Strassel (The Wall Street Journal)\\nTapper: Jake Tapper (CNN)\\nTodd: Chuck Todd (MSNBC)\\nTumulty: Karen Tumulty (The Washington Post)\\nWallace: Chris Wallace (Fox News)\\nWoodruff: Judy Woodruff (PBS)\\nAll moderators in a Python list for easy copy/paste\\n[\\'Arrarás\\', \\'Baier\\', \\'Baker\\', \\'Bartiromo\\', \\'Bash\\', \\'Blitzer\\', \\'Cavuto\\', \\'Cooney\\', \\'Cooper\\', \\'Cordes\\', \\'Cramer\\', \\'Cuomo\\', \\'Dickerson\\', \\'Dinan\\', \\'Epperson\\', \\'Garrett\\', \\'Ham\\', \\'Hannity\\', \\'Harwood\\', \\'Hemmer\\', \\'Hewitt\\', \\'Holt\\', \\'Ifill\\', \\'Kelly\\', \\'Lemon\\', \\'Levesque\\', \\'Lopez\\', \\'Louis\\', \\'MacCallum\\', \\'Maddow\\', \\'Mcelveen\\', \\'Mitchell\\', \\'Muir\\', \"O\\'Reilly\", \\'Obradovich\\', \\'Quick\\', \\'Quintanilla\\', \\'Raddatz\\', \\'Ramos\\', \\'Regan\\', \\'Salinas\\', \\'Santelli\\', \\'Seib\\', \\'Strassel\\', \\'Tapper\\', \\'Todd\\', \\'Tumulty\\', \\'Wallace\\', \\'Woodruff\\']\\nWhat\\'s What?\\n(aka, the Text column)\\nIn general, the Text column contains fully punctuated and appropriately capitalized speech transcriptions. Most parenthesized elements are non-speech elements, with the following exceptions:\\n(c) and (4): are spoken in reference to 501(c)(4)s (tax-exempt lobbying groups)\\n(k): spoken in reference to 401(k)s (individual pension accounts)\\nThe non-speech elements that can be found in parentheses in the Text column are:\\n(ANTHEM): the national anthem is played\\n(APPLAUSE): the audience expresses approval\\n(BELL): a bell or buzzer indicating that a candidate\\'s time has expired\\n(BOOING): the audience expresses disapproval\\n(COMMERCIAL): the televised debate breaks for a commercial advertisement\\n(CROSSTALK): more than one candidate or moderator are speaking at the same time\\n(LAUGHTER): the audience expresses a sense of humor\\n(MOMENT.OF.SILENCE): the debate pauses for a moment of silence\\n(SPANISH): the utterance is in Spanish (for the Democrats\\' Univision-hosted debate on 3/9/16 in Miami)\\n(VIDEO.END): a video clip ends\\n(VIDEO.START): a video clip begins\\n(inaudible): the utterance was inaudible, off-mike, or too indecipherable to transcribe',\n",
       " 'Context\\nMovebank is a free, online database of animal tracking data hosted by the Max Planck Institute for Ornithology. It is designed to help animal tracking researchers to manage, share, protect, analyze, and archive their data. Movebank is an international project with over 11,000 users, including people from research and conservation groups around the world. The animal tracking data accessible through Movebank belongs to researchers all over the world. These researchers can choose to make part or all of their study information and animal tracks visible to other registered users, or to the public.\\nAnimal tracking\\nAnimal tracking data helps us understand how individuals and populations move within local areas, migrate across oceans and continents, and evolve through millennia. This information is being used to address environmental challenges such as climate and land use change, biodiversity loss, invasive species, and the spread of infectious diseases. Read more.',\n",
       " \"Context\\nThis dataset contains energy usage information for every building owned and managed by NYC DCAS. DCAS, or the Department of Citywide Administrative Services, is the arm of the New York City municipal government which handles ownership and management of the city's office facilities and real estate inventory. The organization voluntarily publicly discloses self-measured information about the energy use of its buildings.\\nContent\\nThis data contains information on the name, address, location, and 2015 financial cycle energy usage of every building managed at that time by DCAS.\\nAcknowledgements\\nThis dataset is published as-is by of the City of New York (here).\\nInspiration\\nBy combining this dataset with the New York City Buildings Database, what can you learn about the energy usage of buildings in New York City?\\nCan you use this data to model the energy consumption for city's office space at large?\",\n",
       " 'Short Track Speed Skating Database for Sports Data Analysis\\nWhat is short track speed skating?\\nMaybe some people have never heard of this sport. Short track is a competitive and strategic game in which skaters race on ice. Sometimes the smartest or the luckiest guy, rather than the strongest, wins the game (for example).\\nWhat\\'s in the data?\\nThe database covers all the international short track games in the last 5 years. Currently it contains only men\\'s 500m, but I will keep updating it.\\nDetailed lap data including personal time and ranking in each game from seasons 2012/2013 to present .\\nThe final time results, ranking, starting position, qualified or penalized information of each athlete in each game.\\nAll series of World Cup, World Championship, European Championship and Olympic Games.\\nOriginal data source\\nThe data is collected from the ISU\\'s (International Skating Union) official website. I have already done the cleaning procedure.\\nPlease make sure that the data are only for personal and non-commercial use.\\nExplore the data\\nInteresting questions may be like:\\nWhat will happen in a game when there are more than one athlete from the same team? Are there performance all improved?\\nHow does the performance of athletes change within a season and over seasons?\\nDo some athletes have special patterns in terms of time allocation and surpassing opportunity?\\nWhat is the influence of the implementation of \"no toe starts\" rules on athletes since July 2015?\\nIs there also home advantage like in other sports?\\nWho are the most \"dangerous\" guys that always get penalty?',\n",
       " \"Context\\n« BIXI Montréal is a public bicycle sharing system serving Montréal, Quebec, Canada.\\nLaunched in May 2009, it is North America's first large-scale bike sharing system and the original BIXI brand of systems.\\nThe location of a BIXI bike station is determined by several parameters, including population density, points of interest and activities (universities, bike paths, other transportation networks, and data on travel patterns of the general public. In 2009, 5,000 bikes were deployed in Montreal through a network of pay stations located mainly in the boroughs of Rosemont–La Petite-Patrie, the Plateau-Mont-Royal and Ville-Marie, spilling over into parts of Outremont and the South West. As of 2011, the system has spread to Hochelaga-Maisonneuve, Villeray–Saint-Michel–Parc-Extension, Ahuntsic, Côte-des-Neiges–Notre-Dame-de-Grâce, Westmount and Verdun. » [1]\\nContent\\n1. BIXI - Movements history\\nDatasets containing the details of the travels made via the BIXI Montréal self-service bike network. Each year is a .zip file (ex. BixiMontrealRentals2014.zip) containing several .CSV files (ex. OD_2014-04.csv) for each months.\\n\\nVersion 2 : All .csv are merged per year (OD-year.csv).\\n\\nThe data is extracted from the BIXI Montréal station and bike management system. Trips of less than 1 minute or more than 2 hours are excluded. The station identifiers used correspond to those of the station status data set\\nData Sructure\\nstart_date: Date and time of the start of the trip ( AAAA-MM-JJ hh:mm )\\nstart_station_code: Start station ID\\nend_date: Date and time of the start of the trip ( AAAA-MM-JJ hh:mm )\\nend_station_code : End station ID\\nis_member : Type users. (1 : Suscriber, 0 : Non-suscriber)\\nduration_sec: Total travel time in seconds\\n\\n2. BIXI - The condition of stations\\nThis dataset presents the list of stations in the BIXI Montréal self-service bicycle network, including the geographic position, the number of bicycles available and the number of terminals available. It is a .json file, which corresponds to a dictionary.\\nThe data is produced by the BIXI Montréal station management system with a refresh rate of 5 minutes. Station locations are generally stable over time, but may be subject to change during the season, particularly when the City of Montreal is carrying out work or as part of special events. Temporary storage stations are not included in station status. The data is automatically generated on the BIXI servers, so the date of last update of this dataset does not represent the actual date of update. Information about bikes and bad terminals is available in the JSON format file.\\nData Sructure\\nid: Unique station ID\\ns: Name of the station\\nn: Station terminal ID\\nst: Station status\\nb: Boolean value (true or false) specifying whether the station is blocked\\nsu: Boolean value (true or false) specifying whether the station is suspended\\nm: Boolean value (true or false) specifying whether the station is displayed as out of service\\nread: timestamp of the last update of the data in milliseconds since January 1, 1970.\\nlc: timestamp of the last communication with the server in milliseconds since January 1, 1970.\\nbk: (For future use)\\nbl: (For future use)\\nla: latitude of the station according to the geodesic datum WGS84\\nlo: longitude of the station according to the WGS84 datum\\nda: Number of available terminals at this station\\ndx: Number of unavailable terminals at this station\\nba: Number of available bikes at this station\\nbx: Number of unavailable bicycles at this station\\n3. Geographical boundaries of Montreal (Borough and related city)\\nThis dataset is optional and will be useful mostly for ploting data and doing some choropleth maps.\\nAcknowledgements\\nCreative Commons Attribution 4.0 International\\nFor more details :\\nhttp://donnees.ville.montreal.qc.ca/dataset/bixi-historique-des-deplacements\\nhttp://donnees.ville.montreal.qc.ca/dataset/bixi-etat-des-stations\\nhttp://donnees.ville.montreal.qc.ca/dataset/polygones-arrondissements\\nInspiration\\nCan you find pattern in the behavior of Bixi users?\\nAre there any inefficient stations ?\\nWhat insights can we use from this data for decision making ?\\n[1] https://en.wikipedia.org/wiki/BIXI_Montr%C3%A9al\",\n",
       " \"Context\\nElon Musk is an American business magnate. He was one of the founders of PayPal in the past, and the founder and/or cofounder and/or CEO of SpaceX, Tesla, SolarCity, OpenAI, Neuralink, and The Boring Company in the present. He is known as much for his extremely forward-thinking ideas and huge media presence as he is for his extremely business savvy.\\nMusk is famously active on Twitter. This dataset contains all tweets made by @elonmusk, his official Twitter handle, between November 16, 2012 and September 29, 2017.\\nContent\\nThis dataset includes the body of the tweet and the time it was made, as well as who it was re-tweeted from (if it is a retweet).\\nInspiration\\nCan you figure out Elon Musk's opinions on various things by studying his Twitter statements?\\nHow Elon Musk's post rate increased, decreased, or stayed about the same over time?\",\n",
       " 'Context\\nData Scientists often use crowdsourcing platforms, such as Amazon Mechanical Turk or CrowdFlower to collect labels for their data. Controlling high quality and timeless execution of tasks is an important part of such collection process. It is not possible (or not efficient) to manually check every worker assignment. There is an intuition that there quality could be predicted based on workers task browser behaviour (e.g. key presses, scrolling, mouse clicks, tab switching). In this dataset there are assignment results for 3 different crowdsourcing tasks launched on CrowdFlower, along with associated workers behaviour.\\nContent\\nWe collected data running 3 tasks:\\nImage labelling,\\nReceipt Transcription,\\nBusiness Search.\\nTasks are described in tasks.csv. Results for corresponding tasks are given in files: results_{task_id}.csv. Workers\\'s activity could be found in the following files:\\nactivity_keyboard.csv - timestamps of keyboard keys pressed\\nactivity_mouse.csv - timestamps of mouse clicks with associated HTML elements\\nactivity_tab.csv - timestamps of event task browser tab changes (opened, active, hidden, closed)\\nactivity_page.csv - a summary of events happened in the task page every 2 seconds (boolean keyboard activity, boolean mouse movement activity, boolean scrolling activity, the position of the screen, boolean if text was selected)\\nResult files have a similar structure to the original one given by CrowdFlower:\\n_unit_id: A unique ID number created by the system for each row\\n_created_at: The time the contributor submitted the judgement\\n_golden: This will be \"true\" if this is a test question, otherwise it is \"false\"\\n_id: A unique ID number generated for this specific judgment\\n_missed: This will be \"true\" if the row is an incorrect judgment on a test question.\\n_started_at: The time at which the contributor started working on the judgement\\n_tainted: This will be \"true\" if the contributor has been flagged for falling below the required accuracy. This judgment will not be used in the aggregation.\\n_channel: The work channel that the contributor accessed the job through\\n_trust: The contributor\\'s accuracy. Learn more about trust here\\n_worker_id: A unique ID number assigned to the contributor (in the current dataset MD5 value is given)\\n_country: The country the contributor is from\\n_region: A region code for the area the contributor is from\\n_city: The city the contributor is from\\n_ip: The IP address for the contributor (in the current dataset MD5 value is given)\\n{{field}}: There will be a column for each field in the job, with a header equal to the field\\'s name.\\n{{field}}_gold: The correct answer for the test question\\nAcknowledgements\\nWe thank crowd workers who accomplished our not always exciting tasks on CrowdFlower.',\n",
       " 'Context\\nThis database was used in the paper: \"Covert online ethnography and machine learning for detecting individuals at risk of being drawn into online sex work\". https://www.flinders.edu.au/centre-crime-policy-research/illicit-networks-workshop\\nContent\\nThe database includes data scraped from a European online adult forum. Using covert online ethnography we interviewed a small number of participants and determined their risk to either supply or demand sex services through that forum. This is a great dataset for semi-supervised learning.\\nInspiration\\nHow can we identify individuals at risk of being drawn into online sex work? The spread of online social media enables a greater number of people to be involved into online sex trade; however, detecting deviant behaviors online is limited by the low available of data. To overcome this challenge, we combine covert online ethnography with semi-supervised learning using data from a popular European adult forum.',\n",
       " 'Context\\nCars dataset with features including make, model, year, engine, and other properties of the car used to predict its price.\\nContent\\nScraped from Edmunds and Twitter.\\nAcknowledgements\\nEdmunds and Twitter for providing info Sam Keene\\nInspiration\\nEffects of features on the price How does the brand affect the price? What cars can be consider overpriced? Price VS. popularity?',\n",
       " 'Context\\nThe Federal Election Commission (FEC) is an independent regulatory agency established in 1975 to administer and enforce the Federal Election Campaign Act (FECA), which requires public disclosure of campaign finance information. The FEC publishes campaign finance reports for presidential and legislative election campaign candidates on the Campaign Finance Disclosure Portal.\\nContent\\nThe finance summary report contains one record for each financial report (Form 3P) filed by the presidential campaign committees during the 2016 primary and general election campaigns. Presidential committees file quarterly prior to the election year and monthly during the election year. The campaign expenditures file contains individual operating expenditures made by the campaign committee and reported on Form 3P Line 23 during the same period. Operating expenditures consist of the routine costs of campaigning for president, which include staffing, travel, advertising, voter outreach, and other activities.',\n",
       " \"The New York Public Library is digitizing and transcribing its collection of historical menus. The collection includes about 45,000 menus from the 1840s to the present, and the goal of the digitization project is to transcribe each page of each menu, creating an enormous database of dishes, prices, locations, and so on. As of early November, 2016, the transcribed database contains 1,332,279 dishes from 17,545 menus.\\nThe Data\\nThis dataset is split into four files to minimize the amount of redundant information contained in each (and thus, the size of each file). The four data files are Menu, MenuPage, MenuItem, and Dish. These four files are described briefly here, and in detail in their individual file descriptions below.\\nMenu\\nThe core element of the dataset. Each Menu has a unique identifier and associated data, including data on the venue and/or event that the menu was created for; the location that the menu was used; the currency in use on the menu; and various other fields.\\nEach menu is associated with some number of MenuPage values.\\nMenuPage\\nEach MenuPage refers to the Menu it comes from, via the menu_id variable (corresponding to Menu:id). Each MenuPage also has a unique identifier of its own. Associated MenuPage data includes the page number of this MenuPage, an identifier for the scanned image of the page, and the dimensions of the page.\\nEach MenuPage is associated with some number of MenuItem values.\\nMenuItem\\nEach MenuItem refers to both the MenuPage it is found on -- via the menu_page_id variable -- and the Dish that it represents -- via the dish_id variable. Each MenuItem also has a unique identifier of its own. Other associated data includes the price of the item and the dates when the item was created or modified in the database.\\nDish\\nA Dish is a broad category that covers some number of MenuItems. Each dish has a unique id, to which it is referred by its affiliated MenuItems. Each dish also has a name, a description, a number of menus it appears on, and both date and price ranges.\\nInspiration\\nWhat are some things we can look at with this dataset?\\nHow has the median price of restaurant dishes changed over time? Are there particular types of dishes (alcoholic beverages, seafood, breakfast food) whose price changes have been greater than or less than the average change over time?\\nCan we predict anything about a dish's price based on its name or description?\\n-- There's been some work on how the words used in advertisements for potato chips are reflective of their price; is that also true of the words used in the name of the food?\\n-- Are, for example, French or Italian words more likely to predict a more expensive dish?\\nAcknowledgments\\nThis dataset was downloaded from the New York Public Library's What's on the menu? page. The What's on the menu? data files are updated twice monthly, so expect this dataset to go through multiple versions.\",\n",
       " 'Context\\nI am interested in indoor positioning technology and had been playing with blue-tooth beacons. Typically blue-tooth beacons have accuracy of a range between 1 to 4 meters. I am thinking of maybe using machine learning can produce results of greater accuracy compared to using traditional filtering methods e.g. kalman filters, particle filters.\\nContent\\n3 Kontakt blue-tooth beacons are mounted in a 2.74 meters wide x 4.38 meters long (width x length) room. The 3 beacons are transmitting at a transmit power of -12dbm. A Sony Xperia E3 smartphone with blue-tooth turned on is used as a receiver to the record the data. Recordings are done on several positions in the room of an interval of 30-60 seconds in the same position.\\nBeacons location # Beacon X(meters) Y(meters) BeaconA 0.10 1.80 BeaconB 2.74 2.66 BeaconC 1.22 4.54\\nFields:\\nThe distance in meters between beacon A and the device calculated by using the rssi of this blue-tooth beacon.\\nThe distance in meters between beacon B and the device calculated by using the rssi of this blue-tooth beacon.\\nThe distance in meters between beacon C and the device calculated by using the rssi of this blue-tooth beacon.\\nX coordinate in centimeters rounded to the nearest centimeter measured using a measuring tape with +/-1cm accuracy.\\nY coordinate in centimeters rounded to the nearest centimeter measured using a measuring tape with +/-1cm accuracy.\\nDate and time of the recording.\\nAcknowledgements\\nThe data is collected solely by me.\\nInspiration\\nTo try and improve on the accuracy of indoor position using blue-tooth beacons which typically have accuracy of range between 1 meters to 4 meters.',\n",
       " \"Context\\nThis dataset was collected using our App EC Taximeter.\\nAn easy to use tool developed to compare fees, giving the user an accurate fee based on GPS to calculate a cost of the taxi ride. Due to the ability to verify that you are charged fairly, our App is very popular in several cities. We encourage our users to send us URLs with the taxi/transportation fees in their cities to keep growing our database.\\n★ Our App gets the available fares for your location based on your GPS, perfect when traveling and not getting scammed.\\n★ Users can start a taximeter in their own phone and check they are charged fairly\\n★ Several useful information is displayed to the user during the ride: Speed, Wait time, Distance, GPS update, GPS precision, Range of error.\\n★ Each fare has information available for reference like: Schedule, Minimum fee, Source, Last update.\\n★ It’s possible to surf through several cities and countries which fares are available for use. If a fare is not in the app, now it’s easier than ever to let us know thanks to Questbee Apps.\\nWe invite users to contribute to our project and expect this data set to be useful, please don't hesitate to contact us to info@ashkadata.com to add your city or to contribute with this project.\\nContent\\nThe data is collected from June 2016 until July 20th 2017. The data is not completely clean, many users forget to turn off the taximeter when done with the route. Hence, we encourage data scientist to explore it and trim the data a little bit\\nAcknowledgements\\nWe have to acknowledge the valuable help of our users, who have contributed to generate this dataset and have push our growth by mouth to mouth recommendation.\\nInspiration\\nOur first inspiration for the App was after being scammed in our home city Quito. We started it as a tool for people to be fairly charged when riding a taxi. Currently with other transportation options available, we also help user to compare fares in their cities or the cities which they are visiting.\\nFile descriptions\\nmex_clean.csv - the dataset contains information of routes in Mexico City\\nuio_clean.csv - the dataset contains information of routes in Quito Ecuador\\nbog_clean.csv - the dataset contains information of routes in Bogota\\nall-data_clean.csv - the dataset contains information of routes in different cities\",\n",
       " \"Presidential elections have just finished in France, with two rounds on April 23rd and May 7th 2017. The results of the elections are available online for each ~67000 polling stations around the country. This data can be used to gain insights about both the electorate and the candidates. Examples of basic questions worth looking at are:\\nHow are candidates' scores correlated with each other? Can you infer 'political affinity' of the candidates just looking at the data, without any other a priori knowledge?\\nHow are scores of various candidates correlated with the turnout? Supporters of which candidate are the most 'participative'?\\nHow did the votes get redistributed from the first to the second round? Can you build an a posteriori predictive model of the second round results taking as an input the results of the first round ?\\nThe data provides coordinates of each polling station. Can you gain any insight from the geography?\\nSome preliminary analysis is described in two blog posts here and here. This dataset is inspired by an analogous one for the 2016 US elections by Ben Hamner.\",\n",
       " \"Predict molecular properties\\nContext\\nThis dataset contains molecular properties scraped from the PubChem database. Each file contains properties for thousands of molecules , made up of the elements H, C, N, O, F, Si, P, S, Cl, Br, and I. The dataset is related to a previous one which had fewer number of molecules, where the features were preconstructed.\\nInstead, this dataset is a challenging case for feature engineering and is subject of active research (see references below).\\nData Description\\nThe utilities used to download and process the data can be accessed from my Github repo.\\nEach JSON file contains a list of molecular data. An example molecule is given below:\\n{\\n'En': 37.801,\\n'atoms': [\\n{'type': 'O', 'xyz': [0.3387, 0.9262, 0.46]},\\n{'type': 'O', 'xyz': [3.4786, -1.7069, -0.3119]},\\n{'type': 'O', 'xyz': [1.8428, -1.4073, 1.2523]},\\n{'type': 'O', 'xyz': [0.4166, 2.5213, -1.2091]},\\n{'type': 'N', 'xyz': [-2.2359, -0.7251, 0.027]},\\n{'type': 'C', 'xyz': [-0.7783, -1.1579, 0.0914]},\\n{'type': 'C', 'xyz': [0.1368, -0.0961, -0.5161]},\\n{'type': 'C', 'xyz': [-3.1119, -1.7972, 0.659]},\\n{'type': 'C', 'xyz': [-2.4103, 0.5837, 0.784]},\\n{'type': 'C', 'xyz': [-2.6433, -0.5289, -1.426]},\\n{'type': 'C', 'xyz': [1.4879, -0.6438, -0.9795]},\\n{'type': 'C', 'xyz': [2.3478, -1.3163, 0.1002]},\\n{'type': 'C', 'xyz': [0.4627, 2.1935, -0.0312]},\\n{'type': 'C', 'xyz': [0.6678, 3.1549, 1.1001]},\\n{'type': 'H', 'xyz': [-0.7073, -2.1051, -0.4563]},\\n{'type': 'H', 'xyz': [-0.5669, -1.3392, 1.1503]},\\n{'type': 'H', 'xyz': [-0.3089, 0.3239, -1.4193]},\\n{'type': 'H', 'xyz': [-2.9705, -2.7295, 0.1044]},\\n{'type': 'H', 'xyz': [-2.8083, -1.921, 1.7028]},\\n{'type': 'H', 'xyz': [-4.1563, -1.4762, 0.6031]},\\n{'type': 'H', 'xyz': [-2.0398, 1.417, 0.1863]},\\n{'type': 'H', 'xyz': [-3.4837, 0.7378, 0.9384]},\\n{'type': 'H', 'xyz': [-1.9129, 0.5071, 1.7551]},\\n{'type': 'H', 'xyz': [-2.245, 0.4089, -1.819]},\\n{'type': 'H', 'xyz': [-2.3, -1.3879, -2.01]},\\n{'type': 'H', 'xyz': [-3.7365, -0.4723, -1.463]},\\n{'type': 'H', 'xyz': [1.3299, -1.3744, -1.7823]},\\n{'type': 'H', 'xyz': [2.09, 0.1756, -1.3923]},\\n{'type': 'H', 'xyz': [-0.1953, 3.128, 1.7699]},\\n{'type': 'H', 'xyz': [0.7681, 4.1684, 0.7012]},\\n{'type': 'H', 'xyz': [1.5832, 2.901, 1.6404]}\\n],\\n'id': 1,\\n'shapeM': [259.66, 4.28, 3.04, 1.21, 1.75, 2.55, 0.16, -3.13, -0.22, -2.18, -0.56, 0.21, 0.17, 0.09]\\n}\\nEn: This field is the molecular energy calculated using a force-field method. See references [1,2] for details. This is the target variable which is being predicted.\\natoms: This field contains the name of the element and the position (x,y,z coordinates) and needs to be used for feature engineering.\\nid : This field is the PubChem Id\\nshapeM : This field contains the shape multipoles and can be used for feature engineering. For definition of shape multipoles, see reference [3].\\nNotice that each molecule contains different number and types of atoms, so it is challenging to come up with features that can describe every molecule in a unique way. There are several approaches taken in the literature (see the references), one of which is to use the Coulomb Matrix for a given molecule defined by\\nCIJ=\\nZIZJ\\n|RI−RJ|\\n,(I≠J)CIJ=Z\\n2.4\\nI\\n,(I=J)\\nwhere $Z_I$ are atomic numbers (can be looked up from the periodic table for each element), and ${\\\\vert R_I - R_J \\\\vert}$ is the distance between two atoms I and J. The previous dataset used these features for a subset of molecules given here, where the maximum number of elements in a given molecules was limited by 50. In this case, each molecule has a 50x50 Coulomb matrix where zeroes were used as padding when the molecule had smaller than 50 number of atoms.\\nThere are around 100,000,000 molecules in the whole database. As more files are scraped, new data will be added in time.\\nNote: In the previous dataset, the molecular energies were computed by quantum mechanical simulations. Here, the given energies are computed using another method, so their values are different.\\nInspiration\\nSimulations of molecular properties are computationally expensive. The purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database. In the PubChem database, there are around 100,000,000 molecules. It could take years to do simulations on all of these molecules, however machine learning can be used to predict their properties much faster. As a result, this could open up many possibilities in computational design and discovery of molecules, compounds and new drugs.\\nThis is a regression problem so mean squared error is minimized during training.\\nI am looking for Kagglers to find the best model and reduce mean squared error as much as possible!\\nReferences\\n[1] Halgren TA. Merck Molecular Force Field: I. Basis, Form, Scope, Parameterization and Performance of MMFF94. J. Comp. Chem. 1996;17:490-519.\\n[2] Halgren TA. Merck Molecular Force Field: VI. MMFF94s Option for Energy Minimization Studies. J. Comp. Chem. 1999;20:720-729.\\n[3] Kim, Sunghwan, Evan E Bolton, and Stephen H Bryant. “PubChem3D: Shape Compatibility Filtering Using Molecular Shape Quadrupoles.” J. Cheminf. 3 (2011): 25.\\n[4] Himmetoglu B.: Tree based machine learning framework for predicting ground state energies of molecules, J. Chem. Phys 145, 134101 (2016)\\n[5] Rupp M., Ramakrishnan R., von Lilienfeld OA.: Machine Learning for Quantum Mechanical Properties of Atoms in Molecules, J. Phys. Chem. Lett. , 6(16): 3309–3313 (2015)\\n[6] Montavon G., Rupp M., Gobre V., Vazquez-Mayagoitia A., Hansen K., Tkatchenko A., Müller K-R., von Lilienfeld OA.: Machine learning of molecular electronic properties in chemical compound space, New J. Phys., 15(9): 095003 (2013)\\n[7] Hansen K., Montavon G., Biegler F., Fazli S., Rupp M., Scheffler M., von Lilienfeld OA., Tkatchenko A., Müller K-P.: Assessment and Validation of Machine Learning Methods for Predicting Molecular Atomization Energies, J. Chem. Theory Comput. , 9(8): 3543–3556 (2013)\",\n",
       " 'About This Dataset\\nYou can use this fonts file to generate some Chinese character. Use this image can train a machine learning model to recognize text.\\nDataset is updating\\nTell me if you have other font file or anything related to this topic.',\n",
       " 'Context\\nI have been a comic book fan for many years and, when I started writing web scrapers for practice, it was only natural that I did one inspired by my passion for Marvel.\\nContent\\nThis dataset has 27.290 rows, each one representing a distinct Marvel character, such as Peter Parker, Tony Stark or Jean Grey. As for columns, there are 1822 of them, one for each Marvel universe. All the cells contain a boolean value: true if there is a version of that character from that universe or false otherwise.\\nAcknowledgements\\nI would like to thank the Marvel Wikia for its amazing amount of information, as well as very practical API, and Marvel for having such a huge and diverse multiverse that inspires many possibilities of analysis.\\nInspiration\\nThis was my first attempt at data science. It was challenging but very fun and rewarding. I would really appreciate any feedback or suggestions for next works.',\n",
       " 'Context\\nCan you create a python symptom sorter, in bot style with these data ?\\nSymptom 1: i have an eye problem > the set selects actually all the diseases and symptoms related to the eyes OK\\nthen comes a question: is your nose congested ? and do you have red eye ? Symptom 2: congested nose Yes, red eye No OK\\nthen comes a second question do you cough ? and do you have chills ? Symptom 3: Cough Yes, Chills No OK\\nthe answer: with these symptoms i think you have...\\nThe primer shows the idea.\\nprimer to start with If you want a reduced data input, you download the US_df from the primer So the challenge i have here is, what classification method gives the best results for making a decision tree\\nContent\\nits one of my databases.',\n",
       " \"Overview\\nThis is the original data from Titanic competition plus some changes that I applied to it to be better suited for binary logistic regression:\\nMerged the train and test data.\\nRemoved the 'ticket' and 'cabin' attributes.\\nMoved the 'Survived' attribute to the last column.\\nAdded extra zero columns for categorical inputs to be better suited for One-Hot-Encoding.\\nSubstituted the values of 'Sex' and 'Embarked' attributes with binary and categorical values respectively.\\nFilled the missing values in 'Age' and 'Fare' attributes with the median of the data.\",\n",
       " 'Context\\nThe death penalty was authorized by 32 states, the Federal Government, and the U.S. Military. While Connecticut, Maryland, and New Mexico no longer have death penalty statutes, they do currently incarcerate death-sentenced offenders. Texas leads the nation in the number of executions since the death penalty was reinstated in 1976. California, Florida, Texas, and Pennsylvania have the largest death row populations.\\nThe following crimes are Capital Murder in Texas:\\nmurder of a peace officer or fireman who is acting in the lawful discharge of an official duty and who the person knows is a peace officer or fireman;\\nmurder during the commission or attempted commission of kidnapping, burglary, robbery, aggravated sexual assault, arson, obstruction or retaliation, or terroristic threat;\\nmurder for remuneration or promise of remuneration or employs another to commit murder for remuneration or promise of remuneration;\\nmurder during escape or attempted escape from a penal institution;\\nmurder, while incarcerated in a penal institution, of a correctional employee or with the intent to establish, maintain, or participate in a combination or in the profits of a combination;\\nmurder while incarcerated in a penal institution for a conviction of murder or capital murder;\\nmurder while incarcerated in a penal institution serving a life sentence or a 99 year sentence for a conviction of aggravated kidnapping, aggravated sexual assault, or aggravated robbery;\\nmurder of more than one person during the same criminal transaction or during different criminal transactions but the murders are committed pursuant to the same scheme or course of conduct;\\nmurder of an individual under ten years of age; or\\nmurder in retaliation for or on account of the service or status of the other person as a judge or justice of the supreme court, the court of criminal appeals, a court of appeals, a district court, a criminal district court, a constitutional county court, a statutory county court, a justice court, or a municipal court.\\nContent\\nThe Texas Department of Criminal Justice publishes various details, including the last words, of every inmate on death row they execute. This dataset includes information on the name, age, race, county, date, and last words of Texas death row inmates from 1982 to 2017.\\nAcknowledgments\\nThis dataset on last statements by executed offenders was obtained here: https://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html\\nStart a new kernel',\n",
       " \"Context\\nThis is a general disclosure of Donald Trump's assets, debts, and sources of income. Because the forms are only meant to reveal on potential conflicts of interests a filer might have, they provide far less specificity than tax returns would. This is an unpacked version of the pdf of Trump's form that was made available by the Federal Election Commission June 16th, 2017. It contains some information about his financial interests, but not enough to paint a complete picture of his net worth. It may be possible to use some of these forms to identify his foreign business partners.\\nAcknowledgements\\nThis dataset was kindly extracted from the original PDF and made publicly available by the Center for Responsive Politics.\\nInspiration\\nPrevious work on similar disclosures by Trump has often focused on identifying his foreign business ties. Are you able to find any new ones?\\nYou might also like\\nTrump's 2016 Financial Disclosure\\nTrump's World\\nTrump's Tweets\\nTrump Campaign Expenditures\",\n",
       " \"This dataset provides a complete snapshot of crime, outcome, and stop and search data, as held by the Home Office from late 2014 through mid 2017 for London, both the greater metro and the city.\\nContent\\nThe core fields are as follows:\\nReported by: The force that provided the data about the crime.\\nFalls within: At present, also the force that provided the data about the crime. This is currently being looked into and is likely to change in the near future.\\nLongitude and Latitude: The anonymised coordinates of the crime.\\nLSOA code and LSOA name: References to the Lower Layer Super Output Area that the anonymised point falls into, according to the LSOA boundaries provided by the Office for National Statistics.\\nCrime type: One of the crime types listed in the Police.UK FAQ.\\nLast outcome category: A reference to whichever of the outcomes associated with the crime occurred most recently. For example, this crime's 'Last outcome category' would be 'Offender fined'.\\nContext: A field provided for forces to provide additional human-readable data about individual crimes. Currently, for newly added CSVs, this is always empty.\\nFor additional details, including the steps taken to anonymize the data, please see https://data.police.uk/about/#provenance.\\nAcknowledgements\\nThis dataset was kindly released by the British Home Office under the Open Government License 3.0 at https://data.police.uk/data/. If you are looking for more data, they cover much more than London! All major cities in England and Wales are available, adding up to roughly 2gb of new data per month.\",\n",
       " 'Context:\\nA lemma is the uninflected form of a word. So while “tree” and “trees” are two words, they are the same lemma: “tree”. Similarly, “go”, “went” and “going” are all forms of the underlying lemma “to go”. This dataset contains the most common lemmas in Japanese.\\nContent:\\nThis dataset contains the most common Japanese lemmas from the Internet Corpus, as tagged by the ChaSen morphological tagger for Japanese (http://chasen.naist.jp/hiki/ChaSen/). For each lemma, both the frequency (number of times it occurs in the corpus) and its relative rank to other lemmas is provided.\\nThe total corpus size is 253,071,774 tokens, with a lexicon of 451,963 types.\\nAcknowledgements:\\nThis dataset was developed at the University of Leeds by Centre for Translation Studies(more information: http://corpus.leeds.ac.uk/list.html), and is distributed under a CC-BY license.\\nInspiration:\\nThis dataset is an especially helpful resource for work on Japanese texts.\\nWhat is the distribution of hiragana, katakana and kanji characters among common lemmas?\\nCan you use machine translation to find the equivalent lemmas and their frequency in other languages? Is there a lot of cross-linguistic difference between what concepts are the most frequent?\\nWhich parts of speech are the most common in Japanese? Are these different across languages?',\n",
       " 'Context:\\nBuilding dialogue systems, where a human can have a natural-feeling conversation with a virtual agent, is a difficult task in Natural Language Processing and the focus of much ongoing research. Some of the challenges include linking references to the same entity over time, tracking what’s happened in the conversation previously, and generating appropriate responses. This corpus of naturally-occurring dialogues can be helpful for building and evaluating dialogue systems.\\nContent:\\nThe new Ubuntu Dialogue Corpus consists of almost one million two-person conversations extracted from the Ubuntu chat logs, used to receive technical support for various Ubuntu-related problems. The conversations have an average of 8 turns each, with a minimum of 3 turns. All conversations are carried out in text form (not audio).\\nThe full dataset contains 930,000 dialogues and over 100,000,000 words and is available here. This dataset contains a sample of this dataset spread across .csv files. This dataset contains more than 269 million words of text, spread out over 26 million turns.\\nfolder: The folder that a dialogue comes from. Each file contains dialogues from one folder .\\ndialogueID: An ID number for a specific dialogue. Dialogue ID’s are reused across folders.\\ndate: A timestamp of the time this line of dialogue was sent.\\nfrom: The user who sent that line of dialogue.\\nto: The user to whom they were replying. On the first turn of a dialogue, this field is blank.\\ntext: The text of that turn of dialogue, separated by double quotes (“). Line breaks (\\\\n) have been removed.\\nAcknowledgements:\\nThis dataset was collected by Ryan Lowe, Nissan Pow , Iulian V. Serban† and Joelle Pineau. It is made available here under the Apache License, 2.0. If you use this data in your work, please include the following citation:\\nRyan Lowe, Nissan Pow, Iulian V. Serban and Joelle Pineau, \"The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems\", SIGDial 2015. URL: http://www.sigdial.org/workshops/conference16/proceedings/pdf/SIGDIAL40.pdf\\nInspiration:\\nCan you use these chat logs to build a chatbot that offers help with Ubuntu?',\n",
       " 'Context:\\nUnderstanding the distribution of anopheline vectors of malaria is an important prelude to the design of national malaria control and elimination programmes. A single, geo-coded continental inventory of anophelines using all available published and unpublished data has not been undertaken since the 1960s. We present the largest ever geo-coded database of anophelines in Africa representing a legacy dataset for future updating and identification of knowledge gaps at national levels. The geo-coded and referenced database is made available with the related publication as a reference source for African national malaria control programmes planning their future control and elimination strategies. Information about the underlying research studies can be found at http://kemri-wellcome.org/programme/population-health/.\\nContent:\\nGeocoded info on anopheline inventory. See key below.\\nAcknowledgements:\\nKEMRI-Wellcome Trust assembled the data and distributed it on Dataverse.\\nInspiration:\\nWhere have malarial mosquito populations grown or decreased?\\nCan you predict mosquito population growth trends?\\nDo you seen any correlation between mosquito populations and malaria deaths from this dataset?\\nIs the banner image mosquito capable of carrying malaria?',\n",
       " 'Context\\nThis dataset consists of 384 features extracted from CT images. The class variable is numeric and denotes the relative location of the CT slice on the axial axis of the human body. The data was retrieved from a set of 53500 CT images from 74 different patients (43 male, 31 female).\\nContent\\nEach CT slice is described by two histograms in polar space. The first histogram describes the location of bone structures in the image, the second the location of air inclusions inside of the body. Both histograms are concatenated to form the final feature vector. Bins that are outside of the image are marked with the value -0.25.\\nThe class variable (relative location of an image on the axial axis) was constructed by manually annotating up to 10 different distinct landmarks in each CT Volume with known location. The location of slices in between landmarks was interpolated.\\nField Descriptions:\\npatientId: Each ID identifies a different patient\\nvalue[1-241]: Histogram describing bone structures\\nvalue[242 - 385]: Histogram describing air inclusions\\n386: Relative location of the image on the axial axis (class value).\\nValues are in the range [0; 180] where 0 denotes the top of the head and 180 the soles of the feet.\\nAcknowledgements\\nOriginal dataset was downloaded from UCI Machine learning Repository\\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\\nBanner image acknowledgement:\\nSelf Portre on cat scan, 1997\\nTitle: \"Soon I will be there\"\\nDate: 8 April 1997\\nAuthor: Sérgio Valle Duarte\\nLicense: CC BY 3.0\\nSource: Wikipedia\\nInspiration\\nPredict the relative location of CT slices on the axial axis',\n",
       " 'Content\\nThe World Glacier Inventory contains information for over 130,000 glaciers. Inventory parameters include geographic location, area, length, orientation, elevation, and classification. The WGI is based primarily on aerial photographs and maps with most glaciers having one data entry only. The data set can be viewed as a snapshot of the glacier distribution in the second half of the twentieth century. It was founded on the original WGI from the World Glacier Monitoring Service.\\nAcknowledgements\\nThe National Snow & Ice Data Center continues to work with the World Glacier Monitoring Service to update the glacier inventory database.',\n",
       " 'Content\\nThe Corruption Perceptions Index scores and ranks countries/territories based on how corrupt a country’s public sector is perceived to be. It is a composite index, a combination of surveys and assessments of corruption, collected by a variety of reputable institutions. The CPI is the most widely used indicator of corruption worldwide.\\nCorruption generally comprises illegal activities, which are deliberately hidden and only come to light through scandals, investigations or prosecutions. There is no meaningful way to assess absolute levels of corruption in countries or territories on the basis of hard empirical data. Possible attempts to do so, such as by comparing bribes reported, the number of prosecutions brought or studying court cases directly linked to corruption, cannot be taken as definitive indicators of corruption levels. Instead, they show how effective prosecutors, the courts or the media are in investigating and exposing corruption. Capturing perceptions of corruption of those in a position to offer assessments of public sector corruption is the most reliable method of comparing relative corruption levels across countries.\\nAcknowledgements\\nThe data sources used to calculate the Corruption Perceptions Index scores and ranks were provided by the African Development Bank, Bertelsmann Stiftung Foundation, The Economist, Freedom House, IHS Markit, IMD Business School, Political and Economic Risk Consultancy, Political Risk Services, World Bank, World Economic Forum, World Justice Project, and Varieties of Democracy Project.',\n",
       " 'Content\\nThe Advanced Placement Exam scores for the class of 2016, highlighted in this dataset, show that students continue to demonstrate college-level skills and knowledge in increasing numbers. Even as AP teachers deliver rigor to an ever-diversifying population of students, participation and performance continue to improve. Behind and within these data are the daily sacrifices of AP students and teachers, including the late nights that students put in diligently studying and the weekends that teachers give up to help their students succeed. Their hard work and effort are worth celebrating.\\nAcknowledgements\\nThis data was collected and released by the College Board after the May 2016 exam administration.',\n",
       " 'Context\\nBetween 1901 and 2016, the Nobel Prizes and the Prize in Economic Sciences were awarded 579 times to 911 people and organizations. The Nobel Prize is an international award administered by the Nobel Foundation in Stockholm, Sweden, and based on the fortune of Alfred Nobel, Swedish inventor and entrepreneur. In 1968, Sveriges Riksbank established The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, founder of the Nobel Prize. Each Prize consists of a medal, a personal diploma, and a cash award.\\nA person or organization awarded the Nobel Prize is called Nobel Laureate. The word \"laureate\" refers to being signified by the laurel wreath. In ancient Greece, laurel wreaths were awarded to victors as a sign of honor.\\nContent\\nThis dataset includes a record for every individual or organization that was awarded the Nobel Prize since 1901.\\nAcknowledgements\\nThe Nobel laureate data was acquired from the Nobel Prize API.\\nInspiration\\nWhich country has won the most prizes in each category? What words are most frequently written in the prize motivation? Can you predict the age, gender, and nationality of next year\\'s Nobel laureates?',\n",
       " \"Context:\\nThe CFSAN Adverse Event Reporting System (CAERS) is a database that contains information on adverse event and product complaint reports submitted to FDA for foods, dietary supplements, and cosmetics. The database is designed to support CFSAN's safety surveillance program. Adverse events are coded to terms in the Medical Dictionary for Regulatory Activities (MedDRA) terminology.\\nContent:\\nSee the metadata description in the accompanying README.pdf below or here. Approximately 90k reactions are recorded from 2004-mid 2017, with 12 columns of information regarding type of reaction and related event details.\\nAcknowledgements:\\nThis dataset is collected by the US Food and Drug Administration.\\nInspiration:\\nWhat are the most commonly reported foodstuffs?\\nWhat are the most commonly reported medical reactions to foods?\\nWhere do people in the US most commonly report food-related conditions?\",\n",
       " 'Introduction\\nThis is the keystroke dataset for the study titled \\'High-accuracy detection of early Parkinson\\'s Disease using multiple characteristics of finger movement while typing\\'. This research report is currently under review for publication by PLOS ONE.\\nThe dataset contains keystroke logs collected from over 200 subjects, with and without Parkinson\\'s Disease (PD), as they typed normally on their own computer (without any supervision) over a period of weeks or months (having initially installed a custom keystroke recording app, Tappy). This dataset has been collected and analyzed in order to indicate that the routine interaction with computer keyboards can be used to detect changes in the characteristics of finger movement in the early stages of PD.\\nData\\nThe participants, from the U.S., Canada, UK and Australia, had visited the project website and agreed to participate in the study. The research was approved by the Human Research Ethics Committee at Charles Sturt University, Australia, protocol number H17013.\\nEach data file collected includes the timing information from typing activity as the participants used their various Windows applications (such as email, word processing, web searches and the like). The keystroke acquisition software (\\'Tappy\\') provided timing accuracy of key press and release timestamps to within several milliseconds.\\nThe data files comprise two Zip archives, one with the participant detail files and the other with the keystroke data files for each user.\\nAcknowledgements\\nThis dataset is from the research article \"High-accuracy detection of early Parkinson\\'s Disease using multiple characteristics of finger movement while typing\" by Warwick R. Adams. Read the article here: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0188226#sec008\\nInspiration\\nWhile this is a difficult dataset to work with, there is a rich trove of information. It is a great set to practice preprocessing, attempt to replicate the results of the article, or do your own analysis of keystroke data.',\n",
       " 'DeepSat SAT-4\\n\\n\\nOriginally, images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330,000 scenes spanning the whole of the Continental United States (CONUS). The authors used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a 1-m ground sample distance (GSD) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points.\\nThe images consist of 4 bands - red, green, blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class.\\nOnce labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch.\\nContent\\nEach sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared.\\nThe training and test labels are one-hot encoded 1x4 vectors\\nThe four classes represent the four broad land covers which include barren land, trees, grassland and a class that consists of all land cover classes other than the above three.\\nTraining and test datasets belong to disjoint set of image tiles.\\nEach image patch is size normalized to 28x28 pixels.\\nOnce generated, both the training and testing datasets were randomized using a pseudo-random number generator.\\nCSV files\\nX_train_sat4.csv: 400,000 training images, 28x28 images each with 4 channels\\ny_train_sat4.csv: 400,000 training labels, 1x4 one-hot encoded vectors\\nX_test_sat4.csv: 100,000 training images, 28x28 images each with 4 channels\\ny_test_sat4.csv: 100,000 training labels, 1x4 one-hot encoded vectors\\nThe original MAT file\\ntrain_x: 28x28x4x400000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels)\\ntrain_y: 400000x4 uint8 (containing 4x1 vectors having labels for the 400000 training samples)\\ntest_x: 28x28x4x100000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels)\\ntest_y: 100000x4 uint8 (containing 4x1 vectors having labels for the 100000 test samples)\\nAcknowledgements\\nThe original MATLAB file was converted to multiple CSV files\\nThe original SAT-4 and SAT-6 airborne datasets can be found here:\\nhttp://csc.lsu.edu/~saikat/deepsat/\\nThanks to:\\nSaikat Basu, Robert DiBiano, Manohar Karki and Supratik Mukhopadhyay, Louisiana State University Sangram Ganguly, Bay Area Environmental Research Institute/NASA Ames Research Center Ramakrishna R. Nemani, NASA Advanced Supercomputing Division, NASA Ames Research Center',\n",
       " 'Context\\nWelcome. This is a Women’s Clothing E-Commerce dataset revolving around the reviews written by customers. Its nine supportive features offer a great environment to parse out the text through its multiple dimensions. Because this is real commercial data, it has been anonymized, and references to the company in the review text and body have been replaced with “retailer”.\\nContent\\nThis dataset includes 23486 rows and 10 feature variables. Each row corresponds to a customer review, and includes the variables:\\nClothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\\nAge: Positive Integer variable of the reviewers age.\\nTitle: String variable for the title of the review.\\nReview Text: String variable for the review body.\\nRating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\\nRecommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\\nPositive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.\\nDivision Name: Categorical name of the product high level division.\\nDepartment Name: Categorical name of the product department name.\\nClass Name: Categorical name of the product class name.\\nAcknowledgements\\nAnonymous\\nInspiration\\nI look forward to come quality NLP! There is also some great opportunities for feature engineering, and multivariate analysis.',\n",
       " \"Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 5.8 million products) that was created by extracting data from Flipkart.com, a leading Indian eCommerce store.\\nContent\\nThis dataset has following fields:\\nproduct_url\\nproduct_name\\nproduct_category_tree\\npid\\nretail_price\\ndiscounted_price\\nimage\\nis_FK_Advantage_product\\ndescription\\nproduct_rating\\noverall_rating\\nbrand\\nproduct_specifications\\nAcknowledgements\\nThis dataset was created by PromptCloud's in-house web-crawling service.\\nInspiration\\nAnalyses of the pricing, product specification and brand can be performed.\",\n",
       " \"Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 9.4 million job listings) that was created by extracting data from Naukri.com, a leading job board.\\nContent\\nThis dataset has following fields:\\ncompany\\neducation\\nexperience\\nindustry\\njob description\\njobid\\njoblocation_address\\njob title\\nnumber of positions\\npay rate\\npostdate\\nsite_name\\nskills\\nAcknowledgements\\nThis dataset was created by PromptCloud's in-house web-crawling service.\\nInspiration\\nAnalyses of the pay rate, job title, industry and experience can be performed to name a few starting points.\",\n",
       " \"Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 1.8 million restaurants) that was created by extracting data from Tripadvisor.co.uk.\\nContent\\nThis dataset has following fields:\\nuniq_id\\nurl\\nrestaurant_id\\nrestaurant_location\\nname\\ncategory\\ntitle\\nreview_date\\nreview_text\\nauthor\\nauthor_url\\nlocation\\nrating\\nfood\\nvalue\\nservice\\nvisited_on\\nAcknowledgements\\nThis dataset was created by PromptCloud's in-house web-crawling service.\\nInspiration\\nAnalyses of the restaurant reviews and ratings can be performed.\",\n",
       " \"The TMY3s are data sets of hourly values of solar radiation and meteorological elements for a 1-year period. Their intended use is for computer simulations of solar energy conversion systems and building systems to facilitate performance comparisons of different system types, configurations, and locations in the United States and its territories. Because they represent typical rather than extreme conditions, they are not suited for designing systems to meet the worst-case conditions occurring at a location.\\nPlease note that TMY3 is NOT the state of the art solar data. It was used as a key component of investment analyses for several years, but NREL has released a more recent version based on satellite data and updated meteorological models that provides coverage for the entire United States. That dataset is much too large to publish here, but is highly recommended if you need the best information.\\nContent\\nPlease see the pdf manual for full details of each field; there are several dozen of them.\\nIt's important to know that nearly all of the solar data is modeled based on estimates of cloud cover; less than 1% of the stations directly measured sunlight.\\nThis data is not appropriate for time series analysis. A typical meteorological year is literally twelve months of real data from twelve different years. Please see the manual for further details.\\nAcknowledgements\\nThis dataset was made available by the National Renewable Energy Laboratory. You can find the original dataset here.\\nIf you like\\nIf you liked this dataset, you might also enjoy:\\nGoogle Project Sunroof\\n30 Years of European Wind Generation\\n30 Years of European Solar Generation\",\n",
       " \"We don't always think about industrial scale food, but cheese blocks the size of a small car are important.\\nThe Mandatory Price Reporting Act of 2010 (pdf) was passed on September 27, 2010, the act required USDA to release dairy product sales information on or before Wednesday at 3:00 pm EST (unless affected by a Federal Holiday). The act also required the establishment of an electronic mandatory price reporting system for dairy products reported under Public Law 106-532. These dairy statistics will continue to be collected on a weekly basis, AMS-Dairy Programs will collect, analyze, aggregate, and publish dairy product sales information for selected dairy commodities.\\nAcknowledgements\\nThis data is released by the US Department of Agriculture. You can find the original dataset here.\\nInspiration\\nCan you predict changes in moisture content for 40 pound blocks of cheese?\",\n",
       " 'Context:\\nFew events in American history are better known than the Salem witchcraft trials of 1692. Its popularity is doubtless attributable to a number of things: a persistent fascination with the occult; a perverse pleasure to expose the underbelly of an American culture that boasts of toleration, social harmony, and progress; and an appreciation for a compelling, dramatic narrative replete with heroes and villains. Skeptics, like the preeminent twentieth-century historian Perry Miller, question whether the Salem trials constituted anything more than an inconsequential episode in colonial history. But most historians consider Salem worthy of continuing investigation even if it was less than a major turning point in history. Indeed, Salem has been an unusually fertile field for historical research because it readily lends itself to new approaches, insights, and methodologies. To understand what happened in Salem, historians have profitably applied the perspectives of politics, anthropology, economic and social analysis, religion, social psychology, and demography. If the ultimate meaning of Salem is still elusive, these investigations have broadened and deepened our understanding of the 1692 witchcraft outbreak.\\nContent:\\nThe Salem Witchcraft Website contains eight data sets. They provide only a small portion of the historical record about Salem. They do not contain transcripts of examinations or trials or contemporary narrative accounts, for example. Instead, they provide information, primarily of a quantitative nature, about three major aspects of the outbreak: its chronology, its geographic spread, and the social and economic divisions in Salem Village that shaped events. The data were derived primarily from four published sources: Paul Boyer and Stephen Nissenbaum\\'s three-volume transcription of the legal records of the witchcraft trials, The Salem Witchcraft Papers; the new and now authoritative Records of the Salem Witch-Hunt, edited by Bernard Rosenthal, et. al.; Boyer and Nissenbaum\\'s edited collection of documents, Salem-Village Witchcraft; and Salem Village\\'s Book of Record, which contain tax records and other information relating to Salem Village. Photocopies of the original Salem Village record book and church records were examined at the Danvers Archival Center.\\nThe Accused Witches Data Set contains information about those who were formally accused of witchcraft during the Salem episode. This means that there exists evidence of some form of direct legal involvement, such as a complaint made before civil officials, an arrest warrant, an examination, or court record. Accused witches were almost always detained in jail to await further action by a grand jury, which had the authority to indict and hold the accused for trial. Trials by a special Court of Oyer and Terminer began in June 1692. In October 1692, this court was discontinued due to mounting criticism of its methods. It was replaced by another court, the Superior Court of Judicature, which held trials from January to May 1693.\\nThe \"Accused Witch\" column records the names of the 152 people mentioned in legal records as having been formally accused of witchcraft. Their names are alphabetically arranged. Spelling generally follows that of Paul Boyer and Stephen Nissenbaum, Salem Witchcraft Papers but has been sometimes changed in accordance with the newer Records of the Salem Witch-Hunt and other sources.\\n\"Residence\" identifies the community in which the accused person was living when accused of witchcraft. In a few cases, the residence of an accused witch is problematic. For Elizabeth How, for example, some records cite Ipswich while others name Topsfield as her home. In such cases, the most likely residence has been used. In a few instances, the residence entry does not reflect the actual geographic relationship of the accused with the trials. George Burroughs was living in Wells, Maine, in 1692, but he had been a controversial minister in Salem Village in the early 1680s.\\n\"Month of Accusation\" numerically expresses the month of the year in which an alleged witch was accused: \"1\"=January 1692; \"6\"=June 1692; and \"13\"=January 1693. A negative 1 (-1) indicates that the actual month of accusation is not known with sufficient certainty to be included. Some of these \"unknowns\" can be approximated from available records, and users may choose to substitute their estimate. Users should also recognize an artificial quality to this data: those accused in one month, May (5), for example, may have been charged only a day or two before someone in June (6).\\n\"Month of Execution\" numerically expresses the month in which an alleged witch was executed as a result of the legal process. The data do not include entries for those who died as a result of their incarceration. In one case, Giles Corey, the month of execution does record the month in which he was pressed to death for refusing to plead to the charges against him.\\nThe Towns Data Set provides a convenient way to construct histograms of the communities whose residents were charged with witchcraft in 1692. It contains 25 columns:\\nTwenty-four columns record each town for which at least one formal accusation occurred (Salem Village and Salem Town are listed separately). Each cell lists the month of an accusation, numerically expressed: 1=January 1692, 2=February 1692, and so forth. The negative number, -1, indicates that the month of accusation is unknown.\\nA \"Bin\" column contains the range of months of witchcraft accusations, from 1 (January 1692) to 12 (December 1692), with -1 for unknown months of accusation.\\nBoth the Pro-Parris and Anti-Parris data sets contain the same four columns:\\n\"Name\" identifies each signer of the pro-Parris petition.\\n\"Identification\" indicates the category in the petition under which the signer was placed.\\n\"Sex\" indicates whether the signer was male or female.\\n\"Sort\" locates each signer in the data set so that it can be returned to its original order.\\nTo compare the social make-up of Salem Village\\'s pro- and anti-Parris factions to the village\\'s general population, download the Salem Village Data Set. The data set contains four columns:\\n\"Name\" lists every person in Salem Village who appeared on any village tax assessments for 1690, 1695, and 1697. The 137 names are a good, though imperfect, indicator of the village\\'s adult male population in the period of the witch hunt. Only a few women, all widows, appear. Young men not yet independent or paying taxes do not appear.\\n\"Petition\" notes whether the taxpayer signed either the pro- or anti-Parris petition in 1695. \"NoS\" (no signature) means that the person did not sign either petition.\\n\"Church to 1696\" indicates whether a person was a church member though 1696. No distinction is made as to whether a person was a member of the Salem Village church or another church. The list is compiled from the pro- and anti-Parris petitions as well as from the records of the Salem Village church as recorded by Samuel Parris. Additional information came from the published records of the First Church in Salem Town.\\n\"Sort\" permits data to be easily restored to their original order after a statistical manipulation.\\nThe Committee Yearly Data Set contains information about Salem Village\\'s committees from 1685 to 1698, a period that covers the last years Deodat Lawson\\'s ministry and the entire tenure of Samuel Parris. The data set contains three columns of information for each committee:\\n\"Committee\" lists the names of committeemen for a particular year (in 1688, only four men were elected).\\n\"Petition\" indicates whether the committeeman signed either the pro- or anti-Parris petition in 1695. \"NoS\" (no signature) means that this committeeman did not sign either petition. Signing a petition strongly suggests but does not conclusively establish a petitioner\\'s earlier position regarding Parris or the witchcraft outbreak.\\n\"Social\" indicates whether the committeeman was a church member or a householder. Three committeemen (William Sibley, James Smith, and Jacob Fuller) have been listed as householders in the absence of information linking them to a church.)\\nThe Committee List Data Set provides information about all Salem Village committee members who held office from 1685 to 1698. The data set contains 18 columns:\\n\"Committee Members\" records the names of the thirty-one villagers who held committee office from 1685-1698. They are listed in the order in which they first appeared in the village\\'s Book of Record.\\n\"Petition\" notes whether the committeeman signed either the pro- or anti-Parris petition in 1695. \"NoS\" (no signature) means that this committeeman did not sign either petition.\\n\"Social\" indicates whether the committeeman was a church member or a householder. (Three committeemen, William Sibley, James Smith, and Jacob Fuller, have been listed as householders in the absence of information linking them to a church; the three did not sign either petition.)\\nColumns 4-17 indicate committee membership for each year.\\n\"Sort\" permits data to be easily restored to their original order.\\nThe Tax Comparison Data Set was compiled by listing all Salem Village taxpayers who were assessed rates in the period between 1681 and 1700. The rates are recorded in Salem Village\\'s Book of Record (see Bibliography).\\n\"Name\" lists in alphabetical order all assessments on Salem Village\\'s tax lists for 1681, 1690, 1694, 1695, 1697, and 1700.\\n\"Tax\" records the taxpayer\\'s assessment in shillings. Since the village\\'s revenue needs changed, the total assessment (and individual allocations) changed accordingly.\\n\"Petition\" indicates whether the taxpayer signed either the pro- or anti-Parris petition in 1695. \"NoS\" (no signature) means that this taxpayer did not sign either petition.\\n\"Sort\" permits data to be easily restored to their original order after a statistical manipulation.\\nAcknowledgements:\\nUsers who copy, share, adapt, and re-publish any of the content in Salem Witchcraft Dataset should credit Professor Richard Latner of Tulane University for making this material available. More information and guided exercises can be found on this website.\\nInspiration:\\nWhat was the relationship between economic success and support for Parris?\\nCan you split the list of accused witches and predict who would be accused based on other acquisitions?',\n",
       " 'Context:\\nHow frequently a word occurs in a language is an important piece of information for natural language processing and linguists. In natural language processing, very frequent words tend to be less informative than less frequent one and are often removed during preprocessing. Human language users are also sensitive to word frequency. How often a word is used affects language processing in humans. For example, very frequent words are read and understood more quickly and can be understood more easily in background noise.\\nContent:\\nThis dataset contains the counts of the 333,333 most commonly-used single words on the English language web, as derived from the Google Web Trillion Word Corpus.\\nAcknowledgements:\\nData files were derived from the Google Web Trillion Word Corpus (as described by Thorsten Brants and Alex Franz, and distributed by the Linguistic Data Consortium) by Peter Norvig. You can find more information on these files and the code used to generate them here.\\nThe code used to generate this dataset is distributed under the MIT License.\\nInspiration:\\nCan you tag the part of speech of these words? Which parts of speech are most frequent? Is this similar to other languages, like Japanese?\\nWhat differences are there between the very frequent words in this dataset, and the the frequent words in other corpora, such as the Brown Corpus or the TIMIT corpus? What might these differences tell us about how language is used?',\n",
       " \"Content\\nThe contents of this data set comes from public data available on the city of Portland website. Each individual crime reported is lists the location, time and date of the incident as well as a the neighborhood in which the event occurred.\\nAll data prior to 2015 has the same general format but the newer 2015-17 data needs to be reformatted for easier comparison since it does not match the older organizational scheme. To this end I will be adding new .csv with 2015 , 2016, and 2017 YTD data broken out. Coordinate data will also be added to make the data sets more easily comparable and mappable.\\nUpdate: I created new .csv for each year 2015-2017 changing the formatting from the Portland Police Department's tab separated values to the standard comma separated values. The pre-2015 data still isn't comparable because of the differences in the crime categorization but I will work creating some sort of key so that the full data set can be analyzed as a single batch of information.\\nAcknowledgements\\nBanner image by Zack Spear on Unsplash.\\nAll data gathered from portlandoregon.gov and civicapps.org\",\n",
       " 'Context\\nThe monthly salary of the public workers of the State of São Paulo in Brazil is a Public data available in the transparency portal of the state government at: http://www.transparencia.sp.gov.br/buscaRemunera.html\\nContent\\nThe data is about the salary for all worker in the State for the month of October 2017. There are just over one million records. The names of the employee are anonymous represented by the variable id.\\nInspiration\\nThis database may reveal:\\nHigher salaries\\nThe contribution of extra remuneration to higher salaries\\nBy the rules of the government no employee could receive more than the state governor salary: R$ 21,631.05',\n",
       " \"«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polítics, econòmics, etc...), en els principals índexs borsatils espanyols i de les crypto-monedes.\\nHem seleccionat diferents fonts de dades per generar fitxers «csv», guardar diferents valors en el mateix període de temps. És important destacar que ens interessa més les tendències alcistes o baixes, que podem calcular o recuperar en aquests períodes de temps.\\nContent\\nEn aquest cas el contingut està format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals s’ha generat un fitxer per dia del període de temps estudiat.\\nPel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període, en cada directori un fitxer amb cadascun del noms dels índexs. Degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com.\\nPel que fa als camps, ens interessà detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. Els camps especialment destacats són:\\n• Data: Data de la observació\\n• Nom: Nom empresa o cryptomoneda, per identificar de quina moneda o index estem representant.\\n• Símbol: Símbol de la moneda o del index borsatil, per realitzar gràfic posteriorment d’una forma mes senzilla que el nom.\\n• Preu: Valor en euros d’una acció o una cryptomoneda (transformarem la moneda a euros en el cas de estigui en dòlars amb l'última cotització (un dollar a 0,8501 euro)\\n• Tipus_cotitzacio: Valor nou que agregarem per discretitzar entre la cotització: baix (0 i 1), normal (1 i 100), alt (100 i 1000), molt_alt (&gt;1000)\\nScript R\\nAnàlisis de les observacions i el domini de les dades\\nAnàlisis en especial de Bitcoin i la IOTA.\\nTest de Levene per veure la homogeneitat\\nKmeans per creació de cluster per veure la homegeneitat\\nFreqüències de les distribucions\\nTest de contrast d'hipòtesis de variables dependents (Wilcoxon)\\nTest de Shapiro-Wilk per veure la normalitat de les dades, per normalitzar-les o no\\nCorrelació d'índexs borsatils, per eliminar complexitat dels índexs amb grau més alt de correlació\\nIteració de Regressions lineals per obtenir el model amb més qualitat, observa'n el p-valor i l'índex de correlació\\nValidació de la qualitat del model\\nRepresentació grafica\\nAcknowledgements\\nEn aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a:\\nhttp://www.eleconomista.es\\nhttps://coinmarketcap.com\\nPer aquest fet, les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.\\n[https://www.r4.com/que-necesitas/formacion/diccionario]\\nInspiration\\nHi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes:\\nhttps://arxiv.org/pdf/1410.1231v1.pdf\\nEn aquest cas el «trading» en cryptomoneda és relativament nou, força popular per la seva formulació com a mitja digital d’intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents.\\nLa comunitat podrà respondre, entre altres preguntes, a:\\nEstà afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'Espanya?\\nEls efectes o agents externs afecten per igual a les accions o cryptomonedes?\\nHi ha relacions cause efecte entre les acciones i cryptomonedes?\\nProject repository\\nhttps://github.com/acostasg/scraping\\nDatasets\\nEls fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com:\\nhttps://www.kaggle.com/acostasg/stock-index/\\nhttps://www.kaggle.com/acostasg/crypto-currencies\\nPer una banda, els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil. De forma diferent, les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció.\",\n",
       " 'Context\\nThe Dataset is used by \"A temperature-forecasting problem\" from the \"Deep Learning with Python\" book\\nContent\\nThe data was downloaded from: https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\\nIt represents time period between 2009 and 2016\\nAcknowledgements\\nThe dataset recorded at the Weather Station at the Max Planck Institute for Biogeochemistry in Jena, Germany. https://www.bgc-jena.mpg.de/wetter/\\nIt was reassembled by François Chollet, the author of the \"Deep Learning with Python\" book\\nInspiration\\nThe main purpose of this dataset is to perform RNN exercise (6.3.1 A temperature-forecasting problem) from the \"Deep Learning with Python\" book.',\n",
       " 'The Physician Compare website was created by the Centers for Medicare & Medicaid Services (CMS) in December 2010 as required by the Affordable Care Act (ACA) of 2010 to help patients assess and find doctors and hospitals. This dataset contains the information supplied to patients via that website, including patient satisfaction surveys and performance scores across over 100 metrics.\\nAcknowledgements\\nThis dataset was kindly released by the Centers for Medicare & Medicaid Services. You can find the original copy of the dataset here.',\n",
       " \"Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 4.6 million job listings) that was created by extracting data from Dice.com, a prominent US-based technology job board.\\nContent\\nThis dataset has following fields:\\nadvertiserurl\\ncompany\\nemploymenttype_jobstatus\\njobdescription\\njoblocation_address\\njobtitle\\npostdate\\nshift\\nskills\\nAcknowledgements\\nThis dataset was created by PromptCloud's in-house web-crawling service.\\nInspiration\\nAnalyses of the job description with respect to the job title and skills can be performed.\",\n",
       " 'Context:\\nPLUTO is a master record of the locations and characteristics of buildings in New York City. It’s published by the New York City Department of City Planning on an approximately quarterly-to-half-yearly basis, and is one of the more important datasets for civic analysis in New York City.\\nContent:\\nPLUTO includes information on building height, square footage, location, type, landmark status, number of units, owner, year of construction, and other related fields.\\nAcknowledgements:\\nThis dataset is published as-is by the New York City Department of Planning.\\nInspiration:\\nWhat is the distribution of the heights of buildings in New York City? The age?\\nCan you define neighborhoods by clustering similar buildings within them?\\nWhat (and where) is the split between commercial, residential, and office space in New York City?',\n",
       " 'Context\\nThis dataset contains the salary, pay rate, and total compensation of every New York City employee. In this dataset this information is provided for the 2014, 2015, 2016, and 2017 fiscal years, and provides a transparent lens into who gets paid how much and for what.\\nNote that fiscal years in the New York City budget cycle start on July 1st and end on June 30th (see here). That means that this dataset contains, in its sum, compensation information for all City of New York employees for the period July 1, 2014 to June 30, 2017.\\nContent\\nThis dataset provides columns for fiscal year, employee name, the city department they work for, their job title, and various fields describing their compensation. The most important of these fields is \"Regular Gross Pay\", which provides that employee\\'s total compensation.\\nAcknowledgements\\nThis information was published as-is by the City of New York.\\nInspiration\\nHow many people do the various city agencies employ, and how much does each department spend on salary in total?\\nWhat are the most numerous job titles in civic government employment?\\nWhere does overtime pay seem to be especially common? How much of it is there?\\nHow do New York City employee salaries compare against salaries of city employees in Chicago? Is the difference more or less than the difference in cost of living between the two cities?',\n",
       " 'Context\\nThe New York City Department of Transportation collects daily data about the number of bicycles going over bridges in New York City. This data is used to measure bike utilization as a part of transportation planning. This dataset is a daily record of the number of bicycles crossing into or out of Manhattan via one of the East River bridges (that is, excluding Bronx thruways and the non-bikeable Hudson River tunnels) for a stretch of 9 months.\\nContent\\nA count of the number of bicycles on each of the bridges in question is provided on a day-by-day basis, along with information on maximum and minimum temperature and precipitation.\\nAcknowledgements\\nThis data is published in an Excel format by the City of New York (here). It has been processed into a CSV file for use on Kaggle.\\nInspiration\\nIn this dataset, how many bicycles cross into and out of Manhattan per day?\\nHow strongly do weather conditions affect bike volumes?\\nWhat is the top bridge in terms of bike load?',\n",
       " \"Context:\\nRats in New York City are prevalent, as in many densely populated areas. For a long time, the exact number of rats in New York City was unknown, and a common urban legend was that there were up to four times as many rats as people. In 2014, however, scientists more accurately measured the entire city's rat population to be approximately only 25% of the number of humans; i.e., there were approximately 2 million rats to New York's 8.4 million people at the time of the study.[1][2]\\nContent:\\nNew York City rodent complaints can be made online, or by dialing 3-1-1, and the New York City guide Preventing Rats on Your Property discusses how the New York City Health Department inspects private and public properties for rats. Property owners that fail inspections receive a Commissioner's Order and have five days to correct the problem. If after five days the property fails a second inspection, the owner receives a Notice of Violation and can be fined. The property owner is billed for any clean-up or extermination carried out by the Health Department.\\nData is from 2010-Sept 16th, 2017 and includes date, location (lat/lon), type of structure, borough, and community board.\\nAcknowledgements:\\nData was produced by the City of New York via their 311 portal.\\nInspiration:\\nWhere and when are rats most seen?\\nCan you predict rat sightings from previous data?\\nAre there any trends in rat sightings?\",\n",
       " \"All the data was taken from Open Data Zurich (https://data.stadt-zuerich.ch/dataset/pd-stapo-hundebestand) with the idea of making a useful few Kernel demos from it and let people look at information about the dogs that live here.\\nGerman\\nSince German is the official language of Zurich most of the columns are in German but the translations to English aren't too tricky\\nALTER -> Age\\nGESCHLECHT -> Gender\\nSTADTKREIS -> City Quarter or District\\nRASSE1 -> Dog's Primary Breed\\nRASSE2 -> Dog's Secondary Breed\\nGEBURTSJAHR_HUND -> Dog's Year of Birth\\nGESCHLECHT_HUND -> Dog's Gender\\nHUNDEFARBE -> Dog's Color\\nUtility\\nIt might also help people trying to find apartments in areas with the right kind of dogs\\nCould be used to look at how dog trends have changed in time (by looking at the numbers by birth year)\\nHelpful for picking the right kind of dog to get your 90 year old grandmother (what kind of dogs do other 90 year old women have)\",\n",
       " 'Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext\\nTo understand the Foreign direct investment in India for the last 17 years from 2000-01 to 2016-17.\\nContent\\nThis dataset contains sector and financial year wise data of FDI in India.\\nAcknowledgements\\nMinistry of Commerce and Industry has published Financial Year wise FDI Equity Inflows from 2000-01 to 2016-17 dataset in Open Government Data Platform India under Govt. Open Data License - India.\\nInspiration\\nHow much FDI has changed over the year?\\nHow much has varied since 2014 after Narendra Modi become PM of India?',\n",
       " \"Context\\nThe Major League Soccer Union releases the salaries of every MLS player each year. This is a collection of salaries from 2007 to 2017.\\nContent\\nEach file contains the following fields:\\nclub: Team abbreviation\\nlast_name: Player last name\\nfirst_name: Player first name\\nposition: Position abbreviation\\nbase_salary: Base salary\\nguaranteed_compensation: Guaranteed compensation\\nAcknowledgements\\nJeremy Singer-Vine over at Data is Plural scraped the PDF's released by the MLS Union and put the data in a nice little package of CSV files for everyone.\\nI downloaded this dataset from: https://github.com/data-is-plural/mls-salaries MIT License\\nInspiration\\nWho in the MLS makes the most money? Are they worth it? I make about $900 bazillion each year, can I afford a soccer team?\",\n",
       " 'Context\\nThis dataset is a collection newsgroup documents. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\\nContent\\nThere is file (list.csv) that contains a reference to the document_id number and the newsgroup it is associated with. There are also 20 files that contain all of the documents, one document per newsgroup.\\nIn this dataset, duplicate messages have been removed and the original messages only contain \"From\" and \"Subject\" headers (18828 messages total).\\nEach new message in the bundled file begins with these four headers:\\nNewsgroup: alt.newsgroup\\nDocument_id: xxxxxx\\nFrom: Cat\\nSubject: Meow Meow Meow\\nThe Newsgroup and Document_id can be referenced against list.csv\\nOrganization - Each newsgroup file in the bundle represents a single newsgroup - Each message in a file is the text of some newsgroup document that was posted to that newsgroup.\\nThis is a list of the 20 newsgroups:\\ncomp.graphics\\ncomp.os.ms-windows.misc\\ncomp.sys.ibm.pc.hardware\\ncomp.sys.mac.hardware\\ncomp.windows.x rec.autos\\nrec.motorcycles\\nrec.sport.baseball\\nrec.sport.hockey sci.crypt\\nsci.electronics\\nsci.med\\nsci.space\\nmisc.forsale talk.politics.misc\\ntalk.politics.guns\\ntalk.politics.mideast talk.religion.misc\\nalt.atheism\\nsoc.religion.christian\\nAcknowledgements\\nKen Lang is credited by the source for collecting this data. The source of the data files is here:\\nhttp://qwone.com/~jason/20Newsgroups/\\nInspiration\\nThis dataset text can be used to classify text documents',\n",
       " \"Context\\nDuring the 2016 US presidential election, the phrase “fake news” found its way to the forefront in news articles, tweets, and fiery online debates the world over after misleading and untrue stories proliferated rapidly. BuzzFeed News analyzed over 1,000 stories from hyperpartisan political Facebook pages selected from the right, left, and mainstream media to determine the nature and popularity of false or misleading information they shared.\\nContent\\nThis dataset supports the original story “Hyperpartisan Facebook Pages Are Publishing False And Misleading Information At An Alarming Rate” published October 20th, 2016. Here are more details on the methodology used for collecting and labeling the dataset (reproduced from the story):\\nMore on Our Methodology and Data Limitations\\n“Each of our raters was given a rotating selection of pages from each category on different days. In some cases, we found that pages would repost the same link or video within 24 hours, which caused Facebook to assign it the same URL. When this occurred, we did not log or rate the repeat post and instead kept the original date and rating. Each rater was given the same guide for how to review posts:\\n“Mostly True: The post and any related link or image are based on factual information and portray it accurately. This lets them interpret the event/info in their own way, so long as they do not misrepresent events, numbers, quotes, reactions, etc., or make information up. This rating does not allow for unsupported speculation or claims.\\n“Mixture of True and False: Some elements of the information are factually accurate, but some elements or claims are not. This rating should be used when speculation or unfounded claims are mixed with real events, numbers, quotes, etc., or when the headline of the link being shared makes a false claim but the text of the story is largely accurate. It should also only be used when the unsupported or false information is roughly equal to the accurate information in the post or link. Finally, use this rating for news articles that are based on unconfirmed information.\\n“Mostly False: Most or all of the information in the post or in the link being shared is inaccurate. This should also be used when the central claim being made is false.\\n“No Factual Content: This rating is used for posts that are pure opinion, comics, satire, or any other posts that do not make a factual claim. This is also the category to use for posts that are of the “Like this if you think...” variety.\\n“In gathering the Facebook engagement data, the API did not return results for some posts. It did not return reaction count data for two posts, and two posts also did not return comment count data. There were 70 posts for which the API did not return share count data. We also used CrowdTangle's API to check that we had entered all posts from all nine pages on the assigned days. In some cases, the API returned URLs that were no longer active. We were unable to rate these posts and are unsure if they were subsequently removed by the pages or if the URLs were returned in error.”\\nAcknowledgements\\nThis dataset was originally published on GitHub by BuzzFeed News here: https://github.com/BuzzFeedNews/2016-10-facebook-fact-check\\nInspiration\\nHere are some ideas for exploring the hyperpartisan echo chambers on Facebook:\\nHow do left, mainstream, and right categories of Facebook pages differ in the stories they share?\\nWhich types of stories receive the most engagement from their Facebook followers? Are videos or links more effective for engagement?\\nCan you replicate BuzzFeed’s findings that “the least accurate pages generated some of the highest numbers of shares, reactions, and comments on Facebook”?\\nStart a new kernel\",\n",
       " 'Context\\nThe Journal of the American Chemical Society is the premier journal published by the American Chemical Society and one of the highest ranking journals in all of chemistry. With almost 60,000 papers and over 120,000 authors this collections of papers published between 1996 and 2016, represents the current state of chemistry research.\\nContent\\nThis dataset is presented in 3 database tables, one of published articles, and one of all authors, with a further table relating authors to the journal articles they have published.\\nUpdate 21-12-2017: The previous data was collected by a top level scrape of the table of contents pages from the journal.\\nA couple of months ago I performed a page-level scrape and then forgot about it, but I had some positive reactions to the data-set this week, so I have processed some of the data (although more remains, the raw output from the scrape is an 18 GB csv file).\\nThis new data updates the Articles table to contain many more data fields, including the paper abstract, number of citations and page views (page views are a relatively new feature, so is probably not for the lifetime of some of the older papers).\\nOne interesting project for this data would be to look at the term frequencies in the abstracts of the papers, and use that to see how the focus of chemistry research has changed over the years.\\nIf it is interesting to people, I have the following unprocessed data: - Articles citing papers in this database inc, Title, Journal, Year and Author list - Institutions of authors for each papers (this data is very complicated and requires some difficult parsing)\\nAcknowledgements\\nThis data was scraped from the Table of Contents section of the JACS website and is available online publicly.\\nInspiration\\nThis data could be used to determine the average number of authors per paper, or the connections between authors, to determine if specific research fields can be grouped by the associated authors\\nAlso see if you can find the two papers published by me in this year, and see who my co-authors were!',\n",
       " 'Context\\nThe story behind this data set and analysis is just a combination of my interest in data science and metal music. I was looking for interesting data that I could analyze and this happen to be one of the first I started exploring.\\nContent\\nThe world population information was a direct download so I did not have to do any work to get it. This data set consists of population information for countries on earth from the years 1960 to 2015.\\nThe metal band information was scraped from the website http://metalstorm.net/ and consists of the following - band name - how many fans the band has on the website - when the band formed - when the band split - the country of origin on the band - the styles of the band\\nAcknowledgements\\nThe metal information was compiled from information found on http://metalstorm.net/, the world population information is from http://www.worldbank.org/.\\nInspiration\\nThere has already been some great analysis of metal bands on Kaggle, I wanted to contribute to the discussion by adding some new data and looking at it from a slightly different angle. Also I thought it might be useful to share the process by which I came up with the visualizations and data management since the community has been a big help to me.',\n",
       " 'Context\\nObservations of particles much smaller than us, and various understandings of those particles, have propelled mankind forward in ways once impossible to imagine. \"The elements\" are what we call the sequential patterns in which some of these particles manifest themselves.\\nAs a chemistry student and a coder, I wanted to do what came naturally to me and make my class a bit easier by coding/automating my way around some of the tedious work involved with calculations. Unfortunately, it seems that chemical-related datasets are not yet a thing which have been conveniently formatted into downloadable databases (as far as my research went). I decided that the elements would be a good place to start data collection, so I did that, and I\\'d like to see if this is useful to others as well.\\nOther related data sets I\\'d like to coalesce are some large amount of standard entropies and enthalpies of various compounds, and many of the data sets from the CRC Handbook of Chemistry and Physics. I also think as many diagrams as possible should be documented in a way that can be manipulated and read via code.\\nContent\\nIncluded here are three data sets. Each data set I have included is in three different formats (CSV, JSON, Excel), for a total of nine files.\\nTable of the Elements:\\nThis is the primary data set.\\n118 elements in sequential order\\n72 features\\nReactivity Series:\\n33 rows (in order of reactivity - most reactive at the top)\\n3 features (symbol, name, ion)\\nElectromotive Potentials:\\n284 rows (in order from most negative potential to most positive)\\n3 features (oxidant, reductant, potential)\\nAcknowledgements\\nAll of the data was scraped from 120 pages on Wikipedia using scripts. The links to those scripts are available in the dataset descriptions.\\nExtra\\nIf you are interested in trying the chemistry calculations code I made for completing some of my repetitive class work, it\\'s publicly available on my GitHub. (Chemistry Calculations Repository) I plan to continue updating that as time goes on.',\n",
       " \"Context\\nA dataset of WTA matches including individual statistics.\\nContent\\nIn these datasets there are individual csv files for WTA tournament from 2000 to 2016.\\nThe numbers in the last columns are absolute values, using them you can calculate percentages.\\nAcknowledgement\\nThanks to Jeff Sackmann for the excellent work. Be sure to visit his github profile\\nhttps://github.com/JeffSackmann/tennis_wta\\nInspiration\\nThis dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. I'm planning to add historical odds and injuries data as soon as I have the time to get them.\",\n",
       " 'Context\\nThere are a number of Kaggle datasets that provide spatial data around New York City. For many of these, it may be quite interesting to relate the data to the demographic and economic characteristics of nearby neighborhoods. I hope this data set will allow for making these comparisons without too much difficulty.\\nExploring the data and making maps could be quite interesting as well.\\nContent\\nThis dataset contains two CSV files:\\nnyc_census_tracts.csv\\nThis file contains a selection of census data taken from the ACS DP03 and DP05 tables. Things like total population, racial/ethnic demographic information, employment and commuting characteristics, and more are contained here. There is a great deal of additional data in the raw tables retrieved from the US Census Bureau website, so I could easily add more fields if there is enough interest.\\nI obtained data for individual census tracts, which typically contain several thousand residents.\\ncensus_block_loc.csv\\nFor this file, I used an online FCC census block lookup tool to retrieve the census block code for a 200 x 200 grid containing New York City and a bit of the surrounding area. This file contains the coordinates and associated census block codes along\\nwith the state and county names to make things a bit more readable to users.\\nEach census tract is split into a number of blocks, so one must extract the census tract code from the block code.\\nAcknowledgements\\nThe data here was taken from the American Community Survey 2015 5-year estimates (https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml).\\nThe census block coordinate data was taken from the FCC Census Block Conversions API (https://www.fcc.gov/general/census-block-conversions-api)\\nAs public data from the US government, this is not subject to copyright within the US and should be considered public domain.',\n",
       " 'Context\\nWith this dataset I hope to raise awareness on the trends in crime.\\nContent\\nFor NYPD Complaint Data, each row represents a crime. For information on the columns, please see the attached csv, \"Crime_Column_Description\". Reported crime go back 5 years but I only attached reported crime from 2014-2015 due to file size. The full report can be found at NYC Open Data (https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i)\\nAcknowledgements\\nI would like to thank NYC Open Data for the dataset.\\nInspiration\\nAdditional things I would like to better understand: 1. Differences in crime that exist between the 5 boroughs 2. A mapping of the crimes per borough 3. Where do the most dangerous crimes happen and what time?',\n",
       " 'Context\\nRankings are a constant phenomenon in society, with a persistent interest in the stratification of items in a set across various disciplines. In sports, rankings are a direct representation of the performance of a team or player over a certain period. Given the straightforward nature of rankings in sports (points based system) there is the opportunity to statistically explore rankings of sports disciplines.\\nContent\\nThe dataset comprises monthly rankings data of the Top 100 Chess players between July 2000 and June 2017 . The data is housed in a single csv file.\\nAcknowledgements\\nData was sourced from the official site of the World Chess Federation: fide.com\\nInspiration\\nThis dataset could be of use to anyone interested in the distribution of rankings in competitive events.',\n",
       " 'Context:\\nThe State of the Union is an annual address by the President of the United States before a joint session of congress. In it, the President reviews the previous year and lays out his legislative agenda for the coming year.\\nContent:\\nThis dataset contains the full text of the State of the Union address from 1989 (Regan) to 2017 (Trump).\\nInspiration:\\nThis is a nice, clean set of texts perfect for exploring Natural Language Processing techniques\\nTopic modelling: Which topics have become more popular over time? Which have become less popular?\\nSentiment analysis: Are there differences in tone between different Presidents? Presidents from different parties?\\nParsing: Can you train implement a parser to automatically extract the syntactic relationships between words?\\nAuthorship identification: Can you correctly identify the author of a previously unseen address?',\n",
       " 'Context:\\nColor terms are interesting in natural language processing because it’s an area where it’s possible to link distributional semantics (models of word meanings based on which words are used together in texts) to things in the world. This dataset was created to help link semantic models to images.\\nContent:\\nThis dataset is made up of two smaller files, but were both presented and discussed in the same paper (see Acknowledgements). All data in this dataset is in English.\\nConcrete color terms\\nThis dataset contains a list of common items manually labeled with one of the 11 colors from the set: black, blue, brown, green, grey, orange, pink, purple, red, white, yellow.\\nLiteral vs. nonliteral colors\\nThis dataset is made up of color adjective-noun phrases, randomly drawn from the most frequent 8K nouns and 4K adjectives in the concatenated ukWaC, Wackypedia, and BNC corpora. These were tagged by consensus by two human judges as literal (white towel, black feather) or nonliteral (white wine, white musician, green future). Some phrases had both literal and nonliteral uses, such as blue book in “book that is blue” vs. “automobile price guide”. In these cases, only the most common sense (according to the judges) was taken into account for the present experiment. The dataset consists of 370 phrases.\\nAcknowledgements:\\nIf you use these datasets, please cite:\\nBruni, E., G. Boleda, M. Baroni, N. K. Tran. 2012. Distributional semantics in technicolor. Proceedings of ACL 2012, pages 136-145, Jeju Island, Korea.\\nInspiration:\\nAre some colors used more often in a literal sense?\\nIs there a relationship between how many objects are a given color and how often that color is used in a literal sense?\\nCan you use the color of concrete and an image database of those objects to create an automatic color labeller?',\n",
       " \"Context:\\nSpanish is the second most widely-spoken language on Earth; over one in 20 humans alive today is a native speaker of Spanish. This medium-sized corpus contains 120 million words of modern Spanish taken from the Spanish-Language Wikipedia in 2010.\\nContent:\\nThis dataset is made up of 57 text files. Each contains multiple Wikipedia articles in an XML format. The text of each article is surrounded by tags. The initial tag also contains metadata about the article, including the article’s id and the title of the article. The text “ENDOFARTICLE.” appears at the end of each article, before the closing tag.\\nAcknowledgements:\\nThis dataset was collected by Samuel Reese, Gemma Boleda, Montse Cuadros, Lluís Padró and German Rigau. If you use it in your work, please cite the following paper:\\nSamuel Reese, Gemma Boleda, Montse Cuadros, Lluís Padró, German Rigau. Wikicorpus: A Word-Sense Disambiguated Multilingual Wikipedia Corpus. In Proceedings of 7th Language Resources and Evaluation Conference (LREC'10), La Valleta, Malta. May, 2010.\\nInspiration:\\nCan you create a stop-word list for Spanish based on this corpus? How does it compare to the one in this dataset?\\nCan you build a topic model to cluster together articles on similar topics?\\nYou may also like:\\nBrazilian Portuguese Literature Corpus: 3.7 million word corpus of Brazilian literature published between 1840-1908\\nColonia Corpus of Historical Portuguese: A 5.1 million word corpus of historical Portuguese\\nThe National University of Singapore SMS Corpus: A corpus of more than 67,000 SMS messages in Singapore English & Mandarin\",\n",
       " 'Context\\nThis dataset contains information about deep sea corals and sponges collected by NOAA and NOAA’s partners. Amongst the data are geo locations of deep sea corals and sponges and the whole thing is tailored to the occurrences of azooxanthellates - a subset of all corals and all sponge species (i.e. they don\\'t have symbiotic relationships with certain microbes). Additionally, these records only consists of observations deeper than 50 meters to truly focus on the deep sea corals and sponges.\\nContent:\\nColumn descriptions:\\nCatalogNumber: Unique record identifier assigned by the Deep-Sea Coral Research and Technology Program.\\nDataProvider: The institution, publication, or individual who ultimately deserves credit for acquiring or aggregating the data and making it available.\\nScientificName: Taxonomic identification of the sample as a Latin binomial.\\nVernacularNameCategory: Common (vernacular) name category of the organism.\\nTaxonRank: Identifies the level in the taxonomic hierarchy of the ScientificName term.\\nObservationDate: Time as hh:mm:ss when the sample/observation occurred (UTC).\\nLatitude (degrees North): Latitude in decimal degrees where the sample or observation was collected.\\nLongitude (degrees East): Longitude in decimal degrees where the sample or observation was collected.\\nDepthInMeters: Best single depth value for sample as a positive value in meters.\\nDepthMethod: Method by which best singular depth in meters (DepthInMeters) was determined. \"Averaged\" when start and stop depths were averaged. \"Assigned\" when depth was derived from bathymetry at the location. \"Reported\" when depth was reported based on instrumentation or described in literature.\\nLocality: A specific named place or named feature of origin for the specimen or observation (e.g., Dixon Entrance, Diaphus Bank, or Sur Ridge). Multiple locality names can be separated by a semicolon, arranged in a list from largest to smallest area (e.g., Gulf of Mexico; West Florida Shelf, Pulley Ridge).\\nIdentificationQualifier: Taxonomic identification method and level of expertise. Examples: “genetic ID”; “morphological ID from sample by taxonomic expert”; “ID by expert from image”; “ID by non-expert from video”; etc.\\nSamplingEquipment: Method of data collection. Examples: ROV, submersible, towed camera, SCUBA, etc.\\nRecordType: Denotes the origin and type of record. published literature (\"literature\"); a collected specimen (\"specimen\"); observation from a still image (\"still image\"); observation from video (\"video observation\"); notation without a specimen or image (\"notation\"); or observation from trawl surveys, longline surveys, and/or observer records (\"catch record\").\\nAcknowledgements\\nBig shout out to NOAA and it\\'s partners. Thank you for being scientists! The original and probably more up-to-date dataset can be found here: https://deepseacoraldata.noaa.gov/website/AGSViewers/DeepSeaCorals/mapSites.htm\\nThis dataset hasn\\'t been changed in anyway.\\nNOAA (2015) National Database for Deep-Sea Corals and Sponges (version 20170324-0). https://deepseacoraldata.noaa.gov/; NOAA Deep Sea Coral Research & Technology Program.\\nInspiration\\nWho doesn\\'t love coral and sponges?! I challenge you to find the best algorithm that successfully SAVES the world\\'s corals 100% of the time!',\n",
       " \"Content\\nThe purpose of EPA’s fuel economy estimates is to provide a reliable basis for comparing vehicles. Most vehicles in the database (other than plug-in hybrids) have three fuel economy estimates: a “city” estimate that represents urban driving, in which a vehicle is started in the morning (after being parked all night) and driven in stop-and-go traffic; a “highway” estimate that represents a mixture of rural and interstate highway driving in a warmed-up vehicle, typical of longer trips in free-flowing traffic; and a “combined” estimate that represents a combination of city driving (55%) and highway driving (45%). Estimates for all vehicles are based on laboratory testing under standardized conditions to allow for fair comparisons.\\nThe database provides annual fuel cost estimates, rounded to the nearest $50, for each vehicle. The estimates are based on the assumptions that you travel 15,000 miles per year (55% under city driving conditions and 45% under highway conditions) and that fuel costs $2.33/gallon for regular unleaded gasoline, $2.58/gallon for mid-grade unleaded gasoline, and $2.82/gallon for premium.\\nEPA’s fuel economy values are good estimates of the fuel economy a typical driver will achieve under average driving conditions and provide a good basis to compare one vehicle to another. However, your fuel economy may be slightly higher or lower than EPA’s estimates. Fuel economy varies, sometimes significantly, based on driving conditions, driving style, and other factors.\\nAcknowledgements\\nFuel economy data are produced during vehicle testing at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with EPA oversight.\",\n",
       " 'Content\\nThe Cingranelli-Richards human rights database contains quantitative information on government recognition of 15 internationally recognized human rights in more than 200 countries from 1981-2011. It includes measures of the practices of governments that allow or impede citizens who wish to exercise their physical integrity rights like the rights not to be tortured, summarily executed, disappeared, or imprisoned for political beliefs; civil liberties such as free speech, freedom of association and assembly, freedom of movement, freedom of religion, and the right to participate in the selection of government leaders; employment rights; and rights of women to equal treatment politically, economically, and socially. The database is designed for use by scholars and students who seek to test theories about the causes and consequences of human rights violations, as well as policy makers and analysts who seek to estimate the human rights effects of a wide variety of institutional changes and public policies including democratization, economic aid, military aid, structural adjustment, and humanitarian intervention.\\nThe primary source of information about human rights practices is obtained from the annual United States Department of State’s Country Reports on Human Rights Practices. Coders are instructed to use this source for all variables. For a group of four rights known as \"Physical Integrity Rights\" (the rights to freedom from extrajudicial killing, disappearance, torture, and political imprisonment), coders use Amnesty International’s Annual Report in addition the Department of State reports. If discrepancies exist between the two sources, coders are instructed to treat the Amnesty International report as authoritative; some scholars believe that this step is necessary to remove a potential bias in favor of American allies.\\nAcknowledgements\\nThe human rights database was developed, updated, and published by Professor David Cingranelli of Binghamton University, SUNY, Professor David Richards of the University of Connecticut\\'s Human Rights Institute, and Professor K. Chad Clay of the University of Georgia.',\n",
       " 'Content\\nThe data in this report consists of individuals accused of terrorism and related crimes since September 11, 2001, who are either American citizens or who engaged in terrorist activity within the United States. The data includes some individuals who died before being charged with a crime, but were widely reported to have engaged in terrorist activity.\\nAcknowledgements\\nThis report was produced by the International Security Program at New America.',\n",
       " \"Context\\nThe University of Copenhagen’s Zoological Museum placed a light trap on their roof and for 18 years they documented the types of insects being caught. The data was collected as part of a study to determine insect responses to recent climate change.\\nContent\\nThis file contains the raw data from the light trapping study ordered by: Order (Lepidoptera/Coleoptera), family, name (species), year, date1 (start), date2 (end) and number of individuals\\nAcknowledgements\\nOriginal publication: Thomsen PF, Jørgensen PS, Bruun HH, Pedersen J, Riis-Nielsen T, Jonko K, Słowińska I, Rahbek C, Karsholt O (2016) Resource specialists lead local insect community turnover associated with temperature – analysis of an 18-year full-seasonal record of moths and beetles. Journal of Animal Ecology 85(1): 251–261. http://dx.doi.org/10.1111/1365-2656.12452\\nThe original dataset can be found at http://datadryad.org/resource/doi:10.5061/dryad.s4945/1\\nInspiration\\nClimate change is on everyone's mind for one reason or another and insects are susceptible to climate change just like humans. Using these data, can you determine which species have become more or less prevalent over the 18 years of collection?\",\n",
       " 'Sweden has a surprisingly large number of school fires for a small country (< 10M inhabitants), and many of these fires are due to arson. For instance, according to the Division of Fire Safety Engineering at Lund University, \"Almost every day between one and two school fires occur in Sweden. In most cases arson is the cause of the fire.\" The associated costs can be up to a billion SEK (around 120 million USD) per year.\\nIt is hard to say why these fires are so common in Sweden compared to other countries, and this dataset doesn\\'t address that question - but could it be possible, within a Swedish context, to find out which properties and indicators of Swedish towns (municipalities, to be exact) might be related to a high frequency of school fires?\\nI have collected data on school fire cases in Sweden between 1998 and 2014 through a web site with official statistics from the Swedish Civil Contingencies Agency (https://ida.msb.se/ida2#page=a0087). At least at the time when I collected the data, there was no API to allow easy access to schools fire data, so I had to collect them using a quasi-manual process, downloading XLSX report generated from the database year by year, after which I joined these with an R script into a single table of school fire cases where the suspected reason was arson. (Full details on the data acquisition process are available.)\\nThe number of such cases is reported for each municipality (of which there are currently 290) and year (i e each row is a unique municipality/year combination). The population at the time is also reported.\\nAs a complement to these data, I provide a list of municipal KPI:s (key performance indicators) from 1998 to 2014. There are thousands of these KPI:s, and it would be a futile task for me to try to translate the descriptions from Swedish to English, although I might take a stab at translating a small subset of them at some point. These KPIs were extracted from Kolada (a database of Swedish municipality and county council statistics) by repeatedly querying its API (https://github.com/Hypergene/kolada).\\nI\\'d be very interested to hear if anyone finds some interesting correlations between schools fire cases and municipality indicators!',\n",
       " \"Context\\nExperiment to apply same strategy from Beluga's Keras dataset with PyTorch models. This dataset has the weights for several models included in PyTorch. To use these weights they need to be copied when the kernel runs, like in this example.\\nContent\\nPyTorch models included:\\nDenseNet-161\\nInception-V3\\nResNet18\\nResNet50\\nSqueezeNet 1.0\\nSqueezeNet 1.1\\nAcknowledgements\\nBeluga's Keras dataset\\nPyTorch\",\n",
       " 'What is CDLI?\\nThe Cuneiform Digital Library Initiative (CDLI) is an international digital library project aimed at putting text and images of an estimated 500,000 recovered cuneiform tablets created from between roughly 3350 BC and the end of the pre-Christian era online. The initiative is a joint project of the University of California, Los Angeles, the University of Oxford, and the Max Planck Institute for the History of Science, Berlin.\\nThis dataset includes the full CDLI catalogue (metadata), transliterations of tablets in the catalogue, and word/sign lists from old akkadian and Ur III. This data was downloaded on the 9th of May 2017.\\nTransliterations are in .atf format, find out more about this format here: http://oracc.museum.upenn.edu/doc/help/editinginatf/cdliatf/index.html\\nFind more about CDLI here: http://cdli.ucla.edu/\\nWhat is Cuneiform?\\nCuneiform script, one of the earliest systems of writing, was invented by the Sumerians. It is distinguished by its wedge-shaped marks on clay tablets, made by means of a blunt reed for a stylus. The name cuneiform itself simply means \"wedge shaped\".\\nCuneiform is not a language, nor is it an alphabet. Cuneiform uses between 600-1000 characters to write words or syllables. It has been used by many different cultural groups to represent many different languages, but it was primarily used to write Sumerian and Akkadian. Deciphering cuneiform is very difficult to this day, though the difficulty varies depending on the language.\\nhttps://en.wikipedia.org/wiki/Cuneiform_script\\nWhat is Assyriology?\\nAssyriology is the study of the languages, history, and culture of the people who used the ancient writing system called cuneiform. Cuneiform was used primarily in an area called the Near East, centred on Mesopotamia (modern Iraq and eastern Syria) where cuneiform was invented, but including the Northern Levant (Western Syria and Lebanon), parts of Anatolia, and western Iran. The sources for Assyriology are all archaeological, and include both inscribed and uninscribed objects. Most Assyriologists focus on the rich textual record from the ancient Near East, and specialise in either the study of language, literature, or history of the ancient Near East.\\nAssyriology began as an academic discipline with the recovery of the monuments of ancient Assyria, and the decipherment of cuneiform, in the middle of the 19th century. Large numbers of archaeological objects, including texts, were brought to museums in Europe and later the US, following the early excavations of Nineveh, Kalhu, Babylon, Girsu, Assur and so forth. Today Assyriology is studied in universities across the globe, both as an undergraduate and a graduate subject, and knowledge from the ancient Near East informs students of numerous other disciplines such as the History of Science, Archaeology, Classics, Biblical studies and more.',\n",
       " 'Context\\nIt\\'s possible, using R (and no doubt Python), to \\'listen\\' to Twitter and capture tweets that match a certain description. I decided to test this out by grabbing tweets with the text \\'good morning\\' in them over a 24 hours period, to see if you could see the world waking up from the location information and time-stamp. The main R package used was streamR\\nContent\\nThe tweets have been tidied up quite a bit. First, I\\'ve removed re-tweets, second, I\\'ve removed duplicates (not sure why Twitter gave me them in the first place), third, I\\'ve made sure the tweet contained the words \\'good morning\\' (some tweets were returned that didn\\'t have the text in for some reason) and fourth, I\\'ve removed all the tweets that didn\\'t have a longitude and latitude included. This latter step removed the vast majority. What\\'s left are various aspects of just under 5000 tweets. The columns are,\\ntext\\nretweet_count\\nfavorited\\ntruncated\\nid_str\\nin_reply_to_screen_name\\nsource\\nretweeted\\ncreated_at\\nin_reply_to_status_id_str\\nin_reply_to_user_id_str\\nlang\\nlisted_count\\nverified\\nlocation\\nuser_id_str\\ndescription\\ngeo_enabled\\nuser_created_at\\nstatuses_count\\nfollowers_count\\nfavourites_count\\nprotected\\nuser_url\\nname\\ntime_zone\\nuser_lang\\nutc_offset\\nfriends_count\\nscreen_name\\ncountry_code\\ncountry\\nplace_type\\nfull_name\\nplace_name\\nplace_id\\nplace_lat\\nplace_lon\\nlat\\nlon\\nexpanded_url\\nurl\\nAcknowledgements\\nI used a few blog posts to get the code up and running, including this one\\nCode\\nThe R code I used to get the tweets is as follows (note, I haven\\'t includes the code to set up the connection to Twitter. See the streamR PFD and the link above for that. You need a Twitter account),\\ni = 1\\n\\nwhile (i <= 280) {\\n\\nfilterStream(\"tw_gm.json\", timeout = 300, oauth = my_oauth, track = \\'good morning\\', language = \\'en\\')\\ntweets_gm = parseTweets(\"tw_gm.json\")\\n\\nex = grepl(\\'RT\\', tweets_gm$text, ignore.case = FALSE) #Remove the RTs\\ntweets_gm = tweets_gm[!ex,]\\n\\nex = grepl(\\'good morning\\', tweets_gm$text, ignore.case = TRUE) #Remove anything without good morning in the main tweet text\\ntweets_gm = tweets_gm[ex,]\\n\\nex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information\\ntweets_gm = tweets_gm[!ex,]\\n\\ntweets.all = rbind(tweets.all, tweets_gm) #Add to the collection\\n\\ni=i+1\\n\\nSys.sleep(5)\\n\\n}',\n",
       " 'Context\\nKeystroke dynamics is the study of whether people can be distinguished by their typing rhythms, much like handwriting is used to identify the author of a written text. Possible applications include acting as an electronic fingerprint, or in an access-control mechanism. A digital fingerprint would tie a person to a computer-based crime in the same manner that a physical fingerprint ties a person to the scene of a physical crime. Access control could incorporate keystroke dynamics both by requiring a legitimate user to type a password with the correct rhythm, and by continually authenticating that user while they type on the keyboard.\\nContent\\nThe data are arranged as a table with 34 columns. Each row of data corresponds to the timing information for a single repetition of the password by a single subject. The first column, subject, is a unique identifier for each subject (e.g., s002 or s057). Even though the data set contains 51 subjects, the identifiers do not range from s001 to s051; subjects have been assigned unique IDs across a range of keystroke experiments, and not every subject participated in every experiment. For instance, Subject 1 did not perform the password typing task and so s001 does not appear in the data set. The second column, sessionIndex, is the session in which the password was typed (ranging from 1 to 8). The third column, rep, is the repetition of the password within the session (ranging from 1 to 50).\\nThe remaining 31 columns present the timing information for the password. The name of the column encodes the type of timing information. Column names of the form H.key designate a hold time for the named key (i.e., the time from when key was pressed to when it was released). Column names of the form DD.key1.key2 designate a key down-key down time for the named digraph (i.e., the time from when key1 was pressed to when key2 was pressed). Column names of the form UD.key1.key2 designate a key up-key down time for the named digraph (i.e., the time from when key1 was released to when key2 was pressed). Note that UD times can be negative, and that H times and UD times add up to DD times.\\nConsider the following one-line example of what you will see in the data:\\nsubject sessionIndex rep H.period DD.period.t UD.period.t ... s002 1 1 0.1491 0.3979 0.2488 ...\\nThe example presents typing data for subject 2, session 1, repetition 1. The period key was held down for 0.1491 seconds (149.1 milliseconds); the time between pressing the period key and the t key (key down-key down time) was 0.3979 seconds; the time between releasing the period and pressing the t key (key up-key down time) was 0.2488 seconds; and so on\\nAcknowledgements\\nKevin S. Killourhy and Roy A. Maxion\\nInspiration\\nTo make measurable progress in the field of keystroke dynamics, i shared data. The anomaly-detection task was to discriminate between the typing of a genuine user trying to gain legitimate access to his or her account, and the typing of an impostor trying to gain access illegitimately to that same account. Our intent with this is to share our resources—the typing data, with the research community, and to answer questions that they (or you) might have.\\nFor starters: 1. The typing patterns of different users. 2. The changing typing styles of a user over different attempts. 3. The difference in typing of left-side keys and right-side keys on the keyboard.\\nand so on . . .\\n*TEMPORARY NOTE* Some people are having problem with the main download button, please try downloading from the bottom of the page rather than the main button if issues observed.\\nHappy Machine Learning!',\n",
       " 'The data set contains the details about all the ATP matches played since 1968. The data set has a lot of missing values, especially for the period between 1968 - 1991.\\nThanks to Xiaming Chen for making the data available to the online community.\\nPrimarily, I would like to understand how tennis matches/players have evolved over time and any other insights.',\n",
       " 'This dataset includes SP1 transcription factor binding and non-binding sites on human chromosome1. It can be used for binary classification tasks in bioinformatics. There are 1200 sequences for binding sites (BS) and 1200 sequences for non-biding sites (nBS) We have labeled sequences with 1 for BS and 0 for nBS. Each sequence is 14 nucleobase length, which is converted to numeric string using codes below, assigned to each nucleobase 00 for A 01 for T 10 for C 11 for G',\n",
       " \"The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.\",\n",
       " \"Context\\nSoftware systems are composed of one or more software architectural styles. These styles define the usage patterns of a programmer in order to develop a complex project. These architectural styles are required to analyze for pattern similarity in the structure of multiple groups of projects. The researcher can apply different types of data mining algorithms to analyze the software projects through architectural styles used. The dataset is obtained from an online questionnaire delivered to the world 's best academic and software industry.\\nContent\\nThe content of this dataset are multiple architectural styles utilized by the system. He attributes are Repository, Client Server, Abstract Machine,Object Oriented,Function Oriented,Event Driven,Layered, Pipes & Filters, Data centeric, Blackboard, Rule Based, Publish Subscribe, Asynchronous Messaging, Plug-ins, Microkernel, Peer-to-Peer, Domain Driven, Shared Nothing.\\nAcknowledgements\\nThanks to my honorable teacher Prof.Dr Usman Qamar for guiding me to accomplish this wonderful task.\\nInspiration\\nThe dataset is capable of updating and refinements.Any researcher ,who want to contribute ,plz feel free to ask.\",\n",
       " 'State Energy Data Systems (SEDS) data for all US states, including DC, from 1960 to 2014F\\nContext\\nThis dataset is derived from my general interest in energy systems. It was originally composed for this exercise, as part of this Coursera/John Hopkins Data Science Specilisation.\\nThe code that produced this dataset is in https://www.kaggle.com/nathanto/d/nathanto/seds-1960-2014F/data-wrangling-code-for-seds-1960-2014f\\nContent\\nThe data is a composition of the State Energy Data Systems (SEDS) data for all US states, including DC, from 2016 to 2014F, for data released June 29, 2016. It has been tidied from a wide format to a long format, and includes unit codes for the values associated with the observations for each MSN code for each state for each year.\\nThe \"F\" in the final year number indicates that these are the final observations. There is a lag of some 18 months after year end and final readings.\\nThe columns are:\\nstate - State postal code, composed from the function states.abb and including \"DC\".\\nmsn - A mnemonic series name identifying the value being observed.\\nyear - Year of the observation.\\nvalue - Of the observation.\\nunits_code, representing the units of the value, e.g. BBtu is Billion British Thermal Units.\\nNote that the units_codes are mostly my own invention, based on the EIA Writng Style Guide.\\nAcknowledgements\\nThank you to the US Energy Information Administration for making the data available.\\nSpecial thanks to Yvonne Taylor for guidance on style for the codes.\\nInspiration\\nThe first goal for this data was to support some plotting and forecast testing exercises, which is a work in progress. To what extent do past observations predict future observations? Since the data is readily available, and consistent, within limits, over a long period, this format is a good basis for experimenting with techniques in that space.',\n",
       " 'Context\\nThis is the dataset used in the section \"ANN (Artificial Neural Networks)\" of the Udemy course from Kirill Eremenko (Data Scientist & Forex Systems Expert) and Hadelin de Ponteves (Data Scientist), called Deep Learning A-Z™: Hands-On Artificial Neural Networks. The dataset is very useful for beginners of Machine Learning, and a simple playground where to compare several techniques/skills.\\nIt can be freely downloaded here: https://www.superdatascience.com/deep-learning/\\nThe story: A bank is investigating a very high rate of customer leaving the bank. Here is a 10.000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon.\\nThe story of the story: I\\'d like to compare several techniques (better if not alone, and with the experience of several Kaggle users) to improve my basic knowledge on Machine Learning.\\nContent\\nI will write more later, but the columns names are very self-explaining.\\nAcknowledgements\\nUdemy instructors Kirill Eremenko (Data Scientist & Forex Systems Expert) and Hadelin de Ponteves (Data Scientist), and their efforts to provide this dataset to their students.\\nInspiration\\nWhich methods score best with this dataset? Which are fastest (or, executable in a decent time)? Which are the basic steps with such a simple dataset, very useful to beginners?',\n",
       " \"Context\\nThis data set covers all aspects of the pre-WWI and interwar economies, including production, construction, employment, money, prices, asset market transactions, foreign trade, and government activity. Many series are highly disaggregated, and many exist at the monthly or quarterly frequency. The data set has some coverage of the United Kingdom, France and Germany, although it predominantly covers the United States. For information see:\\nImproving the Accessibility of the NBER's Historical Data , by Daniel Feenberg and Jeff Miron. (NBER Working Paper #5186). Published in the Journal of Business and Economic Statistics, Volume 15 Number 3 (July 1997) pages 293-299.\\nInformation about seasonal adjustments is available, but in most cases only unadjusted series have been made available here.\\nContent\\nThe data.csv is organized in a long format with columns for the date, variable, and value. The dates are always the beginning of period date for whatever period existed in the original data. This means that '1920' was converted to January 1st, 1920 while Q2 1920 was converted to April 1, 1920. This is intended as a convenience to make it easier to work with multiple time series from the original mixed frequency data.\\nThe data is currently organized into 16 chapters:\\nChapter1: Production of Commodities\\nChapter2: Construction\\nChapter3: Transportation and Public Utilities\\nChapter4: Prices\\nChapter5: Stocks of Commodities\\nChapter6: Distribution of Commodities\\nChapter7: Foreign Trade\\nChapter8: Income and Employment\\nChapter9: Financial Status of Business\\nChapter10: Savings and Investment\\nChapter11: Security Markets\\nChapter12: Volume of Transactions\\nChapter13: Interest Rates\\nChapter14: Money and Banking\\nChapter15: Government and Finance\\nChapter16: Leading Indicators\\nThe dataset has been transformed from its original format. You can find the data preparation code here.\\nAcknowledgements\\nThis dataset was kindly made available by the National Bureau of Economic Research (NBER). You can find the original dataset here.\\nInspiration\\nWhich major historical events can you detect from the data?\\nWith roughly 3,500 time series in the dataset, finding relevant information can be challenging. Can you find a better way of organizing or indexing the data?\",\n",
       " \"«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»\\nContext\\nEn aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polítics, econòmics, etc...), en els principals índexs borsatils espanyols i de les crypto-monedes.\\nHem seleccionat diferents fonts de dades per generar fitxers «csv», guardar diferents valors en el mateix període de temps. És important destacar que ens interessa més les tendències alcistes o baixes, que podem calcular o recuperar en aquests períodes de temps.\\nContent\\nEn aquest cas el contingut està format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals s’ha generat un fitxer per dia del període de temps estudiat.\\nPel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període, en cada directori un fitxer amb cadascun del noms dels índexs. Degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com.\\nPel que fa als camps, ens interessà detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. Els camps especialment destacats són:\\n• Nom: Nom empresa o cryptomoneda;\\n• Preu: Valor en euros d’una acció o una cryptomoneda;\\n• Volum: En euros/volum 24 hores,acumulat de les transaccions diàries en milions d’euros\\n• Simbol: Símbol o acrònim de la moneda\\n• Cap de mercat: Valor total de totes les monedes en el moment actual\\n• Oferta circulant: Valor en oportunitat de negoci\\n• % 1h, % 2h i %7d, tant per cent del valor la moneda en 1h, 2h o 7d sobre la resta de cyprtomonedes.\\nAcknowledgements\\nEn aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a:\\nhttp://www.eleconomista.es\\nhttps://coinmarketcap.com\\nPer aquest fet, les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.\\n[https://www.r4.com/que-necesitas/formacion/diccionario]\\nInspiration\\nHi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes:\\nhttps://arxiv.org/pdf/1410.1231v1.pdf\\nEn aquest cas el «trading» en cryptomoneda és relativament nou, força popular per la seva formulació com a mitja digital d’intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents.\\nLa comunitat podrà respondre, entre altres preguntes, a:\\nEstà afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'Espanya?\\nEls efectes o agents externs afecten per igual a les accions o cryptomonedes?\\nHi ha relacions cause efecte entre les acciones i cryptomonedes?\\nProject repository\\nhttps://github.com/acostasg/scraping\\nDatasets\\nEls fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com:\\nhttps://www.kaggle.com/acostasg/stock-index/\\nhttps://www.kaggle.com/acostasg/crypto-currencies\\nPer una banda, els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil. De forma diferent, les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció.\",\n",
       " 'This dataset, from Crowdflower\\'s Data For Everyone Library, provides text of 5000 messages from politicians\\' social media accounts, along with human judgments about the purpose, partisanship, and audience of the messages.\\nHow was it collected?\\nContributors looked at thousands of social media messages from US Senators and other American politicians to classify their content. Messages were broken down into audience (national or the tweeter’s constituency), bias (neutral/bipartisan, or biased/partisan), and finally tagged as the actual substance of the message itself (options ranged from informational, announcement of a media appearance, an attack on another candidate, etc.)\\nAcknowledgments\\nData was provided by the Data For Everyone Library on Crowdflower.\\nOur Data for Everyone library is a collection of our favorite open data jobs that have come through our platform. They\\'re available free of charge for the community, forever.\\nInspiration\\nHere are a couple of questions you can explore with this dataset:\\nwhat words predict partisan v. neutral messages?\\nwhat words predict support messages v. attack messages?\\ndo politicians use Twitter and Facebook for different purposes? (e.g., Twitter for attack messages, Facebook for policy messages)?\\nThe Data\\nThe dataset contains one file, with the following fields:\\n_unit_id: a unique id for the message\\n_golden: always FALSE; (presumably whether the message was in Crowdflower\\'s gold standard)\\n_unit_state: always \"finalized\"\\n_trusted_judgments: the number of trusted human judgments that were entered for this message; an integer between 1 and 3\\n_last_judgment_at: when the final judgment was collected\\naudience: one of national or constituency\\naudience:confidence: a measure of confidence in the audience judgment; a float between 0.5 and 1\\nbias: one of neutral or partisan\\nbias:confidence: a measure of confidence in the bias judgment; a float between 0.5 and 1\\nmessage: the aim of the message. one of: -- attack: the message attacks another politician\\n-- constituency: the message discusses the politician\\'s constituency\\n-- information: an informational message about news in government or the wider U.S.\\n-- media: a message about interaction with the media\\n-- mobilization: a message intended to mobilize supporters\\n-- other: a catch-all category for messages that don\\'t fit into the other\\n-- personal: a personal message, usually expressing sympathy, support or condolences, or other personal opinions\\n-- policy: a message about political policy\\n-- support: a message of political support\\nmessage:confidence: a measure of confidence in the message judgment; a float between 0.5 and 1\\norig__golden: always empty; presumably whether some portion of the message was in the gold standard\\naudience_gold: always empty; presumably whether the audience response was in the gold standard\\nbias_gold: always empty; presumably whether the bias response was in the gold standard\\nbioid: a unique id for the politician\\nembed: HTML code to embed this message\\nid: unique id for the message WITHIN whichever social media site it was pulled from\\nlabel: a string of the form \"From: firstname lastname (position from state)\"\\nmessage_gold: always blank; presumably whether the message response was in the gold standard\\nsource: where the message was posted; one of \"facebook\" or \"twitter\"\\ntext: the text of the message',\n",
       " 'Context\\nLandslides are one of the most pervasive hazards in the world, causing more than 11,500 fatalities in 70 countries since 2007. Saturating the soil on vulnerable slopes, intense and prolonged rainfall is the most frequent landslide trigger.\\nContent\\nThe Global Landslide Catalog (GLC) was developed with the goal of identifying rainfall-triggered landslide events around the world, regardless of size, impacts, or location. The GLC considers all types of mass movements triggered by rainfall, which have been reported in the media, disaster databases, scientific reports, or other sources.\\nAcknowledgements\\nThe GLC has been compiled since 2007 at NASA Goddard Space Flight Center.',\n",
       " 'What is an Independent Expenditure?\\nIndependent expenditures are what some refer to as \"hard money\" in politics -- spending on ads that specifically mention a candidate (either supporting or opposing). The money for these ads must come from PACs that are independent of the candidate and campaign, and the PACs cannot coordinate with the candidate.\\nThe Federal Election Commission (FEC) collects information on independent expenditures to ensure payers\\' independence from candidates.\\nWhat can we look at?\\nI\\'m super interested to see how much spending has increased over the years. The FEC data only goes back to 2004, and it may be the case that the older data is spotty, but I don\\'t doubt that political spending has gone up in the past few years (the 2016 Presidential campaign reportedly involved the most political money since the 1970s).\\nWhat does the data look like?\\nThis dataset includes a ton of information from the independent expenditure reports:\\ncommittee_id : unique id of the PAC that made the payment\\ncommittee_name : name of the PAC that made the payment\\nreport_year : the year the report was file\\nreport_type : one of 24 or 48; whether this is a 24-hour report or a 48-hour report\\nimage_number : unique id of the scanned image of the report\\nline_number : line number in the report\\nfile_number : unique id of the report\\npayee_name : who got paid\\npayee_first_name : if an individual payee, their first name\\npayee_middle_name : if an individual payee, their middle name\\npayee_last_name : if an individual payee, their last name\\npayee_street_1 : payee street address (1 of 2)\\npayee_street_2 : payee street address (2 of 2)\\npayee_city : payee city\\npayee_state : payee state\\npayee_zip : payee ZIP code\\nexpenditure_description : a string describing the expenditure\\nexpenditure_date : when was this expenditure made?\\ndissemination_date : when was the advertisement disseminated?\\nexpenditure_amount : how much was spent?\\noffice_total_ytd : how much has this PAC spent on this office, year-to-date?\\ncategory_code : category of the expenditure (need to find categories!)\\ncategory_code_full : category of the expenditure (need to find categories!)\\nsupport_oppose_indicator : one of S or O; whether the ad is in support of or opposition to the candidate\\nmemo_code :\\nmemo_code_full :\\ncandidate_id : unique id of the candidate\\ncandidate_name : name of the candidate\\ncandidate_prefix : title or prefix of the candidate\\ncandidate_first_name : first name of the candidate\\ncandidate_middle_name : middle name of the candidate\\ncandidate_last_name : last name of the candidate\\ncandidate_suffix : suffix of the candidate\\'s name\\ncandidate_office : office that the candidate is running for -- one of P (President), S (Senate), or H (House)\\ncand_office_state : if House or Senate race, in what state?\\ncand_office_district : if House or Senate race, in what district?\\nconduit_committee_id :\\nconduit_committee_name :\\nconduit_committee_street1 :\\nconduit_committee_street2 :\\nconduit_committee_city :\\nconduit_committee_state :\\nconduit_committee_zip :\\nelection_type : one of P (primary) or G (general)\\nelection_type_full : an id comprising the election type and the year, with no delimiter\\nindependent_sign_name :\\nindependent_sign_date :\\nnotary_sign_name :\\nnotary_sign_date :\\nnotary_commission_expiration_date :\\nback_reference_transaction_id :\\nback_reference_schedule_name :\\nfiler_first_name :\\nfiler_middle_name :\\nfiler_last_name :\\ntransaction_id : unique id identifying the transaction\\noriginal_sub_id :\\naction_code :\\naction_code_full :\\nschedule_type_full :\\nfiling_form :\\nlink_id :\\nsub_id :\\npayee_prefix :\\npayee_suffix :\\nis_notice :\\nmemo_text :\\nfiler_prefix :\\nfiler_suffix :\\nschedule_type :\\npdf_url : link to the scanned form',\n",
       " 'Aedes aegypti and Ae. albopictus are the main vectors transmitting dengue and chikungunya viruses. Despite being pathogens of global public health importance, knowledge of their vectors’ global distribution remains patchy and sparse.\\nA global geographic database of known occurrences of Ae. aegypti and Ae. albopictus between 1960 and 2014 was compiled. The database, which comprises occurrence data linked to point or polygon locations, was derived from peer-reviewed literature and unpublished studies including national entomological surveys and expert networks. The authors describe all data collection processes, as well as geo-positioning methods, database management and quality-control procedures in their 2015 paper cited below.\\nThis is the first comprehensive global database of Ae. aegypti and Ae. albopictus occurrence, consisting of 19,930 and 22,137 geo-positioned occurrence records respectively. The dataset can be used for a variety of mapping and spatial analyses of the vectors and, by inference, the diseases they transmit.\\nCitations\\nKraemer MUG, Sinka ME, Duda KA, Mylne A, Shearer FM, Brady OJ, Messina JP, Barker CM, Moore CG, Carvalho RG, Coelho GE, Van Bortel W, Hendrickx G, Schaffner F, Wint GRW, Elyazar IRF, Teng H, Hay SI (2015) The global compendium of Aedes aegypti and Ae. albopictus occurrence. Scientific Data 2(7): 150035. http://dx.doi.org/10.1038/sdata.2015.35\\nKraemer MUG, Sinka ME, Duda KA, Mylne A, Shearer FM, Brady OJ, Messina JP, Barker CM, Moore CG, Carvalho RG, Coelho GE, Van Bortel W, Hendrickx G, Schaffner F, Wint GRW, Elyazar IRF, Teng H, Hay SI (2015) Data from: The global compendium of Aedes aegypti and Ae. albopictus occurrence. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.47v3c',\n",
       " \"About this Data\\nThis is a list of over 3,500 pizzas from multiple restaurants provided by Datafiniti's Business Database. The dataset includes the category, name, address, city, state, menu information, price range, and more for each pizza restaurant.\\nWhat You Can Do with this Data\\nYou can use this data to discover how much you can expect to pay for pizza across the country. E.g.:\\nWhat are the least and most expensive cities for pizza?\\nWhat is the number of restaurants serving pizza per capita (100,000 residents) across the U.S.?\\nWhat is the median price of a large plain pizza across the U.S.?\\nWhich cities have the most restaurants serving pizza per capita (100,000 residents)?\\nData Schema\\nA full schema for the data is available in our support documentation.\\nAbout Datafiniti\\nDatafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.\\nWant More?\\nYou can get more data like this by joining Datafiniti or requesting a demo.\",\n",
       " 'Context\\nThis official dataset from the Medicare.gov Nursing Home Compare website allows for comparison of over 15,000 Medicare and Medicaid-certified nursing homes in the country.\\nContent\\nSeparate data collections include:\\nDeficiencies, including fire safety, health, and inspection cycle types\\nOwnership details, including ownership percentage\\nPenalties, including filing date, fee, and payment date\\nProvider details, including non or for profit status, staff ratings, and survey scores\\nQuality MSR (Minimum Savings Rate) claims, including adjusted and observed scores\\nMDS (Minimum Data Set) quality measures, scored on a quarterly basis\\nState averages, including total number of quarterly deficiencies, and nurse staffing hours\\nSurvey summaries for each nursing home\\nInspiration\\nHow would you determine what the top ten best nursing homes in the country are? The least?\\nWhich states have the best level of nursing home care? The least?\\nIn general, what are the most common types of complaints and deficiencies?\\nAcknowledgements\\nThis dataset was collected by Medicare.gov, and the original files can be accessed here.',\n",
       " \"Content\\nThis dataset consists of a row for every accredited high school in New York City with its department ID number, school name, borough, building code, street address, latitude/longitude coordinates, phone number, start and end times, student enrollment with race breakdown, and average scores on each SAT test section for the 2014-2015 school year.\\nAcknowledgements\\nThe high school data was compiled and published by the New York City Department of Education, and the SAT score averages and testing rates were provided by the College Board.\\nInspiration\\nWhich public high school's students received the highest overall SAT score? Highest score for each section? Which borough has the highest performing schools? Do schools with lower student enrollment perform better?\",\n",
       " 'Context\\nThis dataset presents speech files recorded for isolated words of Urdu. Language resources for Urdu language are not well developed. In this work, we summarize our work on the development of Urdu speech corpus for isolated words. The Corpus comprises of 250 isolated words of Urdu recorded by ten individuals. The speakers include both native and non-native, male and female individuals. The corpus can be used for both speech and speaker recognition tasks. The sampling frequency is 16000 Hz.\\nContent\\nEach folder name refers to a single speaker. The folder name gives information about the characteristics of each speaker. Each folder contains 250 isolated files i.e. 250 isolated words.\\nSpeaker Name AA AB AC . . . AK\\nGender M for male F for female\\nNative /Non-Native Y for Native N for Non-Native\\nAge Group G1, G2, G3, G4\\nExample: AAMNG1 Speaker Name = AA Gender = Male N = Non-Native G1 = Age Group G1\\nAcknowledgements\\nAll the volunteers community who recorded for us.',\n",
       " 'Context\\nA Hadith is a report describing the words, actions, intentions or habits of the last Prophet and Messenger Muhammed (Peace Be Upon Him). The term literally means report, account or narrative.\\nṢaḥīḥ al-Bukhārī (صحيح البخاري), is one of the six major hadith collections books. It was collected by a Muslim scholar Imam Muhammad al-Bukhari, after being transmitted orally for generations. There are 7,658 full hadiths in this collection narrated by 1,755 narrators/transmitters.\\nImam Bukhari finished his work around 846 A.D.\\nContent\\nThe two main sources of data regarding hadith are works of hadith and works containing biographical information about hadith narrators. The dataset contains 7,658 hadiths in Arabic and the names of 1,755 transmitters. Imam Bukhari followed the following criterion to include a hadith in this book.\\nQuality and soundness of chain of narrators - the lifetime of a narrator should overlap with the lifetime of the authority from whom he narrates.\\nVerifiable - it should be verifiable that narrators have met with their source persons. They should also expressly state that they obtained the narrative from these authorities.\\nPiousness – he only accepted the narratives from only those who, according to his knowledge, not only believed in Islam but practiced its teachings.\\nAcknowledgements\\nMore information on Hadith and Sahih Bukhari can be found from this link - Hadith Books\\nInspiration\\nHere are some ideas worth exploring:\\nThe traditional criteria for determining if a hadith is Sahih (authentic) requires that there should be an uninterrupted chain of narrators; that all those narrators should be highly reliable and there should not be any hidden defects. Can we make a social network graph of all the narrators and then timestamp it with their age and era to see who overlaps who?\\nThe name of a transmitter mentioned in a given hadith is not the full name, and many transmitters have similar names. So identifying who is the transmitter of a given hadith based on the names mentioned in the text might be a good problem to tackle\\nCan we analyze the chains of transmitters for entire collections using Neo4j or some other graph database\\nThere exist different chains that reports the same hadith with little variation of words, can you identify those\\nCan you link the text with other external data sources?\\nCan we produce the word cloud for each chapter of the book?\\nCan we train a neural network to authenticate if the hadith is real or not?\\nCan we find out the specific style or vocabulary of each narrator?\\nCan we develop a system for comparing variant wordings for the same hadith to identify how reliable a given transmitter is.\\nPlease also help me extend this dataset. If you have any other hadith book in CSV or text format, please send me a message and I will add.',\n",
       " 'Context\\nAs I am trying to learn and build an LSTM prediction model for equity prices, I have tried Gold and then want to try crops which may have strong trends in times, so I prepared the dataset for the weekly corn prices.\\nContent\\nThe file composed of simply 2 columns. One is the date (weekend) and the other is corn close price. The period is from 2015-01-04 to 2017-10-01. The original data is downloaded from Quantopian corn futures price.\\nAcknowledgements\\nThanks to Jason of his tutorial about LSTM forecast: https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\\nInspiration\\nWilliam Gann: Time is the most important factor in determining market movements and by studying past price records you will be able to prove to yourself history does repeat and by knowing the past you can tell the future. There is a definite relation between price and time.',\n",
       " 'Content\\nThe dataset consists of 167.053 examples and contains Headlines, Url of Article, Complete Article and Category. I gathered the summarized news from Inshorts and only scraped the news articles from Folha de São Paulo - http://www.folha.uol.com.br/ (Brazilian Newspaper). Time period ranges is between January 2015 and September 2017.',\n",
       " 'Context\\nDataset was created by me for the Major Project of my Dual Degree Course Curriculum.\\nContent\\nDataset comprises of all the ODI Cricket Matches in the interval 1971-2017\\nIt consists of these files:\\n- originalDataset.csv : Raw Dataset File which I scraped using Pandas\\n- CategorialDataset.csv : Categorial Features suitable for models like MLPClassifier & DTClassifier\\n- ContinousDataset.csv : Purely for Experimental Purposes\\n- LabelledDataset.csv : Suitable for Support Vector Machines\\nAcknowledgements\\nESPN CricInfo',\n",
       " 'Description\\nThis is a collection of the Santander (Spain) bike-sharing open data facility, named Tusbic, operated by JCDecaux.\\nI will be updating this dataset form time to time, added new data as I collect it.\\nFormat\\nBike sharing system data\\nThe bikes.csv file contains the information from the bike sharing system. Data is structured as follows:\\nnumber number of the station\\ncontract_name name of the contract of the station\\nname name of the station\\naddress address of the station (raw)\\nlat latitude of the station in WGS84 format\\nlng latitude of the station in WGS84 format\\nbanking indicates whether this station has a payment terminal\\nbonus indicates whether this is a bonus station\\nstatus indicates whether this station is CLOSEDor OPEN\\nbike_stands the number of operational bike stands at this station\\navailable_bike_stands the number of available bike stands at this station\\navailable_bikes the number of available and operational bikes at this station\\nlast_update timestamp indicating the last update time in milliseconds since Epoch\\nBike lane geometries\\nThe bike_lanes.csv file contains the geometries of bike lanes in Santander city, as published by the Santander City Council in its open data platform.\\nayto:WKT contains the geometry in WKT format, using ED50 UTM coordinates (zone 30N).\\nwkt_wsg84 contains the geometry in WKT format, using WGS84 coordinates.\\nayto:Estado shows the status of the bike lane. EJECUTADO means that is has been built and it is operative.\\nLicense\\nThe bike sharing data is being collected from the JCDecaux Developer Open Data platform and is licensed under the Etalab Open License, compatbile with the standards of Open Data licenses (ODC-BY, CC-BY 2.0).\\nThe bike lane geometry is being collected form the Santander Open Data Platform and is licensed under a CC BY 4.0 license.\\nDataste Kaggle logo is a photo licensed under a CC-BY-SA 3.0, authored by Tiia Monto.',\n",
       " 'Gender Info 2007 is a global database of gender statistics and indicators on a wide range of policy areas, including: population, families, health, education, work, and political participation. It can be used by governments, international organizations, advocacy groups, researchers and others in need of statistics for planning, analysis, advocacy and awareness-raising. Users will find in Gender Info an easy-to-use tool to shed light on gender issues through customizable tables, graphs and maps. It is an initiative of the United Nations Statistics Division, produced in collaboration with the United Nations Children’s Fund (UNICEF) and the United Nations Population Fund (UNFPA).\\nThis dataset was last updated in 2008. If you need a more current version of the data please visit http://unstats.un.org/unsd/gender/data.html for other Gender Statistics.\\nAcknowledgements\\nThis dataset was kindly published by the United Nations on the UNData site. You can find the original dataset here.\\nLicense\\nPer the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference.',\n",
       " 'Basic Explanation\\nIt is important to know if a patient will be readmitted in some hospital. The reason is that you can change the treatment, in order to avoid a readmission.\\nIn this database, you have 3 different outputs:\\nNo readmission;\\nA readmission in less than 30 days (this situation is not good, because maybe your treatment was not appropriate);\\nA readmission in more than 30 days (this one is not so good as well the last one, however, the reason can be the state of the patient.\\nIn this context, you can see different objective functions for the problem. You can try to figure out situations where the patient will not be readmitted, or if their are going to be readmitted in less than 30 days (because the problem can the the treatment), etc... Make your choice and let\\'s help them creating new approaches for the problem.\\nContent\\n\"The data set represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. Information was extracted from the database for encounters that satisfied the following criteria.\\nIt is an inpatient encounter (a hospital admission).\\nIt is a diabetic encounter, that is, one during which any kind of diabetes was entered to the system as a diagnosis.\\nThe length of stay was at least 1 day and at most 14 days.\\nLaboratory tests were performed during the encounter.\\nMedications were administered during the encounter.\\nThe data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.\"\\nhttps://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\\nSource\\nThe data are submitted on behalf of the Center for Clinical and Translational Research, Virginia Commonwealth University, a recipient of NIH CTSA grant UL1 TR00058 and a recipient of the CERNER data. John Clore (jclore \\'@\\' vcu.edu), Krzysztof J. Cios (kcios \\'@\\' vcu.edu), Jon DeShazo (jpdeshazo \\'@\\' vcu.edu), and Beata Strack (strackb \\'@\\' vcu.edu). This data is a de-identified abstract of the Health Facts database (Cerner Corporation, Kansas City, MO).\\nOriginal source of the data set\\nhttps://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008',\n",
       " 'The United States Department of Labor tells us that \"Labor Day, the first Monday in September, is a creation of the labor movement and is dedicated to the social and economic achievements of American workers. It constitutes a yearly national tribute to the contributions workers have made to the strength, prosperity, and well-being of our country.\"\\nThis database of state-level union membership and coverage from 1983 to 2015 was originally compiled by Barry Hirsch (Andrew Young School of Policy Studies, Georgia State University) and David Macpherson (Department of Economics, Trinity University). The database, available at unionstats.com provides private and public sector labor union membership, coverage, and density estimates compiled from the monthly household Current Population Survey (CPS) using BLS methods.\\nUse of this data requires citation of the following paper which also includes a description of how the database was created: Barry T. Hirsch and David A. Macpherson, \"Union Membership and Coverage Database from the Current Population Survey: Note,\" Industrial and Labor Relations Review, Vol. 56, No. 2, January 2003, pp. 349-54. (PDF).',\n",
       " 'Context\\nThe Panama Paper Case is the most publicized case in the history of Pakistan. It was heard before the Supreme Court of Pakistan between November 1st 2016 to February 23, 2017. It alleges the corruption, money laundering and wrong doing by the Prime Minister of Pakistan, Nawaz Sharif.\\nAfter subsequent formation of Joint Investigation Committee and its recommendations, court finally disqualified the prime minister on July 28th 2017. Since then, there is a hue and cry in the media by the incumbent political party and one question the ex-Prime Minister is keep asking is \"Mujhe Kiyun Nikala (Why They Kicked Me Out). It seems the poor man has no idea what happened and why.\\nPakistan is a country of 207 million people according to the recent census and no one is giving him the answer of his question. I feel pity for the ex-Prime Minister, and thought to launch this dataset to call my fellow data scientists to run the kernels, dig out the hidden meanings, find patterns and linkages with off-shore companies (I\\'ve posted the complete Panama Papers in another dataset as well) to help him understand \"Unhen Kiyun Nikala - Why did he kicked out?\"\\nHere is a good Wikipedia article with further details. Please do contribute more datasets regarding this case through discussion forums and I will keep updating it as the case/dataset progresses.\\nPlease help me answer the most publicized question in the history of Pakistan -\\nMujhe Kiyun Nikala.\\nContent\\nFull text of Panama Case Verdict is available through CSV file in the Data section. The original and subsequent review decisions by the Supreme Court of Pakistan can be found through these links:\\nCivil Review Petition No. 297 of 2017 in Const. Petition No. 29 of 2016\\nCivil Appeals No. 1406 and 1407 of 2016\\nConstitution Petition No. 29 of 2016\\nReview Decision\\nFull Verdict in Urdu\\nFull Verdict in English\\nAcknowledgements\\nSupreme Court of Pakistan\\nInspiration\\nSome ideas worth exploring:\\nWho are the lawyers from both sides?\\nWhat charges were filled against the ex-Prime Minister\\nWhat charges were found Not True\\nWhat charges were found True\\nWhat other references court has given that resembles this case\\nHow we can join this dataset with Panama dataset\\nHow we can visually present the data\\nHow data science can answer the question - Mujhe Kiyun Nikala',\n",
       " 'Context\\nThe first 100 rows of the data dump of parsed matches from opendota.com (formerly yasp.co) as of mid December 2015.\\nContent\\nColumns:\\nmatch_id - INTEGER, unique match id\\nmatch_seq_num - INTEGER, match sequence number\\nradiant_win - STRING, boolean variable than indicates if radiant won or not in the match\\nstart_time - INTEGER, start time of the match\\nduration - INTEGER, duration of the match\\ntower_status_radiant - INTEGER, remaining health of the towers of the radiant side\\ntower_status_dire - INTEGER, remaining health of the towers of the dire side\\nbarracks_status_radiant - INTEGER, remaining health of the barracks of the radiant side\\nbarracks_status_dire - INTEGER , remaining health of the towers of the direside\\ncluster - INTEGER,\\nfirst_blood_time - INTEGER, time when the first blood occured in the match\\nlobby_type - INTEGER, type of the looby of the match\\nhuman_players - INTEGER, number of human players in the match leagueid - INTEGER, league id\\npositive_votes - INTEGER, number of positive votes\\nnegative_votes - INTEGER, number of negative votes\\ngame_mode - INTEGER, game mode\\nengine - INTEGER, engine\\npicks_bans - STRING, picks and bans\\nparse_status - INTEGER, parse status\\nitem - STRING, a complex JSON that also include all the columns mentioned but may need more processing since the more interesting data are found here (e.g. chats, teamfights, purchase logs, etc. )\\nPast Research\\nThere are already several sites (see dotabuff and onedota) that analyze dota 2 matches.\\nInspiration\\nThe data set could be used by Dota 2 players but are data science enthusiasts as well.\\nThere are lots of questions that can be formulated in this dataset.\\nIt will be helpful if somebody can produce a better friendlier version of the dataset.\\nAcknowledgements\\nCitation:\\nSource: https://yasp.co/blog/33\\nReadme: https://github.com/yasp-dota/yasp/issues/924\\nType: Dataset\\nTags: Dota 2, Games, MOBA\\nAbstract: This is a data dump of all the parsed matches from yasp.co (as of mid December 2015). This is about 3.5 million matches.\\nLicense: CC BY-SA 4.0\\nTerms: We ask that you attribute yasp.co if you create or publish anything related to our data. Also, please seed for as long as possible.\\nhttps://www.opendota.com/\\nhttps://bigquery.cloud.google.com/table/fh-bigquery:public_dump.dota2_yasp_v1\\nHow the dataset was compiled\\nThe details of how the dataset was complied is detailed here: https://github.com/odota/core/issues/924\\nSuggested Way of Reading the File:\\nIn R, the best way to read the file is through the use of read.delim() function since the values are tab separated.\\nexample: dota = read.delim(\"dota2_yasp_v1.txt\", sep=\"\\\\t\")\\nImage Source:\\nhttp://media.steampowered.com/apps/dota2/images/blogfiles/keyart_ezalor.jpg',\n",
       " \"Context\\nOver nine thousand developers took part in the first edition of the State Of JavaScript survey.\\nThey answered questions on topics ranging from front-end frameworks and state management, to build tools and testing libraries.\\nYou'll find out which libraries developers most want to learn next, and which have the highest satisfaction ratings. And hopefully, this data will help you make sense of the ever-changing JavaScript ecosystem.\\nContent\\nhttp://stateofjs.com/2016/introduction/\\nAcknowledgements\\nThanks to http://stateofjs.com/ open the data for download.\",\n",
       " 'Context\\nWhen Brazilian consumers need to resolve a dispute with business the first step is to go to a local Procon (Consumer Protection Agency) and file a complaint. The Procon assists the consumer and intermediates the resolution with the company.\\nContent\\nThis dataset contains information about complaints filed in Procons between 2012 and 2016. This data was download from official Brazilian government open data website',\n",
       " 'Context:\\nBirds use songs and calls of varying length and complexity to attract mates, warn of nearby danger and mark their territory. This dataset contains a recordings of different birdsongs from bird species that can be found in Britain (although the recordings themselves are from many different locations).\\nContent:\\nThis is a dataset of bird sound recordings, a specific subset gathered from the Xeno Canto collection to form a balanced dataset across 88 species commonly heard in the United Kingdom.\\nThe copyright in each audio file is owned by the user who donated the file to Xeno Canto. Please see \"birdsong_metadata.tsv\" for the full listing, which gives the authors\\' names and the CC licences applicable for each file. The audio files are encoded as .flac files.\\nAcknowledgements:\\nThese recordings were collected by 68 separate birding enthusiasts and uploaded to and stored by xeno-canto: www.xeno-canto.org. If you make use of these recordings in your work, please cite the specific recording and include acknowledgement of and a link to the xeno-canto website.\\nInspiration:\\nCan you build a classifier to identify birds based on their songs?\\nCan you visualize the songs of specific birds?\\nCan you generate new birdsongs based on this data?',\n",
       " \"Context\\nWhile the physiological response of humans to emotional events or stimuli is well-investigated for many modalities (like EEG, skin resistance, ...), surprisingly little is known about the exhalation of so-called Volatile Organic Compounds (VOCs) at quite low concentrations in response to such stimuli. VOCs are molecules of relatively small mass that quickly evaporate or sublimate and can be detected in the air that surrounds us. The project introduces a new field of application for data mining, where trace gas responses of people reacting on-line to films shown in cinemas (or movie theaters) are related to the semantic content of the films themselves. To do so, we measured the VOCs from a movie theater over a whole month in intervals of thirty seconds, and annotated the screened films by a controlled vocabulary compiled from multiple sources.\\nContent\\nThe data set consists of two parts, first the measured VOCs, and second the information about the movies. The VOCs are given in the file TOF_CO2_data_30sec.arff which is simply the time of the measurement in the first column, then all measured 400+ VOCs in the other columns. Roughly one measurement was carried out every 30 seconds. The information which movies were shown is given in the file screenings.csv. It gives start time, end time, movie title and how many visitors were in the screening. Additionally, the folder labels_aggregated give a consensus labelling of multiple annotators for the movies. The labels describe the scenes, each label represented by a row, then each column showing if the label is active (1) or not (0). This is available for 6 movies in the data set.\\nThe goal of our initial analysis was the identification of markers, that is, finding certain VOCs that have a relation to certain labels and therefore emotions. For example, given the scene label blood, is there any increase or decrease in the concentration of a specific VOC?\\nFurther information is available in our publications https://doi.org/10.1145/2783258.2783404 and https://doi.org/10.1038/srep25464\\nAcknowledgements\\nIf you use this data set, please cite:\\nJörg Wicker, Nicolas Krauter, Bettina Derstorff, Christof Stönner, Efstratios Bourtsoukidis, Thomas Klüpfel, Jonathan Williams, and Stefan Kramer. 2015. Cinema Data Mining: The Smell of Fear. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '15). ACM, New York, NY, USA, 1295-1304. DOI: https://doi.org/10.1145/2783258.2783404\\nInspiration\\nWhile the first analysis gave already interesting results, we believe that this data set has a high potential for further analysis. We are currently working on increasing the size of the data. Additionally, multiple follow-up publications are being prepared. There are many posssible tasks, we focus mainly on the identification of markers in the VOC data, but there are many potential interesting findings in the data set. Are movies related based on the VOCs? Could we identify similar scenes based on the VOCs?\",\n",
       " \"I need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data. Sorry about the format, it's in text file. Thanks in advance :)\\nContext: Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\\nData Set Characteristics:\\nMultivariate, Time-Series\\nAssociated Tasks: Regression, Clustering\\nData Set Information:\\nThis archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months). Notes: 1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\\n2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\\nAttribute Information: 1.date: Date in format dd/mm/yyyy\\n2.time: time in format hh:mm:ss\\n3.global_active_power: household global minute-averaged active power (in kilowatt)\\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\\n5.voltage: minute-averaged voltage (in volt)\\n6.global_intensity: household global minute-averaged current intensity (in ampere)\\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\",\n",
       " 'Patent Assignment Daily XML (PADX)\\nThis dataset contains daily patent assignment (ownership) text (no drawings/images) for 10/18/2016 derived from patent assignment recordations made at the USPTO.\\nContext\\nEach day, the US Patent and Trademark Office (USPTO) records patent assignments (changes in ownership). These assignments can be used to track chain-of-ownership for patents and patent applications. For more information about Ownership/Assignability of Patents and Applications, please see the USPTO Manual of Patent Examining Procedure (MPEP), Section 301.\\nFrequency: Weekly (MON-SUN)\\nPeriod: 10/18/2016\\nInspiration\\nThis dataset provides insight into where new knowledge and technological advances originate in the United States. To get started working with XML files, fork the kernel Exploring Daily Patent Assignment Files. You can use this dataset to understand what sector is currently up-and-coming or which companies are filing a lot of patents, which can proxy their level of innovation, a corporate strength, or a focus of new research and development.\\nAcknowledgements\\nThe USPTO owns the dataset. These files are extracted nightly from the Assignment Historical Database (AHD).\\nLicense\\nCreative Commons - Public Domain Mark 1.0',\n",
       " 'Context\\nIn 2013 alone, international migrants sent $413 billion home to families and friends. This money is known as \"remittance money\", and the total is more than three times that afforded by total global foreign aid ($135 billion). Remittances are traditionally associated with poor migrants moving outside of their home country to find work, supporting their families back home on their foreign wages; as a result, they make up a significant part of the economic picture for many developing countries in the world.\\nThis dataset, published by the World Bank, provides estimates of 2016 remittance movements between various countries. It also provides historical data on the flow of such money going back to 1970.\\nFor a look at how remittances play into the global economy, watch \"The hidden force in global economics: sending money home\".\\nContent\\nThis dataset contains three files:\\nbilateral-remittance.csv --- Estimated remittances between world countries in the year 2016.\\nremittance-inflow.csv --- Historical remittance money inflow into world countries since 1970. Typically high in developing nations.\\nremittance-outflow.csv --- Historical remittance money outflow from world countries since 1970. Typically high in more developed nations.\\nAll monetary values are in terms of millions of US dollars.\\nFor more information on how this data was generated and calculated, refer to the World Bank Remittance Data FAQ.\\nAcknowledgements\\nThis dataset is a republished version of three of the tables published by the World Bank which has been slightly cleaned up for use on Kaggle. For the original source, and other complimentary materials, check out the dataset home page.\\nInspiration\\nWhat is the historical trend in remittance inflows and outflows for various countries? How does this relate to the developmental character of the countries in question?\\nWhat countries send to most money abroad? What countries receive the most money from abroad? Try combining this dataset with a demographics dataset to see what countries are most and least reliant on income from abroad.\\nHow far do workers migrate for a job? Are they staying near home, or going half the world away? Are there any surprising facts about who send money to who?',\n",
       " \"This database contains a subset of the Memetracker dataset collected by SNAP.\\nThe full Memetracker dataset has observations broken into months. Because of size considerations, however, this version consists of one-half of a month: the first 15 days of Memetracker observations from November 2008.\\nAbout\\nMemetracker tracks the quotes and phrases that appear most frequently over time across the entire online news spectrum. This makes it possible to see how different stories compete for news and blog coverage each day, and how certain stories persist while others fade quickly.\\nOverall Memetracker tracks more than 17 million different phrases and about 54% of the total phrase/quote mentions appear on blogs and 46% in news media.\\nAcknowledgments\\nThis dataset was collected by the Stanford Network Analysis Project. Detailed information about the data and its analysis can be found at the website here.\\nAn analysis of this dataset was published here:\\nJ. Leskovec, L. Backstrom, J. Kleinberg. Meme-tracking and the Dynamics of the News Cycle. ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining, 2009.\\nThe Data\\nThe SQLite database contains three tables:\\narticles: 4,542,920 records, with the following fields:\\narticle_id: a unique id for the article (int)\\nurl: the URL of the article (text)\\ndate: the date of the article (text), in the strptime format '%Y-%m-%d %H:%M:%S'\\nquotes: 7,956,125 records, with the following fields:\\narticle_id: unique id for the article that this quote was found in (int)\\nphrase: the high-frequency phrase found in the article (text)\\nlinks: 16,727,125 records, with the following fields:\\narticle_id: unique id for the article that this link was found in (int)\\nlink_out: the URL of the link out (text)\\nlink_out_id: unique id for the target article (int), if it exists; else NULL\",\n",
       " 'These are the top trending \"How to\" searches on Google, ranked by their spike value. Trending searches are searches with the biggest increase in search interest since the previous time period. Data covers the past 5 years.',\n",
       " \"Context\\nThis dataset is created as a part of my dissertation work for the fulfillment of the Master's degree in Computer Science (Tribhuvan University, Nepal, 2012).\\nContent\\nThe dataset contains three individual categories. Samples are collected from 40 individuals (persons) from different fields and cropped for character boundary.\\nNumerals (288 samples per class, 10 classes)\\nVowels (221 samples per class, 12 classes)\\nConsonants (205 samples per class, 36 classes)\\nCitation\\nPlease cite in your publications if it helps your research:\\n@inproceedings{pant2012off,\\n  title={Off-line Nepali handwritten character recognition using Multilayer Perceptron and Radial Basis Function neural networks},\\n  author={Pant, Ashok Kumar and Panday, Sanjeeb Prasad and Joshi, Shashidhar Ram},\\n  booktitle={2012 Third Asian Himalayas International Conference on Internet},\\n  pages={1--5},\\n  year={2012},\\n  organization={IEEE}\\n}\",\n",
       " \"Context\\nThe dataset is a csv file compiled using a python scrapper developed using Reddit's PRAW API. The raw data is a list of 3-tuples of [username,subreddit,utc timestamp]. Each row represents a single comment made by the user, representing about 5 days worth of Reddit data. Note that the actual comment text is not included, only the user, subreddit and comment timestamp of the users comment. The goal of the dataset is to provide a lens in discovering user patterns from reddit meta-data alone. The original use case was to compile a dataset suitable for training a neural network in developing a subreddit recommender system. That final system can be found here\\nA very unpolished EDA for the dataset can be found here. Note the published dataset is only half of the one used in the EDA and recommender system, to meet kaggle's 500MB size limitation.\\nContent\\nuser - The username of the person submitting the comment\\nsubreddit - The title of the subreddit the user made the comment in\\nutc_stamp - the utc timestamp of when the user made the comment\\nAcknowledgements\\nThe dataset was compiled as part of a school project. The final project report, with my collaborators, can be found here\\nInspiration\\nWe were able to build a pretty cool subreddit recommender with the dataset. A blog post for it can be found here, and the stand alone jupyter notebook for it here. Our final model is very undertuned, so there's definitely improvements to be made there, but I think there are many other cool data projects and visualizations that could be built from this dataset. One example would be to analyze the spread of users through the Reddit ecosystem, whether the average user clusters in close communities, or traverses wide and far to different corners. If you do end up building something on this, please share! And have fun!\\nReleased under Reddit's API licence\",\n",
       " 'Context\\nThis data was compiled as part of our undergrad project that used machine learning to classify songs based on themes or activities songs are associated with. For the project we, four activities were choose.\\nDinner: Songs that sound good when played in a dinner setting or at a restaurant.\\nSleep: Songs that promote sleep when they are played.\\nParty: Songs that sound good when played at a party.\\nWorkout: Songs that sound good when one is exercising/ working out.\\nThe collection of data started with collecting playlist details form Spotify. Spotify web API was used for the collection of the playlist of each category. Track title, album name and artist names were used to extract low level and high level Audio features like MFCC, Spectral centroid, Spectral Roll-off, Spectral Bandwidth, Tempo, Spectral Contrast and Root Mean Square Energy of the songs. For ease of computation, the mean of the values were calculated and added to the tables.\\nData was also curated using Spotify\\'s audio analysis API. A larger set of songs is part of this data set.\\nContent\\nThe data set has eight tables.\\nFour tables with names playlist_audio_features have the signal processing features like MFCC, spectral centroid etc.\\nFour more tables with names playlist_spotify_features have the data extracted from Spotify\\'s audio feature API. These tables have larger number of features. The data set size is quite large.\\nDescription of the \"playlist\"_audio_features columns:\\nThe first column has the simple integer id if the track. (This id is local to that file).\\nThe second column has the name of the track.\\nThe third column name mfcc has the mean of the calculated MFCC for that track. 20 MFC coefficients were extracted from one frame of the track.\\nThe forth column is named scem: This is the mean of Spectral centroid. Spectral centroid was calculated for each frame.\\nThe fifth column is named scom: This is the mean of Spectral contrast. Spectral contrast was calculated for each frame.\\nThe sixth column is named srom: This is the mean of Spectral Roll-off. Spectral roll-off was calculated for each frame.\\nThe seventh column is named sbwm: This is the mean of Spectral Bandwidth. Spectral Bandwidth was calculated for each frame.\\nThe eight column is name tempo: This is the estimated tempo of the track.\\nThe ninth column is name rmse: This is the mean of the RSME was calculated for each frame.\\nDescription of the _spotify_features columns:\\nid: This is the Spotify id of the track.\\nname: This is the name of the track.\\nurl: This is a Spotify uri of the track.\\nartist: This is a one or more artists who worked on the track. 5-13: Description of each of the column can be found at https://developer.spotify.com/web-api/get-audio-features/\\nAcknowledgements\\nWe would like to thank Librosa an opensource audio feature extraction library in python for developing a great tool. We would also thank the large research done on music genre classification using audio feature which helped us in developing this data set as well as the classification. A special thanks to Spotify',\n",
       " 'Content and Context\\nThis dataset is a collection of biomedicine and life science bibliometric data obtained from MEDLINE.\\nThe data covers 26,759,425 papers available thought MEDLINE from 1946 through 2016.\\nThis dataset has been created by processing the publicly available data dump at nih.gov. The data dump consists of ca 23 Million xml articles. From these xml files I have extracted some meta data into more accessible csv files. The processed csvs contain both very basic metadata (i.e. creation date, number of authors, and ids) and the \"Medical Subject Headings\" MeSH classification.\\nThe dataset is divided in two files,\\npaper_details.csv\\nmesh.csv\\nThe paper_details.csv file contains the following columns:\\npmid: this is the unique article identified within the dataset, it is also the official identifier used on PubMed\\ndoi: digital object identifier, this id allows to uniquely identify a paper through the widely used DOI scheme\\nnum_authors: number of authors on the paper\\nyear: year the document has been created\\nmonth: month the document has been created\\nday: day the document has been created\\ntitle: title of the document\\nissn: the ISSN number of the journal the document has been published in\\nissn_type: printed or electronic\\nvolume: volume of the journal the document has been published in\\nissue: issue of the journal the document has been published in\\njournal_title: Name of Journal the document has been published in\\njournal_title_iso: ISO abbreviation of the journal title.\\n5 GB, and 26,759,425 rows\\nIn the mesh.csv file the following fields are available:\\npmid: this is the unique article identified within the dataset, it is also the official identifier used on PubMed\\ndescriptor_id: the id of the MeSH descriptor for the given document\\ndescriptor_name: name of the MeSH descriptor\\ndescriptor_major_topic: Y/N, indicate if the descriptor is a major topic in MeSH\\nqualifiers: a list of MeSH qualifiers (aka subheadings). Subheadings are attached to MeSH headings to describe a specific aspect of a concept.\\nNB: a document can and has more often then not more then one MeSH descriptor associated to it.\\n11 GB, and 247,415,857 rows\\nOther Resources\\nAn introduction to MeSH can be found here and short tutorials from the U.S. National Library of Medicine on how to use MeSH can be found here.\\nFactsheets describing MeSH and MEDLINE:\\nhttps://www.nlm.nih.gov/pubs/factsheets/mesh.html\\nhttps://www.nlm.nih.gov/pubs/factsheets/medline.html\\nFurther datasets, which might be useful to work with MeSH data:\\nhttps://mbr.nlm.nih.gov/Downloads.shtml\\nAcknowledgements\\nThe data is freely available for download from MEDLINE. Specifically through their ftp service at ftp://ftp.ncbi.nlm.nih.gov/pub/.\\nRead the disclaimer from the U.S. National Library of Medicine regarding public use and redistribution of the data here\\nThe data extraction would not have been possible without access to the computing facilities of IMT School for Advanced Studies Lucca.\\nInspiration\\nQuestions which one might want to look into using this dataset could be:\\nPrediction of Mesh Headings\\nDescriptive statistics on the rise and fall of MeSH descriptors over time.\\nWhich Journals ˙have had the most meteoric rise?\\nCompute the co-occurrence probability of a given MeSH descriptor pair over time\\nHow to open the files:\\nDue to the limit on Kaggle for files to be at most 500MB in size, the files have been split. More specifically the two files have been compressed with zip and split. To recombine them do the following. If you are on Linux/MacOS enter the following commands in the terminal\\ncat paper_details.csv.zip.part-* > papers_details.csv.zip\\nunzip papers_details.csv.zip\\nFor Windows machines, this Stock Overflow Answer will help.\\nImage Credit\\nThe beautiful cover image has been made by olsztyn-poland over at unsplash.com.\\nLink: https://unsplash.com/collections/610433/medical?photo=nss2eRzQwgw',\n",
       " 'Context\\nStudy for poem classification. Trying to classified poems with targets age and type. I use two Xgboost predictors to predict target and type separately.\\nContent\\nPlease refer to the website https://www.poetryfoundation.org/ For now I only crawl the data of\\nrenaissance love\\nmodern love\\nrenaissance nature\\nmodern nature\\nrenaissance mythology & folklore\\nmodern mythology & folklore\\nSome have copyrights. I only use for studying :)\\nAcknowledgements\\nhttps://www.poetryfoundation.org/ has the copyright\\nInspiration\\nclassification is fun!!',\n",
       " \"Indian Premier League Database for Data Analysis.\\nCSV Version for the same Dataset\\nContext\\n► 577 matches\\n► 469 Players\\n► Seasons 2008 to 2016\\n► Detailed Player Details (Player_DOB, Player_Batting_Style,Player_Bowling_Style, etc...) for 469 players.\\n► Detailed match events (Toss,Stadium, Date of the match, Who captained the match, who was the keeper, Extras, etc...) for 577 matches.\\nExample SQL Queries on IPL Database\\nMicrosoft SQL Server Version\\nDatabase Diagram\\nContent :\\nDataset includes 21 includes as mentioned below. Some tables created intentionally to practice SQL Queries. You can use Boolean values in some situations instead of joining tables like Toss_Table\\nAcknowledgements :\\nThanks to stackoverflow community\\nInspiration\\nAim is to provide the database for all the Indian Premier League matches. where people can run their own queries to analyze the data.\\nSome of the analysis we most often heard in TV commentary are below.\\nDhoni's Strike rate against Left-ball spinners.\\nWhat will be the result percentage when team lost more than three wickets in powerplay.\\nBelow are the some websites where you can find inspiration for your analysis.\\nKnoema Cricket Analysis\\nPandimi Cricket Analysis\",\n",
       " 'Data on party strength in each US state\\nThe repository contains data on party strength for each state as shown on each state\\'s corresponding party strength Wikipedia page (for example, here is Virginia )\\nEach state has a table of a detailed summary of the state of its governing and representing bodies on Wikipedia but there is no data set that collates these entries. I scraped each state\\'s Wikipedia table and collated the entries into a single dataset. The data are stored in the state_party_strength.csv and state_party_strength_cleaned.csv. The code that generated the file can be found in corresponding Python notebooks.\\nData contents:\\nThe data contain information from 1980 on each state\\'s: 1. governor and party 2. state house and senate composition 3. state representative composition in congress 4. electoral votes\\nClean Version\\nData in the clean version has been cleaned and processed substantially. Namely: - all columns now contain homogenous data within the column - names and Wiki-citations have been removed - only the party counts and party identification have been left The notebook that created this file is here\\nUncleaned Data Version\\nThe data contained herein have not been altered from their Wikipedia tables except in two instances: - Forced column names to be in accord across states - Any needed data modifications (ie concatenated string columns) to retain information when combining columns\\nTo use the data:\\nPlease note that the right encoding for the dataset is \"ISO-8859-1\", not \\'utf-8\\' though in future versions I will try to fix that to make it more accessible.\\nThis means that you will likely have to perform further data wrangling prior to doing any substantive analysis. The notebook that has been used to create this data file is located here\\nRaw scraped data\\nThe raw scraped data can be found in the pickle. This file contains a Python dictionary where each key is a US state name and each element is the raw scraped table in Pandas DataFrame format.\\nHope it proves as useful to you in analyzing/using political patterns at the state level in the US for political and policy research.',\n",
       " 'Context\\nThe complete Global Firepower list for 2017 puts the military powers of the world into full perspective.\\nContent\\nGlobalFirePower.csv Contains all columns without categories. GlobalFirePower_multiindex.csv has 2 level columns, see the kernel for a parsing example.\\nFields list\\nManpower\\nTotal Populations\\nAvailable Manpower\\nManpower Fit-for-Service\\nManpower Reaching Military Age Annually\\nActive Military Manpower\\nActive Reserve Military Manpower\\nAir Power\\nTotal Aircraft Strength\\nFighters & Interceptors\\nAttack Aircraft\\nTransports\\nTrainers\\nTotal Helicopters\\nAttack Helicopters\\nServiceable Airports\\nArmy Strengths\\nCombat Tanks\\nArmored Fighting Vehicles\\nSelf-Propelled Artillery\\nTowed Artillery\\nRocket Projectors\\nNaval Power\\nTotal Naval Strength\\nAircraft Carriers\\nFrigates\\nDestroyers\\nCorvettes\\nSubmarines\\nPatrol Craft\\nMine Warfare\\nFinancial Resources\\nAnnual Defense Budgets\\nExternal Debt\\nReserves of Foreign Exchange and Gold\\nPurchasing Power Parity\\nLogistical Resources\\nLabor Force Strength\\nMerchant Marine Strength\\nMajor Ports & Terminals\\nRoadway Coverage\\nRailway Coverage\\nNatural Resources\\nOil Production\\nOil Consumption\\nProven Oil Reserves\\nGeography\\nSquare Land Areas\\nCoastline\\nShared Borders\\nWaterway Coverage\\nThe finalized Global Firepower ranking relies on over 50 factors to determine a given nation\\'s PowerIndex (\\'PwrIndx\\') score. Our formula allows smaller, though more technologically-advanced, nations to compete with larger, lesser-developed ones. Modifiers (in the form of bonuses and penalties) are added to further refine the list. Some items to observe in regards to the finalized ranking:\\nRanking does not simply rely on the total number of weapons available to any one country but rather focuses on weapon diversity within the number totals to provide a better balance of firepower available (i.e. fielding 100 minesweepers does not equal the strategic and tactical value of fielding 10 aircraft carriers).\\nNuclear stockpiles are NOT taken into account but recognized / suspected nuclear powers receive a bonus.\\nGeographical factors, logistical flexibility, natural resources and local industry influence the final ranking.\\nAvailable manpower is a key consideration; nations with large populations tend to rank higher.\\nLand-locked nations are NOT penalized for lack of a navy; naval powers ARE penalized for lack of diversity in available assets.\\nNATO allies receive a slight bonus due to the theoretical sharing of resources.\\nCurrent political / military leadership is NOT taken into account.\\nFor 2017 there are a total of 133 countries included in the GFP database.\\nAcknowledgements\\nThe CSV files were parsed from http://www.globalfirepower.com/countries-listing.asp .\\n©2017 www.GlobalFirepower.com • Content ©2003-2017 GlobalFirepower.com • All Rights Reserved • The GlobalFirepower.com logo are trademarks and protected by all applicable domestic and international intellectual property laws. All material presented on this site is \"as is\" without any guarantee. Part of the MilitaryFactory.com network of sites.',\n",
       " \"Content\\nThis Dataset is scraped from https://www.thenews.com.pk website. It has news articles from 2015 till date related to business and sports. It Contains the Heading of the particular Article, Its content and its date. The content also contains the place from where the statement or Article was published.\\nImportance\\nThis dataset can be used to detect main patterns between writing pattern of different types of articles. One more thing that can be extracted from it is that we could also detect the main locations from where the different types of articles originate.\\nImprovements\\nSome Data Cleaning could still be done specially in the content area of the dataset. One more thing that could be done is that we could extract the locations from the content and make a separated table for it.\\nAcknowledgements\\nI'd like to thanks developer of Selenium Library. That helped a lot in retrieving the data.\",\n",
       " \"Content\\nDog: LicenseType, Breed, Color, DogName, OwnerZip, ExpYear, ValidDate\\nI've included all the data from 2007 to 2017\\nInspiration\\nI love dogs To see trends in dog breeds, why are some more popular than others?\",\n",
       " \"Content\\ntweet id, contains tweet-stamp\\ndate + time, date and time of day (24hr)\\ntweet text, text of tweet, remove 'b'\\nusage\\nWhat's someone going to do with a bunch of tweets?\\nMaybe someone would want to generate text using this dataset\\nor do sentiment analysis\\nOr find out the most likely time of day Elon would tweet.\\npie his tweets per month, ITS DATA!!\\nEither way its up to you!\\nInspiration:\",\n",
       " 'Content\\nAttached is a list of Twitter users who regularly report on natural and man-made disasters, violence or crime. The accounts may belong to journalists, news media, local fire or police departments, other local authorities, or disaster monitors. Disaster reporting may not be the primary function of the accounts, nevertheless they are a prolific source of disaster/accident reporting, especially at the location they are associated with.\\nBackground\\nDetails of the curation of this dataset, once published, will be added to this entry.\\nDisclaimer\\nThe dataset does not include a measure of credibility for the users. The stories reported by them may or may not be true. Further vetting and verification is required to confirm if the stories that they report are credible.',\n",
       " \"Context\\nThese data are from a couple of sensors of my dad's house.\\nContent\\nThe data are from motion sensors at the front door, which also regularly logs brightness.\\nThe front door motion detection data also includes motions of three cats.\\nData structure is pretty self explanatory. Load the csv files.\\nData Files\\nI already added day-of-year, year, weekday and time of day to the data for easier handling.\\nBrightness\\nThese time-stamped values resemble the brightness readings. They range from dark (low numbers) to bright day light (high numbers).\\nThe brightness nightly mean values differ. The reasons are: Christmas decoration during the winter, and solar lights being set up some time in mid April this year.\\nContacts\\nThese data indicate door and window openings. They are time-stamped. The boolean value isClosed indicates wether the contact has been closed.\\nMotion\\nThere is a motion sensor at the front door which indicates movement. Movement detections also are time-stamped data.\\nQuestions\\nAt what time of the day the newspaper is delivered?\\nHow can I tell from the brightness value, what kind of weather it was on this day?\\nAdditional information\\nNewspaper is not delivered on Sundays\\nNewspaper is in the mailbox by 6:30 AM\\nNewspaper is usually taken out of the mailbox around 7:00 AM (not on Saturdays)\\nThere is regular front door activity - someon leaving the house - at 7:30 (not on Saturdays and Sundays)\\nThere are three cats living at the house\",\n",
       " 'If you play the game, you might understand the columns by themselves, otherwise sometime in this week, ill update the information and provide all details. (Note: The encoding for names is in UTF-8).\\nIf you want to contribute, send me a message.\\nAll Data is collected from the game : Football Manager 2017.\\nTry to predict the next messi/ronaldo ?',\n",
       " 'Context:\\nIrony in language is when a statement is produced with one meaning but the intended meaning is exactly the opposite. For instance, someone who has burned toast might serve it and say ironically “it’s a little underdone”. Automatically detecting when language is ironic is an especially difficult task in Natural Language Processing.\\nContent:\\nThis dataset contains 1950 comments, which have been labeled as ironic (1) or not ironic (-1) by human annotators. The text was taken from Reddit comments.\\nAcknowledgements:\\nThis dataset and analysis of it is presented in the following paper.\\nWallace, B. C., Do Kook Choe, L. K., Kertz, L., & Charniak, E. (2014, April). Humans Require Context to Infer Ironic Intent (so Computers Probably do, too). In ACL (2) (pp. 512-516). Url: http://www.byronwallace.com/static/articles/wallace-irony-acl-2014.pdf\\nMade possible by support from the Army Research Office (ARO), grant 64481-MA / W9111F-13-1-0406 \"Sociolinguistically Informed Natural Language Processing: Automating Irony Detection\"\\nInspiration:\\nIs irony more likely when discussing certain topics?\\nDoes ironic text tend to have more positive or more negative sentiment?\\nWhat novel features can you develop to help detect irony?',\n",
       " 'Context:\\n“Compositionality” is a concept from linguistics where the meaning of a phrase is made up of the meaning of each of its individual words. So “the red apple” refers, literally, to an apple that is red. Sometimes when you combine words, however, the meaning of the phrase isn’t the same as the combined meanings of the individual words. “The Big Apple”, for example, means “New York City”, not a literal large apple.\\nThis dataset contains human judgements of the compositionality of common English phrases with two nouns.\\nContent:\\nThis dataset contains two files: summary data of the human judgements and the annotations by individual judges. All judgements are on a scale of 0 to 5, with 0 being “not literal” and 5 being “literal”.\\nAcknowledgements:\\nThis dataset was collected by Siva Reddy, Diana McCarthy and Suresh Manandhar. If you use this dataset in your work, please cite the following paper:\\nReddy, S., McCarthy, D., & Manandhar, S. (2011, November). An Empirical Study on Compositionality in Compound Nouns. In IJCNLP (pp. 210-218).\\nInspiration:\\nIs there a relationship between how frequent a word is and how often it’s used literally? (You can estimate word frequency using the corpora included in the Natural Language Toolkit, which is already ready to run in any Python kernel!)\\nGiven this dataset, can you predict whether other compound nouns will be literal or not?\\nWhich phrases are the most literal? Which are the least literal? Are there any patterns you notice?',\n",
       " \"Context\\nFrom 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz. Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay. From Sunset to SOMA, and Marina to Excelsior, this dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods.\\nThis dataset was featured in our completed playground competition entitled San Francisco Crime Classification. The goals of the competition were to:\\npredict the category of crime that occurred, given the time and location\\nvisualize the city and crimes (see Mapping and Visualizing Violent Crime for inspiration)\\nContent\\nThis dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. There are 9 variables:\\nDates - timestamp of the crime incident\\nCategory - category of the crime incident (only in train.csv).\\nDescript - detailed description of the crime incident (only in train.csv)\\nDayOfWeek - the day of the week\\nPdDistrict - name of the Police Department District\\nResolution - how the crime incident was resolved (only in train.csv)\\nAddress - the approximate street address of the crime incident\\nX - Longitude\\nY - Latitude\\nAcknowledgements\\nThis dataset is part of our completed playground competition entitled San Francisco Crime Classification. Visit the competition page if you are interested in checking out past discussions, competition leaderboard, or more details regarding the competition. If you are curious to see how your results rank compared to others', you can still make a submission at the competition submission page!\\nThe original dataset is from SF OpenData, the central clearinghouse for data published by the City and County of San Francisco.\",\n",
       " \"Context\\nElectoral integrity refers to international standards and global norms governing the appropriate conduct of elections. These standards have been endorsed in a series of authoritative conventions, treaties, protocols, and guidelines by agencies of the international community and apply universally to all countries throughout the electoral cycle, including during the pre-electoral period, the campaign, on polling day, and in its aftermath.\\nContent\\nThe Perceptions of Electoral Integrity (PEI) survey asks experts to evaluate elections according to 49 indicators, grouped into eleven categories reflecting the whole electoral cycle. The PEI dataset is designed to provide a comprehensive, systematic and reliable way to monitor the quality of elections worldwide. It includes disaggregated scores for each of the individual indicators, summary indices for the eleven dimensions of electoral integrity, and a PEI index score out of 100 to summarize the overall integrity of the election.\\nAcknowledgements\\nThis study was conducted by Pippa Norris, Alessandro Nai, and Max Grömping for Harvard University's Electoral Integrity Project.\",\n",
       " 'Context\\nOn April 23, 2014, the Department of Justice, at the behest of President Obama, announced the Clemency Initiative, inviting petitions for commutation of sentence from nonviolent offenders who, among other criteria, likely would have received substantially lower sentences if convicted of the same offenses today. As expected, the announcement resulted in a record number of petitions – including thousands of petitions involving crimes not included in the initiative, such as terrorism, murder, sex crimes, public corruption, and financial fraud.\\nIn the federal system, commutation of sentence and pardon are different forms of executive clemency, which is a broad term that applies to the President’s constitutional power to exercise leniency toward persons who have committed federal crimes.\\nA commutation of sentence reduces a sentence, either totally or partially, that is then being served, but it does not change the conviction, signify innocence, or remove civil disabilities from the criminal conviction. A commutation may include remission (or release) of the financial obligations that are imposed as part of a sentence, such as payment of a fine or restitution; a remission applies only to the part of the financial obligation that has not already been paid. To be eligible to apply for commutation of sentence, a person must have reported to prison to begin serving his sentence and may not be challenging his conviction in the courts.\\nA pardon is an expression of the President’s forgiveness and is granted in recognition of the applicant’s acceptance of responsibility for the crime and established good conduct for a significant period of time after conviction or completion of sentence. It does not signify innocence. It does, however, remove civil disabilities – such as restrictions on the right to vote, hold state or local office, or sit on a jury – imposed because of the conviction. A person is not eligible to apply for a presidential pardon until a minimum of five years has elapsed since his release from any form of confinement imposed upon him as part of a sentence for his most recent criminal conviction.\\nAcknowledgements\\nThe data was compiled and published by the Office of the Pardon Attorney. The Office of the Pardon Attorney receives and reviews petitions for all forms of executive clemency, including pardon, commutation (reduction) of sentence, remission of fine or restitution, and reprieve, initiates the necessary investigations of clemency requests, and prepares the report and recommendation of the Attorney General to the President.',\n",
       " 'Context\\nThe president can declare an emergency for any occasion or instance when the President determines federal assistance is needed. Emergency declarations supplement State and local or Indian tribal government efforts in providing emergency services, such as the protection of lives, property, public health, and safety, or to lessen or avert the threat of a catastrophe in any part of the United States. The total amount of assistance provided for in a single emergency may not exceed $5 million.\\nThe president can declare a major disaster for any natural event, including any hurricane, tornado, storm, high water, wind-driven water, tidal wave, tsunami, earthquake, volcanic eruption, landslide, mudslide, snowstorm, or drought, or, regardless of cause, fire, flood, or explosion, that the President determines has caused damage of such severity that it is beyond the combined capabilities of state and local governments to respond. A major disaster declaration provides a wide range of federal assistance programs for individuals and public infrastructure, including funds for both emergency and permanent work.\\nContent\\nThis dataset includes a record for every federal emergency or disaster declared by the President of the United States since 1953.\\nAcknowledgements\\nThe disaster database was published by the Federal Emergency Management Agency with data from the National Emergency Management Information System.\\nInspiration\\nWhat type of disaster is the most commonly declared by FEMA? Which disasters or emergencies have lasted the longest? What disaster was declared in the most counties or states? Has the number of disasters declared by FEMA risen or fallen over time?',\n",
       " 'Context\\nThis is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment .\\nContent\\nIt contains the following 6 fields:\\ntarget: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\\nids: The id of the tweet ( 2087)\\ndate: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\\nflag: The query (lyx). If there is no query, then this value is NO_QUERY.\\nuser: the user that tweeted (robotickilldozr)\\ntext: the text of the tweet (Lyx is cool)\\nAcknowledgements\\nThe official link regarding the dataset with resources about how it was generated is here The official paper detailing the approach is here\\nCitation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\\nInspiration\\nTo detect severity from tweets. You may have a look at this.',\n",
       " \"Context\\nThe data here is from the Global Health Observatory (GHO) who provide data on malaria incidence, death and prevention from around the world. I have also included malaria net distribution data the Against Malaria Foundation (AMF). The AMF has consistently been ranked as the most cost effective charity by charity evaluators Give Well - http://www.givewell.org/charities/top-charities\\nContent\\nGHO data is all in narrow format, with variables for a country in a given year being found on different rows.\\nGHO data (there are a number or superfluous columns):\\nGHO (CODE)\\nGHO (DISPLAY) - this is the variable being measured\\nGHO (URL)\\nPUBLISHSTATE (CODE)\\nPUBLISHSTATE (DISPLAY)\\nPUBLISHSTATE (URL)\\nYEAR (CODE)\\nYEAR (DISPLAY)\\nYEAR (URL)\\nREGION (CODE)\\nREGION (DISPLAY)\\nREGION (URL)\\nCOUNTRY (CODE) - can be used to join this data with the AMF data\\nCOUNTRY (DISPLAY)\\nCOUNTRY (URL)\\nDisplay Value - this is the measured value\\nLow - lower confidence interval\\nHigh - higher confidence interval\\nComments\\nAMF distribution data:\\n#_llins - total number of malaria nets distributed\\nlocation - the specific area that received the nets, within the target country\\ncountry - the country in which the nets were distributed\\nwhen - the period the distribution\\nby_whom - the organisation(s) which partnered with the AMF to perform the distribution\\ncountry_code - the country's GHO country code (this will allow joining with the GHO data)\\nFor the current version all data was downloaded 20-08-17 The GHO data covers the years from 2000 to 2015 (not all files have data in all years) The AMF data runs from 2006 - the present.\\nThe GHO data is taken as is from the csv (lists) available here: http://apps.who.int/gho/data/node.main.A1362?lang=en The source of the AMF's distribution data is here: https://www.againstmalaria.com/distributions.aspx - it was assembled into a single csv using Excel (mea culpa)\\nInspiration\\nMalaria is one of the world's most devastating diseases, not least because it largely affects some of the poorest people. Over the past 15 years malaria rates and mortality have dropped (http://www.who.int/malaria/media/world-malaria-report-2016/en/), but there is still a long way to go. Understanding the data is generally one of the most important steps in solving any large problem. I'm excited to see what the Kaggle community can find out about the global trends in malaria over this period, and if we can find out anything about the impact of organisations such as the AMF.\",\n",
       " 'nan',\n",
       " 'Context\\nComputer Network Traffic Data - A ~500K CSV with summary of some real network traffic data from the past. The dataset has ~21K rows and covers 10 local workstation IPs over a three month period. Half of these local IPs were compromised at some point during this period and became members of various botnets.\\nContent\\nEach row consists of four columns:\\ndate: yyyy-mm-dd (from 2006-07-01 through 2006-09-30)\\nl_ipn: local IP (coded as an integer from 0-9)\\nr_asn: remote ASN (an integer which identifies the remote ISP)\\nf: flows (count of connnections for that day)\\nReports of \"odd\" activity or suspicions about a machine\\'s behavior triggered investigations on the following days (although the machine might have been compromised earlier)\\nDate : IP 08-24 : 1 09-04 : 5 09-18 : 4 09-26 : 3 6\\nAcknowledgements\\nThis public dataset was found on http://statweb.stanford.edu/~sabatti/data.html\\nInspiration\\nCan you discover when a compromise has occurred by a change in the pattern of communication?',\n",
       " \"Context\\nI was searching for a master degree program in data-science when I found this awesome website mastersportal, So I just scrapped it to take my time analysing all master programs available around the world.\\nContent\\nThis dataset contains 60442 different master's degree programs from 99 countries around the world.\\nScrapping code\\nhttps://github.com/AnasFullStack/Masters-Portal-Scrapper\",\n",
       " 'Context\\nThis dataset is a cleaned-up and modernized version of \"CAA Database of Battles, Version 1990\", shortnamed \"CDB90\". It contains information on over 600 battles that were fought between 1600 AD and 1973 AD. Descriptive data include battle name, date, and location; the strengths and losses on each side; identification of the victor; temporal duration of the battle; and selected environmental and tactical environment descriptors (such as type of fortifications, type of tactical scheme, weather conditions, width of front, etc.).\\nContent\\nThe data contained therein is split across several files. The most important of these is battles.csv, which is lists and gives information about the battles themselves. Files marked enum describe the keys used by specific fields. Other files provide additional context.\\nAcknowledgements\\nThe original version of this database was distributed by the U.S. Army Concepts Analysis Agency. The version of this dataset you see here is a cleaned-up version created by Jeffrey Arnold. This dataset, cleanup code, and source data are all available here.\\nInspiration\\nHow often were battles fought in various weather conditions?\\nHow often did an attacker or defender achieve the element of surprise? Did it have a significant effect on the outcome?\\nDid prepared fortifications have a significant effect on outcomes?',\n",
       " 'Context\\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includes abundance data on the larvae of over 250 species of fish; larval length frequency data and egg abundance data on key commercial species; and oceanographic and plankton data. The physical, chemical, and biological data collected at regular time and space intervals quickly became valuable for documenting climatic cycles in the California Current and a range of biological responses to them. CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term “El Niño” into the scientific literature.\\nThe California Cooperative Oceanic Fisheries Investigations (CalCOFI) are a unique partnership of the California Department of Fish & Wildlife, NOAA Fisheries Service and Scripps Institution of Oceanography. The organization was formed in 1949 to study the ecological aspects of the sardine population collapse off California. Today our focus has shifted to the study of the marine environment off the coast of California, the management of its living resources, and monitoring the indicators of El Nino and climate change. CalCOFI conducts quarterly cruises off southern & central California, collecting a suite of hydrographic and biological data on station and underway. Data collected at depths down to 500 m include: temperature, salinity, oxygen, phosphate, silicate, nitrate and nitrite, chlorophyll, transmissometer, PAR, C14 primary productivity, phytoplankton biodiversity, zooplankton biomass, and zooplankton biodiversity.\\nContent\\nEach table has several dozen columns. Please see this page for the table details.',\n",
       " \"The State Contract and Procurement Registration System (SCPRS) was established in 2003, as a centralized database of information on State contracts and purchases over $5000. eSCPRS represents the data captured in the State's eProcurement (eP) system, Bidsync, as of March 16, 2009. The data provided is an extract from that system for fiscal years 2012-2013, 2013-2014, and 2014-2015\\nAcknowledgements\\nThis dataset was kindly released by the state of California. You can find the original copy here.\",\n",
       " \"Context\\nIf you've ever had to typeset mathematical expressions, you might have thought: wouldn’t it be great if I could just take a picture of a handwritten expression and have it be recognized automatically? This dataset has all the data you’ll need to build a system to do just that.\\nDescription\\nThe dataset provide more than 11,000 expressions handwritten by hundreds of writers from different countries, merging the data sets from 4 CROHME competitions. Writers were asked to copy printed expressions from a corpus of expressions. The corpus has been designed to cover the diversity proposed by the different tasks and chosen from an existing math corpus and from expressions embedded in Wikipedia pages. Different devices have been used (different digital pen technologies, white-board input device, tablet with sensible screen), thus different scales and resolutions are used. The dataset provides only the on-line signal.\\nIn the last competition CROHME 2013 the test part is completely original and the train part is using 5 existing data sets:\\nMathBrush (University of Waterloo),\\nHAMEX (University of Nantes),\\nMfrDB (Czech Technical University),\\nExpressMatch (University of Sao Paulo),\\nthe KAIST data set.\\nIn CROHME 2014 a new test set has been created with 987 new expressions and 2 new tasks has been added: isolated symbol recognition and matrix recognition. Train and test files as the evaluation scripts for these new tasks are provided. For the isolated symbol datasets, elements are extracted from full expression using the existing datasets, which also includes segmentation errors. For the matrix recognition task, 380 new expressions have been labelled and split into training and test sets.\\nFurthermore, 6 participants of the 2012 competition provide their recognized expressions for the 2012 test part. This data allows research on decision fusion or evaluation metrics.\\nTechnical Details\\nThe ink corresponding to each expression is stored in an InkML file. An InkML file mainly contains three kinds of information:\\nThe ink: a set of traces made of points;\\nThe symbol level ground truth: the segmentation and label information of each symbol in the expression;\\nThe expression level ground truth: the MathML structure of the expression.\\nThe two levels of ground truth information (at the symbol as well as at the expression level) are entered manually. Furthermore, some general information is added in the file:\\nThe channels (here, X and Y);\\nThe writer information (identification, handedness (left/right), age, gender, etc.), if available;\\nThe LaTeX ground truth (without any reference to the ink and hence, easy to render);\\nThe unique identification code of the ink (UI), etc.\\nThe InkML format makes references between the digital ink of the expression, its segmentation into symbols and its MathML representation. Thus, the stroke segmentation of a symbol can be linked to its MathML representation.\\nThe recognized expressions are the outputs of the recognition competitors' systems. It uses the same InkML format, but without the ink information (only segmentation, label and MathML structure).\\nMore details available on CROHME website.\\nAcknowledgements\\nThis dataset was compiled by Harold Mouchère and is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. If you use this dataset in your work, please include the following citation:\\nHarold Mouchère, ICFHR 2014 CROHME: Fourth International Competition on Recognition of Online Handwritten Mathematical Expressions (CROHME-2014) ,2,ID:CROHME-2014_2,URL:http://tc11.cvc.uab.es/datasets/CROHME-2014_2\\nYou might also like:\\nHandwritten math symbols dataset: Over 100 000 image samples\\nArabic Handwritten Digits Dataset\",\n",
       " \"Context:\\nSentimentWortschatz, or SentiWS for short, is a publicly available German-language resource for sentiment analysis, opinion mining etc. It lists positive and negative polarity bearing words weighted within the interval of [-1; 1] plus their part of speech tag, and if applicable, their inflections. The current version of SentiWS (v1.8b) contains 1,650 positive and 1,818 negative words, which sum up to 15,649 positive and 15,632 negative word forms incl. their inflections, respectively. It not only contains adjectives and adverbs explicitly expressing a sentiment, but also nouns and verbs implicitly containing one.\\nFormat:\\nSentiWS is organised in two utf8-encoded text files structured the following way:\\n| \\\\t \\\\t ,..., \\\\n\\nwhere \\\\t denotes a tab, and \\\\n denotes a new line.\\nAcknowledgement:\\nSentiWS is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/). If you use SentiWS in your work, please cite the following paper:\\nR. Remus, U. Quasthoff & G. Heyer: SentiWS - a Publicly Available German-language Resource for Sentiment Analysis. In: Proceedings of the 7th International Language Ressources and Evaluation (LREC'10), 2010\\nThis version of the data set was last updated in March 2012.\",\n",
       " 'Context:\\n“The Universal Product Code (UPC) is a barcode symbology that is widely used in the United States, Canada, United Kingdom, Australia, New Zealand, in Europe and other countries for tracking trade items in stores.\\n“UPC (technically refers to UPC-A) consists of 12 numeric digits, that are uniquely assigned to each trade item. Along with the related EAN barcode, the UPC is the barcode mainly used for scanning of trade items at the point of sale, per GS1 specifications.[1] UPC data structures are a component of GTINs and follow the global GS1 specification, which is based on international standards. But some retailers (clothing, furniture) do not use the GS1 system (rather other barcode symbologies or article number systems). On the other hand, some retailers use the EAN/UPC barcode symbology, but without using a GTIN (for products, brands, sold at such retailers only).”\\n-- Tate. (n.d.). In Wikipedia. Retrieved August 18, 2017, from https://en.wikipedia.org/wiki/Plagiarism. Text reproduced here under a CC-BY-SA 3.0 license.\\nContent:\\nThis dataset contains just over 1 million UPC codes and the names of the products associated with them.\\nAcknowledgements:\\nWhile UPC’s themselves are not copyrightable, the brand names and trademarks in this dataset remain the property of their respective owners.\\nInspiration:\\nCan you use this dataset to generate new product names?\\nCan you use this in conjunction with other datasets to disambiguate products?',\n",
       " 'Context:\\nThe Santa Barbara Corpus of Spoken American English is based on hundreds of recordings of natural speech from all over the United States, representing a wide variety of people of different regional origins, ages, occupations, and ethnic and social backgrounds. It reflects many ways that people use language in their lives: conversation, gossip, arguments, on-the-job talk, card games, city council meetings, sales pitches, classroom lectures, political speeches, bedtime stories, sermons, weddings, and more. The corpus was collected by the University of California, Santa Barbara Center for the Study of Discourse, Director John W. Du Bois (UCSB), Associate Editors: Wallace L. Chafe (UCSB), Charles Meyer (UMass, Boston), and Sandra A. Thompson (UCSB).\\nEach speech file is accompanied by a transcript in which phrases are time stamped with respect to the audio recording. Personal names, place names, phone numbers, etc., in the transcripts have been altered to preserve the anonymity of the speakers and their acquaintances and the audio files have been filtered to make these portions of the recordings unrecognizable. Pitch information is still recoverable from these filtered portions of the recordings, but the amplitude levels in these regions have been reduced relative to the original signal. The audio data consists of MP3 format speech files, recorded in two-channel pcm, at 22050Hz.\\nContents:\\nThis dataset contains part one of the corpus. The other three parts and additional information can be found here. The following information is included in this dataset:\\nRecordings: 14 recordings as .mp3 files\\nTranscripts: Time-aligned transcripts for all 14 recordings, in the CHAT format\\nMetadata: A .csv with demographic information on speakers, as well as which recordings they appear in. (Some talkers appear in more than one recording.)\\nAcknowledgements:\\nThe Santa Barbara Corpus was compiled by researchers in the Linguistics Department of the University of California, Santa Barbara. The Director of the Santa Barbara Corpus is John W. Du Bois, working with Associate Editors Wallace L. Chafe and Sandra A. Thompson (all of UC Santa Barbara), and Charles Meyer (UMass, Boston). For the publication of Parts 3 and 4, the authors are John W. Du Bois and Robert Englebretson.\\nIt is distributed here under an CC BY-ND 3.0 US license.\\nInspiration:\\nCurrently, the transcriptions are close transcriptions and include disfluencies and overlaps. Can you use NLP to convert them to broad transcriptions without this information?\\nCan you create a phone-aligned transcription of this dataset? You might find it helpful to use forced alignment.',\n",
       " 'Context:\\nBike shares are becoming a popular alternative means of transportation. The City of Austin makes data available on >649k bike trips over 2013-2017.\\nContent:\\nThis data includes information on bike trip start location, stop location, duration, type of bike share user. Bike station location data is also provided.\\nDataset Description\\nUse this dataset with BigQuery\\nYou can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too.\\n*austin_bikeshare_trips.csv*\\nbikeid: integer id of bike\\ncheckout_time: HH:MM:SS, see start time for date stamp\\nduration_minutes: int minutes of trip duration\\nend_station_id: integer id of end station\\nend_station_name: string of end station name\\nmonth: month, integer\\nstart_station_id: integer id of start station\\nstart_station_name: string of start station name\\nstart_time: YYYY-MM-DD HH:MM:SS\\nsubscriber_type: membership typ e.g. walk up, annual, other bike share, etc\\ntrip_id: unique trip id int\\nyear: year of trip, int\\n*austin_bikeshare_stations.csv*\\nlatitude: geospatial latitude, precision to 5 places\\nlocation: (lat, long)\\nlongitude: geospatial longitude, precision to 5 places\\nname: station name, str\\nstation_id: unique station id, int\\nstatus: station status (active, closed, moved, ACL-only)\\nAcknowledgements:\\nThis dataset is available from Google Public Data.\\nInspiration:\\nWhat stations are most popular? At certain times?\\nWhat are the average user trip?\\nCan you predict station usage to improve the ability of bike share employees to supply high-use stations?',\n",
       " 'Context:\\nOccupational Safety and Health Administration aka OSHA requires employers to report all severe work-related injuries, defined as an amputation, in-patient hospitalization, or loss of an eye. The requirement began on January 1, 2015.\\nContent:\\nThis dataset provides information from those reports, including a description of the incident and the name and address of the establishment where it happened. Injuries are coded using the Occupational Injury and Illness Classification System. Data covers ~22k incidents Jan 1 2015-Feb 28 2017. 26 columns describe incident, parties involved, employer, injury sustained, and final outcome.\\nAcknowledgements:\\nThis dataset was created by [OSHA](https://www.osha.gov/severeinjury/index.html ) and released to the public.\\nInspiration:\\nWhich industries have the highest rate of worker injuries? Most severe injuries?\\nCan you predict injuries for 2016 based on 2015 data?\\nIn which regions are injuries most common?',\n",
       " \"Telecom customer churn prediction\\nThis data set consists of 100 variables and approx 100 thousand records. This data set contains different variables explaining the attributes of telecom industry and various factors considered important while dealing with customers of telecom industry. The target variable here is churn which explains whether the customer will churn or not. We can use this data set to predict the customers who would churn or who wouldn't churn depending on various variables available.\",\n",
       " \"Context:\\nRestaurant inspections ensure that food served to the public at licensed food establishments follows food safety guidelines. The Food Protection Division of the Chicago Department of Public Health (CDPH) is committed to maintaining the safety of food bought, sold, or prepared for public consumption in Chicago by carrying out science-based inspections of all retail food establishments. These inspections promote public health in areas of food safety and sanitation and prevent the occurrence of food-borne illness. CDPH's licensed, accredited sanitarians inspect retail food establishments such as restaurants, grocery stores, bakeries, convenience stores, hospitals, nursing homes, day care facilities, shelters, schools, and temporary food service events. Inspections focus on food handling practices, product temperatures, personal hygiene, facility maintenance, and pest control. All restaurants are subject to certain recurring inspections. Each year a restaurant is subject to annual inspections to ensure continued compliance with City ordinances and regulations. In addition to recurring inspections, restaurants may also be inspected in response to a complaint. Some of these recurring inspections, such as the inspection by the Buildings Department, will be scheduled, while others will not.\\nGenerally inspections are conducted by the Health Department for sanitation and safe food handling practices, the Buildings Department to ensure the safety of the structure, and the Fire Department to ensure safe fire exits.The City's Dumpster Task Force, a collaborative effort between the Health Department and Streets and Sanitation Department, also inspects restaurants to ensure compliance with sanitation regulations.\\nContent:\\nData includes inspection date, results, violations noted, business name and lat/lon, license# and risk. Data covers 01/02/2013-08/28/17.\\nAcknowledgements:\\nData was collected by City of Chicago Department of Health.\\nInspiration:\\nCan you predict restaurant closings?\\nDo restaurants in certain neighborhoods gather more/less violations?\\nAny seasonal or time anomalies in the data?\",\n",
       " 'Context\\nThis dataset contains the name, job title, department, and salary of every employee that was on the City of Chicago payroll at the time of capture in mid-2017. It provides a transparent lens into who gets paid how much and for what.\\nContent\\nThis dataset provides columns for employee name, the city department they work for, their job title, and various fields describing their compensation. Most employee salaries are covered by the Annual Salary field, but some employees paid hourly are covered by a combination of Typical Hours and Hourly Rate fields.\\nAcknowledgements\\nThis dataset is published as-is by the City of Chicago (here).\\nInspiration\\nHow many people do the various city agencies employ, and how much does each department spend on salary in total?\\nWhat are the most numerous job titles in civic government employment?\\nHow do Chicago employee salaries compare against salaries of city employees in New York City? Is the difference more or less than the difference in cost of living between the two cities?',\n",
       " \"About This Data\\nThis is a list of over 1,500 consumer reviews for Amazon products like the Kindle, Fire TV Stick, and more provided by Datafiniti's Product Database. The dataset includes basic product information, rating, review text, and more for each product.\\nWhat You Can Do With This Data\\nYou can use this data to analyze Amazon’s most successful consumer electronics product launches; discover insights into consumer reviews and assist with machine learning models. E.g.:\\nWhat are the most reviewed Amazon products?\\nWhat are the initial and current number of customer reviews for each product?\\nHow do the reviews in the first 90 days after a product launch compare to the price of the product?\\nHow do the reviews in the first 90 days after a product launch compare to the days available for sale?\\nMap the keywords in the review text against the review ratings to help train sentiment models.\\nData Schema\\nA full schema for the data is available in our support documentation.\\nAbout Datafiniti\\nDatafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.\\nWant More?\\nYou can get more data like this by joining Datafiniti or requesting a demo.\",\n",
       " 'Context\\nThis is a database of airports, train stations, and ferry terminals around the world. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions.\\nContent\\nAirport ID Unique OpenFlights identifier for this airport.\\nName Name of airport. May or may not contain the City name.\\nCity Main city served by airport. May be spelled differently from Name.\\nCountry Country or territory where airport is located. See countries.dat to cross-reference to ISO 3166-1 codes.\\nIATA 3-letter IATA code. Null if not assigned/unknown.\\nICAO 4-letter ICAO code.\\nNull if not assigned.\\nLatitude Decimal degrees, usually to six significant digits. Negative is South, positive is North.\\nLongitude Decimal degrees, usually to six significant digits. Negative is West, positive is East.\\nAltitude In feet.\\nTimezone Hours offset from UTC. Fractional hours are expressed as decimals, eg. India is 5.5.\\nDST Daylight savings time. One of E (Europe), A (US/Canada), S (South America), O (Australia), Z (New Zealand), N (None) or U (Unknown). See also: Help: Time\\nTz database time zone Timezone in \"tz\" (Olson) format, eg. \"America/Los_Angeles\".\\nType Type of the airport. Value \"airport\" for air terminals, \"station\" for train stations, \"port\" for ferry terminals and \"unknown\" if not known.\\nSource Source of this data. \"OurAirports\" for data sourced from OurAirports, \"Legacy\" for old data not matched to OurAirports (mostly DAFIF), \"User\" for unverified user contributions. In airports.csv, only source=OurAirports is included.\\nAcknowledgements\\nThis dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website, so check them out!',\n",
       " 'Context:\\nA record of every public WiFi hotspot in New York City.\\nContent:\\nThe dataset consists of records for every public WiFi hotspot (ones provided by or in partnership with the city) in New York City. It contains over 2500 records overall, and is current as of August 11, 2017.\\nAcknowledgements:\\nThis dataset was published as-is by the New York City Department of Information Technology & Telecommunications.\\nInspiration:\\nDoes free public WiFi tend to cluster around certain (more affluent) areas? Who are the free WiFi providers, and where do they do it? How does NYC WiFi compare to WiFi in other cities, like Buenos Aires? (https://www.kaggle.com/octaviog/buenos-aires-public-wifi-access-points)',\n",
       " 'DeepSat SAT-6\\n\\n\\nOriginally, images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330,000 scenes spanning the whole of the Continental United States (CONUS). The authors used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a 1-m ground sample distance (GSD) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points.\\nThe images consist of 4 bands - red, green, blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class.\\nOnce labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch.\\nContent\\nEach sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared.\\nThe training and test labels are one-hot encoded 1x6 vectors\\nThe six classes represent the six broad land covers which include barren land, trees, grassland, roads, buildings and water bodies.\\nTraining and test datasets belong to disjoint set of image tiles.\\nEach image patch is size normalized to 28x28 pixels.\\nOnce generated, both the training and testing datasets were randomized using a pseudo-random number generator.\\nCSV files\\nX_train_sat6.csv: 324,000 training images, 28x28 images each with 4 channels\\ny_train_sat6.csv: 324,000 training labels, 1x6 one-hot encoded vectors\\nX_test_sat6.csv: 81,000 training images, 28x28 images each with 4 channels\\ny_test_sat6.csv: 81,000 training labels, 1x6 one-hot encoded vectors\\nThe original MAT file\\ntrain_x: 28x28x6x324000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels)\\ntrain_y: 324,000x6 uint8 (containing 6x1 vectors having labels for the 400000 training samples)\\ntest_x: 28x28x6x18000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels)\\ntest_y: 81,000x6 uint8 (containing 6x1 vectors having labels for the 100000 test samples)\\nAcknowledgements\\nThe original MATLAB file was converted to multiple CSV files\\nThe original SAT-4 and SAT-6 airborne datasets can be found here:\\nhttp://csc.lsu.edu/~saikat/deepsat/\\nThanks to:\\nSaikat Basu, Robert DiBiano, Manohar Karki and Supratik Mukhopadhyay, Louisiana State University Sangram Ganguly, Bay Area Environmental Research Institute/NASA Ames Research Center Ramakrishna R. Nemani, NASA Advanced Supercomputing Division, NASA Ames Research Center',\n",
       " 'English Word Vectors from Common Crawl\\nAbout fastText\\nfastText is a library for efficient learning of word representations and sentence classification. One of the key features of fastText word representation is its ability to produce vectors for any words, even made-up ones. Indeed, fastText word vectors are built from vectors of substrings of characters contained in it. This allows you to build vectors even for misspelled words or concatenation of words.\\nAbout the vectors\\nThese pre-trained vectors contain 2 million word vectors trained on Common Crawl (600B tokens).\\nThe first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors, like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency.\\nAcknowledgements\\nThese word vectors are distributed under the Creative Commons Attribution-Share-Alike License 3.0.\\nP. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information\\nA. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification\\nA. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, T. Mikolov, FastText.zip: Compressing text classification models\\n\\n(* These authors contributed equally.)',\n",
       " 'VGG16\\nVery Deep Convolutional Networks for Large-Scale Image Recognition\\nIn this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\\nAuthors: Karen Simonyan, Andrew Zisserman\\nhttps://arxiv.org/abs/1409.1556\\nVGG Architectures\\nWhat is a Pre-trained Model?\\nA pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\\nWhy use a Pre-trained Model?\\nPre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.',\n",
       " 'ResNet-50\\nDeep Residual Learning for Image Recognition\\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.\\nAn ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\\nhttps://arxiv.org/abs/1512.03385\\nArchitecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\\nWhat is a Pre-trained Model?\\nA pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\\nWhy use a Pre-trained Model?\\nPre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.',\n",
       " 'Context\\nThe Convention on International Trade in Endangered Species of Wild Fauna and Flora, or CITES for short, is an international treaty organization tasked with monitoring, reporting, and providing recommendations on the international species trade. CITES is a division of the IUCN, which is one of the principal international organization focused on wildlife conversation at large. It is not a part of the UN (though its reports are read closely by the UN).\\nCITES is one of the oldest conservation organizations in existence. Participation in CITES is voluntary, but almost every member nation in the UN (and, therefore, almost every country worldwide) participates. Countries participating in CITES are obligated to report on roughly 5000 animal species and 29000 plant species brought into or exported out of their countries, and to honor limitations placed on the international trade of these species.\\nProtected species are organized into three appendixes. Appendix I species are those whose trade threatens them with extinction. Two particularly famous examples of Class I species are the black rhinoceros and the African elephant, whose extremely valuable tusks are an alluring target for poachers exporting ivory abroad. There are 1200 such species. Appendix II species are those not threatened with extinction, but whose trade is nevertheless detrimental. Most species in cites, around 21000 of them, are in Appendix II. Finally, Appendix III animals are those submitted to CITES by member states as a control mechanism. There are about 170 such species, and their export or import requires permits from the submitting member state(s).\\nThis dataset records all legal species imports and exports carried out in 2016 (and a few records from 2017) and reported to CITES. Species not on the CITES lists are not included; nor is the significant, and highly illegal, ongoing black market trading activity.\\nContent\\nThis dataset contains records on every international import or export conducted with species from the CITES lists in 2016. It contains columns identifying the species, the import and export countries, and the amount and characteristics of the goods being traded (which range from live animals to skins and cadavers).\\nFor further details on individual rows and columns refer to the metadata on the /data tab. A much more detailed description of each of the fields is available in the original CITES documentation.\\nAcknowledgements\\nThis dataset was originally aggregated by CITES and made available online through this downloader tool. The CITES downloader goes back to 1975, however it is only possible to download fully international data two years at a time (or so) due to limitations in the number of rows allowed by the data exporter. If you would like data going further back, check out the downloader. Be warned, though, this data takes a long time to generate!\\nThis data is prepared for CITES by UNEP, a division of the UN, and hence likely covered by the UN Data License.\\nInspiration\\nWhat is the geospatial distribution of the international plant/animal trade?\\nHow much export/import activity is there for well-known species, like rhinos, elephants, etcetera?\\nWhat percent of the trade is live, as opposed to animal products (ivory, skins, cadavers, etcetera)?',\n",
       " 'Context\\nThe database contains wav recordings from the same optical sensor inserted in-turn into six insectary boxes containing only one mosquito species of both sexes. As the mosquitoes fly randomly through the sensor their wingbeat partially occludes the light from the transmitter to the receiver. The light fluctuation recorded follows the rhythm of the wingbeat. Insect Biometrics, in the context of our work, is a measurable behavioral characteristic of flying insects. Biometric identifiers are related to the shape of the body (main body size, wing shape, wingbeat frequency, pattern movement of the wings). Biometric identification methods use biometric characteristics or traits to verify species/sex identities when insects access endpoint traps following a bait.\\nContent\\n• 279,566 wingbeat recordings correctly labeled\\n• 6 mosquito species (Ae. aegypti, Ae. albopictus, An. arabiensis, An. gambiae, Cu. pipiens, Cu. quinquefasciatus)\\n• 3 genera of mosquito species (Aedes, Anopheles, Culex)\\nAcknowledgements\\nThe data have been recorded at the premises of Biogents, Regensburg, Germany (https://www.biogents.com/) and with the help of IRIDEON SA, Spain (http://irideon.eu/ ). The data have been recorded using the device published in:\\nPotamitis I. and Rigakis I., \"Large Aperture Optoelectronic Devices to Record and Time-Stamp Insects’ Wingbeats,\" in IEEE Sensors Journal, vol. 16, no. 15, pp. 6053-6061, Aug.1, 2016. doi: 10.1109/JSEN.2016.2574762\\nThe REMOSIS project that supported the creation of the database has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No 691131.\\nWe gratefully acknowledge the support of NVIDIA Corporation with the donation of a TITAN-X GPU used for training the deep learning networks used to classify mosquitoes’ spectra.\\nInspiration\\nThe point of having such recordings is to eventually embed optoelectronic sensors in automatic traps that will report counts, species and sex identity of captured mosquitoes. All species of this dataset can be dangerous as they are potential vectors of pathogens that cause serious illnesses. A widespread network of traps for insects of economic importance such as fruit flies and of hygienic importance such as mosquitoes allows the automatic creation of spatiotemporal maps and cuts down significantly the manual cost of visiting the traps. The creation of historical data can lead to the prediction of outbreaks and risk assessment in general.\\nWe provide code to read the data and extract the power spectral density signature of each wingbeat. We also extract Mel-scaled, filter-bank features. How about wavelets and time-varying autoregressive models? The starter code using top-tier shallow classifiers achieves a mean accuracy of 81-84%. Deep-learning performs better. Can you classify genus, perform clustering, apply transfer learning to spectral data?\\nCome aboard and help humanity against killer mosquitoes!',\n",
       " 'Context\\nDataset of Starcraft 2 games, played in different leagues/levels.\\nContent\\nScreen movements aggregated into screen-fixations. -- Time is recorded in terms of timestamps in the StarCraft 2 replay file. When the game is played on \\'faster\\', 1 real-time second is equivalent to roughly 88.5 timestamps.\\nAttribute Information:\\nGameID: Unique ID number for each game (integer)\\nLeagueIndex: Bronze, Silver, Gold, Platinum, Diamond, Master, GrandMaster, and Professional leagues coded 1-8 (Ordinal)\\nAge: Age of each player (integer)\\nHoursPerWeek: Reported hours spent playing per week (integer)\\nTotalHours: Reported total hours spent playing (integer)\\nAPM: Action per minute (continuous)\\nSelectByHotkeys: Number of unit or building selections made using hotkeys per timestamp (continuous)\\nAssignToHotkeys: Number of units or buildings assigned to hotkeys per timestamp (continuous)\\nUniqueHotkeys: Number of unique hotkeys used per timestamp (continuous)\\nMinimapAttacks: Number of attack actions on minimap per timestamp (continuous)\\nMinimapRightClicks: number of right-clicks on minimap per timestamp (continuous)\\nNumberOfPACs: Number of PACs per timestamp (continuous)\\nGapBetweenPACs: Mean duration in milliseconds between PACs (continuous)\\nActionLatency: Mean latency from the onset of a PACs to their first action in milliseconds (continuous)\\nActionsInPAC: Mean number of actions within each PAC (continuous)\\nTotalMapExplored: The number of 24x24 game coordinate grids viewed by the player per timestamp (continuous)\\nWorkersMade: Number of SCVs, drones, and probes trained per timestamp (continuous)\\nUniqueUnitsMade: Unique unites made per timestamp (continuous)\\nComplexUnitsMade: Number of ghosts, infestors, and high templars trained per timestamp (continuous)\\nComplexAbilitiesUsed: Abilities requiring specific targeting instructions used per timestamp (continuous)\\nAcknowledgements\\nSource: 1. Thompson JJ, Blair MR, Chen L, Henrey AJ (2013) Video Game Telemetry as a Critical Tool in the Study of Complex Skill Learning. PLoS ONE 8(9): e75129. [Web Link] -- Results: -- Skip league conditional inference forest classification (Bronze-Gold;Silver-Platinum;Gold-Diamond;Platinum-Masters;Diamond-Professional) showed changing patterns of variable importance with skill.\\nhttp://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset\\nInspiration\\nOrdinal Classification / regression model to determine League Index (\"LeagueIndex\")\\nSuggest additional features to gather and analyze for predicting leagues/performance.\\nAre there features which do not increase/decrease linearly as we go up in the leagues?',\n",
       " 'Overwatch is a team-based multiplayer online first-person shooter video game developed and published by Blizzard Entertainment. It was released in May 2016 for Windows, PlayStation 4, and Xbox One. Overwatch assigns players into two teams of six, with each player selecting from a roster of over 20 characters, known in-game as \"heroes\", each with a unique style of play, whose roles are divided into four general categories: Offense, Defense, Tank, and Support. Players on a team work together to secure and defend control points on a map or escort a payload across the map in a limited amount of time.\\nI discovered this dataset on the Overwatch Subreddit here: https://www.reddit.com/r/Overwatch/comments/7o8hmg/my_friend_has_recorded_every_game_hes_played/\\nIt represents a ridiculous amount of effort in terms of manually recording game results. This data, whilst in some places incomplete, gives an unprecedented view into the experience of a single overwatch player over the course of years of gameplay. From it you can track the ups and downs, shifts in hero preference and all sorts of other exciting in game trends.\\nThanks to JustWingIt for their amazing collecting this data.\\nI cleaned the data a little and put it into a single CSV.',\n",
       " \"Context\\nThis dataset focuses on public assistance in the United States with initial coverage of the WIC Program. The program is formally known as the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC). The program allocates Federal and State funds to help low-income women and children up to age five who are at nutritional risk. Funds are used to provide supplemental foods, baby formula, health care, and nutrition education.\\nContent\\nFiles include participation data and spending for state WIC programs, and poverty data for each state. Data is for fiscal years 2013-2016, which is actually October 2012 through September 2016.\\nMotivation\\nMy original purpose here is two-fold:\\nExplore various aspects of US Public Assistance. Show trends over recent years and better understand differences across state agencies. Although the federal government sponsors the program and provides funding, program are administered at the state level and can widely vary. Indian nations (native Americans) also administer their own programs.\\nShare with the Kaggle Community the joy - and pain - of working with government data. Data is often spread across numerous agency sites and comes in a variety of formats. Often the data is provided in Excel, with the files consisting of multiple tabs. Also, files are formatted as reports and contain aggregated data (sums, averages, etc.) along with base data.\\nAdditional Content Ideas\\nThe dataset can benefit greatly from additional content. Economics, additional demographics, administrative costs and more. I'd like to eventually explore the money trail from taxes and corporate subsidies, through the government agencies, and on to program participants. All community ideas are welcome!\",\n",
       " 'Context\\nfreeCodeCamp is a web-based non-profit organization and learning platform which teaches programming newcomers how to code. It was founded by Quincy Larson in 2014, who in a 2015 interview stated that \"freeCodeCamp is my effort to correct the extremely inefficient and circuitous way I learned to code...all those things that made learning to code a nightmare to me are things that we are trying to fix with freeCodeCamp.\" The original curriculum took approximately 800 hours to complete; today, after several refreshes and additions, there is over 2000 hours of learner\\'s content on the site.\\nfreeCodeCamp also provides several helpful secondary resources for learners. One of them is a freeCodeCamp Gitter chatroom. Gitter is an open-source instant messaging service that lets users share thoughts and ideas with one another. This dataset is a record of activity from this /freeCodeCamp Gitter chatroom, containing posts from students, bots, moderators, and contributors between December 31, 2014 and December 9, 2017.\\nContent\\nThe data includes the usernames, screen names, timestamps, post content, and metadata of every post made to /freeCodeCamp in the aforementioned three year period. This comes out to nearly 5 million records overall.\\nThe data comes in the form of a set of three JSON files, each named freecodecamp_casual_chatroom_[01/02/03].json. The three files make a continuous dataset containing all posts sent during the aforementioned period, but note that they overlap in a few days. The three files were extracted on 01-06-2016, 09-03-2017, and 12-12-2017, respectively. The included convert.py file was used to concatenate these files into a unified CSV file, freecodecamp_casual_chatroom.csv.\\nSome preliminary analyses using this dataset can be found at the Github repository for the freeCodeCamp\\'s Open Data Initiative. For more details on specific elements of the dataset refer to the column metadata tab, or to the detailed documentation provided in the Gitter API Documentation.\\nAcknowledgements\\nThe datasets are a contribution from freeCodeCamp as part of the freeCodeCamp\\'s Open Data Initiative. More information about the rationale of this initiative can be found on this Medium article.\\nThis dataset was extracted using Python code over the Gitter API. All the files were prepared by Evaristo Caraballo (GitHub: @evaristoc).\\nRecords are not anonymised or modified and are presented \"as they are\".\\nThanks to freeCodeCamp team, specially to Quincy Larson for supporting the initiative. Thanks to all freeCodeCamp students who kindly allowed to share their personal progress and to Gitter for making these data available.\\nIf you have questions about this dataset, please contact quincy@freecodecamp.com or get in touch with us through https://gitter.im/FreeCodeCamp/DataScience (Gitter registration might be required).\\nInspiration\\nThis dataset presents three years of developer chat about web technologies. Can you use it to trace the rise and fall over certain technologies, like Angular and React, over time?\\nWhat do programming newcomers tend to get stuck on the most? Are there any canned responses that are particularly common in the freeCodeCamp chatroom?\\nWhat other interesting insights into programmer culture can you glean from examining this dataset?',\n",
       " 'Context\\nThis is a protein data set retrieved from RCS PDB.\\nThe PDB archive is a repository of atomic coordinates and other information describing proteins and other important biological macromolecules. Structural biologists use methods such as X-ray crystallography, NMR spectroscopy, and cryo-electron microscopy to determine the location of each atom relative to each other in the molecule. They then deposit this information, which is then annotated and publicly released into the archive by the wwPDB.\\nThe constantly-growing PDB is a reflection of the research that is happening in laboratories across the world. This can make it both exciting and challenging to use the database in research and education. Structures are available for many of the proteins and nucleic acids involved in the central processes of life, so you can go to the PDB archive to find structures for ribosomes, oncogenes, drug targets, and even whole viruses. However, it can be a challenge to find the information that you need, since the PDB archives so many different structures. You will often find multiple structures for a given molecule, or partial structures, or structures that have been modified or inactivated from their native form.\\nContent\\nThere are two data sets. Both are arranged on \"structureId\" of the protein\\npdb_data_no_dups.csv :- Protein data set deatils of classification, extraction methods, etc. Containing 141401 instances and 14 attributes.\\ndata_seq :- Protein sequence information. Containing 467304 instances and 5 attributes.\\nAcknowledgements\\nOriginal data set down loaded from http://www.rcsb.org/pdb/\\nInspiration\\nProtein data base helped the life science community to study about different diseases and come with new drugs and solution that help the human survival.',\n",
       " 'Context\\nIt is not always easy to find databases from real world manufacturing plants, specially mining plants. So, I would like to share this database with the community, which comes from one of the most important parts of a mining process: a flotation plant!\\nThe main goal is to use this data to predict how much impurity is in the ore concentrate. As this impurity is measured every hour, if we can predict how much silica (impurity) is in the ore concentrate, we can help the engineers, giving them early information to take actions (empowering!). Hence, they will be able to take corrective actions in advance (reduce impurity, if it is the case) and also help the environment (reducing the amount of ore that goes to tailings as you reduce silica in the ore concentrate).\\nContent\\nThe first column shows time and date range (from march of 2017 until september of 2017). Some columns were sampled every 20 second. Others were sampled on a hourly base.\\nThe second and third columns are quality measures of the iron ore pulp right before it is fed into the flotation plant. Column 4 until column 8 are the most important variables that impact in the ore quality in the end of the process. From column 9 until column 22, we can see process data (level and air flow inside the flotation columns, which also impact in ore quality. The last two columns are the final iron ore pulp quality measurement from the lab. Target is to predict the last column, which is the % of silica in the iron ore concentrate.\\nInspiration\\nI have been working in this dataset for at least six months and would like to see if the community can help to answer the following questions:\\nIs it possible to predict % Silica Concentrate every minute?\\nHow many steps (hours) ahead can we predict % Silica in Concentrate? This would help engineers to act in predictive and optimized way, mitigatin the % of iron that could have gone to tailings.\\nIs it possible to predict % Silica in Concentrate whitout using % Iron Concentrate column (as they are highly correlated)?',\n",
       " 'History\\nI have made the database of photos sorted by products and brands. Screenshots were performed only on official brand websites.\\nContent\\nThe main dataset (style.zip) is 894 color images (150x150x3) with 7 brands and 10 products, and the file with labels style.csv. Photo files are in the .png format and the labels are integers and values.\\nThe file StyleColorImages.h5 consists of preprocessing images of this set: image tensors and targets (labels).\\nAcknowledgements\\nI have published the data for absolutely free using by any site visitor. But this database contains the names of famous brands, so it can not be used for commercial purposes.\\nUsage\\nClassification, image recognition and colorizing, etc. in a case of a small number of images are useful exercises. The main question we can try to answer with the help of the data is whether the algorithms can recognize the unique design style well enough. To facilitate the task, I chose the most easily recognizable brands with a bright style.\\nThe example of usage\\nImprovement\\nThere are lots of ways for improving this set and the machine learning algorithms applying to it. At first, it needs to increase the number of photos.',\n",
       " 'Context\\nKernels from Porto Seguro. Some are more overfit than others.\\nContent\\nCopied by hand from Kaggle. (May contain errors, obviously.) Each kernel is identified by a link to the most recent version that has a verified score matching what is in the Kernels tab. (Notebooks are linked to the Code tab, since that\\'s where the scores are.) In many cases this is the most recent version, so if the kernel is subsequently revised, the link may be wrong (since it will then point to the subsequent most recent version, which may not be scored, or its score may not match the Kernels tab). Most of the fields should be self-explanatory, except \"adjusted\", which adjusts the public-private difference by subtracting the median difference over the whole leaderboard. (Private scores are typically higher, presumably because the cases in private portion of the test data were easier to predict, so an \"adjusted\" value of zero represents a kernel that wouldn\\'t have gained or lost much in the shake-up.)\\nAcknowledgements\\nI would like to thank Kaggle, Porto Seguro, and the authors of the kernels.\\nInspiration\\nWhat factors might help predict how much better or worse a kernel will do on the private leaderboard than on the public? It\\'s one competition: just a start, but you\\'ve got to start somewhere.',\n",
       " 'Context\\nI created this dataset to enable everyone to explore local businesses of Pakistan. This dataset might help the local community in gathering information of local businesses. This might also help in local economic development of Pakistan by bridging traders and manufacturers.\\nContent\\nGeography: Pakistan\\nTime period: 1990-2017\\nDataset: The dataset contains information of approx 67000 businesses in Pakistan (~5000 in each csv file)\\nFeatures: The dataset has total 7 columns - Business Name - Contact Name - Telephone - Website - Services (Description of types of products/services provided by the business) - Address - City\\nAcknowledgements\\nThis Dataset was created by scraping this website. I wrote the script in Python using BeautifulSoup Library. Link to script: https://tinyurl.com/ybb4bdky\\nInspiration\\nA lot of questions can be answered and analysis can be done using this dataset. Few interesting ideas I can think of are : - Applying NLP techniques on Services column to extract business category - Clustering of categories of business according to cities',\n",
       " \"Context: Well, we are a head of the largest football event(Soccer, sorry for the American fellows) worldwide, as excited as we were. Well, at least some of us were, by the draw, here comes the data. History of all previous matches, scores and titles of all participating teams to help us predict who will qualify and may even predict who will win. So, lets get our algorithms going.\\nContent: The dataset contains 32 entries(of the 32 participating teams of course), each team will have 3 matches in the group stage, so each match is mentioned vs whom, the history between those 2 teams with wins minus losses, let's say Brazil has beaten Argentina 14 times, Argentina won 12 and there were 3 draws, so that will be +2 for brazil and -2 for Argentina, the same goes for goals scored. You will find some entries with N/A and some with zeros, so what is the difference? The zeros mean that the 2 teams evened out, while N/As means that they have no previous history together and they have never faced each other. So, no data available. The matches history is up to 2012-2014. So, there is a couple of years missing here, and the FIFA rank is up to date, which will be updated every month till we get there.\\nAcknowledgments: Most of the data is pulled from FIFA website except for the matches and scores history, they were pulled manually from various credible sources.\\nInspiration: Well, I think this data can help us predict who will head the groups and who comes second, then may be we can progress through round of 16....final. You can ask for more data and I'll be happy to search and update it.\\nIf you like the data or find it useful enough, don't forget the upvote ;)\",\n",
       " \"Context\\nSpaceX designs, manufactures and launches advanced rockets and spacecraft. The company was founded in 2002 to revolutionize space technology, with the ultimate goal of enabling people to live on other planets - SpaceX\\nContent\\nThe dataset contains mission information for rocket launches conducted by SpaceX (Space Exploration Technologies Corp).\\nAcknowledgements\\nData was obtained via Wikipedia's entry for Falcon 9 and Falcon Heavy launches.\\nInspiration\\nDo you anticipate an increase in launches with the introduction of the Falcon Heavy? How has launch rate increased over time? Do you predict a shift in payload orbits for upcoming launches? How has the customer diversity changed over the years?\",\n",
       " 'Context\\nShanghai uses an auction system to sell a limited number of license plates to fossil-fuel car buyers every month. The average price of this license plate is about $13,000 and it is often referred to as \"the most expensive piece of metal in the world.\" So, our goal is to predict the avg price or the lowest price for the next month. This Data set will be updated every month constantly. Have fun!\\nContent\\nThis data set is gathered by myself with the aid of search engine.\\nInspiration\\nThis data set could be used in your first toy example project. Learning algorithms like RNN+LSTM are well fitted into this time-series prediction problem. So, just have fun!',\n",
       " \"Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 3.7 million products) that was created by extracting data from jcpenney.com, a well known retailer.\\nContent\\nThis dataset has following fields:\\nsku\\nname_title\\ndescription\\nlist_price\\nsale_price\\ncategory\\ncategory_tree\\naverage_product_rating\\nproduct_url\\nproduct_image_urls\\nbrand\\ntotal_number_reviews\\nreviews\\nAcknowledgements\\nThis dataset was created by PromptCloud's in-house web-crawling service.\\nInspiration\\nAnalyses of list price, sale price, rating and reviews can be performed.\",\n",
       " 'Background\\nNLP is a hot topic currently! Team AI really want\\'s to leverage the NLP research and this an attempt for all the NLP researchers to explore exciting insights from bilingual data\\nThe Japanese-English Bilingual Corpus of Wikipedia\\'s Kyoto Articles” aims mainly at supporting research and development relevant to high-performance multilingual machine translation, information extraction, and other language processing technologies.\\nUnique Features\\nA precise and large-scale corpus containing about 500,000 pairs of manually-translated sentences. Can be exploited for research and development of high-performance multilingual machine translation, information extraction, and so on.\\nThe three-step translation process (primary translation -> secondary translation to improve fluency -> final check for technical terms) has been clearly recorded. Enables observation of how translations have been elaborated so it can be applied for uses such as research and development relevant to translation aids and error analysis of human translation.\\nTranslated articles concern Kyoto and other topics such as traditional Japanese culture, religion, and history. Can also be utilized for tourist information translation or to create glossaries for travel guides.\\nThe Japanese-English Bilingual Kyoto Lexicon is also available. This lexicon was created by extracting the Japanese-English word pairs from this corpus.\\nSample\\nOne Wikipedia article is stored as one XML file in this corpus, and the corpus contains 14,111 files in total.\\nThe following is a short quotation from a corpus file titled “Ryoan-ji Temple”. Each tag has different implications. For example:\\n<j>Original Japanese sentence<j> <e type=\"trans\" ver=\"1\">Primary translation</e> <e type=\"trans\" ver=\"2\">Secondary translation</e> <e type=\"check\" ver=\"1\">Final translation</e> <cmt>Comment added by translators</cmt>\\nCategories\\nThe files have been divided into 15 categories: school, railway, family, building, Shinto, person name, geographical name, culture, road, Buddhism, literature, title, history, shrines and temples, and emperor (Click the link to view a sample file for each category).\\nGithub\\nhttps://github.com/venali/BilingualCorpus\\nExplains how to load the corpus\\nAcknowledgements\\nThe National Institute of Information and Communications Technology (NICT) has created this corpus by manually translating Japanese Wikipedia articles (related to Kyoto) into English.\\nLicence\\nUse and/or redistribution of the Japanese-English Bilingual Corpus of Wikipedia\\'s Kyoto Articles and the Japanese-English Bilingual Kyoto Lexicon is permitted under the conditions of Creative Commons Attribution-Share-Alike License 3.0. Details can be found at http://creativecommons.org/licenses/by-sa/3.0/.\\nLink to web\\nhttps://alaginrc.nict.go.jp/WikiCorpus/index_E.html',\n",
       " 'Context\\nThis is data on New York City Taxi Cab trips. It should be useful for adding a lot more training data for the \"New York City Taxi Trip Duration\". The data I am uploading is for 2014. The training data for the competition is for 2016. Hopefully, the underlying data should be very similar over the two years. Compressed, all of the data is over 5 GB, which is more than 10x the allowed data size of Kaggle datasets. This represents a subset of the total data.\\nContent\\nThe data is in a CSV format with the following fields. It was collected both from driver input and from the GPS coordinates of the cab. It is downloaded from New York City\\'s Open Data website.\\nvendor_id pickup_datetime dropoff_datetime passenger_count trip_distance pickup_longitude pickup_latitude store_and_fwd_flag dropoff_longitude dropoff_latitude payment_type fare_amount mta_tax tip_amount tolls_amount total_amount imp_surcharge extra rate_code\\nIn the original dataset published by NYC, there is over 165 million trips. This is only 15 million. It was selected as the first 15 million available records of the year.\\nAcknowledgements\\nThis data comes from the City of New York, who have been leaders in making data available to the public. This comes from their NYC Open Data website (https://opendata.cityofnewyork.us/)\\nInspiration\\nThe inspiration is the Taxi competition: https://www.kaggle.com/c/nyc-taxi-trip-duration',\n",
       " 'Context\\nForex is the largest market in the world, predicting the movement of prices is not a simple task, this dataset pretends to be the gateway for people who want to conduct trading using machine learning.\\nContent\\nThis dataset contains 4479 simulated winning transactions (real data, fictitious money) (3 years 201408-201708) with buy transactions (2771 operations 50.7%) and sell (2208 transactions, 49.3%), to generate this data a script of metatrader was used, operations were performed in time frame 4Hour and fixed stop loss and take profits of 50 pips (4 digits) were used to determine if the operation is winning. Each operation contains a set of classic technical indicators like rsi, mom, bb, emas, etc. (last 24 hours)\\nAcknowledgements\\nThanks to Kaggle for giving me the opportunity to share my passion for machine learning. My profile: https://www.linkedin.com/in/rsx2010/\\nInspiration\\nThe problem of predicting price movement is reduced with this dataset to a classification problem:\\n\"use the variables rsi1 to dayOfWeek to predict the type of correct operation to be performed (field=tipo)\"\\ntipo = 0 ==> Operation buy\\ntipo= 1 ==> Operation = sell:\\nGood luck\\nRodrigo Salas Vallet-Cendre.\\nrasvc@hotmail.com',\n",
       " 'Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.\\nContext:\\n*Annual Health Survey : Mortality Schedule *\\nThis unit level dataset contains the details relating to death occurred to usual residents of sample household during the reference period and it includes information on sex of deceased, date of death, age at death, registration of death and source of medical attention received before death. For infant deaths, data related to symptoms preceding death is also provided. Mortality Schedule also includes information on various determinants of maternal mortality viz. case of deaths associated with pregnancy, information on factors leading/ contributing to death, symptoms preceding death, time between onset of complications and death, etc.\\nThere are total of 770k observations and 121 variables in this dataset.\\nSurvey:\\nBase line survey - 2010-11 (4.14 million households in the sample) 1st update - 2011-12 (4.28 million households in the sample) 2nd update - 2012-13 (4.32 million households in the sample)\\nThe survey was conducted in the below 9 states.\\nA. Empowered Action Group (EAG) States\\nUttarakhand (05)\\nRajasthan (08)\\nUttar Pradesh (09)\\nBihar (10)\\nJharkhand (20)\\nOdisha (21)\\nChhattisgarh (22)\\nMadhya Pradesh (23)\\nB. Assam. (18)\\nThese nine states, which account for about 48 percent of the total population, 59 percent of Births, 70 percent of Infant Deaths, 75 percent of Under 5 Deaths and 62 percent of Maternal Deaths in the country, are the high focus States in view of their relatively higher fertility and mortality.\\nContent:\\nThe files contains the below columns.\\nVariable Names:\\nid\\nm_id\\nclient_m_id\\nhl_id\\nhouse_no\\nhouse_hold_no\\nstate\\ndistrict\\nrural\\nstratum_code\\npsu_id\\nm_serial_no\\ndeceased_sex\\ndate_of_death\\nmonth_of_death\\nyear_of_death\\nage_of_death_below_one_month\\nage_of_death_below_eleven_month\\nage_of_death_above_one_year\\ntreatment_source\\nplace_of_death\\nis_death_reg\\nis_death_certificate_received\\nserial_num_of_infant_mother\\norder_of_birth\\ndeath_symptoms\\nis_death_associated_with_pregnan\\ndeath_period\\nmonths_of_pregnancy\\nfactors_contributing_death\\nfactors_contributing_death_2\\nsymptoms_of_death\\ntime_between_onset_of_complicati\\nnearest_medical_facility\\nm_expall_status\\nfield38\\nhh_id\\nclient_hh_id\\ncurrently_dead_or_out_migrated\\nhh_serial_no\\nsex\\nusual_residance\\nrelation_to_head\\nmember_identity\\nfather_serial_no\\nmother_serial_no\\ndate_of_birth\\nmonth_of_birth\\nyear_of_birth\\nage\\nreligion\\nsocial_group_code\\nmarital_status\\ndate_of_marriage\\nmonth_of_marriage\\nyear_of_marriage\\ncurrently_attending_school\\nreason_for_not_attending_school\\nhighest_qualification\\noccupation_status\\ndisability_status\\ninjury_treatment_type\\nillness_type\\nsymptoms_pertaining_illness\\nsought_medical_care\\ndiagnosed_for\\ndiagnosis_source\\nregular_treatment\\nregular_treatment_source\\nchew\\nsmoke\\nalcohol\\nstatus\\nhh_expall_status\\nclient_hl_id\\nserial_no\\nbuilding_no\\nhouse_status\\nhouse_structure\\nowner_status\\ndrinking_water_source\\nis_water_filter\\nwater_filteration\\ntoilet_used\\nis_toilet_shared\\nhousehold_have_electricity\\nlighting_source\\ncooking_fuel\\nno_of_dwelling_rooms\\nkitchen_availability\\nis_radio\\nis_television\\nis_computer\\nis_telephone\\nis_washing_machine\\nis_refrigerator\\nis_sewing_machine\\nis_bicycle\\nis_scooter\\nis_car\\nis_tractor\\nis_water_pump\\ncart\\nland_possessed\\nhl_expall_status\\nfid\\nisdeadmigrated\\nresidancial_status\\niscoveredbyhealthscheme\\nhealthscheme_1\\nhealthscheme_2\\nhousestatus\\nhouseholdstatus\\nisheadchanged\\nfidh\\nfidx\\nas\\nwt\\nx\\nschedule_id\\nyear\\nFile content: Mortality_data_dictionary.xlsx : This data dictionary excel work book has the detailed information about each and every column and codes used in the data.\\nAcknowledgements\\nDepartment of Health and Family Welfare, Govt. of India has published this dataset in Open Govt Data Platform India portal under Govt. Open Data License - India.',\n",
       " \"The below information is from the project page: https://nlp.stanford.edu/projects/glove/\\nContext\\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\\nContent\\nDue to size constraints, only the 25 dimension version is uploaded. Please visit the project page for GloVe of other dimensions.\\nThis dataset (https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation) contains GloVe extracted from Wikipedia 2014 + Gigaword 5.\\nNearest neighbors The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. Sometimes, the nearest neighbors according to this metric reveal rare but relevant words that lie outside an average human's vocabulary.\\nLinear substructures The similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words. This simplicity can be problematic since two given words almost always exhibit more intricate relationships than can be captured by a single number. For example, man may be regarded as similar to woman in that both words describe human beings; on the other hand, the two words are often considered opposites since they highlight a primary axis along which humans differ from one another.\\nIn order to capture in a quantitative way the nuance necessary to distinguish man from woman, it is necessary for a model to associate more than a single number to the word pair. A natural and simple candidate for an enlarged set of discriminative numbers is the vector difference between the two word vectors. GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words.\\nAcknowledgements\\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.\\nInspiration\\nThe dataset specifically includes tokens extracted from Twitter, which unlike tokens from Wikipedia, include many abbreviations that have interesting content.\",\n",
       " 'Context\\nThis dataset comes from \"The Exempt Organization Business Master File Extract\" (EO BMF) which includes cumulative information on tax-exempt organizations.\\nContent\\nData is current up to: 8/14/2017\\nEIN: Employer Identification Number (EIN)\\nNAME: Primary Name of Organization\\nICO: In Care of Name\\nSTREET: Street Address\\nCITY: City\\nSTATE: State\\nZIP: Zip Code\\nGROUP: Group Exemption Number\\nSUBSECTION: Subsection Code\\nAFFILIATION: Affiliation Code\\nCLASSIFICATION: Classification Code(s)\\nRULING: Ruling Date\\nDEDUCTIBILITY: Deductibility Code\\nFOUNDATION: Foundation Code\\nACTIVITY: Activity Codes\\nORGANIZATION: Organization Code\\nSTATUS: Exempt Organization Status Code\\nTAX_PERIOD: Tax Period\\nASSET_CD: Asset Code\\nINCOME_CD: Income Code\\nFILING_REQ_CD: Filing Requirement Code\\nPF_FILING_REQ_CD: PF Filing Requirement Code\\nACCT_PD: Accounting Period\\nASSET_AMT: Asset Amount\\nINCOME_AMT: Income Amount (includes negative sign if amount is negative)\\nREVENUE_AMT: Form 990 Revenue Amount (includes negative sign if amount is negative)\\nNTEE_CD: National Taxonomy of Exempt Entities (NTEE) Code\\nSORT_NAME: Sort Name (Secondary Name Line)\\nThere are six data files separated by regions\\neo1:\\nCT\\nMA\\nME\\nNH\\nNJ\\nNY\\nRI\\nVT\\neo2:\\nDC\\nDE\\nIA\\nIL\\nIN\\nKY\\nMD\\nMI\\nMN\\nNC\\nND\\nNE\\nOH\\nPA\\nSC\\nSD\\nVA\\nWI\\nWV\\neo3:\\nAK\\nAL\\nAR\\nAZ\\nCA\\nCO\\nFL\\nGA\\nHI\\nID\\nKS\\nLA\\nMO\\nMS\\nMT\\nNM\\nNV\\nOK\\nOR\\nTN\\nTX\\nUT\\nWA\\nWY\\neo4:\\nAA\\nAE\\nAP\\nAS\\nFM\\nGU\\nMH\\nMP\\nPR\\nPW\\nVI\\neo_pr:\\nPuerto Rico\\neo_xx:\\nVarious international non-profits (too many countries to list). See columns 5 and 6.\\nAcknowledgements\\nMore information and updated data an be found here',\n",
       " \"Context\\nThis data contains 'real-time' traffic information from locations where NYCDOT picks up sensor feeds within the five boroughs of NYC, mostly on major arterials and highways. NYCDOT uses this information for emergency response and management, see Acknowledgements.\\nContent\\nNYC Real Time Traffic Speed Data Feed for the year 2016, separated in monthly files of 5 minutes intervals of 'real-time' traffic information within the five boroughs of NYC. Each row represents a given street section (link), for which the average speed, travel time, timestamp and an id of the street section (link) is given. For each link id, information about this link is given in the linkinfo.csv file, e.g., geo coordinates.\\nAcknowledgements\\nhttp://data.beta.nyc/dataset/nyc-real-time-traffic-speed-data-feed-archived https://data.cityofnewyork.us/Transportation/Real-Time-Traffic-Speed-Data/xsat-x5sa\",\n",
       " \"Context\\nPresenting a compendium of crowdsourced journalism from the psuedo-news site The Examiner.\\nThis dataset contains the headlines of 3 million articles written by 21000+ authors over 6 years.\\nWhile The Examiner was never praised for its quality, it consistently churned out 1000s of articles per day over several years.\\nAt their height in 2011, The Examiner was ranked highly in google search and had enormous shares on social media. At one point it was the 10th largest site on mobile and was attracting 20 M unique visitors a month.\\nAs a platform driven towards advert revenue, most of their content was rushed, unsourced and factually sparse. It still manages to capture in great detail, the trending topics over a long period of time.\\nPrepared by Rohit Kulkarni\\nContent\\nFormat: CSV Rows: 3,089,781\\nColumn 1: publish_date Date when the article was published on the site in yyyyMMdd format\\nColumn 2: headline_text Text of the headline in English with rare utf8 chars (<1k)\\nStart Date: 2010-01-01 End Date: 2015-21-31\\nAnother copy of the file with headlines tokenised to lowercase ascii only is included.\\nAcknowledgements\\nCreated using Jsoup, Java and Bash.\\nSimilar news datasets exploring other attributes, countries and topics can be seen on my profile.\\nThis dataset is free to use with citation:\\nRohit Kulkarni (2017), The Examiner - Spam ClickBait News 2010-2015 [CSV data files], doi:10.7910/DVN/I4HKOO, Retrieved from [this url]\\nInspiration\\nThe Examiner had emerged as an early winner in the digital content landscape of the 2000's using catchy headlines.\\nIt changed many roles over the years, from leftist citizen news to a multiuser blogging platform to a content farm.\\nWith falling views its operations were absorbed by AXS in 2014 and the website was finally shut down in June 2016.\\nThe original site and content no longer exists: http://www.examiner.com\\nThis is the last surviving record of its existence.\",\n",
       " 'This dataset contains a collection of interaction sequences between allies in online Diplomacy [1] games. A sequence consists of consecutive game seasons during which the two players exchange messages and help each other in the game. Half of the sequences end with betrayal, while the other half are part of lasting friendships.\\nDescription\\nDiplomacy [1] is a popular and engaging strategic board game that is often played online [2, 3]. It is based heavily on communication between the players. Due to its military domination setting, Diplomacy is a well suited environment for studying naturally occurring betrayal and deception.\\nFrom a collection of Diplomacy game logs, we identified and extracted ongoing, established, and reciprocal friendships: relationships that contain at least two consecutive and reciprocated acts of support that span at least three seasons in game time, with no more than five seasons passing between two acts of friendship.\\nWe then identified 250 betrayals: the subset of friendships described above that are followed by at least two attacks. To match each betrayal, we selected a friendship that is not followed by any offensive action, but is otherwise nearly identical (in terms of length and relative time within the game). The current dataset consists of these selected betrayals and friendships only.\\nEach relationship contains a sequence of seasons. Within each season, we provide features extracted from the messages sent by each player.\\nAcknowledgements:\\nThis dataset is distributed under the Open Data Commons Attribution (ODC-By 1.0) license. It was collected by Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber and Cristian Danescu-Niculescu-Mizil.\\nIf you use this dataset, please cite this paper:\\nNiculae, V., Kumar, S., Boyd-Graber, J., & Danescu-Niculescu-Mizil, C. (2015). Linguistic harbingers of betrayal: A case study on an online strategy game. In Proceedings of the ACL.\\nData format\\nThe dataset is a UTF-8 encoded JSON file, which can be loaded into a Python kernel with the following code:\\n>>> import json\\n>>> from io import open\\n>>> with open(\"diplomacy_data.json\", \"r\") as f:\\n...     diplomacy = json.load(f)\\n...\\nIt is structured as a list of dictionaries, one for each of the 500 sequences.\\n>>> len(diplomacy)\\n500\\nThis is an example of one such entry, with the fields explained:\\n>>> entry = diplomacy[0]\\n>>> entry\\n{\\n    \\'idx\\': 0,           # unique identifier of the dataset entry\\n    \\'game\\': 74,         # unique identifier of the game it comes from\\n    \\'betrayal\\': True,   # whether the friendship ended in betrayal\\n    \\'people\\': u\\'AT\\',    # the countries represented by the two players\\n                        # (in this case, Austria and Turkey)\\n    \\'seasons\\': ...\\n}\\nThe \\'seasons\\' field is again a list of dictionaries, one for each game season in the friendship sequence. In the example below, there are 8 seasons, each identified by the game year. Decimal notation is used to denote the season in each year. For example, 1906.0 is the spring of 1906 and 1906.5 is the fall of 1906. Each season is also marked with what interaction the two players have at the end of the discussion: whether the players supported one another (\\'support\\'), attacked one another (\\'attack\\'), or did not have explicit military interactions (null).\\n>>> seasons = entry[\\'seasons\\']\\n>>> len(seasons)\\n8\\n>>> seasons[0]\\n{\\n    \\'season\\': 1906.5,           # fall of the year 1906 (game time)\\n    \\'interaction\\': {\\n        \\'victim\\': u\\'support\\',   # the victim supported the betrayer\\n        \\'betrayer\\': u\\'support\\'  # the betrayer supported the victim\\n    },\\n    \\'messages\\': {\\n        \\'victim\\': ...,\\n        \\'betrayer\\': ...\\n    }\\n}\\nThe [\\'messages\\'][\\'victim\\'] and [\\'messages\\'][\\'betrayer\\'] fields are lists of features of each message sent by the victim to the betrayer, and by the betrayer to the victim, respectively:\\n>>> msgs = seasons[0][\\'messages\\'][\\'betrayer\\']\\n>>> len(msgs)\\n6\\n>>> msgs[0]\\n{\\n    \"n_words\": 146,             # number of words in the message\\n    \"n_sentences\": 9,           # number of sentences in the message\\n\\n    \"n_requests\": 7,            # number of request sentences\\n    \"politeness\": 0.8320,       # politeness of the requests (from 0 to 1)\\n                                # (using the Stanford Politeness\\n                                # Classifier available at [4])\\n    \"sentiment\": {\\n        \"positive\": 1,          # no. sentences with positive sentiment\\n        \"neutral\": 3,           #      \"      \"      neutral sentiment\\n        \"negative\": 5           #      \"      \"      negative sentiment\\n    },                          # (using Stanford Sentiment Analysis [5])\\n\\n    \"lexicon_words\": {          # words and phrases matching several\\n        \"disc_expansion\": [     # linguistic and psycholinguistic lexicons\\n            \"until\",            # (see below for details)\\n            \"yet\",\\n            \"instead\"\\n        ],\\n        \"premise\": [\\n            \"for\",\\n            \"for\"\\n        ],\\n        ...\\n    },\\n    \"frequent_words\": [         # frequent words in the message\\n        \"more\",                 # (occurring in at least 50 messages\\n        \"let\",                  # and 5 friendships overall)\\n        \"keep\",\\n        \"...\\n    ]\\n}\\nThe words in each list are in random order. The order of messages within a season is also randomized. This measure is in place to protect the privacy of the players and of their conversations.\\nThe lexicons used to construct the \"lexicon_words\" field are:\\n\\'claim\\', \\'premise\\': Argumentation structure markers [6]\\n\\'allsubj\\': Subjective markers [7]\\n\\'disc_*\\': Discourse markers from the Penn Discourse Treebank. [8] Includes \\'disc_comparison\\', \\'disc_expansion\\', \\'disc_contingency\\', \\'disc_temporal_future\\' and \\'disc_temporal_rest\\' (we manually split \\'temporal\\' from PDT into \\'temporal_future\\' and \\'temporal_rest\\' to capture planning).\\nReferences\\n[1] https://en.wikipedia.org/wiki/Diplomacy_%28game%29 [2] http://www.floc.net/dpjudge/ [3] http://usak.asciiking.com/ [4] http://politeness.mpi-sws.org/ [5] http://nlp.stanford.edu/sentiment/ [6] C. Stab and I. Gurevych. Identifying Argumentative Discourse Structures in Persuasive Essays. In: Proceedings of EMNLP, 2014. https://www.ukp.tu-darmstadt.de/data/argumentation-mining/ [7] E. Riloff and J. Wiebe. Learning extraction patterns for subjective expressions. In: Proceedings of EMNLP, 2003. http://www.anthology.aclweb.org/W/W03/W03-1014.pdf [8] https://www.seas.upenn.edu/~pdtb/',\n",
       " 'Context:\\n“A blog (a truncation of the expression \"weblog\") is a discussion or informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (\"posts\"). Posts are typically displayed in reverse chronological order, so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual, occasionally of a small group, and often covered a single subject or topic.” -- Wikipedia article “Blog”\\nThis dataset contains text from blogs written on or before 2004, with each blog being the work of a single user.\\nContent:\\nThe Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\\nEach blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\\nAll bloggers included in the corpus fall into one of three age groups: * 8240 \"10s\" blogs (ages 13-17), * 8086 \"20s\" blogs(ages 23-27) * 2994 \"30s\" blogs (ages 33-47).\\nFor each age group there are an equal number of male and female bloggers.\\nEach blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.\\nAcknowledgements\\nThe corpus may be freely used for non-commercial research purposes. Any resulting publications should cite the following:\\nJ. Schler, M. Koppel, S. Argamon and J. Pennebaker (2006). Effects of Age and Gender on Blogging in Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs. URL: http://www.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf\\nInspiration:\\nThis dataset contains information on writers demographics, including their age, gender and zodiac sign. Can you build a classifier to guess someone’s zodiac sign from blog posts they’ve written?\\nWhich are bigger: differences between demographic groups or differences between blogs on different topics?\\nYou may also like:\\nNews and Blog Data Crawl: Content section from over 160,000 news and blog articles\\n20 Newsgroups: A collection of ~18,000 newsgroup documents from 20 different newsgroups',\n",
       " 'Context:\\nSentiment analysis is the task of computationally labeling whether the content of text is positive or negative. One common approach to this is to compile lists of words which have a positive connotation (like “wonderful”, “lovely” and “best”) and a negative connotation (like “bad”, “horrible” or “awful”). Then you count how many positive and how many negative\\nContent:\\nThis dataset contains three lists of Thai words:\\nswear words (94 words)\\npositive words (512 words)\\nnegative words (1218 words)\\nEach list a .txt file with one word per line. The character encoding is UTF-8.\\nAcknowledgements:\\nThis dataset was compiled by Wannaphong Phatthiyaphaibun and is reproduced here under a CC-BY-SA 4.0 license. (You may also be interested in his translation of Python 3 documentation into Thai on this blog.)\\nInspiration:\\nCan you analyze the sentiment in this corpus of Thai? Is there a difference in sentiment between the WIkipedia and Government parts of the corpus?',\n",
       " 'Context:\\nAnticipating public nuisances and allocating proper resources is a critical part of public duties.\\nContent:\\nThis dataset contains 5 years (2008-2011, 2016) worth of public incidents, both criminal and non-criminal. Data includes time, location, description, and unique key.\\nAcknowledgements:\\nThis dataset was compiled by the City of Austin and published on Google Cloud Public Data.\\nDataset Description\\nUse this dataset with BigQuery You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too.\\nInspiration:\\nAre there notable time variations in incidences?\\nCan you predict incidence patterns for 2016 based on the 2008-2011 training data?\\nDo weather patterns or other changes related to incidence changes?',\n",
       " \"Context\\nAt Shadow Robot, we are leaders in robotic grasping and manipulation. As part of our Smart Grasping System development, we're developing different algorithms using machine learning.\\nThis first public dataset was created to investigate the use of machine learning to predict the stability of a grasp. Due to the limitations of the current simulation, it is a restricted dataset - only grasping a ball. The dataset is annotated with an objective grasp quality and contains the different data gathered from the joints (position, velocity, effort).\\nYou can find all the explanations for this dataset over on Medium.\\nInspiration\\nI'll be more than happy to discuss this dataset as well as which dataset you'd like to have to try your hands at solving real world robotic problems focused on grasping using machine learning. Let's connect on twitter (@ugocupcic)!\",\n",
       " 'Context\\nThe data were collected as the SCITOS G5 navigated through the room following the wall in a clockwise direction, for 4 rounds. To navigate, the robot uses 24 ultrasound sensors arranged circularly around its \"waist\". The numbering of the ultrasound sensors starts at the front of the robot and increases in clockwise direction.\\nThe provided files comprise three diferent data sets.\\nThe first one contains the raw values of the measurements of all 24 ultrasound sensors and the corresponding class label (Moving forward, turning left, etc). Sensor readings are sampled at a rate of 9 samples per second.\\nThe second one contains four sensor readings named \\'simplified distances\\' and the corresponding class label l (Moving forward, turning left, etc). These simplified distances are referred to as the \\'front distance\\', \\'left distance\\', \\'right distance\\' and \\'back distance\\'. They consist, respectively, of the minimum sensor readings among those within 60 degree arcs located at the front, left, right and back parts of the robot.\\nThe third one contains only the front and left simplified distances and the corresponding class labell (Moving forward, turning left, etc).\\nIt is worth mentioning that the 24 ultrasound readings and the simplified distances were collected at the same time step, so each file has the same number of rows (one for each sampling time step).\\nThe wall-following task and data gathering were designed to test the hypothesis that this apparently simple navigation task is indeed a non-linearly separable classification task. Thus, linear classifiers, such as the Perceptron network, are not able to learn the task and command the robot around the room without collisions. Nonlinear neural classifiers, such as the MLP network, are able to learn the task and command the robot successfully without collisions.\\nIf some kind of short-term memory mechanism is provided to the neural classifiers, their performances are improved in general. For example, if past inputs are provided together with current sensor readings, even the Perceptron becomes able to learn the task and command the robot successfully. If a recurrent neural network, such as the Elman network, is used to learn the task, the resulting dynamical classifier is able to learn the task using less hidden neurons than the MLP network.\\nFiles with different number of sensor readings were built in order to evaluate the performance of the classifiers with respect to the number of inputs.\\nContent\\nFile sensor_readings_24.csv:\\nUS1: ultrasound sensor at the front of the robot (reference angle: 180°) - (numeric: real)\\nUS2: ultrasound reading (reference angle: -165°) - (numeric: real)\\nUS3: ultrasound reading (reference angle: -150°) - (numeric: real)\\nUS4: ultrasound reading (reference angle: -135°) - (numeric: real)\\nUS5: ultrasound reading (reference angle: -120°) - (numeric: real)\\nUS6: ultrasound reading (reference angle: -105°) - (numeric: real)\\nUS7: ultrasound reading (reference angle: -90°) - (numeric: real)\\nUS8: ultrasound reading (reference angle: -75°) - (numeric: real)\\nUS9: ultrasound reading (reference angle: -60°) - (numeric: real)\\nUS10: ultrasound reading (reference angle: -45°) - (numeric: real)\\nUS11: ultrasound reading (reference angle: -30°) - (numeric: real)\\nUS12: ultrasound reading (reference angle: -15°) - (numeric: real)\\nUS13: reading of ultrasound sensor situated at the back of the robot (reference angle: 0°) - (numeric: real)\\nUS14: ultrasound reading (reference angle: 15°) - (numeric: real)\\nUS15: ultrasound reading (reference angle: 30°) - (numeric: real)\\nUS16: ultrasound reading (reference angle: 45°) - (numeric: real)\\nUS17: ultrasound reading (reference angle: 60°) - (numeric: real)\\nUS18: ultrasound reading (reference angle: 75°) - (numeric: real)\\nUS19: ultrasound reading (reference angle: 90°) - (numeric: real)\\nUS20: ultrasound reading (reference angle: 105°) - (numeric: real)\\nUS21: ultrasound reading (reference angle: 120°) - (numeric: real)\\nUS22: ultrasound reading (reference angle: 135°) - (numeric: real)\\nUS23: ultrasound reading (reference angle: 150°) - (numeric: real)\\nUS24: ultrasound reading (reference angle: 165°) - (numeric: real)\\nClasses: Move-Forward, Slight-Right-Turn, Sharp-Right-Turn, Slight-Left-Turn\\nFile: sensor_readings_4.csv:\\nSD_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)\\nSD_left: minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric: real)\\nSD_right: minimum sensor reading within a 60 degree arc located at the right of the robot - (numeric: real)\\nSD_back: minimum sensor reading within a 60 degree arc located at the back of the robot - (numeric: real)\\nClasses: Move-Forward, Slight-Right-Turn, Sharp-Right-Turn, Slight-Left-Turn\\nFile: sensor_readings_2.csv:\\nSD_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)\\nSD_left: minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric: real)\\nClasses: Move-Forward, Slight-Right-Turn, Sharp-Right-Turn, Slight-Left-Turn\\nAcknowledgements\\nThese datasets were downlaoded from the UCI Machine Learning Repository\\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\\nInspiration\\nUse these ultrasound readings to predict the class, i.e. given these readings, is the robot moving straight? turning left?',\n",
       " \"Context\\nThe drug-development process is time-consuming and expensive. In High-Throughput Screening (HTS), batches of compounds are tested against a biological target to test the compound's ability to bind to the target. Targets might be antibodies for example. If the compound binds to the target then it is active for that target and known as a hit.\\nVirtual screening is the computational or in silico screening of biological compounds and complements the HTS process. It is used to aid the selection of compounds for screening in HTS bioassays or for inclusion in a compound-screening library.\\nDrug discovery is the first stage of the drug-development process and involves finding compounds to test and screen against biological targets. This first stage is known as primary-screening and usually involves the screening of thousands of compounds.\\nThis dataset is a collection of 21 bioassays (screens) that measure the activity of various compounds against different biological targets.\\nContent\\nEach bioassay is split into test and train files.\\nHere are some descriptions of some of the assays compounds. The source, unfortunately, does not have descriptions for every assay. That's the nature of the beast for finding this kind data and was also pointed out in the original study.\\nPrimary screens\\nAID362 details the results of a primary screening bioassay for Formylpeptide Receptor Ligand Binding University from the New Mexico Center for Molecular Discovery. It is a relatively small dataset with 4279 compounds and with a ratio of 1 active to 70 inactive compounds (1.4% minority class). The compounds were selected on the basis of preliminary virtual screening of approximately 480,000 drug-like small molecules from Chemical Diversity Laboratories.\\nAID604 is a primary screening bioassay for Rho kinase 2 inhibitors from the Scripps Research Institute Molecular Screening Center. The bioassay contains activity information of 59,788 compounds with a ratio of 1 active compound to 281 inactive compounds (1.4%). 57,546 of the compounds have known drug-like properties.\\nAID456 is a primary screen assay from the Burnham Center for Chemical Genomics for inhibition of TNFa induced VCAM-1 cell surface expression and consists of 9,982 compounds with a ratio of 1 active compound to 368 inactive compounds (0.27% minority). The compounds have been selected for their known drug-like properties and 9,431 meet the Rule of 5 [19].\\nAID688 is the result of a primary screen for Yeast eIF2B from the Penn Center for Molecular Discovery and contains activity information of 27,198 compounds with a ratio of 1 active compound to 108 inactive compounds (0.91% minority). The screen is a reporter-gene assay and 25,656 of the compounds have known drug-like properties.\\nAID373 is a primary screen from the Scripps Research Institute Molecular Screening Center for endothelial differentiation, sphingolipid G-protein-coupled receptor, 3. 59,788 compounds were screened with a ratio of 1 active compound to 963 inactive compounds (0.1%). 57,546 of the compounds screened had known drug-like properties.\\nAID746 is a primary screen from the Scripps Research Institute Molecular Screening Center for Mitogen-activated protein kinase. 59,788 compounds were screened with a ratio of 1 active compound to 162 inactive compounds (0.61%). 57,546 of the compounds screened had known drug-like properties.\\nAID687 is the result of a primary screen for coagulation factor XI from the Penn Center for Molecular Discovery and contains activity information of 33,067 compounds with a ratio of 1 active compound to 350 inactive compounds (0.28% minority). 30,353 of the compounds screened had known drug-like properties.\\nPrimary and Confirmatory\\nAID604 (primary) with AID644 (confirmatory)\\nAID746 (primary) with AID1284 (confirmatory)\\nAID373 (primary) with AID439 (confirmatory)\\nAID746 (primary) with AID721 (confirmatory)\\nConfirmatory\\nAID1608 is a different type of screening assay that was used to identify compounds that prevent HttQ103-induced cell death. National Institute of Neurological Disorders and Stroke Approved Drug Program. The compounds that prevent a release of a certain chemical into the growth medium are labelled as active and the remaining compounds are labelled as having inconclusive activity. AID1608 is a small dataset with 1,033 compounds and a ratio of 1 active to 14 inconclusive compounds (6.58% minority class).\\nAID644\\nAID1284\\nAID439\\nAID721\\nAID1608\\nAID644\\nAID1284\\nAID439\\nAID721\\nAcknowledgements\\nOriginal study: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2820499/\\nData downloaded form UCI ML repository:\\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\\nInspiration\\nDrug development is expensive. Use this virtual bio assay data to classify compounds as hits (active) against their biological targets.\",\n",
       " \"Fireballs and bolides are astronomical terms for exceptionally bright meteors that are spectacular enough to to be seen over a very wide area. A world map shows a visual representation of the data table that provides a chronological data summary of fireball and bolide events provided by U.S. Government sensors. Ground-based observers sometimes also witness these events at night, or much more rarely in daylight, as impressive atmospheric light displays. This website is not meant to be a complete list of all fireball events. Only the brightest fireballs are noted.\\nContent\\nThe accompanying table provides information on the date and time of each reported fireball event with its approximate total optical radiated energy and its calculated total impact energy. When reported, the event’s geographic location, altitude and velocity at peak brightness are also provided. Note that data are not provided in real-time and not all fireballs are reported. A blank (empty) field in the table indicates the associated value was not reported.\\nFor more information about fireballs and bolides, please see https://cneos.jpl.nasa.gov/fireballs/intro.html.\\nField legend\\nPeak Brightness Date/Time (UT) The date and time in UT (Universal Time) of this event's peak brightness.\\nLatitude (deg.) Geodetic latitude in degrees north (N) or south (S) of the equator for this event.\\nLongitude (deg.) Geodetic longitude in degrees east (E) or west (W) of the prime meridian for this event.\\nAltitude (km) Altitude in kilometers (km) above the reference geoid for this event.\\nVelocity (km/s) The magnitude of the meteor's pre-impact velocity in kilometers per second (km/s).\\nVelocity Components (km/s) The magnitude of the meteor's pre-impact velocity in a geocentric Earth-fixed reference frame defined as follows: the z-axis is directed along the Earth's rotation axis towards the celestial north pole, the x-axis lies in the Earth's equatorial plane, directed towards the prime meridian, and the y-axis completes the right-handed coordinate system.\\nTotal Radiated Energy (J) The approximate total radiated energy in the atmosphere in Joules [a unit of energy given in kilograms times velocity squared, or kg × (m/s)2]\\nCalculated Total Impact Energy (kt) The impact energy of the event in kilotons of TNT (kt) computed from an empirical expression relating radiated and impact energy\\nAcknowledgements\\nThis dataset was kindly made available by NASA. You can find the original dataset at https://cneos.jpl.nasa.gov/fireballs/\\nYou might also be interested in their Planetary Defense FAQ.\",\n",
       " \"About this Data\\nThis is a list of 10,000 different food listings and their ingredients provided by Datafiniti's Product Database. The dataset includes the brand, name, manufacturer, category, features, and more for each product.\\nWhat You Can Do With This Data\\nA similar dataset was used to determine if it's cheaper to eat in or eat out. Discover insights into ingredients used in various foods. E.g.:\\nWhat's the distribution of the number of ingredients per listing?\\nWhat are the most common ingredients used?\\nWhat is the total cost of ingredients needed for a homemade meal versus restaurant meal?\\nData Schema\\nA full schema for the data is available in our support documentation.\\nAbout Datafiniti\\nDatafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.\\nWant More?\\nYou can get more data like this by joining Datafiniti or requesting a demo.\",\n",
       " \"Context\\nThis is a pre-crawled dataset, taken as subset of a bigger dataset (more than 3.7 million products) that was created by extracting data from jcpenney.com, a well known retailer.\\nContent\\nThis dataset has following fields:\\nsku\\nname_title\\ndescription\\nlist_price\\nsale_price\\ncategory\\ncategory_tree\\naverage_product_rating\\nproduct_url\\nproduct_image_urls\\nbrand\\ntotal_number_reviews\\nreviews\\nAcknowledgements\\nThis dataset was created by PromptCloud's in-house web-crawling service.\\nInspiration\\nAnalyses of list price, sale price, rating and reviews can be performed.\",\n",
       " \"Context\\nOpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.\\nContent\\nThis dataset contains one data file for each of these countries:\\nStates included in this dataset:\\nField descriptions:\\nLON - Longitude\\nLAT - Latitude\\nNUMBER - Street number\\nSTREET - Street name\\nUNIT - Unit or apartment number\\nCITY - City name\\nDISTRICT - ?\\nREGION - ?\\nPOSTCODE - Postcode or zipcode\\nID - ?\\nHASH - ?\\nAcknowledgements\\nData collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).\\nAddress data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.\\nData licenses can be found in LICENSE.txt.\\nData source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources\\nInspiration\\nUse this dataset to create maps in conjunction with other datasets to map weather, crime, or how your next canoing trip.\",\n",
       " 'Background\\nNLP is a hot topic currently! Team AI really want\\'s to leverage the NLP research and this an attempt for all the NLP researchers to explore exciting insights from bilingual data\\nThe Japanese-English Bilingual Corpus of Wikipedia\\'s Kyoto Articles” aims mainly at supporting research and development relevant to high-performance multilingual machine translation, information extraction, and other language processing technologies.\\nUnique Features\\nA precise and large-scale corpus containing about 500,000 pairs of manually-translated sentences. Can be exploited for research and development of high-performance multilingual machine translation, information extraction, and so on.\\nThe three-step translation process (primary translation -> secondary translation to improve fluency -> final check for technical terms) has been clearly recorded. Enables observation of how translations have been elaborated so it can be applied for uses such as research and development relevant to translation aids and error analysis of human translation.\\nTranslated articles concern Kyoto and other topics such as traditional Japanese culture, religion, and history. Can also be utilized for tourist information translation or to create glossaries for travel guides.\\nThe Japanese-English Bilingual Kyoto Lexicon is also available. This lexicon was created by extracting the Japanese-English word pairs from this corpus.\\nSample\\nOne Wikipedia article is stored as one XML file in this corpus, and the corpus contains 14,111 files in total.\\nThe following is a short quotation from a corpus file titled “Ryoan-ji Temple”. Each tag has different implications. For example:\\n<j>Original Japanese sentence<j> <e type=\"trans\" ver=\"1\">Primary translation</e> <e type=\"trans\" ver=\"2\">Secondary translation</e> <e type=\"check\" ver=\"1\">Final translation</e> <cmt>Comment added by translators</cmt>\\nCategories\\nThe files have been divided into 15 categories: school, railway, family, building, Shinto, person name, geographical name, culture, road, Buddhism, literature, title, history, shrines and temples, and emperor (Click the link to view a sample file for each category).\\nGithub\\nhttps://github.com/venali/BilingualCorpus\\nExplains how to load the corpus\\nAcknowledgements\\nThe National Institute of Information and Communications Technology (NICT) has created this corpus by manually translating Japanese Wikipedia articles (related to Kyoto) into English.\\nLicence\\nUse and/or redistribution of the Japanese-English Bilingual Corpus of Wikipedia\\'s Kyoto Articles and the Japanese-English Bilingual Kyoto Lexicon is permitted under the conditions of Creative Commons Attribution-Share-Alike License 3.0. Details can be found at http://creativecommons.org/licenses/by-sa/3.0/.\\nLink to web\\nhttps://alaginrc.nict.go.jp/WikiCorpus/index_E.html',\n",
       " \"DW-Nominate scores of congressional voting behavior regularly appears in media such as the New York Times, Washington Post, and 538. This dataset contains the voting records used to generate those scores and additional features related to the DW-NOMINATE calculations.\\nContent\\nThis dataset contains descriptive data as well as ideological data for congressional rollcalls, individual member votes, members of congress, and parties. You can find information such the descriptions of rollcalls, what proportion of voting members were correctly classified by the ideological cutting line for that rollcall, the ideological position of members of congress, and more.\\nBoth the rollcall data and the data on members are split into chambers and congresses. The data on parties is a dataset with some metadata about all of the different parties as well as their average ideological position and membership size broken down by congress and chamber.\\nThe full details behind the DW-NOMINATE calculations may be helpful in interpreting some of this data. The technical details of the DW-NOMINATE model can be found in Poole's Spatial Models of Parliamentary Voting. Poole and Rosenthal's Ideology and Congress explores the nature of voting in Congress and the political history of the United States through the lens of the ideological dimensions recovered by DW-NOMINATE.\\nAcknowledgements\\nThis dataset was prepared by the team at VoteView. Please visit their site if you require up-to-date records. You may also be interested in their blog.\\nInspiration\\n-Using national scores as a training set, can you develop polarization scores for you own state legislature? -Can you find correlates that help explain changes in DW-NOMINATE scores?\",\n",
       " \"Unlike other Kaggle competitions, the leaderboard for March Madness changes as the NCAA Basketball tournament progresses. Part of the fun is seeing how the rankings will change due to upcoming games. To enable easier analysis, I've cleansed and processed the predictions dataset that William Cukierski created.\",\n",
       " 'Context\\nThis dataset is created for the deep learning based Devanagari character recognition research evaluation, 2015.\\nContent\\nThe dataset contains mixed categories for Devanagari numerals (10 classes) and consonants (36 classes). Dataset is explicitly separated into train and test set. Train set contains total 78,200 samples with 1700 samples per class for total 46 classes and test set contains total 13,800 samples with 300 samples per class for total 46 classes. Dataset is collected from the school level students.\\nThis dataset is collected and maintained by the following research members,\\nShailesh Acharya\\nAshok Kumar Pant\\nPrashnna Kumar Gyawali\\nCitation\\nPlease cite in your publications if it helps your research:\\n@inproceedings{ashok2015deep, \\n    author={S. Acharya and A. K. Pant and P. K. Gyawali}, \\n    booktitle={2015 9th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)}, \\n    title={Deep learning based large scale handwritten Devanagari character recognition}, \\n    year={2015}, \\n    pages={1-6},\\n    month={Dec}\\n }',\n",
       " 'Mercedes-Benz Greener Manufacturing Competition\\nMercedes-Benz hosted a Kaggle competition in June-July 2017. Participants were invited to predict the time it takes to test the car using an anonymized set of 377 variables.\\nIn this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing.\\nWhat is in the dataset\\nCurrent dataset represents Public and Private Leaderboard standings for every day of competition (from 31-May-2017 to 11-07-2017). Data has been collected from Kaggle leaderboard tables after the competition has ended.\\nAcknowledgements\\nWe would like to thank @BreakfastPirat for inspiration, Daimler team for hosting the competition, Kaggle team for support and all fellow participants in the Mercedes data challenge for their courage and perseverance.\\nQuestions for Inspiration\\n1) Validate the leaderboard shakeup statistics\\n2) Try to reproduce the leaderboard progression visualizations\\n3) When did the teams start overfitting? What makes one prone to overfitting?\\n4) Investigate public submissions statistics. Can the pattern of submissions predict the final score?\\n5) Can you detect which teams have has a robust cross-validation strategy from their submissions stats?',\n",
       " \"From Peter Norvig's classic How to Write a Spelling Corrector\\nOne week in 2007, two friends (Dean and Bill) independently told me they were amazed at Google's spelling correction. Type in a search like [speling] and Google instantly comes back with Showing results for: spelling. I thought Dean and Bill, being highly accomplished engineers and mathematicians, would have good intuitions about how this process works. But they didn't, and come to think of it, why should they know about something so far outside their specialty?\\nI figured they, and others, could benefit from an explanation. The full details of an industrial-strength spell corrector are quite complex (you can read a little about it here or here). But I figured that in the course of a transcontinental plane ride I could write and explain a toy spelling corrector that achieves 80 or 90% accuracy at a processing speed of at least 10 words per second in about half a page of code.\\nA Kernel has been added with Peter's basic spell.py and evaluation code to set a baseline. Minimal modifications were made so that it runs on this environment.\\nData files\\nbig.txt is required by the code. That's how it learns the probabilities of English words. You can prepend more text data to it, but be sure to leave in the little Python snippet at the end.\\nTesting files\\nThe other files are for testing the accuracy. The baseline code should get 75% of 270 correct on spell-testset1.txt, and 68% of 400 correct on spell-testset2.txt.\\nI've also added some other files for more extensive testing. The example Kernel runs all of them but birkbeck.txt by default. Here's the output:\\nTesting spell-testset1.txt\\n75% of 270 correct (6% unknown) at 32 words per second \\nTesting spell-testset2.txt\\n68% of 400 correct (11% unknown) at 28 words per second \\nTesting wikipedia.txt\\n61% of 2455 correct (24% unknown) at 21 words per second \\nTesting aspell.txt\\n43% of 531 correct (23% unknown) at 15 words per second \\nThe larger datasets take a few minutes to run. birkbeck.txt takes more than a few minutes.\\nYou can try adding other datasets, or splitting these ones in meaningful ways - for example a dataset of only words of 5 characters or less, or 10 characters or more, or without uppercase - to understand the effect of changes you make on different types of words.\\nLanguages\\nThe data and testing files include English only for now. In principle it is easily generalisable to other languages.\",\n",
       " 'Context\\nGoodreads is the world’s largest site for readers and book recommendations. Their mission is to help people find and share books they love. In addition to tracking the books you\\'re reading, have read, and want to read, users on Goodreads can see what their friends are reading, get personalized recommendations, and browse community reviews. The website is also a great place to look for inspirational quotes from authors.\\nContent & Inspiration\\nThis dataset contains the most popular and recent quotes shared on Goodreads. Along with each quote, you\\'ll get its tags, the number of likes it received, and the author of the quote. You can use this dataset to study which tags receive the most likes, what characteristics the most popular quotes have in common, use NLP techniques like sentiment analysis, or even generate your own novel quotes (pun intended).\\nBefore you begin, an inspirational quote from Arthur Conan Doyle, Sherlock Holmes:\\n\"It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts.\"',\n",
       " 'This dataset consists of 5547 breast histology images of size 50 x 50 x 3, curated from Andrew Janowczyk website and used for a data science tutorial at Epidemium. The goal is to classify cancerous images (IDC : invasive ductal carcinoma) vs non-IDC images.',\n",
       " 'Context\\nThe assembly election results for Uttar Pradesh(UP) were surprising to say the least. Never in the past has any single party secured a similar mandate. UP with a population of around 220 million is as big as the whole of united states. It has 403 constituencies each having its own demographic breakup. The election was conducted in 7 phases.\\nContent\\nThe dataset has 8 variables:\\nseat_allotment: As some of you might be aware that there was a coalition between INC and SP which materialized pretty late into the campaign. Hence, in a few constituencies the high command of the 2 parties could not convince contestants to forfeit their nomination. In such constituencies, there is a situation that is called a friendly fight(FF) where candidates from both parties INC and SP are contesting instead of just one. These constituencies are marked by the flag FF (Friendly Fight). Others are INC (contested by INC), SP(Contested by SP) and DNC(Contested by none)\\nphase: The phase in which the election was conducted.\\nac_no: Assembly constituency number\\nac: Assembly constituency name\\ndistrict: District to which the ac belongs\\ncandidate: Candidate name\\nparty: Party name\\nvotes: votes for each candidate\\nSource: Scraped from eciresults.nic.in',\n",
       " \"This dataset was obtained here and their description is reproduced below.\\nAstronomical background\\nGalaxies are fundamental structures in the Universe. Our Sun lives in the Milky Way Galaxy we can see as a patchy band of light across the sky. The components of a typical galaxy are: a vast number of stars (total mass ~106-1011 Mo where Mo is the unit of a solar mass), a complex interstellar medium of gas and dust from which stars form (typically 1-100% of the stellar component mass), a single supermassive black hole at the center (typically <1% of the stellar component mass), and a poorly understood component called Dark Matter with mass ~5-10-times all the other components combined.\\nOver the ~14 billion years since the Big Bang, the rate at which galaxies convert interstellar matter into stars has not been constant, and thus the brightness and color of galaxies change with cosmic time. This phenomenon has several names in the astronomical community: the history of star formation in the Universe, chemical evolution of galaxies, or simply galaxy evolution. A major effort over several decades has been made to quantify and understand galaxy evolution using telescopes at all wavelengths.\\nThe traditional tool for such studies has been optical spectroscopy which easily reveals signatures of star formation in nearby galaxies. However, to study star formation in the galaxies recently emerged after the Big Bang, we must examine extremely faint galaxies which are too faint for spectroscopy, even using the biggest available telescopes. A feasible alternative is to obtain images of faint galaxies at random locations in the sky in narrow spectral bands, and thereby construct crude spectra. First, statistical analysis of such multiband photometric datasets are used to classify galaxies, stars and quasars. Second, for the galaxies, multivariate regression is made to develop photometric estimates of redshift, which is a measure both of distance from us and age since the Big Bang. Third, one can examine galaxy colors as a function of redshift (after various corrections are made) to study the evolution of star formation. The present dataset is taken after these first two steps are complete.\\nContents\\nWolf et al. (2004) provide the first public catalog of a large dataset (63,501 objects) with brightness measurements in 17 bands in the visible band. (Note that the Sloan Digital Sky Survey provides a much larger dataset of 108 objects with measurements in 5 bands.) We provide here a subset of their catalog with 65 columns of information on 3,462 galaxies. These are objects in the Chandra Deep Field South field which Wolf and colleagues have classified as `Galaxies'. The column headings are formally described in their Table 3, and the columns we provide are summarized here with brief commentary:\\nCol 1: Nr, object number\\nCol 2-3: Total R (red band) magnitude and its error. This was the band at which the basic catalog was constructed. Magnitudes are inverted logarithmic measures of brightness. A galaxy with R=21 is 100-times brighter than one with R=26. The error is the standard deviation derived from detailed knowledge of the measurement process. This dataset is an excellent example of astronomical datasets where each variable is accompanied by heteroscedastic measurement errors of known variances.\\nCol 4-5: ApDRmag is the difference between the total and aperture magnitude in the R band. This is a rough measure of the size of the galaxy in the image where ApDRmag=0 corresponds to a point source. Negative values are not physically meaningful. mu_max is the central surface brightness of the object in the R band. The difference between Rmag and mu_max should also be an indicator of galaxy size.\\nCol 6-9: Mcz and MCzml are two redshift estimates. Mcz is the preferred value. e.Mcz is its estimated error, and chi2red is the reduced chi-squared value of the least-squares fit of the 17-band magnitudes to the best-fit template galaxy spectrum. Galaxies with large e.Mcz or chi2red might be omitted as unreliable.\\nCol 10-29: These give the absolute magnitudes (i.e. intrinsic luminosities) of the galaxy in 10 bands, with their measurement errors. They are based on the measured magnitudes and the redshifts, and represent the intrinsic luminosities of the galaxies; a galaxy with M=-15 is 100-times less luminous than one with M=-20. These magnitudes are not all independent of each others, but the are important for representing intrinsic properties of the galaxies. Below is one of several redshift-stratified plots of the B-band absolute magnitude (abscissa) against the difference of magnitude (i.e. ratio of luminosities) between the 2800A ultraviolet and blue band, which is a sensitive indicator of star formation. A redshift-dependent bimodal distribution is seen.\\nCol 30-55: Observed brightnesses in 13 bands in sequence from 420 nm in the ultraviolet to 915 nm in the far red. These are given in linear variables with units of photon flux densities, photons/m2/s/nm. Again, each measurement is accompanied by a measurement error which can be used to distinguish measurement from intrinsic dispersions in the distributions.\\nCol 56-65: Observed brightnesses in 5 traditional broad spectral bands, UBVRI. These are largely redundant with the 13 bands in the previous columns.\\nStatistical exercises\\nExamine basic characteristics of the survey which are not of scientific interest. These include the absence of high-redshift (i.e. distant) high-absolute-magnitude (i.e. faint) galaxies; the dropoff in flux with redshift; the dropoff in image size with redshift.\\nStudy these two populations as a function of redshift to investigate the evolution of star formation.\",\n",
       " \"Context\\nThis dataset includes some of the basic information of the websites we daily use. While scrapping this info, I learned quite a lot in R programming, system speed, memory usage etc. and developed my niche in Web Scrapping. It took about 4-5 hrs for scrapping this data through my system (4GB RAM) and nearly about 4-5 days working out my idea through this project.\\nContent\\nThe dataset contains Top 50 ranked sites from each 191 countries along with their traffic (global) rank. Here, country_rank represent the traffic rank of that site within the country, and traffic_rank represent the global traffic rank of that site.\\nSince most of the columns meaning can be derived from their name itself, its pretty much straight forward to understand this dataset. However, there are some instances of confusion which I would like to explain in here:\\n1) most of the numeric values are in character format, hence, contain spaces which you might need to clean on.\\n2) There are multiple instances of same website. for.e.g. Yahoo. com is present in 179 rows within this dataset. This is due to their different country rank in each country.\\n3)The information provided in this dataset is for the top 50 websites in 191 countries as on 25th May 2017 and is subjected to change in future time due to the dynamic structure of ranking.\\n4) The dataset inactual contains 9540 rows instead of 9550(50*191 rows). This was due to the unavailability of information for 10 websites.\\nPS: in case if there are anymore queries, comment on this, I'll add an answer to that in above list.\\nAcknowledgements\\nI wouldn't have done this without the help of others. I've scrapped this information from publicly available (open to all) websites namely: 1) http://data.danetsoft.com/ 2) http://www.alexa.com/topsites , of which i'm highly grateful. I truly appreciate and thanks the owner of these sites for providing us with the information that I included today in this dataset.\\nInspiration\\nI feel that there this a lot of scope for exploring & visualization this dataset to find out the trends in the attributes of these websites across countries. Also, one could try predicting the traffic(global) rank being a dependent factor on the other attributes of the website. In any case, this dataset will help you find out the popular sites in your area.\",\n",
       " \"Context\\nJust throwing it out their for anyone and everyone that is interested in heart disease.\\nContent\\nDataset consisting of 14 attributes and 303 observations that were used for the purpose of heart disease classification of a given patient.\\nAcknowledgements\\nHungarian Institute of Cardiology. Budapest: Andras Janosi, M.D. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D\\nInspiration\\nI'd like to have outside input on this model and create a hybrid classifier that can be used by MD's in underserved communities.\",\n",
       " \"# Context\\nFirst, I am new to ML, and just in case I slip up, apologies in advance!! So, I am doing an online ML course and this is an assignment where we are supposed to practice scikit-learn's PCA routine. Since the course has been ARCHIVED - which means the discussion posts are not answered!! - hence my posting of the problem here.\\nWhat better way to learn than to get so many experts giving me feedback ... right?\\n# Content\\nThe data was taken over a 2-month period in India with 25 features ( eg, red blood cell count, white blood cell count, etc). The target is the 'classification', which is either 'ckd' or 'notckd' - ckd=chronic kidney disease. There are 400 rows\\nThe data needs cleaning: in that it has NaNs and the numeric features need to be forced to floats. Basically, we were instructed to get rid of ALL ROWS with Nans, with no threshold - meaning, any row that has even one NaN, gets deleted.\\nPart 1: We are asked to choose 3 features (bgr, rc, wc), visualize them, then run the PCA with n_components=2. the PCA is to be run twice: one with no scaling and the second run WITH scaling. And this is where my issue starts ... in that after scaling I can hardly see any difference!\\nI will stop here for now till I get feedback and then move to Part 2.\\nAcknowledgements\\nThe dataset is available at: https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease\\nInspiration\\nI would like to get an intuitive and a practical understanding of PCA.\",\n",
       " \"Context\\nnifty50.csv The NIFTY 50 index is National Stock Exchange of India's benchmark stock market index for Indian equity market. It is a well diversified 50 stock index accounting for 22 sectors of the economy. It is used for a variety of purposes such as bench-marking fund portfolios, index based derivatives and index funds.\\nbanknifty.csv Bank Nifty represents the 12 most liquid and large capitalized stocks from the banking sector which trade on the National Stock Exchange (NSE). It provides investors and market intermediaries a benchmark that captures the capital market performance of Indian banking sector.\\nContent\\nA data frame with 8 variables: index, date, time, open, high, low, close and id. For each year from 2013 to 2016, the number of trading data of each minute of given each date. The currency of the price is Indian Rupee (INR).\\nindex : market id\\ndate: numerical value (Ex. 20121203- to be converted to 2012/12/03)\\ntime: factor (Ex. 09:16)\\nopen: numeric (opening price)\\nhigh: numeric (high price)\\nlow: numeric (low price)\\nclose: numeric (closing price)\\nInspiration\\nInitial raw data sets are very complex and mixed datatypes. These are processed properly using R libraries like dplyr, stringr and other data munging packages. The desired outputs are then converted into a CSV format to use for further analysis.\",\n",
       " \"Context\\nData gathered from the James Comey testimony to the Senate Intelligence Committee on June 8th, 2017 regarding possible Russian influence in the 2016 U.S. presidential election.\\nContent\\nContent includes the full transcript in transcript.csv as well as a breakdown of questions asked by each senator, their political affiliation, and Comey's response.\\nAll CSVs are UTF-8.\\nAcknowledgements\\nRod Castor - Initial Python script. AppliedAI.\\nInspiration\\nInitially I did analysis to determine length of question by party, length of Comey response by party, number of times each word is used and words with a large difference by party. (Clinton used 16x more by Republicans).\\nFurther analysis to follow as time permits.\",\n",
       " 'Context\\nChemicals have health effects: some recognized and some suspected. A comprehensive list of those chemicals and their health effects would be beneficial to those wishing to associate those chemicals\\' health effects with other sets of data. The datasets uploaded here represent the \"blending\" of various individual chemical health effect datasets as compiled by Scorecard and hosted by the GoodGuide web site.\\nContent\\nAs a general rule, each dataset has a chemical\\'s Chemical Abstract Society Registry Number (CASRN), for example \"100-00-5\", its name (e.g. \"P-NITROCHLOROBENZENE\"), one or more health effect categories (e.g. \"recognized\" or \"suspected\"), and one or more health effects (e.g. \"kidney\" and/or \"neurotoxicity\") , as well as the organization of provenance (e.g. \"HAZMAP\" and/or \"RTECS\") for the recognized and/or suspected health effects.\\nAcknowledgements\\nA team from the Environmental Defense Fund created the individual datasets that presently reside at GoodGuide\\'s Scorecard\\'s Health Effects web page. The \"blended\" datasets uploaded herein are various compilations of those individual datasets into ones more suitable for inclusion in data analyses and visualizations.\\nInspiration\\nInitially, the \"blended\" datasets were used to associate health effects with fracking well chemical disclosures, as such joining of data was not readily available to most data analysts. Examples of such datasets can be found at FrackingData.org\\'s FracFocus Data web page.',\n",
       " 'Context\\nBehavioral Context refers to a wide range of attributes describing what is going on with you: where you are (home, school, work, at the beach, at a restaurant), what you are doing (sleeping, eating, in a meeting, computer work, exercising, shower), who you are with (family, friends, co-workers), your body posture state (sitting, standing, walking, running), and so on. The ability to automatically (effortlessly, frequently, objectively) recognize behavioral context can serve many domains. Medical applications can monitor physical activity or eating habits; aging-at-home programs can log older adults\\' physical, social, and mental behavior; personal assistant systems can better server the user if they are aware of the context. In-the-wild (in real life), natural behavior is complex, composed of different aspects, and has high variability. You can run outside at the beach, with friends with your phone in the pocket; you can also run indoors, at the gym, on a treadmill, with your phone motionless next to you. This high variability makes context-recognition a hard task to perform in-the-wild.\\nContent\\nThe ExtraSensory Dataset was collected from 60 participants where each person participated approximately 7 days. We installed our data-collection mobile app on their personal phone and it was used to collect both sensor-measurements and context-labels. The sensor-measurements were recorded automatically for a window of 20-seconds every minute. This included accelerometer, gyroscope, magnetometer, audio, location, and phone-state from the person\\'s phone, as well as accelerometer and compass from an additional smartwatch that we provided. In addition, the app\\'s interface had many mechanisms for self-reporting the relevant context-labels, including reporting past context, near future, responding to notifications, and more. The flexible interface allowed to collect many labels with minimal effort and interaction-time, to avoid interfering with the natural behavior. The data was collected in-the-wild: participants used their phone in any way that was convenient to them, they engaged in their regular behavior and reported an combinations of labels that fit their context.\\nFor every participant (or \"user\"), the dataset has a CSV file with pre-computed features that we extracted from the sensors and with labels. Each row has a separate example (representing 1 minute) and is indexed by the timestamp (seconds since the epoch). There are columns for the sensor-features, with the prefix of the column name indicating the sensor it came from (e.g. prefix \"raw_acc:\" indicating a feature came from the raw phone accelerometer measurements). There are columns for 51 diverse context-labels and the value for an example-label pair is either 1 (the label is relevant for the example), 0 (the label is not relevant), or \\'NaN\\' (missing information).\\nHere, we provide data for 2 of the 60 participants. You can use this partial data to get familiar with the data and practice algorithms. The full dataset is publicly available at http://extrasensory.ucsd.edu. The website has additional parts of the data (such as a wider range of the original reported labels, location coordinates, mood labels from part of the participants). If you use the data for your publications, you are required to cite our original paper Vaizman, Y., Ellis, K., and Lanckriet, G. \"Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches\". IEEE Pervasive Computing, vol. 16, no. 4, October-December 2017, pp. 62-74. Read the information at http://extrasensory.ucsd.edu and the original paper for more details.\\nAcknowledgements\\nThe dataset was collected by Yonatan Vaizman and Katherine Ellis, under the supervision of prof. Gert Lanckriet, all from the department of Electrical and Computer Engineering, University of California, San Diego.\\nInspiration\\nThe ExtraSensory Dataset can serve as a benchmark to compare methods for context-recognition (or context-awareness, activity recognition, daily activity detection). You can focus on specific sensors or on specific context-labels. You can suggest new models and classifiers, train them on the data and evaluate their performance on the data.',\n",
       " 'Context:\\nThe Occupational Employment Statistics (OES) and National Compensation Survey (NCS) programs have produced estimates by borrowing from the strength and breadth of each survey to provide more details on occupational wages than either program provides individually. Modeled wage estimates provide annual estimates of average hourly wages for occupations by selected job characteristics and within geographical location. The job characteristics include bargaining status (union and nonunion), part- and full-time work status, incentive- and time-based pay, and work levels by occupation.\\nDirect estimates are based on survey responses only from the particular geographic area to which the estimate refers. In contrast, modeled wage estimates use survey responses from larger areas to fill in information for smaller areas where the sample size is not sufficient to produce direct estimates. Modeled wage estimates require the assumption that the patterns to responses in the larger area hold in the smaller area.\\nThe sample size for the NCS is not large enough to produce direct estimates by area, occupation, and job characteristic for all of the areas for which the OES publishes estimates by area and occupation. The NCS sample consists of 6 private industry panels with approximately 3,300 establishments sampled per panel, and 1,600 sampled state and local government units. The OES full six-panel sample consists of nearly 1.2 million establishments.\\nThe sample establishments are classified in industry categories based on the North American Industry Classification System (NAICS). Within an establishment, specific job categories are selected to represent broader occupational definitions. Jobs are classified according to the Standard Occupational Classification (SOC) system.\\nContent:\\nSummary: Average hourly wage estimates for civilian workers in occupations by job characteristic and work levels. These data are available at the national, state, metropolitan, and nonmetropolitan area levels.\\nFrequency of Observations: Data are available on an annual basis, typically in May.\\nData Characteristics: All hourly wages are published to the nearest cent.\\nAcknowledgements:\\nThis dataset was taken directly from the Bureau of Labor Statistics and converted to CSV format.\\nInspiration:\\nThis dataset contains the estimated wages of civilian workers in the United States. Wage changes in certain industries may be indicators for growth or decline. Which industries have had the greatest increases in wages? Combine this dataset with the Bureau of Labor Statistics Consumer Price Index dataset and find out what kinds of jobs you would need to afford your snacks and instant coffee!',\n",
       " 'Content\\nThe Supreme Court database is the definitive source for researchers, students, journalists, and citizens interested in the United States Supreme Court. The database contains more than two hundred variables regarding each case decided by the Court between the 1946 and 2015 terms. Examples include the identity of the court whose decision the Supreme Court reviewed, the parties to the suit, the legal provisions considered in the case, and the votes of the Justices. The database codebook is available here.\\nAcknowledgements\\nThe database was compiled by Professor Spaeth of Washington University Law and funded with a grant from the National Science Foundation.',\n",
       " 'Content\\nThe Satellite Database is a listing of active satellites currently in orbit around the Earth. The database includes basic information about the satellites and their orbits, but does not contain the detailed information necessary to locate individual satellites. The information included in the database is publicly accessible and free and was collected from corporate, scientific, government, military, non-governmental, and academic websites available to the public. No copyrighted material was used, nor did we subscribe to any commercial databases for information.\\nWe have attempted to include all currently active satellites. However, satellites are constantly being launched, decommissioned, or simply abandoned, and the list may inadvertently contain some satellites that are no longer active but for which we have not yet received information.\\nAcknowledgements\\nThe Satellite Database is produced and updated quarterly by Teri Grimwood.',\n",
       " \"Context\\nThousands gather at Gobbler’s Knob in Punxsutawney, Pennsylvania, on the second day of February to await the spring forecast from a groundhog known as Punxsutawney Phil. According to legend, if Phil sees his shadow the United States is in store for six more weeks of winter weather. But, if Phil doesn’t see his shadow, the country should expect warmer temperatures and the arrival of an early spring.\\nAcknowledgements\\nThe historical weather predictions were provided by the Punxsutawney Groundhog Club, and the average monthly temperatures were published by NOAA's National Climatic Data Center.\",\n",
       " \"Context:\\nThe New York Philharmonic played its first concert on December 7, 1842. Since then, it has merged with the New York Symphony, the New/National Symphony, and had a long-running summer season at New York's Lewisohn Stadium. The Performance History database documents all known concerts of all of these organizations, amounting to more than 20,000 performances.\\nContent:\\nDataset is a single csv with over 800k rows. Data contains information on season, orchestra, venue, date, time, conductor, work title, composer, movement, and soloists.\\nAcknowledgements:\\nThis dataset was compiled by the New York Philharmonic. Original json files hosted here. Original json files were flattened and joined on guid to form a single csv file. Image courtesy of Larisa Birta.\\nInspiration:\\nNearly 175 years of performance history, covering over 11k unique works--which composers are most popular? Have there been any trends in popularity by conductor or by season?\",\n",
       " 'The Challenge\\nThe online sports gambling industry employs teams of data analysts to build forecast models that turn the odds at sports games in their favour. While several betting strategies have been proposed to beat bookmakers, from expert prediction models and arbitrage strategies to odds bias exploitation, their returns have been inconsistent and it remains to be shown that a betting strategy can outperform the online sports betting market. We designed a strategy to beat football bookmakers with their own numbers:\\n\"Beating the bookies with their own numbers - and how the online sports betting market is rigged\", by Lisandro Kaunitz, Shenjun Zhong and Javier Kreiner.\\nHere, we make the full dataset publicly available to the Kaggle community. We also provide the codes, raw SQL database and the online real-time dashboard that were used for our study on github.\\nOur strategy proved profitable in a 10-year historical simulation using closing odds, a 6-month historical simulation using minute to minute odds, and a 5-month period during which we staked real money with the bookmakers. We would like to challenge the Kaggle community to improve our results:\\nCan your strategy consistently beat the sports betting market over thousands of bets across leagues around the world?\\nDo time series odds movements offer insightful information that a betting strategy can exploit?\\nCan you outperform the bookmakers’ predictions included in the odds data by creating a better model?\\nWhat\\'s inside the Beat The Bookie dataset\\n10 year historical closing odds:\\n479,440 football games from 818 leagues around the world\\nGames from 2005-01-01 to 2015-07-30.\\nMaximum, average and count of active odds at closing time (start of the match)\\nBetting odds from up to 32 providers\\nDetails about the match: date and time, league, teams, 90-minute score\\n14-months time series odds:\\n92,647 football games from 1005 leagues around the world\\nGames from 2015-09-01 to 2016-11-22\\nHourly sampled odds time series, from up to 32 bookmakers from 72 hours before the start of each game\\nDetails about the match: date and time, league, teams, 90-minute score\\nThe dataset was assembled over months of scraping online sport portals.\\nWe hope you enjoy your sports betting simulations (but remember... the house always wins in the end).\\nAcknowledgements\\nBen Fulcher was of great help when we were drafting the paper. Ben has also developed a very nice toolbox for time-series analysis, which might be relevant for the analysis of this dataset.',\n",
       " \"This a blend dataset that contains historic Swedish interest rates from 1908-2001 Source/Källa: Sveriges riksbank and Swedish inflation rate 1908-2001 fetched from Sweden's statistic central bureau SCB.\\nContent: Blend of Swedish historic central bank interest rate diskkonto and Swedish SCB Consument price index\\nAcknowledgements / Original data sets:\\nSwedish central bank interest rate diskkonto http://www.riksbank.se/sv/Rantor-och-valutakurser/Sok-rantor-och-valutakurser/\\nConsumer price index http://www.scb.se/sv_/Hitta-statistik/Statistik-efter-amne/Priser-och-konsumtion/Konsumentprisindex/Konsumentprisindex-KPI/33772/33779/Konsumentprisindex-KPI/33831/\\nData set cover images: Wikipedia https://sv.wikipedia.org/wiki/Enkronan#/media/File:1_Krona_1927,_1.jpg https://en.wikipedia.org/wiki/Flag_of_Sweden#/media/File:Flag_of_Sweden.svg\\nInspiration: Question: How does central bank interest rate effect inflation? What are the interest rate inflation rate delays? Verify ROC R^2 inflation/interest rate causation.\\nContent:\\nInterestrate and inflation Sweden 1908-2001.csv\\nColumns\\nPeriod Year\\nCentral bank interest rate diskonto average Percent\\nInflation Percent\\nPrice level Integer\",\n",
       " 'This dataset is taken from UCI : https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\\nThe dataset in UCI is imbalanced. A balanced sample was taken from that dataset to create this. Features remain the same as the original one.',\n",
       " \"Context\\nThe Brazilian government compiles motor insurance data from ago/2000 to ago/2016 and makes it available for public consumption.\\nContent\\nThere's information about the performance of Brazilian insurance motor like Premium, claim and commission.\\nAcknowledgements\\nSUSEP is a governmental office responsible for collect, house and share this information: http://www2.susep.gov.br/menuestatistica/SES/principal.aspx\\nInspiration\\nThis information can answer questions about performance and trends regarding to Brazilian motor line of business and make user able to gain insight about this market.\\nI know this data well but my inspiration to share this data on Kaggle is to discuss and see different points of view.\",\n",
       " \"Context\\nAudio classification is often proposed as MFCC classification problem. With this dataset, we intend to give attention to raw audio classification, as performed in the Wavenet network.\\nContent\\nThe dataset consists in 50 WAV files sampled at 16KHz for 50 different classes.\\nTo each one of the classes, corresponds 40 audio sample of 5 seconds each. All of these audio files have been concatenated by class in order to have 50 wave files of 3 min. 20sec.\\nIn our example notebook, we show how to access the data and visualize a piece of it.\\nAcknowledgements\\nWe have not much credit in proposing the dataset here. Much of the work have been done by the authors of the ESC-50 Dataset for Environmental Sound Classification. In order to fit on Kaggle, we processed the files with the to_wav.py file present in the original repository. You might also notice that we transformed the data from OGG to WAV as the former didn't seem to be supported in Anaconda.\\nInspiration\\nYou might use this dataset to challenge your algorithms in classifying from raw audio ;)\",\n",
       " '1.8 million positions of racing king chess variant\\nRacing kings is a popular chess variant.\\nEach player has a standard set of pieces without pawns. The opening setup is as below.\\nIn this game, check is entirely forbidden: not only is it forbidden to move ones king into check, but it is also forbidden to check the opponents king. The purpose of the game is to be the first player that moves his king to the eight row. When white moves their king to the eight row, and black moves directly after that also their king to the last row, the game is a draw (this rule is to compensate for the advantage of white that they may move first.) Apart from the above, pieces move and capture precisely as in normal chess.\\nTo learn a little bit more about a game and to experience the evaluation of the position, you can play a couple of games here. Do not forget to select Racing kings chess variant and to analyse the game at the end with the machine. Keep in mind that the evaluation score on lichess website is from -10 to 10 and slightly different than in my dataset.\\nWhat you get:\\n2 csv files train.csv and validate.csv with 1.5 mln and ~0.35 mln positions. Both have an identical structure: FEN of the position and the score.\\nThe score is real value in [-1, 1] range. The closer it is to 1/-1, the more probable is the win of a white/black player. Due to the symmetry I will explain the score only for a white player (for black is the same just with a negative sign.\\n1 means that white already won (the game is already finished)\\n0.98 white has a guaranteed(*) win in maximum 1 move\\n0.96 ... 2 moves\\n0.94 ... 3 moves\\n....\\n0.82 ... in 9 moves\\n0.80 ... in 10 or more moves\\nfrom 0.4 to 0.8 - white has big advantage. For a good player it is not hard to win in such situation\\nfrom 0.2 to 0.4 - white has some advantage. Might be hard to convert it to a win\\nfrom 0 to 0.2 - white has tiny advantage\\n0 means that the position is either a draw or very close to a draw\\n(*) Guaranteed means that the machine has found a forced sequence of moves that allows white player to win no matter what moves the opponent will make. If the opponent makes the best moves - the game will finish in x moves, but it can finish faster if the black player makes a mistake.\\nYour goal is to use predict a score of the position knowing its FEN.\\nUse train.csv to build your model and evaluate the performance on the validate.csv dataset (without looking/using it). I used MAE score in my analysis.\\nConstruction of the dataset\\nDataset was constructed by me. I created a bot that plays many games against itself. The bot takes 1 second to analyse the position and selects the move based on the score of position. It took almost a month to generate these positions.\\nWhat is the purpose?\\nCurrently the plan is to use ML + reinforcement learning to build my own chess bot that will not use alpha-beta prunning for position evaluation and self-play. In a couple of days I will release my own findings as kernels.',\n",
       " \"The following dataset contains data on blog posts from MarginalRevolution.com. For posts from Jan. 1, 2010 to 9/17/2016, the following attributes are gathered.\\nAuthor Name\\nPost Title\\nPost Date\\nPost content (words)\\nNumber of Words in post\\nNumber of Comments in post\\nDummy variable for several commonly used categories\\nThe data was scraped using Python's Beautiful Soup package, and cleaned in R. See my github page (https://github.com/wnowak10/) for the Python and R code.\",\n",
       " \"Gives a bunch of data regarding parkinson's disease. Some of these variables have a very high correlation with each other.\",\n",
       " 'Context\\nI was curious about the hot topics in quantum physics as reflected by the quant-ph category on arXiv. Citation counts have a long lag, and so do journal publications, and I wanted a more immediate measure of interest. SciRate is fairly well known in this community, and I noticed that after the initial two-three weeks, the number of Scites a paper gets hardly increases further. So the number of Scites is both immediate and near constant after a short while.\\nContent\\nThe main dataset (scirate_quant-ph.csv) is the metadata of all papers published in quant-ph between 2012-01-01 and 2016-12-31 that had at least ten Scites, as crawled on 2016-12-31. It has six columns:\\nThe id column as exported by pandas.\\nThe arXiv id.\\nThe year of publication.\\nThe month of publication.\\nThe day of publication.\\nThe number of Scites (this column defines the order).\\nThe title.\\nAll authors separates by a semicolon.\\nThe abstract.\\nThe author names were subjected to normalization and the chances are high that the same author only appears with a unique name.\\nThe name normalization was the difficult part in compiling this collection, and this is why the number of Scites was lower bounded. A second file (scirate_quant-ph_unnormalized.csv) includes all papers that appeared between 2012-2016 irrespective of the number of Scites, but the author names are not normalized. The actual number of Scites for each paper may show a slight variation between the two datasets because the unnormalized version was compiled more than a month later.\\nAcknowledgements\\nMany thanks to SciRate for tolerating my crawling trials and not blacklisting my IP address.\\nInspiration\\nUnleash topic models and author analysis to find out what or who is hot in quantum physics today. Build a generative model to write trendy fake titles like SnarXiv does it for hep-th.',\n",
       " 'Creating a rental market database for data analysis and machine learning.\\nHow does it work ?\\nYou scrape the property ads (sale or rent) on internet and you get a dataset.\\nThen 3 fancy solutions are possible:\\nRun your webcrawler everyday for a specific place, upload the data in your data warehouse, and monitor the trends in real estate market prices.\\nApply machine learning to your database and get a sense of the relative expensiveness of the properties.\\nLocalize every property ads on a Google map using color-coded points in order to visualize the most cheap and expensive neighborhoods.\\nOriginal Data Source\\nFor the sake of example, and for proximity reasons, we fetched information from a mid-sized Swiss city, called Lausanne, based in the south of Switzerland. The country has the particularity that people get often puzzled by the level of prices swarming almost everywhere in the rental markets. This is mostly related to the very high living standards prevailing over here. So we used one of the public property ads available in this french-speaking part of the country : https://www.homegate.ch/\\nBecause the booming Swiss housing market is mainly a rental market (foreign investments have been riding high for the sales of property, and mortgage loans are closed to record low), I focused on real estate for rent ads in the Homegate website.\\nBuilding a webcrawler\\nIn the Kernels section, you will find out how the Python looks like. I used BeautifulSoup and Urllib Python libraries to grab data from the website. As you can figure out, the code is simple, but really efficient.\\nWhat you get\\nIn this example, I extracted data as of 03/17/2017, and I named the DataFrame \"Output\", available in CSV format to make the data compatible with most commonly preferred tools for analysis. It allows you to get a DataFrame with 12 columns:\\nthe date\\nis it a rent or a buy\\nthe location\\nthe address of the property\\nthe zip code\\nthe available description of the property\\nthe number of rooms\\nthe surface\\nthe floor\\nthe price\\nthe source\\nMachine learning\\nIn the Kernels section, you will see a very simple ML algorithm applied to the dataset in order to the \"theoretical\" price of each asset, at the end of the code. For the sake of simplicity, I ran a very straightforward linear regression using only 3 features (the 3 only quantitative factors I have at hand) :\\nthe number of rooms\\nthe floor\\nthe surface\\nI know what you\\'re thinking right at the moment : those 3 features can barely explain the price of a property. Other determinants, such as the location, the neighborhood, the fact that it is outdated, badly maintained by a students roommate partying every night, ... , are of interest when it comes to assessing an appartment. But straightaway, I reduced the model to this.\\nGoogle Map display of the property ads and their relative expensiveness\\ncf Capture.PNG file\\nUpcoming improvements\\nAdd new features to machine learning process, especially a dummy variable accounting for the neighborhood to which the property pertains.\\nSee to what extent a logistic regression could overcome a linear regressor.\\nTest more complex machine learning algorithms.\\nDisplay trends in rental property prices, for each neighborhood, after establishing a larger database (with a few weeks of scraped data).',\n",
       " \"Context\\nI got all these .csv files using pandas data reader but getting every single kospi data through pandas data reader is annoying. so I decided to share this files.\\nContent\\nFiles\\nkospi.csv contains average kospi price. you can use this for checking whether if korean stock is day-off or not. xxxxxx.csv contains each single price records. xxxxxx is it's unique ticker.\\nColumns\\nDate\\nformat - \\\\d{4}-\\\\d{2}-\\\\d{2}\\nOpen\\nformat - \\\\d{1,}\\\\.\\\\d{1}\\nHigh\\nformat - \\\\d{1,}\\\\.\\\\d{1}\\nLow\\nformat - \\\\d{1,}\\\\.\\\\d{1}\\nClose\\nformat - \\\\d{1,}\\\\.\\\\d{1}\\nAdj Close\\nformat - \\\\d{1,}\\\\.\\\\d{1}\\nVolume\\nformat - \\\\d+\\nAcknowledgements\\nblog post which describes how i got these data's. you might need this to update csv files.\\ngit repository git repository\\nInspiration\\nGood luck.\",\n",
       " \"Context\\nIf you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. Some of our strongest geographic and cultural associations are tied to a region's local foods.\\nThis dataset was featured in our completed playground competition entitled What's Cooking? The objective of the competition was to predict the category of a dish's cuisine given a list of its ingredients.\\nContent\\nThe data are stored in JSON format.\\ntrain.json - the training set containing recipes id, type of cuisine, and list of ingredients\\ntest.json - the test set containing recipes id, and list of ingredients\\nAn example of a recipe node in train.json can be found here or in the file preview section below.\\nAcknowledgements\\nThis unique dataset was provided by Yummly and featured in a Kaggle playground competition for fun and practice. Visit the competition page if you are interested in checking out past discussions, competition leaderboard, or more details regarding the competition. If you are curious to see how your results rank compared to others', you can still make a submission at the competition submission page!\",\n",
       " \"Introduction\\nOn 9 August 2017, the Treasurer, under the Census and Statistics Act 1905, directed the Australian Statistician to collect and publish statistical information from all eligible Australians on the Commonwealth Electoral Roll, about their views on whether or not the law should be changed to allow same-sex couples to marry.\\nThe voluntary survey asked one question: should the law be changed to allow same-sex couples to marry? Respondents were asked to mark one box – Yes or No – on the survey form.\\nSurvey materials were mailed to eligible Australians on the Commonwealth Electoral Roll as at 24 August 2017.\\nA range of strategies were implemented to assist all eligible Australians who wished to complete the survey to do so. A survey response was received from 12,727,920 (79.5%) eligible Australians.\\nThe ABS implemented robust systems and controls for the processing, coding and publication of statistical data. Detailed information on the systems used as well as the accuracy and integrity of the data is available in the Survey Process and the Quality and Integrity Statement.\\nThe official statistics include a count of responses (Yes, No and Response Not Clear) by Federal Electoral Division (FED), State/Territory and National. This also includes a count of eligible Australians who have not participated in the survey.\\nInformation from the Commonwealth Electoral Roll has been used to independently produce a participation rate by age and gender for each FED, State/Territory and National. This rate has been published by gender, for each of the following age groups: 18-19 years, 20-24 years, 25-29 years, 30-34 years, 35-39 years, 40-44 years, 45-49 years, 50-54 years, 55-59 years, 60-64 years, 65-69 years, 70-74 years, 75-79 years, 80-84 years, and 85+ years.\\nNational results\\nShould the law be changed to allow same-sex couples to marry?\\nOf the eligible Australians who expressed a view on this question, the majority indicated that the law should be changed to allow same-sex couples to marry, with 7,817,247 (61.6%) responding Yes and 4,873,987 (38.4%) responding No. Nearly 8 out of 10 eligible Australians (79.5%) expressed their view.\\nAll states and territories recorded a majority Yes response. 133 of the 150 Federal Electoral Divisions recorded a majority Yes response, and 17 of the 150 Federal Electoral Divisions recorded a majority No response.\\nData Source\\nAll data presented here comes from the official ABS website: https://marriagesurvey.abs.gov.au/\\nThis data was cleaned by Myles O'Neill and Richard Dear to make it easier to work with.\",\n",
       " 'Tatoeba Sentences Corpus\\nThis data is directly from the Tatoeba project: https://tatoeba.org/ It is a large collection of sentences in multiple languages. Many of the sentences are contained with translations in multiple languages. It is a valuable resource for Machine Translation and many Natural Language Processing projects.',\n",
       " 'Context\\nSadly, Seoul, South Korea has some of the most polluted air in the world. Since Seoul also represents 25-50% of the South Korean population, the air quality is a concern to many.\\nIt used to be that in Korea, we have bad air quality in spring (yellow wind blowing from the Chinese Yellow River), and clear air in autumn. Now with more industries in China, the air is getting worse in Korea in a different seasonality pattern. This is known as Asian Dust.\\nContent\\nHourly measurement on several air pollutants in dozens of districts in Seoul.\\nAcknowledgements\\nData downloaded from here. http://data.seoul.go.kr/openinf/sheetview.jsp?infId=OA-2275&tMenu=11 We thank Seoul Open Data Plaza for making the datasets available. http://english.seoul.go.kr/policy-information/key-policies/informatization/seoul-open-data-plaza/\\nThe banner photos are via JEONGUK HA on Unsplash\\nInspiration\\nRecently, fine dusts are posing a big problem in Korea. https://www.ft.com/content/b49a9878-141b-11e7-80f4-13e067d5072c',\n",
       " \"Context:\\nThis dataset contains survey responses to a survey that people could complete when they signed up for the 5-Day Data Challenge.\\nOn December 12, 2017 survey responses for the second 5-Day Data Challenge were added. For this version of the challenge, participants could sign up for either an intro version or a more in-depth regression challenge.\\nContent:\\nThe optional survey included four multiple-choice questions:\\nHave you ever taken a course in statistics?\\nYep\\nYes, but I've forgotten everything\\nNope\\nDo you have any previous experience with programming?\\nNope\\nI have a little bit of experience\\nI have quite a bit of experience\\nI have a whole lot of experience\\nWhat's your interest in data science?\\nJust curious\\nIt will help me in my current job\\nI want to get a job where I use data science\\nOther\\nJust for fun, do you prefer dogs or cat?\\nDogs 🐶\\nCats 🐱\\nBoth 🐱🐶\\nNeither 🙅\\nIn order to protect privacy, the data has been shuffled (so there’s no temporal order to the responses) and a random 2% of the data has been removed (so even if you know that someone completed the survey, you cannot be sure that their responses are included in this dataset). In addition, all incomplete responses have been removed, and any text entered in the “other” free response field has been replaced with the text “other”.\\nAcknowledgements:\\nThanks to everyone who completed the survey! :)\\nInspiration:\\nIs there a relationship between how much programming experience someone has and why they’re interested in data science?\\nAre more experienced programmers more likely to have taken statistics?\\nDo people tend to prefer dogs, cats, both or neither? Is there a relationship between what people prefer and why they’re interested in data science?\",\n",
       " \"Context\\nAs a huge LOTR fan, I was excited to have acquired this character data from the Lord of the Rings Wiki. I scraped this data using F#; the repository can be found here: https://github.com/MokoSan/FSharpAdvent.\\nContent\\nData consists of character names, the url in the wiki and the respective race.\\nAcknowledgements\\nWouldn't have been able to publish this data set unless it was for the work done by the great people of the wiki page.\",\n",
       " 'Context\\nThis data set is a collection of all StarCraft pro-player 2 matches. The data is taken from the site - http://aligulac.com/\\nContent\\nDataset data - 18 October 2017. You can parse actual data. Just use my script (Github)\\nThis dataset contains 10 variables:\\nmatch_date -Date of match in format mm/dd/yyyy\\nplayer_1 - Player 1 Nickname\\nplayer_1_match_status - Match status for Player 1: winner or loser\\nscore - match score (example: 1-0, 1-2 etc)\\nplayer_2 - Player 2 Nickname\\nplayer_2_match_status - Match status for Player 2: winner or loser\\nplayer_1_race - Player 1 Race: Z - Zerg, P - Protoss, T - Terran\\nplayer_2_race - Player 2 Race: Z - Zerg, P - Protoss, T - Terran\\naddon - Game addon: WoL- Wings of Liberty, HotS - Heart of the Swarm, LotV - Legacy of the Void\\ntournament_type - online or offline\\nAcknowledgements\\nThe source is http://aligulac.com/\\nInspiration\\nQuestions worth exploring:\\nPredict the outcome of a match between two players\\nor whatever you want ....',\n",
       " 'I work with UK company information on a daily basis, and I thought it would be useful to publish a list of all active companies, in a way that could be used for machine learning.\\nThere are 3,801,733 rows in the dataset, one for each active company. The postcode which is included in the dataset has been geolocated, and the resultant latitude and longitudes have been included, along with the Standard Industrial Classification Code, and date of incorporation.\\nThe company list is from the publicly available 1st November 2017 Companies House snapshot.\\nThe postcode geolocations and SIC Codes are from the gov.uk website.\\nIn the file AllCompanies.csv each row is formatted as follows:\\nCompanyNumber - in the format of 99999999 for England/Wales, SC999999 for Scotland and NI999999 for Northern Ireland.\\nIncorporationDate - in British date format, dd/mm/yyyy\\nRegisteredAddressPostCode - standard British format Postcode\\nLatitude - to 6 decimal places\\nLongitude - to 6 decimal places\\nSIC - 5 digits or if not known, None - see separate file for description of each code.\\nInspiration Possible uses for this data is to see where certain types of companies are located in the UK, and how over time they multiply and spread throughout the UK.\\nTraining ML algorithms to predict where there are a high (or low) density of certain types of companies, and where would be a good area for a company to be located, if it wanted minimal competition, or the inverse, where there are clusters of high densities, where it might be easier to recruit specialised staff.\\nA useful addition would be to overlay population density, which I am currently working on as an option for this dataset.\\nI am sure there are many more possible uses for this data in ways, that I cannot imagine.\\nThis is my first go at publishing a dataset on any medium, so any useful tips and hints would be extremely welcome.\\nLinks to the raw data sources are here:\\nCompanies House http://download.companieshouse.gov.uk/en_output.html\\nPostcode to Geolocation https://data.gov.uk/dataset/national-statistics-postcode-lookup-uk\\nSIC Codes https://www.gov.uk/government/publications/standard-industrial-classification-of-economic-activities-sic',\n",
       " \"Context\\nSciELO (Scientific Electronic Library Online) is an international research communication program launched in 1998 and implemented through a decentralized network of national collections of peer reviewed journals from 15 countries – 12 from Latin America, Portugal, Spain and South Africa – that jointly publish over 1 thousand journals and about 50 thousand articles per year. A thematic collection on Public Health is also operated by the SciELO Program. All collections are accessible via the network portal – http://www.scielo.org.\\nSciELO aims at the progress of research through the improvement of peer reviewed journals from all disciplines published by scientific and professional associations, academic institutions and public or private research and development institutions. The specific objectives are to increase in a sustainable way the quality, visibility, usage, impact and credibility of the indexed journals and the research they communicate. A key characteristic of SciELO is multilingual publishing, so journals can publish articles in one or multiple languages including the simultaneous publishing of the same article in more than one language.\\nSciELO Program develops itself according to three principles. First, the conception that scientific knowledge is a public good and therefore should be available openly in the Web. Second, the network operation envisaging to strengthen collaboration and interchange of information and experience, creating scale and lessen the costs. Third, quality control as an essential policy and practice at the level of articles, journals and collections, adoption and compliance with bibliographic and interoperability standards.\\nSciELO operation and development are carried out following three main action lines. The first is professionalization, which means to produce journals according to the state of art. The second is internationalization, which means to strengthen the active participation of SciELO Journals and Program in the international flow of scientific information. The third is operational and financial sustainability, which means to develop conditions to assure journals to be published on time with a well-established financial model.\\nAll collections follow the SciELO Publishing Model, which comprises three main functions. First, the indexing of journals with metadata of articles, including the bibliographic references of the indexed articles and of the articles they cite. Second, the full text of articles which are available in HTML, PDF and progressively in XML JATS compatible according the SciELO Publishing Schema. Third, the dissemination and interoperability of journals and articles with bibliographic indexes and systems.\\nSciELO Brazil, led by SciELO / FAPESP Program, acts as the SciELO Network secretariat and coordinates the maintenance of the methodological and technological platform, while the operation of the network collections and journals are decentralized and led by national research agencies.\\nContent\\nThis dataset contains publication and access reports for the documents of the SciELO Brazil collection between 1998 and October 2017.\\nThe reports present metadata of journals, totals of issues and published documents, thematic areas, authors' affiliation, bibliographic references, use licenses and more. Further details can be found at http://docs.scielo.org/projects/scielo-processing/pt/latest/public_reports.html (in Portuguese, with notes in English).\",\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_descriptions = [str(description) for description in df['Description']]\n",
    "all_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pattern = r\"\\b\\w{3,}\\b\"\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=40, stop_words='english', token_pattern=pattern).fit(all_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review feratures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of features = 885\n",
      "\n",
      "['000' '100' '1000' '1st' '200' '2000' '2008' '2009' '2010' '2011' '2012'\n",
      " '2013' '2014' '2015' '2016' '2017' '500' 'able' 'access' 'accessible'\n",
      " 'according' 'account' 'accuracy' 'accurate' 'acknowledgements' 'acquired'\n",
      " 'act' 'action' 'active' 'activities' 'activity' 'actual' 'actually' 'add'\n",
      " 'added' 'addition' 'additional' 'address' 'administration' 'age']\n"
     ]
    }
   ],
   "source": [
    "print ('len of features = {:,}\\n'.format(len(vectorizer.get_feature_names_out())))\n",
    "print (vectorizer.get_feature_names_out()[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t2\n",
      "  (0, 63)\t1\n",
      "  (0, 80)\t1\n",
      "  (0, 82)\t1\n",
      "  (0, 91)\t1\n",
      "  (0, 108)\t1\n",
      "  (0, 127)\t1\n",
      "  (0, 130)\t3\n",
      "  (0, 132)\t2\n",
      "  (0, 139)\t1\n",
      "  (0, 169)\t3\n",
      "  (0, 179)\t1\n",
      "  (0, 191)\t1\n",
      "  (0, 195)\t1\n",
      "  (0, 198)\t3\n",
      "  (0, 201)\t4\n",
      "  (0, 202)\t1\n",
      "  (0, 206)\t1\n",
      "  (0, 222)\t1\n",
      "  (0, 224)\t1\n",
      "  (0, 267)\t1\n",
      "  (0, 272)\t1\n",
      "  :\t:\n",
      "  (2148, 817)\t3\n",
      "  (2148, 860)\t4\n",
      "  (2148, 881)\t2\n",
      "  (2149, 24)\t1\n",
      "  (2149, 113)\t1\n",
      "  (2149, 170)\t1\n",
      "  (2149, 172)\t1\n",
      "  (2149, 173)\t1\n",
      "  (2149, 198)\t2\n",
      "  (2149, 205)\t3\n",
      "  (2149, 229)\t1\n",
      "  (2149, 295)\t1\n",
      "  (2149, 332)\t1\n",
      "  (2149, 381)\t1\n",
      "  (2149, 457)\t1\n",
      "  (2149, 491)\t3\n",
      "  (2149, 497)\t2\n",
      "  (2149, 513)\t3\n",
      "  (2149, 712)\t2\n",
      "  (2149, 719)\t1\n",
      "  (2149, 748)\t1\n",
      "  (2149, 758)\t1\n",
      "  (2149, 802)\t1\n",
      "  (2149, 864)\t2\n",
      "  (2149, 881)\t2\n"
     ]
    }
   ],
   "source": [
    "newsgroup_data_vectorized = vectorizer.transform(all_descriptions)\n",
    "print(newsgroup_data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create gensim corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(11, 1),\n",
       "  (13, 1),\n",
       "  (21, 1),\n",
       "  (22, 2),\n",
       "  (63, 1),\n",
       "  (80, 1),\n",
       "  (82, 1),\n",
       "  (91, 1),\n",
       "  (108, 1),\n",
       "  (127, 1),\n",
       "  (130, 3),\n",
       "  (132, 2),\n",
       "  (139, 1),\n",
       "  (169, 3),\n",
       "  (179, 1),\n",
       "  (191, 1),\n",
       "  (195, 1),\n",
       "  (198, 3),\n",
       "  (201, 4),\n",
       "  (202, 1),\n",
       "  (206, 1),\n",
       "  (222, 1),\n",
       "  (224, 1),\n",
       "  (267, 1),\n",
       "  (272, 1),\n",
       "  (282, 4),\n",
       "  (283, 3),\n",
       "  (316, 1),\n",
       "  (327, 1),\n",
       "  (337, 1),\n",
       "  (346, 3),\n",
       "  (376, 1),\n",
       "  (378, 1),\n",
       "  (390, 1),\n",
       "  (421, 2),\n",
       "  (454, 1),\n",
       "  (483, 2),\n",
       "  (518, 1),\n",
       "  (519, 1),\n",
       "  (530, 1),\n",
       "  (544, 1),\n",
       "  (574, 1),\n",
       "  (606, 1),\n",
       "  (609, 1),\n",
       "  (652, 1),\n",
       "  (670, 1),\n",
       "  (674, 1),\n",
       "  (675, 1),\n",
       "  (700, 1),\n",
       "  (708, 1),\n",
       "  (772, 1),\n",
       "  (792, 2),\n",
       "  (801, 1),\n",
       "  (836, 1),\n",
       "  (841, 1),\n",
       "  (843, 1),\n",
       "  (845, 1),\n",
       "  (846, 1)],\n",
       " [(0, 3),\n",
       "  (6, 1),\n",
       "  (14, 2),\n",
       "  (17, 2),\n",
       "  (18, 2),\n",
       "  (43, 1),\n",
       "  (49, 1),\n",
       "  (55, 1),\n",
       "  (73, 5),\n",
       "  (84, 1),\n",
       "  (104, 1),\n",
       "  (108, 1),\n",
       "  (118, 1),\n",
       "  (130, 1),\n",
       "  (131, 1),\n",
       "  (140, 1),\n",
       "  (141, 1),\n",
       "  (143, 3),\n",
       "  (146, 1),\n",
       "  (148, 1),\n",
       "  (154, 1),\n",
       "  (168, 1),\n",
       "  (175, 1),\n",
       "  (181, 1),\n",
       "  (193, 1),\n",
       "  (198, 10),\n",
       "  (199, 2),\n",
       "  (201, 4),\n",
       "  (205, 1),\n",
       "  (221, 1),\n",
       "  (232, 2),\n",
       "  (251, 1),\n",
       "  (252, 1),\n",
       "  (264, 1),\n",
       "  (267, 1),\n",
       "  (271, 3),\n",
       "  (275, 1),\n",
       "  (276, 1),\n",
       "  (281, 1),\n",
       "  (283, 1),\n",
       "  (302, 1),\n",
       "  (305, 5),\n",
       "  (306, 3),\n",
       "  (315, 1),\n",
       "  (319, 1),\n",
       "  (333, 1),\n",
       "  (334, 1),\n",
       "  (340, 3),\n",
       "  (346, 3),\n",
       "  (351, 1),\n",
       "  (359, 1),\n",
       "  (360, 1),\n",
       "  (361, 1),\n",
       "  (364, 1),\n",
       "  (380, 2),\n",
       "  (384, 1),\n",
       "  (385, 1),\n",
       "  (386, 1),\n",
       "  (401, 1),\n",
       "  (405, 1),\n",
       "  (418, 2),\n",
       "  (421, 1),\n",
       "  (430, 1),\n",
       "  (431, 1),\n",
       "  (434, 1),\n",
       "  (449, 3),\n",
       "  (451, 1),\n",
       "  (454, 1),\n",
       "  (457, 2),\n",
       "  (467, 1),\n",
       "  (468, 4),\n",
       "  (485, 1),\n",
       "  (497, 1),\n",
       "  (505, 2),\n",
       "  (510, 1),\n",
       "  (530, 2),\n",
       "  (532, 1),\n",
       "  (534, 1),\n",
       "  (537, 1),\n",
       "  (547, 1),\n",
       "  (563, 1),\n",
       "  (564, 7),\n",
       "  (581, 1),\n",
       "  (582, 1),\n",
       "  (598, 1),\n",
       "  (599, 1),\n",
       "  (605, 1),\n",
       "  (608, 1),\n",
       "  (621, 1),\n",
       "  (626, 1),\n",
       "  (679, 1),\n",
       "  (684, 1),\n",
       "  (692, 1),\n",
       "  (695, 1),\n",
       "  (696, 2),\n",
       "  (709, 2),\n",
       "  (731, 2),\n",
       "  (732, 1),\n",
       "  (736, 1),\n",
       "  (768, 1),\n",
       "  (776, 4),\n",
       "  (777, 4),\n",
       "  (792, 2),\n",
       "  (812, 1),\n",
       "  (818, 1),\n",
       "  (822, 1),\n",
       "  (830, 1),\n",
       "  (831, 1),\n",
       "  (835, 2),\n",
       "  (841, 2),\n",
       "  (842, 1),\n",
       "  (844, 1),\n",
       "  (850, 1),\n",
       "  (856, 1),\n",
       "  (863, 1),\n",
       "  (869, 3),\n",
       "  (873, 1),\n",
       "  (880, 1)],\n",
       " [(1, 1),\n",
       "  (8, 1),\n",
       "  (17, 1),\n",
       "  (18, 2),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (36, 1),\n",
       "  (49, 2),\n",
       "  (55, 4),\n",
       "  (56, 2),\n",
       "  (82, 1),\n",
       "  (97, 1),\n",
       "  (99, 1),\n",
       "  (115, 1),\n",
       "  (136, 1),\n",
       "  (142, 3),\n",
       "  (148, 1),\n",
       "  (152, 1),\n",
       "  (166, 1),\n",
       "  (168, 1),\n",
       "  (169, 1),\n",
       "  (195, 1),\n",
       "  (198, 8),\n",
       "  (199, 2),\n",
       "  (201, 9),\n",
       "  (222, 1),\n",
       "  (232, 1),\n",
       "  (246, 2),\n",
       "  (249, 1),\n",
       "  (272, 4),\n",
       "  (273, 1),\n",
       "  (287, 1),\n",
       "  (288, 3),\n",
       "  (289, 1),\n",
       "  (299, 1),\n",
       "  (309, 1),\n",
       "  (312, 1),\n",
       "  (316, 1),\n",
       "  (318, 2),\n",
       "  (321, 1),\n",
       "  (322, 2),\n",
       "  (326, 1),\n",
       "  (328, 1),\n",
       "  (337, 1),\n",
       "  (350, 1),\n",
       "  (358, 1),\n",
       "  (359, 1),\n",
       "  (374, 1),\n",
       "  (376, 1),\n",
       "  (380, 1),\n",
       "  (381, 1),\n",
       "  (388, 1),\n",
       "  (393, 1),\n",
       "  (395, 1),\n",
       "  (398, 2),\n",
       "  (399, 1),\n",
       "  (400, 2),\n",
       "  (401, 3),\n",
       "  (405, 1),\n",
       "  (410, 1),\n",
       "  (424, 1),\n",
       "  (431, 2),\n",
       "  (432, 2),\n",
       "  (434, 1),\n",
       "  (439, 1),\n",
       "  (440, 1),\n",
       "  (456, 2),\n",
       "  (482, 1),\n",
       "  (485, 3),\n",
       "  (505, 5),\n",
       "  (506, 2),\n",
       "  (525, 1),\n",
       "  (527, 4),\n",
       "  (530, 3),\n",
       "  (534, 1),\n",
       "  (557, 2),\n",
       "  (575, 1),\n",
       "  (577, 1),\n",
       "  (581, 1),\n",
       "  (587, 1),\n",
       "  (599, 1),\n",
       "  (601, 1),\n",
       "  (611, 1),\n",
       "  (622, 1),\n",
       "  (623, 1),\n",
       "  (624, 2),\n",
       "  (631, 1),\n",
       "  (656, 1),\n",
       "  (658, 1),\n",
       "  (668, 1),\n",
       "  (683, 1),\n",
       "  (706, 1),\n",
       "  (712, 1),\n",
       "  (719, 3),\n",
       "  (721, 2),\n",
       "  (722, 1),\n",
       "  (728, 1),\n",
       "  (731, 2),\n",
       "  (742, 1),\n",
       "  (750, 1),\n",
       "  (764, 1),\n",
       "  (782, 1),\n",
       "  (788, 1),\n",
       "  (792, 2),\n",
       "  (812, 1),\n",
       "  (817, 1),\n",
       "  (835, 1),\n",
       "  (836, 1),\n",
       "  (839, 1),\n",
       "  (840, 1),\n",
       "  (844, 1),\n",
       "  (849, 3),\n",
       "  (873, 1)],\n",
       " [(0, 2),\n",
       "  (1, 1),\n",
       "  (10, 1),\n",
       "  (14, 2),\n",
       "  (15, 2),\n",
       "  (18, 5),\n",
       "  (19, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (31, 1),\n",
       "  (35, 2),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (41, 1),\n",
       "  (49, 2),\n",
       "  (52, 1),\n",
       "  (67, 2),\n",
       "  (69, 1),\n",
       "  (71, 1),\n",
       "  (80, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (89, 1),\n",
       "  (109, 1),\n",
       "  (113, 1),\n",
       "  (119, 1),\n",
       "  (123, 1),\n",
       "  (140, 6),\n",
       "  (143, 1),\n",
       "  (148, 2),\n",
       "  (157, 1),\n",
       "  (160, 1),\n",
       "  (165, 1),\n",
       "  (167, 1),\n",
       "  (170, 1),\n",
       "  (171, 2),\n",
       "  (172, 2),\n",
       "  (198, 18),\n",
       "  (199, 10),\n",
       "  (214, 4),\n",
       "  (222, 1),\n",
       "  (237, 1),\n",
       "  (241, 1),\n",
       "  (247, 1),\n",
       "  (254, 1),\n",
       "  (255, 1),\n",
       "  (270, 2),\n",
       "  (289, 1),\n",
       "  (295, 1),\n",
       "  (296, 1),\n",
       "  (299, 1),\n",
       "  (304, 1),\n",
       "  (309, 1),\n",
       "  (311, 1),\n",
       "  (318, 7),\n",
       "  (319, 1),\n",
       "  (325, 2),\n",
       "  (347, 1),\n",
       "  (359, 2),\n",
       "  (363, 3),\n",
       "  (364, 2),\n",
       "  (372, 2),\n",
       "  (373, 1),\n",
       "  (376, 4),\n",
       "  (386, 1),\n",
       "  (397, 1),\n",
       "  (399, 1),\n",
       "  (406, 1),\n",
       "  (419, 1),\n",
       "  (445, 1),\n",
       "  (461, 2),\n",
       "  (477, 3),\n",
       "  (488, 1),\n",
       "  (497, 4),\n",
       "  (505, 1),\n",
       "  (508, 3),\n",
       "  (510, 1),\n",
       "  (513, 2),\n",
       "  (519, 1),\n",
       "  (521, 1),\n",
       "  (522, 2),\n",
       "  (525, 2),\n",
       "  (529, 1),\n",
       "  (534, 1),\n",
       "  (542, 3),\n",
       "  (545, 3),\n",
       "  (552, 2),\n",
       "  (557, 1),\n",
       "  (569, 1),\n",
       "  (573, 1),\n",
       "  (585, 1),\n",
       "  (586, 3),\n",
       "  (596, 1),\n",
       "  (600, 1),\n",
       "  (611, 2),\n",
       "  (613, 2),\n",
       "  (614, 2),\n",
       "  (615, 2),\n",
       "  (619, 1),\n",
       "  (624, 2),\n",
       "  (644, 2),\n",
       "  (649, 1),\n",
       "  (656, 1),\n",
       "  (660, 1),\n",
       "  (662, 1),\n",
       "  (664, 1),\n",
       "  (666, 1),\n",
       "  (671, 2),\n",
       "  (675, 1),\n",
       "  (676, 1),\n",
       "  (679, 3),\n",
       "  (689, 1),\n",
       "  (697, 1),\n",
       "  (701, 1),\n",
       "  (709, 1),\n",
       "  (712, 2),\n",
       "  (729, 1),\n",
       "  (731, 1),\n",
       "  (732, 2),\n",
       "  (742, 15),\n",
       "  (745, 2),\n",
       "  (746, 3),\n",
       "  (748, 1),\n",
       "  (761, 4),\n",
       "  (764, 1),\n",
       "  (776, 1),\n",
       "  (779, 1),\n",
       "  (782, 3),\n",
       "  (792, 4),\n",
       "  (799, 1),\n",
       "  (807, 3),\n",
       "  (810, 1),\n",
       "  (825, 1),\n",
       "  (826, 3),\n",
       "  (828, 3),\n",
       "  (831, 1),\n",
       "  (834, 1),\n",
       "  (835, 4),\n",
       "  (836, 1),\n",
       "  (838, 9),\n",
       "  (839, 1),\n",
       "  (840, 1),\n",
       "  (841, 1),\n",
       "  (846, 2),\n",
       "  (851, 1),\n",
       "  (861, 3),\n",
       "  (862, 2),\n",
       "  (867, 3),\n",
       "  (876, 4),\n",
       "  (880, 1)],\n",
       " [(7, 1),\n",
       "  (10, 1),\n",
       "  (24, 1),\n",
       "  (30, 1),\n",
       "  (55, 2),\n",
       "  (89, 1),\n",
       "  (114, 1),\n",
       "  (135, 1),\n",
       "  (136, 1),\n",
       "  (157, 1),\n",
       "  (166, 1),\n",
       "  (170, 1),\n",
       "  (171, 1),\n",
       "  (172, 1),\n",
       "  (194, 5),\n",
       "  (198, 7),\n",
       "  (230, 2),\n",
       "  (233, 1),\n",
       "  (234, 1),\n",
       "  (237, 1),\n",
       "  (262, 1),\n",
       "  (288, 1),\n",
       "  (290, 1),\n",
       "  (292, 1),\n",
       "  (302, 1),\n",
       "  (334, 1),\n",
       "  (338, 1),\n",
       "  (362, 1),\n",
       "  (380, 1),\n",
       "  (381, 1),\n",
       "  (385, 1),\n",
       "  (407, 1),\n",
       "  (431, 2),\n",
       "  (452, 1),\n",
       "  (459, 1),\n",
       "  (466, 2),\n",
       "  (483, 1),\n",
       "  (485, 1),\n",
       "  (499, 1),\n",
       "  (519, 1),\n",
       "  (525, 2),\n",
       "  (552, 1),\n",
       "  (557, 1),\n",
       "  (587, 1),\n",
       "  (589, 1),\n",
       "  (605, 1),\n",
       "  (613, 2),\n",
       "  (641, 2),\n",
       "  (642, 1),\n",
       "  (656, 1),\n",
       "  (661, 1),\n",
       "  (680, 1),\n",
       "  (684, 1),\n",
       "  (689, 1),\n",
       "  (694, 1),\n",
       "  (712, 1),\n",
       "  (715, 1),\n",
       "  (731, 1),\n",
       "  (772, 1),\n",
       "  (786, 1),\n",
       "  (792, 3),\n",
       "  (794, 1),\n",
       "  (831, 1),\n",
       "  (848, 1),\n",
       "  (855, 2),\n",
       "  (858, 1)]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(newsgroup_data_vectorized, documents_columns=False)\n",
    "[item for item in corpus][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create id2word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{202: 'datasets',\n",
       " 169: 'contains',\n",
       " 191: 'credit',\n",
       " 708: 'september',\n",
       " 11: '2013',\n",
       " 267: 'european',\n",
       " 201: 'dataset',\n",
       " 519: 'occurred',\n",
       " 206: 'days',\n",
       " 337: 'highly',\n",
       " 574: 'positive',\n",
       " 130: 'class',\n",
       " 21: 'account',\n",
       " 378: 'input',\n",
       " 846: 'variables',\n",
       " 675: 'result',\n",
       " 390: 'issues',\n",
       " 609: 'provide',\n",
       " 530: 'original',\n",
       " 283: 'features',\n",
       " 82: 'background',\n",
       " 376: 'information',\n",
       " 198: 'data',\n",
       " 518: 'obtained',\n",
       " 792: 'time',\n",
       " 282: 'feature',\n",
       " 700: 'seconds',\n",
       " 836: 'used',\n",
       " 272: 'example',\n",
       " 179: 'cost',\n",
       " 421: 'learning',\n",
       " 674: 'response',\n",
       " 845: 'variable',\n",
       " 772: 'takes',\n",
       " 843: 'value',\n",
       " 108: 'case',\n",
       " 316: 'given',\n",
       " 22: 'accuracy',\n",
       " 841: 'using',\n",
       " 63: 'area',\n",
       " 132: 'classification',\n",
       " 139: 'collected',\n",
       " 670: 'research',\n",
       " 454: 'machine',\n",
       " 327: 'group',\n",
       " 346: 'http',\n",
       " 91: 'big',\n",
       " 483: 'mining',\n",
       " 224: 'detection',\n",
       " 222: 'details',\n",
       " 195: 'current',\n",
       " 544: 'past',\n",
       " 606: 'projects',\n",
       " 652: 'related',\n",
       " 801: 'topics',\n",
       " 80: 'available',\n",
       " 127: 'cite',\n",
       " 13: '2015',\n",
       " 199: 'database',\n",
       " 49: 'analysis',\n",
       " 0: '000',\n",
       " 468: 'matches',\n",
       " 564: 'players',\n",
       " 181: 'countries',\n",
       " 6: '2008',\n",
       " 14: '2016',\n",
       " 777: 'teams',\n",
       " 73: 'attributes',\n",
       " 850: 'video',\n",
       " 305: 'game',\n",
       " 709: 'series',\n",
       " 364: 'including',\n",
       " 831: 'updates',\n",
       " 776: 'team',\n",
       " 434: 'line',\n",
       " 175: 'coordinates',\n",
       " 221: 'detailed',\n",
       " 467: 'match',\n",
       " 271: 'events',\n",
       " 319: 'goal',\n",
       " 818: 'types',\n",
       " 193: 'cross',\n",
       " 505: 'new',\n",
       " 768: 'table',\n",
       " 168: 'containing',\n",
       " 731: 'source',\n",
       " 252: 'easily',\n",
       " 842: 'usually',\n",
       " 232: 'different',\n",
       " 863: 'websites',\n",
       " 140: 'collection',\n",
       " 598: 'processing',\n",
       " 457: 'make',\n",
       " 430: 'life',\n",
       " 251: 'easier',\n",
       " 148: 'commercial',\n",
       " 835: 'use',\n",
       " 55: 'api',\n",
       " 143: 'com',\n",
       " 692: 'scores',\n",
       " 880: 'www',\n",
       " 822: 'understand',\n",
       " 141: 'column',\n",
       " 306: 'games',\n",
       " 608: 'property',\n",
       " 449: 'look',\n",
       " 732: 'sources',\n",
       " 104: 'called',\n",
       " 485: 'missing',\n",
       " 844: 'values',\n",
       " 17: 'able',\n",
       " 43: 'algorithm',\n",
       " 361: 'include',\n",
       " 386: 'international',\n",
       " 497: 'national',\n",
       " 418: 'league',\n",
       " 736: 'specific',\n",
       " 856: 'want',\n",
       " 333: 'help',\n",
       " 360: 'improve',\n",
       " 18: 'access',\n",
       " 605: 'project',\n",
       " 315: 'github',\n",
       " 359: 'important',\n",
       " 510: 'note',\n",
       " 547: 'people',\n",
       " 384: 'interested',\n",
       " 696: 'scripts',\n",
       " 621: 'python',\n",
       " 118: 'changed',\n",
       " 146: 'comes',\n",
       " 695: 'script',\n",
       " 563: 'player',\n",
       " 873: 'work',\n",
       " 830: 'updated',\n",
       " 276: 'exploring',\n",
       " 302: 'fun',\n",
       " 451: 'lot',\n",
       " 380: 'insights',\n",
       " 534: 'overview',\n",
       " 537: 'page',\n",
       " 401: 'kernels',\n",
       " 812: 'try',\n",
       " 351: 'ideas',\n",
       " 581: 'predict',\n",
       " 131: 'classes',\n",
       " 340: 'home',\n",
       " 869: 'win',\n",
       " 679: 'right',\n",
       " 281: 'far',\n",
       " 334: 'high',\n",
       " 626: 'random',\n",
       " 405: 'know',\n",
       " 84: 'base',\n",
       " 582: 'predicting',\n",
       " 684: 'running',\n",
       " 431: 'like',\n",
       " 532: 'output',\n",
       " 264: 'estimate',\n",
       " 154: 'compare',\n",
       " 275: 'explore',\n",
       " 599: 'produce',\n",
       " 385: 'interesting',\n",
       " 205: 'day',\n",
       " 656: 'released',\n",
       " 115: 'certain',\n",
       " 152: 'companies',\n",
       " 456: 'major',\n",
       " 1: '100',\n",
       " 482: 'million',\n",
       " 623: 'question',\n",
       " 374: 'industry',\n",
       " 326: 'great',\n",
       " 557: 'place',\n",
       " 742: 'start',\n",
       " 624: 'questions',\n",
       " 764: 'summary',\n",
       " 399: 'kaggle',\n",
       " 658: 'removed',\n",
       " 849: 'version',\n",
       " 668: 'request',\n",
       " 527: 'order',\n",
       " 358: 'impact',\n",
       " 721: 'similar',\n",
       " 712: 'set',\n",
       " 288: 'fields',\n",
       " 782: 'terms',\n",
       " 506: 'news',\n",
       " 99: 'built',\n",
       " 432: 'likely',\n",
       " 322: 'good',\n",
       " 400: 'kernel',\n",
       " 273: 'examples',\n",
       " 309: 'general',\n",
       " 388: 'introduction',\n",
       " 299: 'format',\n",
       " 398: 'just',\n",
       " 439: 'listed',\n",
       " 56: 'appear',\n",
       " 8: '2010',\n",
       " 318: 'global',\n",
       " 23: 'accurate',\n",
       " 440: 'lists',\n",
       " 142: 'columns',\n",
       " 166: 'contain',\n",
       " 395: 'json',\n",
       " 722: 'simple',\n",
       " 587: 'previous',\n",
       " 719: 'shows',\n",
       " 249: 'duration',\n",
       " 706: 'separate',\n",
       " 289: 'file',\n",
       " 839: 'users',\n",
       " 246: 'don',\n",
       " 631: 'ratings',\n",
       " 683: 'run',\n",
       " 136: 'code',\n",
       " 577: 'posted',\n",
       " 750: 'status',\n",
       " 525: 'open',\n",
       " 788: 'things',\n",
       " 424: 'let',\n",
       " 622: 'quality',\n",
       " 287: 'field',\n",
       " 350: 'idea',\n",
       " 728: 'small',\n",
       " 381: 'inspiration',\n",
       " 817: 'type',\n",
       " 410: 'labels',\n",
       " 575: 'possible',\n",
       " 97: 'build',\n",
       " 393: 'job',\n",
       " 328: 'groups',\n",
       " 321: 'going',\n",
       " 24: 'acknowledgements',\n",
       " 312: 'generated',\n",
       " 601: 'product',\n",
       " 840: 'uses',\n",
       " 611: 'provides',\n",
       " 36: 'additional',\n",
       " 172: 'context',\n",
       " 876: 'world',\n",
       " 52: 'annual',\n",
       " 304: 'future',\n",
       " 363: 'includes',\n",
       " 552: 'period',\n",
       " 109: 'cases',\n",
       " 671: 'researchers',\n",
       " 761: 'study',\n",
       " 828: 'university',\n",
       " 170: 'content',\n",
       " 15: '2017',\n",
       " 614: 'publication',\n",
       " 397: 'june',\n",
       " 825: 'unit',\n",
       " 445: 'location',\n",
       " 477: 'media',\n",
       " 67: 'articles',\n",
       " 119: 'changes',\n",
       " 545: 'patterns',\n",
       " 810: 'trends',\n",
       " 542: 'particular',\n",
       " 649: 'regions',\n",
       " 31: 'actual',\n",
       " 508: 'non',\n",
       " 745: 'state',\n",
       " 569: 'political',\n",
       " 254: 'economic',\n",
       " 729: 'social',\n",
       " 214: 'department',\n",
       " 513: 'number',\n",
       " 689: 'science',\n",
       " 779: 'technology',\n",
       " 521: 'office',\n",
       " 10: '2012',\n",
       " 167: 'contained',\n",
       " 666: 'representing',\n",
       " 522: 'official',\n",
       " 826: 'united',\n",
       " 746: 'states',\n",
       " 325: 'government',\n",
       " 615: 'publications',\n",
       " 662: 'reports',\n",
       " 295: 'following',\n",
       " 160: 'conditions',\n",
       " 701: 'section',\n",
       " 644: 'refer',\n",
       " 600: 'produced',\n",
       " 585: 'present',\n",
       " 867: 'wide',\n",
       " 861: 'web',\n",
       " 586: 'presented',\n",
       " 113: 'center',\n",
       " 85: 'based',\n",
       " 838: 'user',\n",
       " 372: 'individual',\n",
       " 373: 'individuals',\n",
       " 461: 'management',\n",
       " 529: 'organization',\n",
       " 664: 'represent',\n",
       " 834: 'usage',\n",
       " 697: 'search',\n",
       " 851: 'view',\n",
       " 171: 'contents',\n",
       " 573: 'position',\n",
       " 41: 'agency',\n",
       " 296: 'follows',\n",
       " 347: 'https',\n",
       " 862: 'website',\n",
       " 19: 'accessible',\n",
       " 613: 'public',\n",
       " 247: 'download',\n",
       " 35: 'addition',\n",
       " 237: 'distributed',\n",
       " 619: 'purpose',\n",
       " 123: 'check',\n",
       " 660: 'reported',\n",
       " 157: 'complete',\n",
       " 676: 'results',\n",
       " 71: 'associated',\n",
       " 241: 'documentation',\n",
       " 270: 'event',\n",
       " 488: 'modified',\n",
       " 37: 'address',\n",
       " 807: 'training',\n",
       " 406: 'knowledge',\n",
       " 799: 'tools',\n",
       " 89: 'best',\n",
       " 596: 'process',\n",
       " 419: 'learn',\n",
       " 86: 'basic',\n",
       " 311: 'generate',\n",
       " 748: 'statistics',\n",
       " 69: 'asked',\n",
       " 165: 'contact',\n",
       " 255: 'edu',\n",
       " 407: 'known',\n",
       " 7: '2009',\n",
       " 234: 'digital',\n",
       " 642: 'recorded',\n",
       " 499: 'need',\n",
       " 641: 'record',\n",
       " 114: 'central',\n",
       " 466: 'market',\n",
       " 292: 'financial',\n",
       " 362: 'included',\n",
       " 338: 'historical',\n",
       " 194: 'csv',\n",
       " 290: 'files',\n",
       " 452: 'low',\n",
       " 135: 'close',\n",
       " 855: 'volume',\n",
       " 589: 'price',\n",
       " 30: 'activity',\n",
       " 794: 'timestamp',\n",
       " 230: 'did',\n",
       " 262: 'error',\n",
       " 661: 'reporting',\n",
       " 680: 'risk',\n",
       " 848: 'various',\n",
       " 459: 'making',\n",
       " 233: 'difficult',\n",
       " 694: 'scraping',\n",
       " 786: 'thank',\n",
       " 715: 'share',\n",
       " 858: 'way',\n",
       " 161: 'conducted',\n",
       " 766: 'survey',\n",
       " 158: 'comprehensive',\n",
       " 637: 'received',\n",
       " 420: 'learned',\n",
       " 874: 'working',\n",
       " 377: 'initial',\n",
       " 659: 'report',\n",
       " 716: 'shared',\n",
       " 494: 'multiple',\n",
       " 724: 'single',\n",
       " 681: 'row',\n",
       " 243: 'does',\n",
       " 629: 'rates',\n",
       " 536: 'package',\n",
       " 816: 'txt',\n",
       " 512: 'november',\n",
       " 491: 'month',\n",
       " 2: '1000',\n",
       " 864: 'week',\n",
       " 48: 'analyses',\n",
       " 633: 'read',\n",
       " 403: 'kind',\n",
       " 182: 'country',\n",
       " 495: 'named',\n",
       " 53: 'answer',\n",
       " 647: 'regarding',\n",
       " 669: 'required',\n",
       " 370: 'indicates',\n",
       " 438: 'list',\n",
       " 442: 'live',\n",
       " 76: 'august',\n",
       " 484: 'minutes',\n",
       " 802: 'total',\n",
       " 610: 'provided',\n",
       " 628: 'rate',\n",
       " 657: 'relevant',\n",
       " 310: 'generally',\n",
       " 759: 'students',\n",
       " 707: 'separated',\n",
       " 402: 'key',\n",
       " 300: 'free',\n",
       " 298: 'form',\n",
       " 145: 'come',\n",
       " 540: 'paper',\n",
       " 475: 'measurements',\n",
       " 594: 'problems',\n",
       " 821: 'uci',\n",
       " 663: 'repository',\n",
       " 686: 'samples',\n",
       " 607: 'properties',\n",
       " 228: 'development',\n",
       " 371: 'indicators',\n",
       " 83: 'bank',\n",
       " 632: 'raw',\n",
       " 32: 'actually',\n",
       " 211: 'deep',\n",
       " 507: 'nlp',\n",
       " 184: 'course',\n",
       " 561: 'play',\n",
       " 219: 'description',\n",
       " 163: 'considered',\n",
       " 203: 'date',\n",
       " 627: 'range',\n",
       " 752: 'stock',\n",
       " 81: 'average',\n",
       " 699: 'second',\n",
       " 435: 'lines',\n",
       " 248: 'downloaded',\n",
       " 235: 'directly',\n",
       " 375: 'info',\n",
       " 144: 'combined',\n",
       " 408: 'label',\n",
       " 523: 'ones',\n",
       " 774: 'task',\n",
       " 269: 'evaluation',\n",
       " 12: '2014',\n",
       " 783: 'test',\n",
       " 882: 'years',\n",
       " 740: 'split',\n",
       " 583: 'prediction',\n",
       " 789: 'think',\n",
       " 489: 'money',\n",
       " 285: 'feel',\n",
       " 749: 'stats',\n",
       " 735: 'special',\n",
       " 738: 'speed',\n",
       " 217: 'described',\n",
       " 566: 'points',\n",
       " 332: 'health',\n",
       " 25: 'acquired',\n",
       " 726: 'sites',\n",
       " 54: 'answered',\n",
       " 734: 'space',\n",
       " 470: 'mean',\n",
       " 884: 'zip',\n",
       " 137: 'codes',\n",
       " 516: 'observations',\n",
       " 229: 'dictionary',\n",
       " 187: 'create',\n",
       " 584: 'predictive',\n",
       " 486: 'model',\n",
       " 354: 'identify',\n",
       " 751: 'step',\n",
       " 805: 'train',\n",
       " 559: 'plan',\n",
       " 847: 'variety',\n",
       " 872: 'words',\n",
       " 677: 'review',\n",
       " 870: 'won',\n",
       " 576: 'post',\n",
       " 164: 'consists',\n",
       " 687: 'scale',\n",
       " 678: 'reviews',\n",
       " 691: 'score',\n",
       " 795: 'title',\n",
       " 218: 'describing',\n",
       " 648: 'region',\n",
       " 379: 'inside',\n",
       " 553: 'person',\n",
       " 815: 'twitter',\n",
       " 693: 'scraped',\n",
       " 829: 'update',\n",
       " 881: 'year',\n",
       " 705: 'sentiment',\n",
       " 785: 'text',\n",
       " 487: 'models',\n",
       " 533: 'overall',\n",
       " 117: 'change',\n",
       " 39: 'age',\n",
       " 713: 'sets',\n",
       " 320: 'goes',\n",
       " 447: 'long',\n",
       " 250: 'early',\n",
       " 853: 'visit',\n",
       " 860: 'weather',\n",
       " 780: 'temperature',\n",
       " 580: 'pre',\n",
       " 46: 'allows',\n",
       " 617: 'publish',\n",
       " 58: 'applied',\n",
       " 481: 'methods',\n",
       " 45: 'allow',\n",
       " 471: 'meaning',\n",
       " 469: 'maximum',\n",
       " 129: 'city',\n",
       " 297: 'food',\n",
       " 603: 'products',\n",
       " 34: 'added',\n",
       " 618: 'published',\n",
       " 758: 'structure',\n",
       " 833: 'url',\n",
       " 110: 'categories',\n",
       " 128: 'cities',\n",
       " 515: 'numeric',\n",
       " 366: 'increase',\n",
       " 688: 'school',\n",
       " 744: 'starting',\n",
       " 367: 'increased',\n",
       " 579: 'power',\n",
       " 20: 'according',\n",
       " 103: 'california',\n",
       " 756: 'street',\n",
       " 394: 'journal',\n",
       " 531: 'originally',\n",
       " 207: 'death',\n",
       " 121: 'characteristics',\n",
       " 220: 'descriptions',\n",
       " 178: 'corresponding',\n",
       " 280: 'family',\n",
       " 509: 'north',\n",
       " 47: 'american',\n",
       " 352: 'identified',\n",
       " 61: 'april',\n",
       " 549: 'perform',\n",
       " 356: 'image',\n",
       " 730: 'software',\n",
       " 62: 'archive',\n",
       " 349: 'ics',\n",
       " 72: 'attribute',\n",
       " 634: 'real',\n",
       " 741: 'standard',\n",
       " 443: 'local',\n",
       " 414: 'largest',\n",
       " 720: 'significant',\n",
       " 238: 'distribution',\n",
       " 16: '500',\n",
       " 520: 'october',\n",
       " 685: 'sample',\n",
       " 524: 'online',\n",
       " 426: 'levels',\n",
       " 465: 'march',\n",
       " 640: 'recognition',\n",
       " 568: 'policy',\n",
       " 796: 'today',\n",
       " 554: 'personal',\n",
       " 455: 'main',\n",
       " 866: 'weights',\n",
       " 266: 'estimates',\n",
       " 279: 'factors',\n",
       " 602: 'production',\n",
       " 765: 'support',\n",
       " 335: 'higher',\n",
       " 336: 'highest',\n",
       " 274: 'experience',\n",
       " 60: 'approximately',\n",
       " 702: 'seen',\n",
       " 413: 'large',\n",
       " 565: 'point',\n",
       " 102: 'calculated',\n",
       " 33: 'add',\n",
       " 66: 'article',\n",
       " 357: 'images',\n",
       " 44: 'algorithms',\n",
       " 727: 'size',\n",
       " 784: 'testing',\n",
       " 151: 'community',\n",
       " 514: 'numbers',\n",
       " 667: 'represents',\n",
       " 444: 'located',\n",
       " 422: 'left',\n",
       " 70: 'assigned',\n",
       " 174: 'converted',\n",
       " 428: 'license',\n",
       " 176: 'copyright',\n",
       " 762: 'subject',\n",
       " 433: 'limited',\n",
       " 78: 'authors',\n",
       " 27: 'action',\n",
       " 257: 'election',\n",
       " 591: 'primary',\n",
       " 183: 'county',\n",
       " 261: 'entire',\n",
       " 436: 'link',\n",
       " 425: 'level',\n",
       " 96: 'book',\n",
       " 122: 'characters',\n",
       " 120: 'character',\n",
       " 188: 'created',\n",
       " 345: 'html',\n",
       " 528: 'org',\n",
       " 156: 'compiled',\n",
       " 101: 'business',\n",
       " 308: 'gender',\n",
       " 307: 'gathered',\n",
       " 714: 'sex',\n",
       " 258: 'end',\n",
       " 704: 'self',\n",
       " 460: 'male',\n",
       " 286: 'female',\n",
       " 578: 'potential',\n",
       " 314: 'getting',\n",
       " 90: 'better',\n",
       " 616: 'publicly',\n",
       " 550: 'performance',\n",
       " 98: 'building',\n",
       " 773: 'target',\n",
       " 453: 'lower',\n",
       " 216: 'derived',\n",
       " 180: 'count',\n",
       " 343: 'hours',\n",
       " 389: 'involved',\n",
       " 837: 'useful',\n",
       " 9: '2011',\n",
       " 698: 'season',\n",
       " 562: 'played',\n",
       " 138: 'collect',\n",
       " 479: 'metadata',\n",
       " 480: 'method',\n",
       " 368: 'index',\n",
       " 473: 'measure',\n",
       " 265: 'estimated',\n",
       " 590: 'prices',\n",
       " 291: 'final',\n",
       " 588: 'previously',\n",
       " 185: 'cover',\n",
       " 827: 'units',\n",
       " 225: 'determine',\n",
       " 212: 'defined',\n",
       " 149: 'common',\n",
       " 112: 'census',\n",
       " 496: 'names',\n",
       " 500: 'negative',\n",
       " 877: 'worth',\n",
       " 365: 'income',\n",
       " 450: 'looking',\n",
       " 382: 'instead',\n",
       " 723: 'simply',\n",
       " 824: 'unique',\n",
       " 344: 'house',\n",
       " 650: 'regression',\n",
       " 396: 'july',\n",
       " 655: 'release',\n",
       " 204: 'dates',\n",
       " 412: 'languages',\n",
       " 437: 'links',\n",
       " 355: 'ids',\n",
       " 763: 'subset',\n",
       " 57: 'applications',\n",
       " 803: 'track',\n",
       " 857: 'wanted',\n",
       " 339: 'history',\n",
       " 755: 'story',\n",
       " 767: 'systems',\n",
       " 804: 'traffic',\n",
       " 197: 'daily',\n",
       " 153: 'company',\n",
       " 278: 'extracted',\n",
       " 570: 'popular',\n",
       " 200: 'databases',\n",
       " 504: 'neural',\n",
       " 503: 'networks',\n",
       " 743: 'started',\n",
       " 638: 'recent',\n",
       " 441: 'little',\n",
       " 787: 'thanks',\n",
       " 87: 'basis',\n",
       " 771: 'taken',\n",
       " 147: 'comments',\n",
       " 502: 'network',\n",
       " 654: 'relative',\n",
       " 548: 'percentage',\n",
       " 240: 'divided',\n",
       " 710: 'service',\n",
       " 555: 'photo',\n",
       " 809: 'trend',\n",
       " 832: 'uploaded',\n",
       " 797: 'took',\n",
       " 770: 'tags',\n",
       " 260: 'english',\n",
       " 190: 'creative',\n",
       " 150: 'commons',\n",
       " 493: 'months',\n",
       " 711: 'services',\n",
       " 597: 'processed',\n",
       " 769: 'tables',\n",
       " 718: 'short',\n",
       " 323: 'google',\n",
       " 859: 'ways',\n",
       " 690: 'scientific',\n",
       " 790: 'thought',\n",
       " 415: 'later',\n",
       " 639: 'recently',\n",
       " 636: 'reasons',\n",
       " 798: 'tool',\n",
       " 64: 'areas',\n",
       " 682: 'rows',\n",
       " 879: 'written',\n",
       " 268: 'evaluate',\n",
       " 411: 'language',\n",
       " 558: 'places',\n",
       " 793: 'times',\n",
       " 256: 'education',\n",
       " 476: 'measures',\n",
       " 383: 'institute',\n",
       " 301: 'frequency',\n",
       " 50: 'analyze',\n",
       " 107: 'care',\n",
       " 604: 'program',\n",
       " 673: 'resources',\n",
       " 253: 'easy',\n",
       " 478: 'medical',\n",
       " 556: 'physical',\n",
       " 511: 'notes',\n",
       " 313: 'geographic',\n",
       " 517: 'obtain',\n",
       " 717: 'sharing',\n",
       " 458: 'makes',\n",
       " 472: 'means',\n",
       " 868: 'wikipedia',\n",
       " 800: 'topic',\n",
       " 811: 'true',\n",
       " 105: 'came',\n",
       " 427: 'library',\n",
       " 133: 'classify',\n",
       " 814: 'tweets',\n",
       " 612: 'providing',\n",
       " 737: 'speech',\n",
       " 348: 'human',\n",
       " 474: 'measured',\n",
       " 259: 'energy',\n",
       " 231: 'difference',\n",
       " 651: 'regular',\n",
       " 646: 'references',\n",
       " 177: 'corpus',\n",
       " 134: 'cleaned',\n",
       " 77: 'author',\n",
       " 567: 'police',\n",
       " 342: 'hour',\n",
       " 29: 'activities',\n",
       " 387: 'internet',\n",
       " 653: 'relationship',\n",
       " 595: 'proceedings',\n",
       " 162: 'conference',\n",
       " 111: 'category',\n",
       " 643: 'records',\n",
       " 551: 'performed',\n",
       " 93: 'blog',\n",
       " 391: 'items',\n",
       " 778: 'techniques',\n",
       " 854: 'visualization',\n",
       " 747: 'statistical',\n",
       " 330: 'hard',\n",
       " 635: 'really',\n",
       " 417: 'law',\n",
       " 65: 'art',\n",
       " 775: 'tasks',\n",
       " 329: 'hand',\n",
       " 865: 'weight',\n",
       " 196: 'currently',\n",
       " 739: 'spent',\n",
       " 560: 'platform',\n",
       " 4: '200',\n",
       " 284: 'federal',\n",
       " 538: 'pages',\n",
       " 242: 'documents',\n",
       " 236: 'distance',\n",
       " 492: 'monthly',\n",
       " 630: 'rating',\n",
       " 852: 'vision',\n",
       " 883: 'york',\n",
       " 392: 'january',\n",
       " 546: 'pdf',\n",
       " 416: 'latitude',\n",
       " 448: 'longitude',\n",
       " 223: 'detect',\n",
       " 823: 'understanding',\n",
       " 593: 'problem',\n",
       " 51: 'analyzing',\n",
       " 409: 'labeled',\n",
       " 665: 'represented',\n",
       " 875: 'works',\n",
       " 213: 'demographic',\n",
       " 263: 'especially',\n",
       " 116: 'challenge',\n",
       " 341: 'hope',\n",
       " 186: 'covers',\n",
       " 592: 'prior',\n",
       " 541: 'papers',\n",
       " 42: 'air',\n",
       " 79: 'automatically',\n",
       " 446: 'locations',\n",
       " 245: 'domain',\n",
       " 95: 'body',\n",
       " 124: 'children',\n",
       " 806: 'trained',\n",
       " 125: 'citation',\n",
       " 293: 'focus',\n",
       " 317: 'gives',\n",
       " 501: 'net',\n",
       " 227: 'developed',\n",
       " 155: 'competition',\n",
       " 189: 'creating',\n",
       " 498: 'natural',\n",
       " 106: 'car',\n",
       " 331: 'having',\n",
       " 324: 'gov',\n",
       " 26: 'act',\n",
       " 625: 'race',\n",
       " 791: 'thousands',\n",
       " 672: 'resource',\n",
       " 753: 'stop',\n",
       " 543: 'parts',\n",
       " 725: 'site',\n",
       " 94: 'board',\n",
       " 59: 'approach',\n",
       " 173: 'control',\n",
       " 463: 'map',\n",
       " 215: 'depth',\n",
       " 571: 'population',\n",
       " 192: 'crime',\n",
       " 277: 'extract',\n",
       " 813: 'trying',\n",
       " 645: 'reference',\n",
       " 5: '2000',\n",
       " 209: 'decided',\n",
       " 100: 'bureau',\n",
       " 754: 'stored',\n",
       " 539: 'pairs',\n",
       " 464: 'maps',\n",
       " 126: 'citations',\n",
       " 462: 'manually',\n",
       " 159: 'computer',\n",
       " 68: 'arxiv',\n",
       " 429: 'licensed',\n",
       " 819: 'typical',\n",
       " 703: 'selected',\n",
       " 572: 'portal',\n",
       " 620: 'purposes',\n",
       " 760: 'studies',\n",
       " 226: 'develop',\n",
       " 404: 'kindly',\n",
       " 28: 'active',\n",
       " 781: 'term',\n",
       " 353: 'identifier',\n",
       " 208: 'december',\n",
       " 820: 'typically',\n",
       " 733: 'south',\n",
       " 871: 'word',\n",
       " 239: 'district',\n",
       " 244: 'doi',\n",
       " 40: 'agencies',\n",
       " 38: 'administration',\n",
       " 423: 'length',\n",
       " 757: 'string',\n",
       " 369: 'india',\n",
       " 210: 'decision',\n",
       " 808: 'tree',\n",
       " 92: 'bigger',\n",
       " 74: 'attribution',\n",
       " 3: '1st',\n",
       " 294: 'follow',\n",
       " 526: 'opportunity',\n",
       " 490: 'monitoring',\n",
       " 303: 'function',\n",
       " 878: 'wouldn',\n",
       " 88: 'benefit',\n",
       " 535: 'owe',\n",
       " 75: 'attributions'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_map = dict((v, k) for k, v in vectorizer.vocabulary_.items()) \n",
    "id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=6, id2word=id_map, passes=25, random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.066*\"data\" + 0.020*\"context\" + 0.019*\"content\" + 0.017*\"inspiration\" + 0.017*\"acknowledgements\" + 0.014*\"world\" + 0.013*\"dataset\" + 0.013*\"research\" + 0.012*\"day\" + 0.011*\"weather\"'),\n",
       " (1,\n",
       "  '0.080*\"dataset\" + 0.033*\"description\" + 0.033*\"does\" + 0.016*\"data\" + 0.015*\"images\" + 0.013*\"class\" + 0.013*\"contains\" + 0.012*\"different\" + 0.012*\"column\" + 0.011*\"image\"'),\n",
       " (2,\n",
       "  '0.041*\"data\" + 0.019*\"csv\" + 0.013*\"dataset\" + 0.012*\"information\" + 0.010*\"year\" + 0.008*\"health\" + 0.008*\"number\" + 0.008*\"states\" + 0.008*\"state\" + 0.007*\"code\"'),\n",
       " (3,\n",
       "  '0.359*\"university\" + 0.069*\"state\" + 0.033*\"trained\" + 0.027*\"model\" + 0.021*\"pre\" + 0.019*\"california\" + 0.015*\"features\" + 0.015*\"dataset\" + 0.014*\"institute\" + 0.011*\"data\"'),\n",
       " (4,\n",
       "  '0.047*\"data\" + 0.022*\"time\" + 0.019*\"2017\" + 0.018*\"dataset\" + 0.018*\"date\" + 0.016*\"content\" + 0.014*\"number\" + 0.013*\"context\" + 0.013*\"times\" + 0.012*\"acknowledgements\"'),\n",
       " (5,\n",
       "  '0.043*\"data\" + 0.034*\"dataset\" + 0.015*\"com\" + 0.014*\"content\" + 0.013*\"contains\" + 0.012*\"context\" + 0.011*\"acknowledgements\" + 0.010*\"csv\" + 0.010*\"using\" + 0.010*\"file\"')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=6,num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_names= ['Education', 'Computers & IT', 'Religion', 'Sports', 'Science', 'Society & Lifestyle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It's my understanding that the freezing will start to occur because of the\n",
      "growing distance of Pluto and Charon from the Sun, due to it's\n",
      "elliptical orbit. It is not due to shadowing effects. \n",
      "\n",
      "\n",
      "Pluto can shadow Charon, and vice-versa.\n",
      "\n",
      "George Krumins\n",
      "-- \n"
     ]
    }
   ],
   "source": [
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"] \n",
    "print (new_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.04166859),\n",
       "  (1, 0.041689582),\n",
       "  (2, 0.041702054),\n",
       "  (3, 0.0416686),\n",
       "  (4, 0.791568),\n",
       "  (5, 0.041703176)]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorized= vectorizer.transform(new_doc)\n",
    "\n",
    "new_doc_corpus = gensim.matutils.Sparse2Corpus(doc_vectorized, documents_columns=False)\n",
    "doc_topics = ldamodel.get_document_topics(new_doc_corpus)\n",
    "list(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Science'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def elicit_topic_name(doc_topics):\n",
    "    return topics_names[np.squeeze(np.array(doc_topics))[:, 1].argmax()]\n",
    "\n",
    "elicit_topic_name(doc_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
